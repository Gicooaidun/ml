{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from dataloader import *\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S2_bands': {'B01': {'mean': 0.13021514, 'std': 0.017152175, 'min': 1e-04, 'max': 1.1213, 'p1': 0.1273, 'p99': 0.1074}, 'B02': {'mean': 0.1363337, 'std': 0.018509913, 'min': 1e-04, 'max': 1.8768, 'p1': 0.1366, 'p99': 0.1128}, 'B03': {'mean': 0.16427371, 'std': 0.02087248, 'min': 0.0411, 'max': 1.7888, 'p1': 0.1692, 'p99': 0.1364}, 'B04': {'mean': 0.13865142, 'std': 0.025569845, 'min': 0.0121, 'max': 1.7232, 'p1': 0.1445, 'p99': 0.1184}, 'B05': {'mean': 0.20296873, 'std': 0.028621713, 'min': 0.0672, 'max': 1.6344, 'p1': 0.2157, 'p99': 0.1591}, 'B06': {'mean': 0.38582557, 'std': 0.070499, 'min': 0.0758, 'max': 1.6699, 'p1': 0.3286, 'p99': 0.2766}, 'B07': {'mean': 0.4361872, 'std': 0.086211845, 'min': 0.0573, 'max': 1.6645, 'p1': 0.3621, 'p99': 0.2278}, 'B08': {'mean': 0.4448093, 'std': 0.08623231, 'min': 0.0737, 'max': 1.6976, 'p1': 0.3588, 'p99': 0.2122}, 'B8A': {'mean': 0.4580875, 'std': 0.08798952, 'min': 0.0772, 'max': 1.6709, 'p1': 0.3775, 'p99': 0.26}, 'B09': {'mean': 0.45806482, 'std': 0.08441155, 'min': 0.0066, 'max': 1.7178, 'p1': 0.3818, 'p99': 0.2655}, 'B11': {'mean': 0.2941879, 'std': 0.04741236, 'min': 0.0994, 'max': 1.5738, 'p1': 0.3279, 'p99': 0.5299}, 'B12': {'mean': 0.19771282, 'std': 0.0438055, 'min': 0.0974, 'max': 1.6086, 'p1': 0.22, 'p99': 0.1688}}, 'BM': {'bm': {'mean': 50.714764, 'std': 35.78637, 'min': 0.0, 'max': 312.03958, 'p1': 0.0, 'p99': 127.57817}, 'std': {'mean': 7.036375, 'std': 4.400113, 'min': 0.0, 'max': 145.97275, 'p1': 0.0, 'p99': 2.180725}}, 'Sentinel_metadata': {'S2_vegetation_score': {'mean': 96.60825, 'std': 11.659279, 'min': 20.0, 'max': 100.0, 'p1': 100.0, 'p99': 48.0}, 'S2_date': {'mean': 470.45947, 'std': 169.251, 'min': 1.0, 'max': 620.0, 'p1': 212.0, 'p99': 122.0}}, 'GEDI': {'agbd': {'mean': 77.14966, 'std': 79.39693, 'min': 0.66613775, 'max': 499.96765, 'p1': 1.0619253, 'p99': 75.21779}, 'agbd_se': {'mean': 7.2501493, 'std': 2.1052322, 'min': 2.981795, 'max': 12.131867, 'p1': 3.004118, 'p99': 11.066575}, 'rh98': {'mean': 12.344467, 'std': 9.239699, 'min': 0.030004844, 'max': 96.50001, 'p1': 2.9900095, 'p99': 33.88001}, 'date': {'mean': 394.3948, 'std': 149.13177, 'min': 46.0, 'max': 531.0, 'p1': 158.0, 'p99': 503.0}}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in binary mode for reading\n",
    "with open('data/normalization_values.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Now you can analyze the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the dataset mapping (train, test, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training tiles:  45\n",
      "['53UNQ', '52UFV', '53TPN', '53ULQ', '51UWQ', '53UNT', '52UFU', '51UXS', '52UFC', '51UVP', '53UMR', '50UQA', '51UVT', '51TWM', '53UNS', '51TVM', '52UFA', '54UUU', '54UVV', '54UVU', '51UVR', '51UYT', '52UGB', '51UYS', '53ULT', '51UXR', '53UPS', '54UUC', '51UUQ', '52UEA', '54UVA', '54UUA', '52UFB', '52UDA', '53TPL', '54UUV', '50TPT', '53UMT', '51UWS', '50TQT', '51UXQ', '51UWR', '53UPT', '51UVQ', '53UMS']\n",
      "validation tiles:  10\n",
      "['51UVS', '52UEV', '52UEC', '53UPP', '51TVN', '52UEB', '52UDC', '54UUB', '53ULR', '51UUP']\n",
      "testing tiles:  15\n",
      "['53TQN', '53UPQ', '51UWT', '52UCA', '51TWN', '51UUT', '51TVL', '50TPS', '52UGU', '51UWP', '53UMQ', '54UVC', '52UDB', '50UQB', '53UNR']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {'train': [], 'val': [], 'test': []} \n",
    "path_h5 = '/scratch2/biomass_estimation/code/ml/data/data_no_outliers/'\n",
    "\n",
    "all_tiles = []\n",
    "# Iterate over all the h5 files\n",
    "for fname in os.listdir(path_h5):\n",
    "    if fname.endswith('.h5'):\n",
    "        with h5py.File(os.path.join(path_h5, fname), 'r') as f:\n",
    "            # Get the list of all tiles in the file\n",
    "            all_tiles.extend(list(f.keys()))\n",
    "\n",
    "train_tiles, test_and_val_tiles = train_test_split(all_tiles, test_size=0.35, random_state=42)\n",
    "val_tile, test_tile = train_test_split(test_and_val_tiles, test_size=0.6, random_state=42)\n",
    "data['val'].extend(val_tile)\n",
    "data['test'].extend(test_tile)\n",
    "data['train'].extend(train_tiles)\n",
    "\n",
    "print(\"training tiles: \", len(data['train']))\n",
    "print(data['train'])\n",
    "print(\"validation tiles: \", len(data['val']))\n",
    "print(data['val'])\n",
    "print(\"testing tiles: \", len(data['test']))\n",
    "print(data['test'])\n",
    "# Pickle the DataFrame and save it to a file\n",
    "with open('/scratch2/biomass_estimation/code/ml/data/mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining loss and training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    \"\"\" \n",
    "        Weighted RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.mse = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def __call__(self, prediction, target, weights = 1):\n",
    "        # prediction = prediction[:, 0]\n",
    "        return torch.sqrt(torch.mean(weights * self.mse(prediction,target)))\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_no_outliers_0-5.h5', 'data_no_outliers_1-5.h5', 'data_no_outliers_2-5.h5', 'data_no_outliers_3-5.h5', 'data_no_outliers_4-5.h5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128, 64, 32, 16),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        # print(layers)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.conv_output(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = SimpleFCN()\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = models\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            self.models = [model.cuda() for model in self.models]\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        mean = torch.mean(torch.stack(outputs), dim=0)\n",
    "        std = torch.std(torch.stack(outputs), dim=0)\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training simple FCN for 10 epochs with learning rate 0.001 (model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs = 10, modelname = 'overwritten_ensemble', patience = 5):\n",
    "    wandb.init(name=modelname)\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    mode = 'train'\n",
    "    ds_training = GEDIDataset({'h5':path_h5, 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "    trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "    mode = 'val'\n",
    "    ds_validation = GEDIDataset({'h5':path_h5, 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "    validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "    min_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        i=0\n",
    "        for inputs, targets in trainloader:\n",
    "            i+=1\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            # print(loss.item())\n",
    "            if i%20==0:\n",
    "                # print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "                wandb.log({'train_loss': train_loss / i})\n",
    "\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        i=0\n",
    "        model.eval()\n",
    "        for inputs, targets in validloader:\n",
    "            i+=1\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            # loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "            loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "            valid_loss += loss.item()\n",
    "            if i%20==0:\n",
    "                # print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    "                wandb.log({'valid_loss': valid_loss / i})\n",
    "    \n",
    "        print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "        \n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "            min_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), f'models/{modelname}.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                early_stop = True\n",
    "                break\n",
    "\n",
    "        if early_stop:\n",
    "            print(\"Stopped\")\n",
    "            break\n",
    "\n",
    "        # print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_architecture, model_path = 'models/model3.pth', ensemble_models = [], experiment_name = 'testing'):\n",
    "    wandb.init(name=experiment_name)\n",
    "    if model_architecture == 'SimpleFCN':\n",
    "        model = SimpleFCN()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    elif model_architecture == 'EnsembleModel':\n",
    "        models = []\n",
    "        for path in ensemble_models:\n",
    "            temp_model = SimpleFCN()\n",
    "            temp_model.load_state_dict(torch.load(path))\n",
    "            temp_model.eval()\n",
    "            models.append(temp_model)\n",
    "        model = EnsembleModel(models)\n",
    "    else:\n",
    "        print('Model architecture not found')\n",
    "        return\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # print('Using GPU')\n",
    "        model = model.cuda()\n",
    "\n",
    "    mode = 'test'\n",
    "    ds_testing = GEDIDataset({'h5':path_h5, 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "    testloader = DataLoader(dataset = ds_testing, batch_size = 256, shuffle = False, num_workers = 8)\n",
    "\n",
    "    # Testing loop\n",
    "    wandb.watch(model, log_freq=100)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    batch_test_loss = 0.0\n",
    "    i = 0\n",
    "    for inputs, targets in testloader:\n",
    "        i += 1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        outputs, std = model(inputs)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        test_loss += loss.item()\n",
    "        batch_test_loss += loss.item()\n",
    "        if i % 20 == 0:\n",
    "            # print(f'Batch {i} \\t Testing Loss: {test_loss / i}')\n",
    "            wandb.log({'20_batches_test_loss': batch_test_loss / 20})\n",
    "            batch_test_loss = 0.0\n",
    "            wandb.log({'test_loss': test_loss / i})\n",
    "\n",
    "    print(f'Testing Loss SimpleFCN: {test_loss / len(testloader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsenti\u001b[0m (\u001b[33mdose\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch2/biomass_estimation/code/ml/wandb/run-20240526_112410-xmyjhw9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dose/code-ml/runs/xmyjhw9q' target=\"_blank\">early_stopping_200epochs_sub_ensemble_model_0</a></strong> to <a href='https://wandb.ai/dose/code-ml' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dose/code-ml' target=\"_blank\">https://wandb.ai/dose/code-ml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dose/code-ml/runs/xmyjhw9q' target=\"_blank\">https://wandb.ai/dose/code-ml/runs/xmyjhw9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "models = []\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model0 = SimpleFCN()\n",
    "models.append(model0)\n",
    "torch.manual_seed(42+1)\n",
    "np.random.seed(42+1)\n",
    "model1 = SimpleFCN()\n",
    "models.append(model1)\n",
    "torch.manual_seed(42+2)\n",
    "np.random.seed(42+2)\n",
    "model2 = SimpleFCN()\n",
    "models.append(model2)\n",
    "torch.manual_seed(42+3)\n",
    "np.random.seed(42+3)\n",
    "model3 = SimpleFCN()\n",
    "models.append(model3)\n",
    "torch.manual_seed(42+4)\n",
    "np.random.seed(42+4)\n",
    "model4 = SimpleFCN()\n",
    "models.append(model4)\n",
    "\n",
    "experiment_name = 'early_stopping_200epochs'\n",
    "i=0\n",
    "for model in models:\n",
    "    train(model, epochs = 200, modelname = f'{experiment_name}_sub_ensemble_model_{i}', patience = 20)\n",
    "    model.eval()\n",
    "    i+=1\n",
    "\n",
    "ensemble_models = [f'models/{experiment_name}_sub_ensemble_model_{i}.pth' for i in range(len(models))]\n",
    "\n",
    "model = EnsembleModel(models)\n",
    "torch.save(model.state_dict(), f'models/{experiment_name}_ensemble.pth')\n",
    "\n",
    "test(model_architecture='EnsembleModel', ensemble_models = ensemble_models, experiment_name = f'{experiment_name}_testing')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
