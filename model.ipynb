{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataloader import *\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S2_bands': {'B01': {'mean': 0.13021514, 'std': 0.017152175, 'min': 1e-04, 'max': 1.1213, 'p1': 0.1273, 'p99': 0.1074}, 'B02': {'mean': 0.1363337, 'std': 0.018509913, 'min': 1e-04, 'max': 1.8768, 'p1': 0.1366, 'p99': 0.1128}, 'B03': {'mean': 0.16427371, 'std': 0.02087248, 'min': 0.0411, 'max': 1.7888, 'p1': 0.1692, 'p99': 0.1364}, 'B04': {'mean': 0.13865142, 'std': 0.025569845, 'min': 0.0121, 'max': 1.7232, 'p1': 0.1445, 'p99': 0.1184}, 'B05': {'mean': 0.20296873, 'std': 0.028621713, 'min': 0.0672, 'max': 1.6344, 'p1': 0.2157, 'p99': 0.1591}, 'B06': {'mean': 0.38582557, 'std': 0.070499, 'min': 0.0758, 'max': 1.6699, 'p1': 0.3286, 'p99': 0.2766}, 'B07': {'mean': 0.4361872, 'std': 0.086211845, 'min': 0.0573, 'max': 1.6645, 'p1': 0.3621, 'p99': 0.2278}, 'B08': {'mean': 0.4448093, 'std': 0.08623231, 'min': 0.0737, 'max': 1.6976, 'p1': 0.3588, 'p99': 0.2122}, 'B8A': {'mean': 0.4580875, 'std': 0.08798952, 'min': 0.0772, 'max': 1.6709, 'p1': 0.3775, 'p99': 0.26}, 'B09': {'mean': 0.45806482, 'std': 0.08441155, 'min': 0.0066, 'max': 1.7178, 'p1': 0.3818, 'p99': 0.2655}, 'B11': {'mean': 0.2941879, 'std': 0.04741236, 'min': 0.0994, 'max': 1.5738, 'p1': 0.3279, 'p99': 0.5299}, 'B12': {'mean': 0.19771282, 'std': 0.0438055, 'min': 0.0974, 'max': 1.6086, 'p1': 0.22, 'p99': 0.1688}}, 'BM': {'bm': {'mean': 50.714764, 'std': 35.78637, 'min': 0.0, 'max': 312.03958, 'p1': 0.0, 'p99': 127.57817}, 'std': {'mean': 7.036375, 'std': 4.400113, 'min': 0.0, 'max': 145.97275, 'p1': 0.0, 'p99': 2.180725}}, 'Sentinel_metadata': {'S2_vegetation_score': {'mean': 96.60825, 'std': 11.659279, 'min': 20.0, 'max': 100.0, 'p1': 100.0, 'p99': 48.0}, 'S2_date': {'mean': 470.45947, 'std': 169.251, 'min': 1.0, 'max': 620.0, 'p1': 212.0, 'p99': 122.0}}, 'GEDI': {'agbd': {'mean': 77.14966, 'std': 79.39693, 'min': 0.66613775, 'max': 499.96765, 'p1': 1.0619253, 'p99': 75.21779}, 'agbd_se': {'mean': 7.2501493, 'std': 2.1052322, 'min': 2.981795, 'max': 12.131867, 'p1': 3.004118, 'p99': 11.066575}, 'rh98': {'mean': 12.344467, 'std': 9.239699, 'min': 0.030004844, 'max': 96.50001, 'p1': 2.9900095, 'p99': 33.88001}, 'date': {'mean': 394.3948, 'std': 149.13177, 'min': 46.0, 'max': 531.0, 'p1': 158.0, 'p99': 503.0}}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in binary mode for reading\n",
    "with open('data/normalization_values.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Now you can analyze the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the dataset mapping (train, test, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training tiles:  45\n",
      "['53UNQ', '52UFV', '53TPN', '53ULQ', '51UWQ', '53UNT', '52UFU', '51UXS', '52UFC', '51UVP', '53UMR', '50UQA', '51UVT', '51TWM', '53UNS', '51TVM', '52UFA', '54UUU', '54UVV', '54UVU', '51UVR', '51UYT', '52UGB', '51UYS', '53ULT', '51UXR', '53UPS', '54UUC', '51UUQ', '52UEA', '54UVA', '54UUA', '52UFB', '52UDA', '53TPL', '54UUV', '50TPT', '53UMT', '51UWS', '50TQT', '51UXQ', '51UWR', '53UPT', '51UVQ', '53UMS']\n",
      "validation tiles:  10\n",
      "['51UVS', '52UEV', '52UEC', '53UPP', '51TVN', '52UEB', '52UDC', '54UUB', '53ULR', '51UUP']\n",
      "testing tiles:  15\n",
      "['53TQN', '53UPQ', '51UWT', '52UCA', '51TWN', '51UUT', '51TVL', '50TPS', '52UGU', '51UWP', '53UMQ', '54UVC', '52UDB', '50UQB', '53UNR']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {'train': [], 'val': [], 'test': []} \n",
    "path_h5 = '/scratch2/biomass_estimation/code/ml/data/data_no_outliers/'\n",
    "\n",
    "all_tiles = []\n",
    "# Iterate over all the h5 files\n",
    "for fname in os.listdir(path_h5):\n",
    "    if fname.endswith('.h5'):\n",
    "        with h5py.File(os.path.join(path_h5, fname), 'r') as f:\n",
    "            # Get the list of all tiles in the file\n",
    "            all_tiles.extend(list(f.keys()))\n",
    "\n",
    "train_tiles, test_and_val_tiles = train_test_split(all_tiles, test_size=0.35, random_state=42)\n",
    "val_tile, test_tile = train_test_split(test_and_val_tiles, test_size=0.6, random_state=42)\n",
    "data['val'].extend(val_tile)\n",
    "data['test'].extend(test_tile)\n",
    "data['train'].extend(train_tiles)\n",
    "\n",
    "print(\"training tiles: \", len(data['train']))\n",
    "print(data['train'])\n",
    "print(\"validation tiles: \", len(data['val']))\n",
    "print(data['val'])\n",
    "print(\"testing tiles: \", len(data['test']))\n",
    "print(data['test'])\n",
    "# Pickle the DataFrame and save it to a file\n",
    "with open('/scratch2/biomass_estimation/code/ml/data/mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining loss and training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    \"\"\" \n",
    "        Weighted RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.mse = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def __call__(self, prediction, target, weights = 1):\n",
    "        # prediction = prediction[:, 0]\n",
    "        return torch.sqrt(torch.mean(weights * self.mse(prediction,target)))\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_nonan_0-5.h5', 'data_nonan_1-5.h5', 'data_nonan_2-5.h5', 'data_nonan_3-5.h5', 'data_nonan_4-5.h5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the SimpleFCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        print(layers)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "        # self.fc = nn.Linear(15*15*num_outputs, 1)  # Fully connected layer to get a single output value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_output(x)\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        # predictions = self.fc(x)\n",
    "        # return predictions.squeeze()  # Remove the extra dimension\n",
    "        return x\n",
    "    \n",
    "model = SimpleFCN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training simple FCN for 10 epochs with learning rate 0.001 (model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ozqtmdmk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2e49a771bb485c94e1030a84155113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-gorge-19</strong> at: <a href='https://wandb.ai/dose/code-ml/runs/ozqtmdmk' target=\"_blank\">https://wandb.ai/dose/code-ml/runs/ozqtmdmk</a><br/> View project at: <a href='https://wandb.ai/dose/code-ml' target=\"_blank\">https://wandb.ai/dose/code-ml</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_114930-ozqtmdmk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ozqtmdmk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8334d925f96248108fbc5f51909dba8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111243073311117, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch2/biomass_estimation/code/ml/wandb/run-20240613_115158-30tc7btv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dose/code-ml/runs/30tc7btv' target=\"_blank\">earnest-deluge-20</a></strong> to <a href='https://wandb.ai/dose/code-ml' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dose/code-ml' target=\"_blank\">https://wandb.ai/dose/code-ml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dose/code-ml/runs/30tc7btv' target=\"_blank\">https://wandb.ai/dose/code-ml/runs/30tc7btv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "Epoch 1 \t Batch 20 \t Training Loss: 136.6802490234375\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 136.43809700012207\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 136.23353907267253\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 135.10922145843506\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 134.84507965087892\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 134.0436440149943\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 132.95523894173758\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 131.9072479248047\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 130.53986612955728\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 129.05633422851562\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 127.45527822321111\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 125.8928354581197\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 124.09635414710412\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 122.25959949493408\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 120.38932151794434\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 118.49178922176361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2503167/1413230757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init()\n",
    "model = SimpleFCN()\n",
    "wandb.watch(model, log_freq=100)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(10):  # 10 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "            wandb.log({'train_loss': train_loss / i})\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    "            wandb.log({'valid_loss': valid_loss / i})\n",
    "            np.save(f'training_predictions/outputs_epoch{epoch+1}_batch{i}.npy', outputs.detach().cpu().numpy())\n",
    "        \n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model3.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining bigger FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiggerFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128, 256, 512),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(BiggerFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.conv_output(x)\n",
    "        return x\n",
    "    \n",
    "model = BiggerFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Batch 20 \t Training Loss: 110.96225547790527\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 107.67209758758545\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 101.05285097757975\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 93.23420357704163\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 85.79349407196045\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 80.52298367818197\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 76.5239510672433\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 73.53724029064179\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 71.26314665476481\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 69.52043266296387\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 67.91062923778188\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 66.49623942375183\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 65.39369429074802\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 64.4995620727539\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 63.679258804321286\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 62.976606941223146\n",
      "Epoch 1 \t Batch 340 \t Training Loss: 62.33444068011116\n",
      "Epoch 1 \t Batch 360 \t Training Loss: 61.76926401986016\n",
      "Epoch 1 \t Batch 380 \t Training Loss: 61.237123880888284\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 60.70992929458618\n",
      "Epoch 1 \t Batch 420 \t Training Loss: 60.294830939883276\n",
      "Epoch 1 \t Batch 440 \t Training Loss: 59.949697615883565\n",
      "Epoch 1 \t Batch 460 \t Training Loss: 59.481460770316744\n",
      "Epoch 1 \t Batch 480 \t Training Loss: 59.134289892514545\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 58.87759677886963\n",
      "Epoch 1 \t Batch 520 \t Training Loss: 58.58180530254658\n",
      "Epoch 1 \t Batch 540 \t Training Loss: 58.33218734882496\n",
      "Epoch 1 \t Batch 560 \t Training Loss: 58.057140241350446\n",
      "Epoch 1 \t Batch 580 \t Training Loss: 57.78861550955937\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 57.57404867808024\n",
      "Epoch 1 \t Batch 620 \t Training Loss: 57.376583831541\n",
      "Epoch 1 \t Batch 640 \t Training Loss: 57.17892336845398\n",
      "Epoch 1 \t Batch 660 \t Training Loss: 56.9645563472401\n",
      "Epoch 1 \t Batch 680 \t Training Loss: 56.81188611423268\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 56.65156123025077\n",
      "Epoch 1 \t Batch 720 \t Training Loss: 56.48217346933153\n",
      "Epoch 1 \t Batch 740 \t Training Loss: 56.32664554699047\n",
      "Epoch 1 \t Batch 760 \t Training Loss: 56.20642511468185\n",
      "Epoch 1 \t Batch 780 \t Training Loss: 56.0519822194026\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 55.8889439535141\n",
      "Epoch 1 \t Batch 820 \t Training Loss: 55.751300407037505\n",
      "Epoch 1 \t Batch 840 \t Training Loss: 55.65692692257109\n",
      "Epoch 1 \t Batch 860 \t Training Loss: 55.53203521994657\n",
      "Epoch 1 \t Batch 880 \t Training Loss: 55.4256563056599\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 55.31681309170193\n",
      "Epoch 1 \t Batch 20 \t Validation Loss: 31.635834169387817\n",
      "Epoch 1 \t Batch 40 \t Validation Loss: 30.551535177230836\n",
      "Epoch 1 \t Batch 60 \t Validation Loss: 31.7991423924764\n",
      "Epoch 1 \t Batch 80 \t Validation Loss: 32.03209134340286\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 31.24517704963684\n",
      "Epoch 1 \t Batch 120 \t Validation Loss: 30.92811795870463\n",
      "Epoch 1 \t Batch 140 \t Validation Loss: 30.41154567854745\n",
      "Epoch 1 \t Batch 160 \t Validation Loss: 31.688007813692092\n",
      "Epoch 1 \t Batch 180 \t Validation Loss: 34.42811838255988\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 35.23710624217987\n",
      "Epoch 1 \t Batch 220 \t Validation Loss: 35.93048896356063\n",
      "Epoch 1 \t Batch 240 \t Validation Loss: 36.075316139062245\n",
      "Epoch 1 \t Batch 260 \t Validation Loss: 37.64939104960515\n",
      "Epoch 1 \t Batch 280 \t Validation Loss: 38.31259245872498\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 39.35678173700968\n",
      "Epoch 1 \t Batch 320 \t Validation Loss: 39.79431272745133\n",
      "Epoch 1 \t Batch 340 \t Validation Loss: 39.685108942144055\n",
      "Epoch 1 \t Batch 360 \t Validation Loss: 39.51340000364515\n",
      "Epoch 1 \t Batch 380 \t Validation Loss: 39.668588678460374\n",
      "Epoch 1 \t Batch 400 \t Validation Loss: 39.33138882637024\n",
      "Epoch 1 \t Batch 420 \t Validation Loss: 39.41798274630592\n",
      "Epoch 1 \t Batch 440 \t Validation Loss: 39.253935952620076\n",
      "Epoch 1 \t Batch 460 \t Validation Loss: 39.47099304199219\n",
      "Epoch 1 \t Batch 480 \t Validation Loss: 39.98899187644323\n",
      "Epoch 1 \t Batch 500 \t Validation Loss: 39.66986949539184\n",
      "Epoch 1 \t Batch 520 \t Validation Loss: 39.448386753522435\n",
      "Epoch 1 \t Batch 540 \t Validation Loss: 39.10927919635066\n",
      "Epoch 1 \t Batch 560 \t Validation Loss: 38.84181722572872\n",
      "Epoch 1 \t Batch 580 \t Validation Loss: 38.60614662828117\n",
      "Epoch 1 \t Batch 600 \t Validation Loss: 38.78552556991577\n",
      "Epoch 1 Training Loss: 55.24997385039584 Validation Loss: 39.51507000799303\n",
      "Validation Loss Decreased(inf--->24341.283124923706) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 20 \t Training Loss: 49.64289493560791\n",
      "Epoch 2 \t Batch 40 \t Training Loss: 49.406539344787596\n",
      "Epoch 2 \t Batch 60 \t Training Loss: 49.564073181152345\n",
      "Epoch 2 \t Batch 80 \t Training Loss: 49.493405199050905\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 49.866230430603025\n",
      "Epoch 2 \t Batch 120 \t Training Loss: 50.11052522659302\n",
      "Epoch 2 \t Batch 140 \t Training Loss: 50.126308277675086\n",
      "Epoch 2 \t Batch 160 \t Training Loss: 50.07642571926117\n",
      "Epoch 2 \t Batch 180 \t Training Loss: 49.96032155354818\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 50.02135623931885\n",
      "Epoch 2 \t Batch 220 \t Training Loss: 50.099186064980245\n",
      "Epoch 2 \t Batch 240 \t Training Loss: 50.30380522410075\n",
      "Epoch 2 \t Batch 260 \t Training Loss: 50.21892132392296\n",
      "Epoch 2 \t Batch 280 \t Training Loss: 50.14027790342058\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 50.14895840962728\n",
      "Epoch 2 \t Batch 320 \t Training Loss: 50.179298758506775\n",
      "Epoch 2 \t Batch 340 \t Training Loss: 50.25277165805592\n",
      "Epoch 2 \t Batch 360 \t Training Loss: 50.1936602168613\n",
      "Epoch 2 \t Batch 380 \t Training Loss: 50.161365237988925\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 50.208186540603634\n",
      "Epoch 2 \t Batch 420 \t Training Loss: 50.1261611575172\n",
      "Epoch 2 \t Batch 440 \t Training Loss: 50.08342033733021\n",
      "Epoch 2 \t Batch 460 \t Training Loss: 50.03552337314772\n",
      "Epoch 2 \t Batch 480 \t Training Loss: 50.03958521684011\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 49.98678639984131\n",
      "Epoch 2 \t Batch 520 \t Training Loss: 50.03246687375582\n",
      "Epoch 2 \t Batch 540 \t Training Loss: 50.01399533307111\n",
      "Epoch 2 \t Batch 560 \t Training Loss: 49.996558271135605\n",
      "Epoch 2 \t Batch 580 \t Training Loss: 49.98081274361446\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 50.0108574295044\n",
      "Epoch 2 \t Batch 620 \t Training Loss: 49.99692596927766\n",
      "Epoch 2 \t Batch 640 \t Training Loss: 49.96362827420235\n",
      "Epoch 2 \t Batch 660 \t Training Loss: 49.94423809629498\n",
      "Epoch 2 \t Batch 680 \t Training Loss: 49.94275195177864\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 49.95766597202846\n",
      "Epoch 2 \t Batch 720 \t Training Loss: 49.96450979974535\n",
      "Epoch 2 \t Batch 740 \t Training Loss: 49.95128341365505\n",
      "Epoch 2 \t Batch 760 \t Training Loss: 49.93227035120914\n",
      "Epoch 2 \t Batch 780 \t Training Loss: 49.9017641214224\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 49.93937173366547\n",
      "Epoch 2 \t Batch 820 \t Training Loss: 49.91945829624083\n",
      "Epoch 2 \t Batch 840 \t Training Loss: 49.9041691507612\n",
      "Epoch 2 \t Batch 860 \t Training Loss: 49.88860101478044\n",
      "Epoch 2 \t Batch 880 \t Training Loss: 49.880507252433084\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 49.88879450480143\n",
      "Epoch 2 \t Batch 20 \t Validation Loss: 19.840473985672\n",
      "Epoch 2 \t Batch 40 \t Validation Loss: 21.949557065963745\n",
      "Epoch 2 \t Batch 60 \t Validation Loss: 21.593690983454387\n",
      "Epoch 2 \t Batch 80 \t Validation Loss: 22.685034728050233\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 23.737811431884765\n",
      "Epoch 2 \t Batch 120 \t Validation Loss: 24.86342810789744\n",
      "Epoch 2 \t Batch 140 \t Validation Loss: 25.211781004496984\n",
      "Epoch 2 \t Batch 160 \t Validation Loss: 26.98300274014473\n",
      "Epoch 2 \t Batch 180 \t Validation Loss: 30.060378291871814\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 31.253096413612365\n",
      "Epoch 2 \t Batch 220 \t Validation Loss: 32.37655443278226\n",
      "Epoch 2 \t Batch 240 \t Validation Loss: 32.74326010942459\n",
      "Epoch 2 \t Batch 260 \t Validation Loss: 34.64859671592713\n",
      "Epoch 2 \t Batch 280 \t Validation Loss: 35.66772904736655\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 36.62505556424459\n",
      "Epoch 2 \t Batch 320 \t Validation Loss: 37.0468281596899\n",
      "Epoch 2 \t Batch 340 \t Validation Loss: 37.060856754639566\n",
      "Epoch 2 \t Batch 360 \t Validation Loss: 36.94777344332801\n",
      "Epoch 2 \t Batch 380 \t Validation Loss: 37.23951456923234\n",
      "Epoch 2 \t Batch 400 \t Validation Loss: 36.98889718770981\n",
      "Epoch 2 \t Batch 420 \t Validation Loss: 37.09688831738063\n",
      "Epoch 2 \t Batch 440 \t Validation Loss: 36.934777487408034\n",
      "Epoch 2 \t Batch 460 \t Validation Loss: 37.20302628434223\n",
      "Epoch 2 \t Batch 480 \t Validation Loss: 37.71882771452268\n",
      "Epoch 2 \t Batch 500 \t Validation Loss: 37.45799318885803\n",
      "Epoch 2 \t Batch 520 \t Validation Loss: 37.26879870524773\n",
      "Epoch 2 \t Batch 540 \t Validation Loss: 37.002303937629414\n",
      "Epoch 2 \t Batch 560 \t Validation Loss: 36.81115876095635\n",
      "Epoch 2 \t Batch 580 \t Validation Loss: 36.587905490809476\n",
      "Epoch 2 \t Batch 600 \t Validation Loss: 36.799421461423236\n",
      "Epoch 2 Training Loss: 49.89170411004885 Validation Loss: 37.452851705736926\n",
      "Validation Loss Decreased(24341.283124923706--->23070.956650733948) Saving The Model\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 20 \t Training Loss: 48.121444129943846\n",
      "Epoch 3 \t Batch 40 \t Training Loss: 48.507242393493655\n",
      "Epoch 3 \t Batch 60 \t Training Loss: 49.033769098917645\n",
      "Epoch 3 \t Batch 80 \t Training Loss: 49.131727027893064\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 49.13284927368164\n",
      "Epoch 3 \t Batch 120 \t Training Loss: 49.26318511962891\n",
      "Epoch 3 \t Batch 140 \t Training Loss: 49.40038018907819\n",
      "Epoch 3 \t Batch 160 \t Training Loss: 49.56010539531708\n",
      "Epoch 3 \t Batch 180 \t Training Loss: 49.48133214314779\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 49.41101360321045\n",
      "Epoch 3 \t Batch 220 \t Training Loss: 49.43015613555908\n",
      "Epoch 3 \t Batch 240 \t Training Loss: 49.492906173070274\n",
      "Epoch 3 \t Batch 260 \t Training Loss: 49.53610358605018\n",
      "Epoch 3 \t Batch 280 \t Training Loss: 49.577797290257045\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 49.45120906829834\n",
      "Epoch 3 \t Batch 320 \t Training Loss: 49.33368159532547\n",
      "Epoch 3 \t Batch 340 \t Training Loss: 49.246969458636116\n",
      "Epoch 3 \t Batch 360 \t Training Loss: 49.27044841978285\n",
      "Epoch 3 \t Batch 380 \t Training Loss: 49.21239834835655\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 49.14879199028015\n",
      "Epoch 3 \t Batch 420 \t Training Loss: 49.140773228236604\n",
      "Epoch 3 \t Batch 440 \t Training Loss: 49.09100240360607\n",
      "Epoch 3 \t Batch 460 \t Training Loss: 49.12558759606403\n",
      "Epoch 3 \t Batch 480 \t Training Loss: 49.07003756364187\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 49.1068385925293\n",
      "Epoch 3 \t Batch 520 \t Training Loss: 49.0515904206496\n",
      "Epoch 3 \t Batch 540 \t Training Loss: 49.15901041384097\n",
      "Epoch 3 \t Batch 560 \t Training Loss: 49.13349212237767\n",
      "Epoch 3 \t Batch 580 \t Training Loss: 49.16731354943637\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 49.16185821533203\n",
      "Epoch 3 \t Batch 620 \t Training Loss: 49.147235076658184\n",
      "Epoch 3 \t Batch 640 \t Training Loss: 49.13324482440949\n",
      "Epoch 3 \t Batch 660 \t Training Loss: 49.089819070064664\n",
      "Epoch 3 \t Batch 680 \t Training Loss: 49.09526567459106\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 49.1139801461356\n",
      "Epoch 3 \t Batch 720 \t Training Loss: 49.095734633339774\n",
      "Epoch 3 \t Batch 740 \t Training Loss: 49.140492681554846\n",
      "Epoch 3 \t Batch 760 \t Training Loss: 49.190509981858106\n",
      "Epoch 3 \t Batch 780 \t Training Loss: 49.21322289491311\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 49.19482919216156\n",
      "Epoch 3 \t Batch 820 \t Training Loss: 49.17424308032524\n",
      "Epoch 3 \t Batch 840 \t Training Loss: 49.13812784013294\n",
      "Epoch 3 \t Batch 860 \t Training Loss: 49.151823354321856\n",
      "Epoch 3 \t Batch 880 \t Training Loss: 49.13304526589133\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 49.10735304514567\n",
      "Epoch 3 \t Batch 20 \t Validation Loss: 26.367181301116943\n",
      "Epoch 3 \t Batch 40 \t Validation Loss: 27.117975282669068\n",
      "Epoch 3 \t Batch 60 \t Validation Loss: 26.867522843678792\n",
      "Epoch 3 \t Batch 80 \t Validation Loss: 27.35708327293396\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 27.885700244903564\n",
      "Epoch 3 \t Batch 120 \t Validation Loss: 28.382410033543906\n",
      "Epoch 3 \t Batch 140 \t Validation Loss: 28.426054286956788\n",
      "Epoch 3 \t Batch 160 \t Validation Loss: 30.156337678432465\n",
      "Epoch 3 \t Batch 180 \t Validation Loss: 33.71355373594496\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 35.196856813430784\n",
      "Epoch 3 \t Batch 220 \t Validation Loss: 36.53725405606357\n",
      "Epoch 3 \t Batch 240 \t Validation Loss: 36.95753107468287\n",
      "Epoch 3 \t Batch 260 \t Validation Loss: 39.06184698985173\n",
      "Epoch 3 \t Batch 280 \t Validation Loss: 40.23188323293414\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 41.305646375020345\n",
      "Epoch 3 \t Batch 320 \t Validation Loss: 41.6827992618084\n",
      "Epoch 3 \t Batch 340 \t Validation Loss: 41.54825033861048\n",
      "Epoch 3 \t Batch 360 \t Validation Loss: 41.34567954805162\n",
      "Epoch 3 \t Batch 380 \t Validation Loss: 41.57890434767071\n",
      "Epoch 3 \t Batch 400 \t Validation Loss: 41.18152366161346\n",
      "Epoch 3 \t Batch 420 \t Validation Loss: 41.127484921046666\n",
      "Epoch 3 \t Batch 440 \t Validation Loss: 40.83401662219654\n",
      "Epoch 3 \t Batch 460 \t Validation Loss: 41.001721315798555\n",
      "Epoch 3 \t Batch 480 \t Validation Loss: 41.399992946783705\n",
      "Epoch 3 \t Batch 500 \t Validation Loss: 41.05902730178833\n",
      "Epoch 3 \t Batch 520 \t Validation Loss: 40.86060306475713\n",
      "Epoch 3 \t Batch 540 \t Validation Loss: 40.56741827858819\n",
      "Epoch 3 \t Batch 560 \t Validation Loss: 40.31570145062038\n",
      "Epoch 3 \t Batch 580 \t Validation Loss: 40.161560397312556\n",
      "Epoch 3 \t Batch 600 \t Validation Loss: 40.29492826461792\n",
      "Epoch 3 Training Loss: 49.12765032020494 Validation Loss: 40.985196865998304\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 20 \t Training Loss: 48.271524620056155\n",
      "Epoch 4 \t Batch 40 \t Training Loss: 48.15513877868652\n",
      "Epoch 4 \t Batch 60 \t Training Loss: 48.67790203094482\n",
      "Epoch 4 \t Batch 80 \t Training Loss: 48.5879264831543\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 48.49671138763428\n",
      "Epoch 4 \t Batch 120 \t Training Loss: 48.42600933710734\n",
      "Epoch 4 \t Batch 140 \t Training Loss: 48.495386259896414\n",
      "Epoch 4 \t Batch 160 \t Training Loss: 48.40870189666748\n",
      "Epoch 4 \t Batch 180 \t Training Loss: 48.498815621270076\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 48.62695072174072\n",
      "Epoch 4 \t Batch 220 \t Training Loss: 48.63354067368941\n",
      "Epoch 4 \t Batch 240 \t Training Loss: 48.5565669854482\n",
      "Epoch 4 \t Batch 260 \t Training Loss: 48.61081051459679\n",
      "Epoch 4 \t Batch 280 \t Training Loss: 48.732921954563686\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 48.69625049591065\n",
      "Epoch 4 \t Batch 320 \t Training Loss: 48.743146288394925\n",
      "Epoch 4 \t Batch 340 \t Training Loss: 48.736984443664554\n",
      "Epoch 4 \t Batch 360 \t Training Loss: 48.70212775336371\n",
      "Epoch 4 \t Batch 380 \t Training Loss: 48.73433380126953\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 48.77283784866333\n",
      "Epoch 4 \t Batch 420 \t Training Loss: 48.75910911560059\n",
      "Epoch 4 \t Batch 440 \t Training Loss: 48.811433289267804\n",
      "Epoch 4 \t Batch 460 \t Training Loss: 48.82330848859704\n",
      "Epoch 4 \t Batch 480 \t Training Loss: 48.872277450561526\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 48.905290893554685\n",
      "Epoch 4 \t Batch 520 \t Training Loss: 48.875521322397084\n",
      "Epoch 4 \t Batch 540 \t Training Loss: 48.892065359044956\n",
      "Epoch 4 \t Batch 560 \t Training Loss: 48.89372590609959\n",
      "Epoch 4 \t Batch 580 \t Training Loss: 48.87419561188796\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 48.85403482437134\n",
      "Epoch 4 \t Batch 620 \t Training Loss: 48.85676764826621\n",
      "Epoch 4 \t Batch 640 \t Training Loss: 48.84521569609642\n",
      "Epoch 4 \t Batch 660 \t Training Loss: 48.816512726292466\n",
      "Epoch 4 \t Batch 680 \t Training Loss: 48.767844575994154\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 48.795317322867255\n",
      "Epoch 4 \t Batch 720 \t Training Loss: 48.78559270434909\n",
      "Epoch 4 \t Batch 740 \t Training Loss: 48.79535299249598\n",
      "Epoch 4 \t Batch 760 \t Training Loss: 48.830184449647604\n",
      "Epoch 4 \t Batch 780 \t Training Loss: 48.807707962623006\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 48.80963492393494\n",
      "Epoch 4 \t Batch 820 \t Training Loss: 48.77977613123452\n",
      "Epoch 4 \t Batch 840 \t Training Loss: 48.77762338093349\n",
      "Epoch 4 \t Batch 860 \t Training Loss: 48.76995798155319\n",
      "Epoch 4 \t Batch 880 \t Training Loss: 48.768485654484145\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 48.77196410285102\n",
      "Epoch 4 \t Batch 20 \t Validation Loss: 20.6640962600708\n",
      "Epoch 4 \t Batch 40 \t Validation Loss: 22.77962565422058\n",
      "Epoch 4 \t Batch 60 \t Validation Loss: 22.833951457341513\n",
      "Epoch 4 \t Batch 80 \t Validation Loss: 23.534181439876555\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 24.661034936904908\n",
      "Epoch 4 \t Batch 120 \t Validation Loss: 25.53391799132029\n",
      "Epoch 4 \t Batch 140 \t Validation Loss: 26.003880412237987\n",
      "Epoch 4 \t Batch 160 \t Validation Loss: 28.102736538648607\n",
      "Epoch 4 \t Batch 180 \t Validation Loss: 31.982400772306654\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 33.58544809818268\n",
      "Epoch 4 \t Batch 220 \t Validation Loss: 35.16904445561496\n",
      "Epoch 4 \t Batch 240 \t Validation Loss: 35.79183190266291\n",
      "Epoch 4 \t Batch 260 \t Validation Loss: 38.04451084136963\n",
      "Epoch 4 \t Batch 280 \t Validation Loss: 39.26039258411952\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 40.43170003255209\n",
      "Epoch 4 \t Batch 320 \t Validation Loss: 40.91665272712707\n",
      "Epoch 4 \t Batch 340 \t Validation Loss: 40.85328770244823\n",
      "Epoch 4 \t Batch 360 \t Validation Loss: 40.7121061431037\n",
      "Epoch 4 \t Batch 380 \t Validation Loss: 41.00770451897069\n",
      "Epoch 4 \t Batch 400 \t Validation Loss: 40.68330402374268\n",
      "Epoch 4 \t Batch 420 \t Validation Loss: 40.65753486724127\n",
      "Epoch 4 \t Batch 440 \t Validation Loss: 40.42534226504239\n",
      "Epoch 4 \t Batch 460 \t Validation Loss: 40.632191873633346\n",
      "Epoch 4 \t Batch 480 \t Validation Loss: 41.07579252719879\n",
      "Epoch 4 \t Batch 500 \t Validation Loss: 40.76508207321167\n",
      "Epoch 4 \t Batch 520 \t Validation Loss: 40.60666180390578\n",
      "Epoch 4 \t Batch 540 \t Validation Loss: 40.30945280569571\n",
      "Epoch 4 \t Batch 560 \t Validation Loss: 40.05380945886884\n",
      "Epoch 4 \t Batch 580 \t Validation Loss: 39.8469729324867\n",
      "Epoch 4 \t Batch 600 \t Validation Loss: 39.9701962629954\n",
      "Epoch 4 Training Loss: 48.75204149951332 Validation Loss: 40.53243128045813\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 20 \t Training Loss: 47.247809982299806\n",
      "Epoch 5 \t Batch 40 \t Training Loss: 47.434485912323\n",
      "Epoch 5 \t Batch 60 \t Training Loss: 47.86081256866455\n",
      "Epoch 5 \t Batch 80 \t Training Loss: 48.13800964355469\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 48.26368282318115\n",
      "Epoch 5 \t Batch 120 \t Training Loss: 48.38496475219726\n",
      "Epoch 5 \t Batch 140 \t Training Loss: 48.20579054696219\n",
      "Epoch 5 \t Batch 160 \t Training Loss: 48.28403623104096\n",
      "Epoch 5 \t Batch 180 \t Training Loss: 48.19665942721897\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 48.13987564086914\n",
      "Epoch 5 \t Batch 220 \t Training Loss: 48.27013164867054\n",
      "Epoch 5 \t Batch 240 \t Training Loss: 48.255510584513345\n",
      "Epoch 5 \t Batch 260 \t Training Loss: 48.421072783836955\n",
      "Epoch 5 \t Batch 280 \t Training Loss: 48.48161945343018\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 48.45377997080485\n",
      "Epoch 5 \t Batch 320 \t Training Loss: 48.42780703306198\n",
      "Epoch 5 \t Batch 340 \t Training Loss: 48.41745771520278\n",
      "Epoch 5 \t Batch 360 \t Training Loss: 48.36041004392836\n",
      "Epoch 5 \t Batch 380 \t Training Loss: 48.320891018917685\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 48.30996982574463\n",
      "Epoch 5 \t Batch 420 \t Training Loss: 48.28468816848028\n",
      "Epoch 5 \t Batch 440 \t Training Loss: 48.28638201626865\n",
      "Epoch 5 \t Batch 460 \t Training Loss: 48.31420689458432\n",
      "Epoch 5 \t Batch 480 \t Training Loss: 48.2944326877594\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 48.324926002502444\n",
      "Epoch 5 \t Batch 520 \t Training Loss: 48.302897218557504\n",
      "Epoch 5 \t Batch 540 \t Training Loss: 48.293631942183886\n",
      "Epoch 5 \t Batch 560 \t Training Loss: 48.33298121179853\n",
      "Epoch 5 \t Batch 580 \t Training Loss: 48.32038772188384\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 48.3201285425822\n",
      "Epoch 5 \t Batch 620 \t Training Loss: 48.30354648713143\n",
      "Epoch 5 \t Batch 640 \t Training Loss: 48.29610541462898\n",
      "Epoch 5 \t Batch 660 \t Training Loss: 48.31149764783455\n",
      "Epoch 5 \t Batch 680 \t Training Loss: 48.33811856438132\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 48.34159583500453\n",
      "Epoch 5 \t Batch 720 \t Training Loss: 48.36976330545213\n",
      "Epoch 5 \t Batch 740 \t Training Loss: 48.3818284008954\n",
      "Epoch 5 \t Batch 760 \t Training Loss: 48.395078603844894\n",
      "Epoch 5 \t Batch 780 \t Training Loss: 48.42389236352383\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 48.46928061962128\n",
      "Epoch 5 \t Batch 820 \t Training Loss: 48.478832375131\n",
      "Epoch 5 \t Batch 840 \t Training Loss: 48.438968127114435\n",
      "Epoch 5 \t Batch 860 \t Training Loss: 48.47210889417072\n",
      "Epoch 5 \t Batch 880 \t Training Loss: 48.47886780825528\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 48.44761079152425\n",
      "Epoch 5 \t Batch 20 \t Validation Loss: 19.83184185028076\n",
      "Epoch 5 \t Batch 40 \t Validation Loss: 20.77542917728424\n",
      "Epoch 5 \t Batch 60 \t Validation Loss: 21.039448976516724\n",
      "Epoch 5 \t Batch 80 \t Validation Loss: 21.90870887041092\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 22.742273626327513\n",
      "Epoch 5 \t Batch 120 \t Validation Loss: 23.854497090975443\n",
      "Epoch 5 \t Batch 140 \t Validation Loss: 24.277581814357212\n",
      "Epoch 5 \t Batch 160 \t Validation Loss: 26.20094726085663\n",
      "Epoch 5 \t Batch 180 \t Validation Loss: 29.974571047888862\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 31.565528888702392\n",
      "Epoch 5 \t Batch 220 \t Validation Loss: 32.93501642400568\n",
      "Epoch 5 \t Batch 240 \t Validation Loss: 33.4698388616244\n",
      "Epoch 5 \t Batch 260 \t Validation Loss: 35.61947376544659\n",
      "Epoch 5 \t Batch 280 \t Validation Loss: 36.79103287628719\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 37.94843189557393\n",
      "Epoch 5 \t Batch 320 \t Validation Loss: 38.44213697612285\n",
      "Epoch 5 \t Batch 340 \t Validation Loss: 38.42214651388281\n",
      "Epoch 5 \t Batch 360 \t Validation Loss: 38.239325388272604\n",
      "Epoch 5 \t Batch 380 \t Validation Loss: 38.51734059735348\n",
      "Epoch 5 \t Batch 400 \t Validation Loss: 38.19994252920151\n",
      "Epoch 5 \t Batch 420 \t Validation Loss: 38.22608192307609\n",
      "Epoch 5 \t Batch 440 \t Validation Loss: 38.02311565876007\n",
      "Epoch 5 \t Batch 460 \t Validation Loss: 38.279646581152214\n",
      "Epoch 5 \t Batch 480 \t Validation Loss: 38.77085700233777\n",
      "Epoch 5 \t Batch 500 \t Validation Loss: 38.482305028915405\n",
      "Epoch 5 \t Batch 520 \t Validation Loss: 38.2709462917768\n",
      "Epoch 5 \t Batch 540 \t Validation Loss: 38.03867564554567\n",
      "Epoch 5 \t Batch 560 \t Validation Loss: 37.83512860536575\n",
      "Epoch 5 \t Batch 580 \t Validation Loss: 37.570123549165395\n",
      "Epoch 5 \t Batch 600 \t Validation Loss: 37.81449599424998\n",
      "Epoch 5 Training Loss: 48.455542943989826 Validation Loss: 38.40252856929581\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 20 \t Training Loss: 48.751102447509766\n",
      "Epoch 6 \t Batch 40 \t Training Loss: 48.28693809509277\n",
      "Epoch 6 \t Batch 60 \t Training Loss: 48.275003560384114\n",
      "Epoch 6 \t Batch 80 \t Training Loss: 48.093993997573854\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 47.8606674194336\n",
      "Epoch 6 \t Batch 120 \t Training Loss: 48.035650444030765\n",
      "Epoch 6 \t Batch 140 \t Training Loss: 48.046206774030416\n",
      "Epoch 6 \t Batch 160 \t Training Loss: 48.26283838748932\n",
      "Epoch 6 \t Batch 180 \t Training Loss: 48.30098508199056\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 48.37315795898437\n",
      "Epoch 6 \t Batch 220 \t Training Loss: 48.38746960379861\n",
      "Epoch 6 \t Batch 240 \t Training Loss: 48.26426197687785\n",
      "Epoch 6 \t Batch 260 \t Training Loss: 48.254677479083725\n",
      "Epoch 6 \t Batch 280 \t Training Loss: 48.31781563077654\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 48.28038141886393\n",
      "Epoch 6 \t Batch 320 \t Training Loss: 48.383427309989926\n",
      "Epoch 6 \t Batch 340 \t Training Loss: 48.347801668503706\n",
      "Epoch 6 \t Batch 360 \t Training Loss: 48.4302165667216\n",
      "Epoch 6 \t Batch 380 \t Training Loss: 48.39475284375642\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 48.420002460479736\n",
      "Epoch 6 \t Batch 420 \t Training Loss: 48.42602043151855\n",
      "Epoch 6 \t Batch 440 \t Training Loss: 48.431654713370584\n",
      "Epoch 6 \t Batch 460 \t Training Loss: 48.40774162126624\n",
      "Epoch 6 \t Batch 480 \t Training Loss: 48.40797795454661\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 48.45065757751465\n",
      "Epoch 6 \t Batch 520 \t Training Loss: 48.43880466314462\n",
      "Epoch 6 \t Batch 540 \t Training Loss: 48.44690885897036\n",
      "Epoch 6 \t Batch 560 \t Training Loss: 48.457953575679234\n",
      "Epoch 6 \t Batch 580 \t Training Loss: 48.44446245390793\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 48.477311032613116\n",
      "Epoch 6 \t Batch 620 \t Training Loss: 48.438569302712715\n",
      "Epoch 6 \t Batch 640 \t Training Loss: 48.42001370191574\n",
      "Epoch 6 \t Batch 660 \t Training Loss: 48.426742848483\n",
      "Epoch 6 \t Batch 680 \t Training Loss: 48.40331297481761\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 48.3660098866054\n",
      "Epoch 6 \t Batch 720 \t Training Loss: 48.37708053588867\n",
      "Epoch 6 \t Batch 740 \t Training Loss: 48.35980212752884\n",
      "Epoch 6 \t Batch 760 \t Training Loss: 48.28222345552946\n",
      "Epoch 6 \t Batch 780 \t Training Loss: 48.286757317567485\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 48.26897792339325\n",
      "Epoch 6 \t Batch 820 \t Training Loss: 48.19902254895466\n",
      "Epoch 6 \t Batch 840 \t Training Loss: 48.244499397277835\n",
      "Epoch 6 \t Batch 860 \t Training Loss: 48.25743134520775\n",
      "Epoch 6 \t Batch 880 \t Training Loss: 48.21262037970803\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 48.22410099453396\n",
      "Epoch 6 \t Batch 20 \t Validation Loss: 28.44399461746216\n",
      "Epoch 6 \t Batch 40 \t Validation Loss: 26.943004488945007\n",
      "Epoch 6 \t Batch 60 \t Validation Loss: 28.270578304926556\n",
      "Epoch 6 \t Batch 80 \t Validation Loss: 28.787781047821046\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 28.311946630477905\n",
      "Epoch 6 \t Batch 120 \t Validation Loss: 28.33886643250783\n",
      "Epoch 6 \t Batch 140 \t Validation Loss: 28.116903959001814\n",
      "Epoch 6 \t Batch 160 \t Validation Loss: 29.242110228538515\n",
      "Epoch 6 \t Batch 180 \t Validation Loss: 31.593262492285834\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 32.49027788162231\n",
      "Epoch 6 \t Batch 220 \t Validation Loss: 33.07442425814542\n",
      "Epoch 6 \t Batch 240 \t Validation Loss: 33.14499009052913\n",
      "Epoch 6 \t Batch 260 \t Validation Loss: 34.65465064782363\n",
      "Epoch 6 \t Batch 280 \t Validation Loss: 35.32639484405517\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 36.05087405522664\n",
      "Epoch 6 \t Batch 320 \t Validation Loss: 36.311871248483655\n",
      "Epoch 6 \t Batch 340 \t Validation Loss: 36.26896767335779\n",
      "Epoch 6 \t Batch 360 \t Validation Loss: 36.060713858074614\n",
      "Epoch 6 \t Batch 380 \t Validation Loss: 36.233534692463124\n",
      "Epoch 6 \t Batch 400 \t Validation Loss: 36.03157766819\n",
      "Epoch 6 \t Batch 420 \t Validation Loss: 36.132213083903\n",
      "Epoch 6 \t Batch 440 \t Validation Loss: 35.97689293948087\n",
      "Epoch 6 \t Batch 460 \t Validation Loss: 36.29905495436295\n",
      "Epoch 6 \t Batch 480 \t Validation Loss: 36.83756430149079\n",
      "Epoch 6 \t Batch 500 \t Validation Loss: 36.57933235168457\n",
      "Epoch 6 \t Batch 520 \t Validation Loss: 36.47620050723736\n",
      "Epoch 6 \t Batch 540 \t Validation Loss: 36.28621661927965\n",
      "Epoch 6 \t Batch 560 \t Validation Loss: 36.164760163852144\n",
      "Epoch 6 \t Batch 580 \t Validation Loss: 36.02275568863441\n",
      "Epoch 6 \t Batch 600 \t Validation Loss: 36.27708641052246\n",
      "Epoch 6 Training Loss: 48.20668333879344 Validation Loss: 36.93669741184681\n",
      "Validation Loss Decreased(23070.956650733948--->22753.005605697632) Saving The Model\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 20 \t Training Loss: 48.5288911819458\n",
      "Epoch 7 \t Batch 40 \t Training Loss: 47.21015405654907\n",
      "Epoch 7 \t Batch 60 \t Training Loss: 47.03581085205078\n",
      "Epoch 7 \t Batch 80 \t Training Loss: 47.25496382713318\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 47.7608141708374\n",
      "Epoch 7 \t Batch 120 \t Training Loss: 47.93658266067505\n",
      "Epoch 7 \t Batch 140 \t Training Loss: 47.902208055768696\n",
      "Epoch 7 \t Batch 160 \t Training Loss: 48.0865959405899\n",
      "Epoch 7 \t Batch 180 \t Training Loss: 48.097340456644694\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 48.05765245437622\n",
      "Epoch 7 \t Batch 220 \t Training Loss: 47.99386220411821\n",
      "Epoch 7 \t Batch 240 \t Training Loss: 48.037910254796344\n",
      "Epoch 7 \t Batch 260 \t Training Loss: 48.00437578054575\n",
      "Epoch 7 \t Batch 280 \t Training Loss: 47.99709347316197\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 48.04457111358643\n",
      "Epoch 7 \t Batch 320 \t Training Loss: 48.02500706911087\n",
      "Epoch 7 \t Batch 340 \t Training Loss: 48.022173903970156\n",
      "Epoch 7 \t Batch 360 \t Training Loss: 48.03237288792928\n",
      "Epoch 7 \t Batch 380 \t Training Loss: 48.07788398140355\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 48.02953104972839\n",
      "Epoch 7 \t Batch 420 \t Training Loss: 48.07567537398565\n",
      "Epoch 7 \t Batch 440 \t Training Loss: 48.0597374309193\n",
      "Epoch 7 \t Batch 460 \t Training Loss: 48.11432254625404\n",
      "Epoch 7 \t Batch 480 \t Training Loss: 48.107703638076785\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 48.14002972412109\n",
      "Epoch 7 \t Batch 520 \t Training Loss: 48.115029958578255\n",
      "Epoch 7 \t Batch 540 \t Training Loss: 48.125505715829355\n",
      "Epoch 7 \t Batch 560 \t Training Loss: 48.05785711833409\n",
      "Epoch 7 \t Batch 580 \t Training Loss: 48.02729673714473\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 48.04331687291463\n",
      "Epoch 7 \t Batch 620 \t Training Loss: 48.03409280469341\n",
      "Epoch 7 \t Batch 640 \t Training Loss: 48.054665476083755\n",
      "Epoch 7 \t Batch 660 \t Training Loss: 48.025775250521576\n",
      "Epoch 7 \t Batch 680 \t Training Loss: 47.99123203053194\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 47.935490471976145\n",
      "Epoch 7 \t Batch 720 \t Training Loss: 47.91340832180447\n",
      "Epoch 7 \t Batch 740 \t Training Loss: 47.981265810373664\n",
      "Epoch 7 \t Batch 760 \t Training Loss: 47.99270386444895\n",
      "Epoch 7 \t Batch 780 \t Training Loss: 47.98416296641032\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 47.98299119472504\n",
      "Epoch 7 \t Batch 820 \t Training Loss: 48.00439723875464\n",
      "Epoch 7 \t Batch 840 \t Training Loss: 48.00723993664696\n",
      "Epoch 7 \t Batch 860 \t Training Loss: 47.9968534070392\n",
      "Epoch 7 \t Batch 880 \t Training Loss: 47.98656357418407\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 47.983417150709364\n",
      "Epoch 7 \t Batch 20 \t Validation Loss: 17.908319330215456\n",
      "Epoch 7 \t Batch 40 \t Validation Loss: 19.455167365074157\n",
      "Epoch 7 \t Batch 60 \t Validation Loss: 19.578389263153078\n",
      "Epoch 7 \t Batch 80 \t Validation Loss: 20.525937187671662\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 21.73800421714783\n",
      "Epoch 7 \t Batch 120 \t Validation Loss: 23.013552117347718\n",
      "Epoch 7 \t Batch 140 \t Validation Loss: 23.57013421058655\n",
      "Epoch 7 \t Batch 160 \t Validation Loss: 25.708881944417953\n",
      "Epoch 7 \t Batch 180 \t Validation Loss: 29.504517369800144\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 31.23614981174469\n",
      "Epoch 7 \t Batch 220 \t Validation Loss: 32.74214438958602\n",
      "Epoch 7 \t Batch 240 \t Validation Loss: 33.36268248558044\n",
      "Epoch 7 \t Batch 260 \t Validation Loss: 35.660762801537146\n",
      "Epoch 7 \t Batch 280 \t Validation Loss: 36.91175913129534\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 38.017355314890544\n",
      "Epoch 7 \t Batch 320 \t Validation Loss: 38.50919064283371\n",
      "Epoch 7 \t Batch 340 \t Validation Loss: 38.49987734626321\n",
      "Epoch 7 \t Batch 360 \t Validation Loss: 38.31927955945333\n",
      "Epoch 7 \t Batch 380 \t Validation Loss: 38.64208305760434\n",
      "Epoch 7 \t Batch 400 \t Validation Loss: 38.29276388645172\n",
      "Epoch 7 \t Batch 420 \t Validation Loss: 38.340565100170316\n",
      "Epoch 7 \t Batch 440 \t Validation Loss: 38.131424981897524\n",
      "Epoch 7 \t Batch 460 \t Validation Loss: 38.446475306801176\n",
      "Epoch 7 \t Batch 480 \t Validation Loss: 38.90780732234319\n",
      "Epoch 7 \t Batch 500 \t Validation Loss: 38.63373360824585\n",
      "Epoch 7 \t Batch 520 \t Validation Loss: 38.47076092866751\n",
      "Epoch 7 \t Batch 540 \t Validation Loss: 38.20989171840527\n",
      "Epoch 7 \t Batch 560 \t Validation Loss: 37.98280485187258\n",
      "Epoch 7 \t Batch 580 \t Validation Loss: 37.72593208674727\n",
      "Epoch 7 \t Batch 600 \t Validation Loss: 37.95113412062327\n",
      "Epoch 7 Training Loss: 48.00236964251769 Validation Loss: 38.55309886282141\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 20 \t Training Loss: 47.386888885498045\n",
      "Epoch 8 \t Batch 40 \t Training Loss: 47.70776376724243\n",
      "Epoch 8 \t Batch 60 \t Training Loss: 47.673283131917316\n",
      "Epoch 8 \t Batch 80 \t Training Loss: 47.6026394367218\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 47.89011180877686\n",
      "Epoch 8 \t Batch 120 \t Training Loss: 47.664101632436115\n",
      "Epoch 8 \t Batch 140 \t Training Loss: 47.61695938110351\n",
      "Epoch 8 \t Batch 160 \t Training Loss: 47.49898357391358\n",
      "Epoch 8 \t Batch 180 \t Training Loss: 47.403333303663466\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 47.430797004699706\n",
      "Epoch 8 \t Batch 220 \t Training Loss: 47.43127085945823\n",
      "Epoch 8 \t Batch 240 \t Training Loss: 47.54817279179891\n",
      "Epoch 8 \t Batch 260 \t Training Loss: 47.54327107943021\n",
      "Epoch 8 \t Batch 280 \t Training Loss: 47.593351759229385\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 47.592235527038575\n",
      "Epoch 8 \t Batch 320 \t Training Loss: 47.67911307811737\n",
      "Epoch 8 \t Batch 340 \t Training Loss: 47.68787022758933\n",
      "Epoch 8 \t Batch 360 \t Training Loss: 47.69069242477417\n",
      "Epoch 8 \t Batch 380 \t Training Loss: 47.721012938650034\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 47.76471371650696\n",
      "Epoch 8 \t Batch 420 \t Training Loss: 47.75574565160842\n",
      "Epoch 8 \t Batch 440 \t Training Loss: 47.81972027691928\n",
      "Epoch 8 \t Batch 460 \t Training Loss: 47.779666900634766\n",
      "Epoch 8 \t Batch 480 \t Training Loss: 47.802500144640604\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 47.76988510131836\n",
      "Epoch 8 \t Batch 520 \t Training Loss: 47.71345838400034\n",
      "Epoch 8 \t Batch 540 \t Training Loss: 47.82519686663592\n",
      "Epoch 8 \t Batch 560 \t Training Loss: 47.81967859268188\n",
      "Epoch 8 \t Batch 580 \t Training Loss: 47.79791391964616\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 47.790837891896565\n",
      "Epoch 8 \t Batch 620 \t Training Loss: 47.84293983828637\n",
      "Epoch 8 \t Batch 640 \t Training Loss: 47.850269109010696\n",
      "Epoch 8 \t Batch 660 \t Training Loss: 47.82866223653158\n",
      "Epoch 8 \t Batch 680 \t Training Loss: 47.8145803788129\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 47.819111731392994\n",
      "Epoch 8 \t Batch 720 \t Training Loss: 47.81265978283352\n",
      "Epoch 8 \t Batch 740 \t Training Loss: 47.822375539831214\n",
      "Epoch 8 \t Batch 760 \t Training Loss: 47.81639580475657\n",
      "Epoch 8 \t Batch 780 \t Training Loss: 47.82664340092585\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 47.82091463565826\n",
      "Epoch 8 \t Batch 820 \t Training Loss: 47.83523530262273\n",
      "Epoch 8 \t Batch 840 \t Training Loss: 47.85439684277489\n",
      "Epoch 8 \t Batch 860 \t Training Loss: 47.86906605653984\n",
      "Epoch 8 \t Batch 880 \t Training Loss: 47.862083365700464\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 47.86800629933675\n",
      "Epoch 8 \t Batch 20 \t Validation Loss: 21.70837287902832\n",
      "Epoch 8 \t Batch 40 \t Validation Loss: 23.073440742492675\n",
      "Epoch 8 \t Batch 60 \t Validation Loss: 23.006460237503052\n",
      "Epoch 8 \t Batch 80 \t Validation Loss: 23.269202852249144\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 24.301080379486084\n",
      "Epoch 8 \t Batch 120 \t Validation Loss: 25.254383977254232\n",
      "Epoch 8 \t Batch 140 \t Validation Loss: 25.575970540727887\n",
      "Epoch 8 \t Batch 160 \t Validation Loss: 27.62745233774185\n",
      "Epoch 8 \t Batch 180 \t Validation Loss: 31.422461292478772\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 33.00536592960358\n",
      "Epoch 8 \t Batch 220 \t Validation Loss: 34.582448062029755\n",
      "Epoch 8 \t Batch 240 \t Validation Loss: 35.19483868678411\n",
      "Epoch 8 \t Batch 260 \t Validation Loss: 37.50464720359216\n",
      "Epoch 8 \t Batch 280 \t Validation Loss: 38.7611894096647\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 39.82778817812602\n",
      "Epoch 8 \t Batch 320 \t Validation Loss: 40.326112660765645\n",
      "Epoch 8 \t Batch 340 \t Validation Loss: 40.23575117728289\n",
      "Epoch 8 \t Batch 360 \t Validation Loss: 40.01296021143595\n",
      "Epoch 8 \t Batch 380 \t Validation Loss: 40.33111652575041\n",
      "Epoch 8 \t Batch 400 \t Validation Loss: 39.90826180934906\n",
      "Epoch 8 \t Batch 420 \t Validation Loss: 39.8695555528005\n",
      "Epoch 8 \t Batch 440 \t Validation Loss: 39.584985548799686\n",
      "Epoch 8 \t Batch 460 \t Validation Loss: 39.83661671721417\n",
      "Epoch 8 \t Batch 480 \t Validation Loss: 40.28075981338819\n",
      "Epoch 8 \t Batch 500 \t Validation Loss: 39.97759992027283\n",
      "Epoch 8 \t Batch 520 \t Validation Loss: 39.775921271397515\n",
      "Epoch 8 \t Batch 540 \t Validation Loss: 39.44743934913918\n",
      "Epoch 8 \t Batch 560 \t Validation Loss: 39.179013000215804\n",
      "Epoch 8 \t Batch 580 \t Validation Loss: 38.84994734073507\n",
      "Epoch 8 \t Batch 600 \t Validation Loss: 39.02047925949097\n",
      "Epoch 8 Training Loss: 47.82688071397555 Validation Loss: 39.62737593712745\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 20 \t Training Loss: 48.8183614730835\n",
      "Epoch 9 \t Batch 40 \t Training Loss: 47.767907333374026\n",
      "Epoch 9 \t Batch 60 \t Training Loss: 48.019175084431964\n",
      "Epoch 9 \t Batch 80 \t Training Loss: 47.950012397766116\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 47.70629211425781\n",
      "Epoch 9 \t Batch 120 \t Training Loss: 47.67440230051677\n",
      "Epoch 9 \t Batch 140 \t Training Loss: 47.77538326808384\n",
      "Epoch 9 \t Batch 160 \t Training Loss: 47.68634617328644\n",
      "Epoch 9 \t Batch 180 \t Training Loss: 47.58736434512668\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 47.53727365493774\n",
      "Epoch 9 \t Batch 220 \t Training Loss: 47.37050243724476\n",
      "Epoch 9 \t Batch 240 \t Training Loss: 47.453032938639325\n",
      "Epoch 9 \t Batch 260 \t Training Loss: 47.49942384866568\n",
      "Epoch 9 \t Batch 280 \t Training Loss: 47.49393562589373\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 47.556274795532225\n",
      "Epoch 9 \t Batch 320 \t Training Loss: 47.590832591056824\n",
      "Epoch 9 \t Batch 340 \t Training Loss: 47.62295132805319\n",
      "Epoch 9 \t Batch 360 \t Training Loss: 47.57058047188653\n",
      "Epoch 9 \t Batch 380 \t Training Loss: 47.73178240123548\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 47.738626670837405\n",
      "Epoch 9 \t Batch 420 \t Training Loss: 47.74056237538655\n",
      "Epoch 9 \t Batch 440 \t Training Loss: 47.73420796827836\n",
      "Epoch 9 \t Batch 460 \t Training Loss: 47.74589331253715\n",
      "Epoch 9 \t Batch 480 \t Training Loss: 47.778863867123924\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 47.79955451965332\n",
      "Epoch 9 \t Batch 520 \t Training Loss: 47.82549291023841\n",
      "Epoch 9 \t Batch 540 \t Training Loss: 47.810384319446705\n",
      "Epoch 9 \t Batch 560 \t Training Loss: 47.78309327534267\n",
      "Epoch 9 \t Batch 580 \t Training Loss: 47.77928420757425\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 47.83158447265625\n",
      "Epoch 9 \t Batch 620 \t Training Loss: 47.832269643968154\n",
      "Epoch 9 \t Batch 640 \t Training Loss: 47.82327298521996\n",
      "Epoch 9 \t Batch 660 \t Training Loss: 47.79616329308712\n",
      "Epoch 9 \t Batch 680 \t Training Loss: 47.79925891651827\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 47.801977533612934\n",
      "Epoch 9 \t Batch 720 \t Training Loss: 47.78509501881069\n",
      "Epoch 9 \t Batch 740 \t Training Loss: 47.7602296262174\n",
      "Epoch 9 \t Batch 760 \t Training Loss: 47.76461303610551\n",
      "Epoch 9 \t Batch 780 \t Training Loss: 47.77896132346911\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 47.756022472381595\n",
      "Epoch 9 \t Batch 820 \t Training Loss: 47.72084096582925\n",
      "Epoch 9 \t Batch 840 \t Training Loss: 47.70846573511759\n",
      "Epoch 9 \t Batch 860 \t Training Loss: 47.68905651624813\n",
      "Epoch 9 \t Batch 880 \t Training Loss: 47.71182310364463\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 47.705666101243764\n",
      "Epoch 9 \t Batch 20 \t Validation Loss: 21.07897424697876\n",
      "Epoch 9 \t Batch 40 \t Validation Loss: 21.331773376464845\n",
      "Epoch 9 \t Batch 60 \t Validation Loss: 21.972084283828735\n",
      "Epoch 9 \t Batch 80 \t Validation Loss: 22.526023042201995\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 23.276090993881226\n",
      "Epoch 9 \t Batch 120 \t Validation Loss: 24.139839220046998\n",
      "Epoch 9 \t Batch 140 \t Validation Loss: 24.555924006870814\n",
      "Epoch 9 \t Batch 160 \t Validation Loss: 26.532703256607057\n",
      "Epoch 9 \t Batch 180 \t Validation Loss: 30.21685331132677\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 31.846606998443605\n",
      "Epoch 9 \t Batch 220 \t Validation Loss: 33.307776581157334\n",
      "Epoch 9 \t Batch 240 \t Validation Loss: 33.91377970774968\n",
      "Epoch 9 \t Batch 260 \t Validation Loss: 36.1704446719243\n",
      "Epoch 9 \t Batch 280 \t Validation Loss: 37.37823825223105\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 38.45875648816427\n",
      "Epoch 9 \t Batch 320 \t Validation Loss: 38.94826540052891\n",
      "Epoch 9 \t Batch 340 \t Validation Loss: 38.900688886642456\n",
      "Epoch 9 \t Batch 360 \t Validation Loss: 38.67179835372501\n",
      "Epoch 9 \t Batch 380 \t Validation Loss: 39.0276575414758\n",
      "Epoch 9 \t Batch 400 \t Validation Loss: 38.77141651391983\n",
      "Epoch 9 \t Batch 420 \t Validation Loss: 38.789478658494495\n",
      "Epoch 9 \t Batch 440 \t Validation Loss: 38.65900698358362\n",
      "Epoch 9 \t Batch 460 \t Validation Loss: 38.98810758383378\n",
      "Epoch 9 \t Batch 480 \t Validation Loss: 39.45225115418434\n",
      "Epoch 9 \t Batch 500 \t Validation Loss: 39.14204191017151\n",
      "Epoch 9 \t Batch 520 \t Validation Loss: 39.09216435505794\n",
      "Epoch 9 \t Batch 540 \t Validation Loss: 38.844419436984595\n",
      "Epoch 9 \t Batch 560 \t Validation Loss: 38.60816076483045\n",
      "Epoch 9 \t Batch 580 \t Validation Loss: 38.366549870063515\n",
      "Epoch 9 \t Batch 600 \t Validation Loss: 38.57528961181641\n",
      "Epoch 9 Training Loss: 47.67322997048481 Validation Loss: 39.16189593773384\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 20 \t Training Loss: 48.232781410217285\n",
      "Epoch 10 \t Batch 40 \t Training Loss: 47.93444957733154\n",
      "Epoch 10 \t Batch 60 \t Training Loss: 47.955058352152506\n",
      "Epoch 10 \t Batch 80 \t Training Loss: 47.81079187393188\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 47.67430011749268\n",
      "Epoch 10 \t Batch 120 \t Training Loss: 47.661550490061444\n",
      "Epoch 10 \t Batch 140 \t Training Loss: 47.77798524584089\n",
      "Epoch 10 \t Batch 160 \t Training Loss: 47.86761531829834\n",
      "Epoch 10 \t Batch 180 \t Training Loss: 47.81359318627251\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 47.76228061676025\n",
      "Epoch 10 \t Batch 220 \t Training Loss: 47.54354816783558\n",
      "Epoch 10 \t Batch 240 \t Training Loss: 47.665459108352664\n",
      "Epoch 10 \t Batch 260 \t Training Loss: 47.57532784388616\n",
      "Epoch 10 \t Batch 280 \t Training Loss: 47.584915379115515\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 47.52042500813802\n",
      "Epoch 10 \t Batch 320 \t Training Loss: 47.37726862430573\n",
      "Epoch 10 \t Batch 340 \t Training Loss: 47.3746145697201\n",
      "Epoch 10 \t Batch 360 \t Training Loss: 47.365276453230116\n",
      "Epoch 10 \t Batch 380 \t Training Loss: 47.315410132157176\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 47.30094864845276\n",
      "Epoch 10 \t Batch 420 \t Training Loss: 47.33568864549909\n",
      "Epoch 10 \t Batch 440 \t Training Loss: 47.37429937882857\n",
      "Epoch 10 \t Batch 460 \t Training Loss: 47.412896280703336\n",
      "Epoch 10 \t Batch 480 \t Training Loss: 47.46355357170105\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 47.443719818115234\n",
      "Epoch 10 \t Batch 520 \t Training Loss: 47.45071586462168\n",
      "Epoch 10 \t Batch 540 \t Training Loss: 47.4996115649188\n",
      "Epoch 10 \t Batch 560 \t Training Loss: 47.46231576374599\n",
      "Epoch 10 \t Batch 580 \t Training Loss: 47.476142100630135\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 47.48214209238688\n",
      "Epoch 10 \t Batch 620 \t Training Loss: 47.46874696054766\n",
      "Epoch 10 \t Batch 640 \t Training Loss: 47.44160134792328\n",
      "Epoch 10 \t Batch 660 \t Training Loss: 47.41028233152447\n",
      "Epoch 10 \t Batch 680 \t Training Loss: 47.44596661960377\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 47.473119485037664\n",
      "Epoch 10 \t Batch 720 \t Training Loss: 47.51499127282037\n",
      "Epoch 10 \t Batch 740 \t Training Loss: 47.4965655610368\n",
      "Epoch 10 \t Batch 760 \t Training Loss: 47.49706766228927\n",
      "Epoch 10 \t Batch 780 \t Training Loss: 47.501328091743666\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 47.4881983089447\n",
      "Epoch 10 \t Batch 820 \t Training Loss: 47.504411711343906\n",
      "Epoch 10 \t Batch 840 \t Training Loss: 47.49203706014724\n",
      "Epoch 10 \t Batch 860 \t Training Loss: 47.495307975591615\n",
      "Epoch 10 \t Batch 880 \t Training Loss: 47.522100318561904\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 47.501333440144855\n",
      "Epoch 10 \t Batch 20 \t Validation Loss: 16.594529008865358\n",
      "Epoch 10 \t Batch 40 \t Validation Loss: 17.088509464263915\n",
      "Epoch 10 \t Batch 60 \t Validation Loss: 17.523457702000936\n",
      "Epoch 10 \t Batch 80 \t Validation Loss: 18.256713235378264\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 19.90758063316345\n",
      "Epoch 10 \t Batch 120 \t Validation Loss: 21.4190482378006\n",
      "Epoch 10 \t Batch 140 \t Validation Loss: 22.173443739754813\n",
      "Epoch 10 \t Batch 160 \t Validation Loss: 24.217741346359254\n",
      "Epoch 10 \t Batch 180 \t Validation Loss: 27.45200320879618\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 28.923369183540345\n",
      "Epoch 10 \t Batch 220 \t Validation Loss: 30.21837087544528\n",
      "Epoch 10 \t Batch 240 \t Validation Loss: 30.795890311400097\n",
      "Epoch 10 \t Batch 260 \t Validation Loss: 32.96222793138944\n",
      "Epoch 10 \t Batch 280 \t Validation Loss: 34.18301383086613\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 35.051549959182736\n",
      "Epoch 10 \t Batch 320 \t Validation Loss: 35.542305174469945\n",
      "Epoch 10 \t Batch 340 \t Validation Loss: 35.60178506234113\n",
      "Epoch 10 \t Batch 360 \t Validation Loss: 35.458777305814955\n",
      "Epoch 10 \t Batch 380 \t Validation Loss: 35.849627775894966\n",
      "Epoch 10 \t Batch 400 \t Validation Loss: 35.70361558914185\n",
      "Epoch 10 \t Batch 420 \t Validation Loss: 35.8260758854094\n",
      "Epoch 10 \t Batch 440 \t Validation Loss: 35.751388367739594\n",
      "Epoch 10 \t Batch 460 \t Validation Loss: 36.13116164414779\n",
      "Epoch 10 \t Batch 480 \t Validation Loss: 36.683048804601036\n",
      "Epoch 10 \t Batch 500 \t Validation Loss: 36.47057634353638\n",
      "Epoch 10 \t Batch 520 \t Validation Loss: 36.44051342927492\n",
      "Epoch 10 \t Batch 540 \t Validation Loss: 36.21784598915665\n",
      "Epoch 10 \t Batch 560 \t Validation Loss: 36.04064066239766\n",
      "Epoch 10 \t Batch 580 \t Validation Loss: 35.84211631150081\n",
      "Epoch 10 \t Batch 600 \t Validation Loss: 36.0993060986201\n",
      "Epoch 10 Training Loss: 47.530273200381416 Validation Loss: 36.775612358923084\n",
      "Validation Loss Decreased(22753.005605697632--->22653.77721309662) Saving The Model\n",
      "Epoch 10 completed\n",
      "Epoch 11 \t Batch 20 \t Training Loss: 46.289933586120604\n",
      "Epoch 11 \t Batch 40 \t Training Loss: 47.25142707824707\n",
      "Epoch 11 \t Batch 60 \t Training Loss: 47.69786542256673\n",
      "Epoch 11 \t Batch 80 \t Training Loss: 47.99863080978393\n",
      "Epoch 11 \t Batch 100 \t Training Loss: 47.894509887695314\n",
      "Epoch 11 \t Batch 120 \t Training Loss: 47.745090611775716\n",
      "Epoch 11 \t Batch 140 \t Training Loss: 47.885139438084195\n",
      "Epoch 11 \t Batch 160 \t Training Loss: 47.80867915153503\n",
      "Epoch 11 \t Batch 180 \t Training Loss: 47.891255929734974\n",
      "Epoch 11 \t Batch 200 \t Training Loss: 47.888388805389404\n",
      "Epoch 11 \t Batch 220 \t Training Loss: 47.90030285228382\n",
      "Epoch 11 \t Batch 240 \t Training Loss: 47.965748659769694\n",
      "Epoch 11 \t Batch 260 \t Training Loss: 47.94790533505953\n",
      "Epoch 11 \t Batch 280 \t Training Loss: 47.84006470271519\n",
      "Epoch 11 \t Batch 300 \t Training Loss: 47.71805913289388\n",
      "Epoch 11 \t Batch 320 \t Training Loss: 47.7531720161438\n",
      "Epoch 11 \t Batch 340 \t Training Loss: 47.7231751722448\n",
      "Epoch 11 \t Batch 360 \t Training Loss: 47.69056723912557\n",
      "Epoch 11 \t Batch 380 \t Training Loss: 47.60874947999653\n",
      "Epoch 11 \t Batch 400 \t Training Loss: 47.62111347198486\n",
      "Epoch 11 \t Batch 420 \t Training Loss: 47.59681297483898\n",
      "Epoch 11 \t Batch 440 \t Training Loss: 47.66303235834295\n",
      "Epoch 11 \t Batch 460 \t Training Loss: 47.62308934253195\n",
      "Epoch 11 \t Batch 480 \t Training Loss: 47.65895709196727\n",
      "Epoch 11 \t Batch 500 \t Training Loss: 47.59842825317383\n",
      "Epoch 11 \t Batch 520 \t Training Loss: 47.62376917325533\n",
      "Epoch 11 \t Batch 540 \t Training Loss: 47.58587780705205\n",
      "Epoch 11 \t Batch 560 \t Training Loss: 47.53307050977435\n",
      "Epoch 11 \t Batch 580 \t Training Loss: 47.47273339896366\n",
      "Epoch 11 \t Batch 600 \t Training Loss: 47.45306638717651\n",
      "Epoch 11 \t Batch 620 \t Training Loss: 47.46465544546804\n",
      "Epoch 11 \t Batch 640 \t Training Loss: 47.46906692385674\n",
      "Epoch 11 \t Batch 660 \t Training Loss: 47.46687511675285\n",
      "Epoch 11 \t Batch 680 \t Training Loss: 47.498559082255646\n",
      "Epoch 11 \t Batch 700 \t Training Loss: 47.524014167785644\n",
      "Epoch 11 \t Batch 720 \t Training Loss: 47.51729761229621\n",
      "Epoch 11 \t Batch 740 \t Training Loss: 47.48746377068597\n",
      "Epoch 11 \t Batch 760 \t Training Loss: 47.46825847123799\n",
      "Epoch 11 \t Batch 780 \t Training Loss: 47.46082215431409\n",
      "Epoch 11 \t Batch 800 \t Training Loss: 47.4414284324646\n",
      "Epoch 11 \t Batch 820 \t Training Loss: 47.44013238302091\n",
      "Epoch 11 \t Batch 840 \t Training Loss: 47.41763824281239\n",
      "Epoch 11 \t Batch 860 \t Training Loss: 47.41020445712777\n",
      "Epoch 11 \t Batch 880 \t Training Loss: 47.42372888218273\n",
      "Epoch 11 \t Batch 900 \t Training Loss: 47.426913074917266\n",
      "Epoch 11 \t Batch 20 \t Validation Loss: 23.666348934173584\n",
      "Epoch 11 \t Batch 40 \t Validation Loss: 23.237790656089782\n",
      "Epoch 11 \t Batch 60 \t Validation Loss: 23.825306797027586\n",
      "Epoch 11 \t Batch 80 \t Validation Loss: 23.914026081562042\n",
      "Epoch 11 \t Batch 100 \t Validation Loss: 24.406520948410034\n",
      "Epoch 11 \t Batch 120 \t Validation Loss: 25.323965295155844\n",
      "Epoch 11 \t Batch 140 \t Validation Loss: 25.622261674063548\n",
      "Epoch 11 \t Batch 160 \t Validation Loss: 27.4451305270195\n",
      "Epoch 11 \t Batch 180 \t Validation Loss: 31.1347085263994\n",
      "Epoch 11 \t Batch 200 \t Validation Loss: 32.65426057338715\n",
      "Epoch 11 \t Batch 220 \t Validation Loss: 34.092030468854034\n",
      "Epoch 11 \t Batch 240 \t Validation Loss: 34.67956815163294\n",
      "Epoch 11 \t Batch 260 \t Validation Loss: 36.96773404708276\n",
      "Epoch 11 \t Batch 280 \t Validation Loss: 38.1749709878649\n",
      "Epoch 11 \t Batch 300 \t Validation Loss: 39.2030792427063\n",
      "Epoch 11 \t Batch 320 \t Validation Loss: 39.65838810801506\n",
      "Epoch 11 \t Batch 340 \t Validation Loss: 39.556840470257924\n",
      "Epoch 11 \t Batch 360 \t Validation Loss: 39.28423726558685\n",
      "Epoch 11 \t Batch 380 \t Validation Loss: 39.52511753032082\n",
      "Epoch 11 \t Batch 400 \t Validation Loss: 39.13493418931961\n",
      "Epoch 11 \t Batch 420 \t Validation Loss: 39.07580916313898\n",
      "Epoch 11 \t Batch 440 \t Validation Loss: 38.76043166464025\n",
      "Epoch 11 \t Batch 460 \t Validation Loss: 38.97157224364903\n",
      "Epoch 11 \t Batch 480 \t Validation Loss: 39.42116631468137\n",
      "Epoch 11 \t Batch 500 \t Validation Loss: 39.103890562057494\n",
      "Epoch 11 \t Batch 520 \t Validation Loss: 38.86177245103396\n",
      "Epoch 11 \t Batch 540 \t Validation Loss: 38.565733155497796\n",
      "Epoch 11 \t Batch 560 \t Validation Loss: 38.326858825342995\n",
      "Epoch 11 \t Batch 580 \t Validation Loss: 38.038821807400936\n",
      "Epoch 11 \t Batch 600 \t Validation Loss: 38.23681317170461\n",
      "Epoch 11 Training Loss: 47.40898220515693 Validation Loss: 38.83692376799398\n",
      "Epoch 11 completed\n",
      "Epoch 12 \t Batch 20 \t Training Loss: 47.41057147979736\n",
      "Epoch 12 \t Batch 40 \t Training Loss: 46.257573795318606\n",
      "Epoch 12 \t Batch 60 \t Training Loss: 46.74571781158447\n",
      "Epoch 12 \t Batch 80 \t Training Loss: 47.01860332489014\n",
      "Epoch 12 \t Batch 100 \t Training Loss: 47.157110900878905\n",
      "Epoch 12 \t Batch 120 \t Training Loss: 47.02106161117554\n",
      "Epoch 12 \t Batch 140 \t Training Loss: 46.974994904654366\n",
      "Epoch 12 \t Batch 160 \t Training Loss: 46.90944275856018\n",
      "Epoch 12 \t Batch 180 \t Training Loss: 47.0028190612793\n",
      "Epoch 12 \t Batch 200 \t Training Loss: 47.02142127990723\n",
      "Epoch 12 \t Batch 220 \t Training Loss: 46.950608253479004\n",
      "Epoch 12 \t Batch 240 \t Training Loss: 46.94594739278158\n",
      "Epoch 12 \t Batch 260 \t Training Loss: 46.97544897519625\n",
      "Epoch 12 \t Batch 280 \t Training Loss: 46.97106865474156\n",
      "Epoch 12 \t Batch 300 \t Training Loss: 46.965121281941734\n",
      "Epoch 12 \t Batch 320 \t Training Loss: 46.96342097520828\n",
      "Epoch 12 \t Batch 340 \t Training Loss: 46.97554574854234\n",
      "Epoch 12 \t Batch 360 \t Training Loss: 46.91950012842814\n",
      "Epoch 12 \t Batch 380 \t Training Loss: 46.94284644879793\n",
      "Epoch 12 \t Batch 400 \t Training Loss: 47.04920268058777\n",
      "Epoch 12 \t Batch 420 \t Training Loss: 47.03376360393706\n",
      "Epoch 12 \t Batch 440 \t Training Loss: 47.021790357069534\n",
      "Epoch 12 \t Batch 460 \t Training Loss: 47.067999690511954\n",
      "Epoch 12 \t Batch 480 \t Training Loss: 47.04633937676748\n",
      "Epoch 12 \t Batch 500 \t Training Loss: 47.060386489868165\n",
      "Epoch 12 \t Batch 520 \t Training Loss: 47.15822255061223\n",
      "Epoch 12 \t Batch 540 \t Training Loss: 47.185656208462184\n",
      "Epoch 12 \t Batch 560 \t Training Loss: 47.20884925978525\n",
      "Epoch 12 \t Batch 580 \t Training Loss: 47.26677913008065\n",
      "Epoch 12 \t Batch 600 \t Training Loss: 47.245962931315105\n",
      "Epoch 12 \t Batch 620 \t Training Loss: 47.28262590592907\n",
      "Epoch 12 \t Batch 640 \t Training Loss: 47.253634941577914\n",
      "Epoch 12 \t Batch 660 \t Training Loss: 47.26349748553652\n",
      "Epoch 12 \t Batch 680 \t Training Loss: 47.258174021103805\n",
      "Epoch 12 \t Batch 700 \t Training Loss: 47.26043428148542\n",
      "Epoch 12 \t Batch 720 \t Training Loss: 47.238806533813474\n",
      "Epoch 12 \t Batch 740 \t Training Loss: 47.239598944380475\n",
      "Epoch 12 \t Batch 760 \t Training Loss: 47.25795376426295\n",
      "Epoch 12 \t Batch 780 \t Training Loss: 47.25443965227176\n",
      "Epoch 12 \t Batch 800 \t Training Loss: 47.264443316459655\n",
      "Epoch 12 \t Batch 820 \t Training Loss: 47.27671054281839\n",
      "Epoch 12 \t Batch 840 \t Training Loss: 47.263104234422954\n",
      "Epoch 12 \t Batch 860 \t Training Loss: 47.26848914124245\n",
      "Epoch 12 \t Batch 880 \t Training Loss: 47.27019609104503\n",
      "Epoch 12 \t Batch 900 \t Training Loss: 47.24316930558946\n",
      "Epoch 12 \t Batch 20 \t Validation Loss: 15.56641435623169\n",
      "Epoch 12 \t Batch 40 \t Validation Loss: 16.22826752662659\n",
      "Epoch 12 \t Batch 60 \t Validation Loss: 16.93603172302246\n",
      "Epoch 12 \t Batch 80 \t Validation Loss: 17.449269425868987\n",
      "Epoch 12 \t Batch 100 \t Validation Loss: 19.26924595832825\n",
      "Epoch 12 \t Batch 120 \t Validation Loss: 20.914356152216595\n",
      "Epoch 12 \t Batch 140 \t Validation Loss: 21.946740695408412\n",
      "Epoch 12 \t Batch 160 \t Validation Loss: 24.134294426441194\n",
      "Epoch 12 \t Batch 180 \t Validation Loss: 27.793197785483468\n",
      "Epoch 12 \t Batch 200 \t Validation Loss: 29.629572863578797\n",
      "Epoch 12 \t Batch 220 \t Validation Loss: 31.12426182573492\n",
      "Epoch 12 \t Batch 240 \t Validation Loss: 31.89033797184626\n",
      "Epoch 12 \t Batch 260 \t Validation Loss: 34.22529366933382\n",
      "Epoch 12 \t Batch 280 \t Validation Loss: 35.575430498804366\n",
      "Epoch 12 \t Batch 300 \t Validation Loss: 36.55010443051656\n",
      "Epoch 12 \t Batch 320 \t Validation Loss: 37.05654105842113\n",
      "Epoch 12 \t Batch 340 \t Validation Loss: 37.14561044188107\n",
      "Epoch 12 \t Batch 360 \t Validation Loss: 36.99350231223636\n",
      "Epoch 12 \t Batch 380 \t Validation Loss: 37.387868416936776\n",
      "Epoch 12 \t Batch 400 \t Validation Loss: 37.18736079931259\n",
      "Epoch 12 \t Batch 420 \t Validation Loss: 37.31373401596433\n",
      "Epoch 12 \t Batch 440 \t Validation Loss: 37.207368415052244\n",
      "Epoch 12 \t Batch 460 \t Validation Loss: 37.53973250389099\n",
      "Epoch 12 \t Batch 480 \t Validation Loss: 38.027848249673845\n",
      "Epoch 12 \t Batch 500 \t Validation Loss: 37.800781949996946\n",
      "Epoch 12 \t Batch 520 \t Validation Loss: 37.765242094259996\n",
      "Epoch 12 \t Batch 540 \t Validation Loss: 37.57432600127326\n",
      "Epoch 12 \t Batch 560 \t Validation Loss: 37.469287771838054\n",
      "Epoch 12 \t Batch 580 \t Validation Loss: 37.3674176627192\n",
      "Epoch 12 \t Batch 600 \t Validation Loss: 37.62007715066274\n",
      "Epoch 12 Training Loss: 47.24425547359554 Validation Loss: 38.258106645051534\n",
      "Epoch 12 completed\n",
      "Epoch 13 \t Batch 20 \t Training Loss: 47.03491973876953\n",
      "Epoch 13 \t Batch 40 \t Training Loss: 47.280211353302\n",
      "Epoch 13 \t Batch 60 \t Training Loss: 47.34155775705973\n",
      "Epoch 13 \t Batch 80 \t Training Loss: 47.29320945739746\n",
      "Epoch 13 \t Batch 100 \t Training Loss: 47.2716869354248\n",
      "Epoch 13 \t Batch 120 \t Training Loss: 47.15987119674683\n",
      "Epoch 13 \t Batch 140 \t Training Loss: 46.939830098833355\n",
      "Epoch 13 \t Batch 160 \t Training Loss: 46.774106812477115\n",
      "Epoch 13 \t Batch 180 \t Training Loss: 46.78035380045573\n",
      "Epoch 13 \t Batch 200 \t Training Loss: 46.74807994842529\n",
      "Epoch 13 \t Batch 220 \t Training Loss: 46.81148698980158\n",
      "Epoch 13 \t Batch 240 \t Training Loss: 46.83008023897807\n",
      "Epoch 13 \t Batch 260 \t Training Loss: 46.92026879237248\n",
      "Epoch 13 \t Batch 280 \t Training Loss: 46.93252379553659\n",
      "Epoch 13 \t Batch 300 \t Training Loss: 46.99218119303385\n",
      "Epoch 13 \t Batch 320 \t Training Loss: 46.992160403728484\n",
      "Epoch 13 \t Batch 340 \t Training Loss: 46.971480032976935\n",
      "Epoch 13 \t Batch 360 \t Training Loss: 47.01100877126058\n",
      "Epoch 13 \t Batch 380 \t Training Loss: 46.90085621883995\n",
      "Epoch 13 \t Batch 400 \t Training Loss: 46.95327374458313\n",
      "Epoch 13 \t Batch 420 \t Training Loss: 46.986826006571455\n",
      "Epoch 13 \t Batch 440 \t Training Loss: 46.948850232904604\n",
      "Epoch 13 \t Batch 460 \t Training Loss: 46.95000139319379\n",
      "Epoch 13 \t Batch 480 \t Training Loss: 46.989897394180296\n",
      "Epoch 13 \t Batch 500 \t Training Loss: 47.03784959411621\n",
      "Epoch 13 \t Batch 520 \t Training Loss: 47.04339353121244\n",
      "Epoch 13 \t Batch 540 \t Training Loss: 47.00448621114095\n",
      "Epoch 13 \t Batch 560 \t Training Loss: 47.02830363001142\n",
      "Epoch 13 \t Batch 580 \t Training Loss: 47.02983912763924\n",
      "Epoch 13 \t Batch 600 \t Training Loss: 47.063311983744306\n",
      "Epoch 13 \t Batch 620 \t Training Loss: 47.07363784543929\n",
      "Epoch 13 \t Batch 640 \t Training Loss: 47.0476332962513\n",
      "Epoch 13 \t Batch 660 \t Training Loss: 47.01912925604618\n",
      "Epoch 13 \t Batch 680 \t Training Loss: 47.01130198310403\n",
      "Epoch 13 \t Batch 700 \t Training Loss: 47.022494141714915\n",
      "Epoch 13 \t Batch 720 \t Training Loss: 47.05351201163398\n",
      "Epoch 13 \t Batch 740 \t Training Loss: 47.03764511830098\n",
      "Epoch 13 \t Batch 760 \t Training Loss: 47.03901512246383\n",
      "Epoch 13 \t Batch 780 \t Training Loss: 47.07442841652112\n",
      "Epoch 13 \t Batch 800 \t Training Loss: 47.069087715148925\n",
      "Epoch 13 \t Batch 820 \t Training Loss: 47.06941372010766\n",
      "Epoch 13 \t Batch 840 \t Training Loss: 47.112195741562616\n",
      "Epoch 13 \t Batch 860 \t Training Loss: 47.135422950567204\n",
      "Epoch 13 \t Batch 880 \t Training Loss: 47.14984497590498\n",
      "Epoch 13 \t Batch 900 \t Training Loss: 47.16050473531087\n",
      "Epoch 13 \t Batch 20 \t Validation Loss: 15.381106185913087\n",
      "Epoch 13 \t Batch 40 \t Validation Loss: 16.84868950843811\n",
      "Epoch 13 \t Batch 60 \t Validation Loss: 17.00075559616089\n",
      "Epoch 13 \t Batch 80 \t Validation Loss: 17.615586483478545\n",
      "Epoch 13 \t Batch 100 \t Validation Loss: 19.50365761756897\n",
      "Epoch 13 \t Batch 120 \t Validation Loss: 21.11404390335083\n",
      "Epoch 13 \t Batch 140 \t Validation Loss: 21.981557423727853\n",
      "Epoch 13 \t Batch 160 \t Validation Loss: 24.2720543384552\n",
      "Epoch 13 \t Batch 180 \t Validation Loss: 27.99916599591573\n",
      "Epoch 13 \t Batch 200 \t Validation Loss: 29.76306242465973\n",
      "Epoch 13 \t Batch 220 \t Validation Loss: 31.401807884736495\n",
      "Epoch 13 \t Batch 240 \t Validation Loss: 32.12515533367793\n",
      "Epoch 13 \t Batch 260 \t Validation Loss: 34.582433634537914\n",
      "Epoch 13 \t Batch 280 \t Validation Loss: 36.01053285939353\n",
      "Epoch 13 \t Batch 300 \t Validation Loss: 36.9641069316864\n",
      "Epoch 13 \t Batch 320 \t Validation Loss: 37.47430817782879\n",
      "Epoch 13 \t Batch 340 \t Validation Loss: 37.483288879955516\n",
      "Epoch 13 \t Batch 360 \t Validation Loss: 37.29335803455777\n",
      "Epoch 13 \t Batch 380 \t Validation Loss: 37.73285973197535\n",
      "Epoch 13 \t Batch 400 \t Validation Loss: 37.50911536693573\n",
      "Epoch 13 \t Batch 420 \t Validation Loss: 37.55473069917588\n",
      "Epoch 13 \t Batch 440 \t Validation Loss: 37.433184517513624\n",
      "Epoch 13 \t Batch 460 \t Validation Loss: 37.77617538908254\n",
      "Epoch 13 \t Batch 480 \t Validation Loss: 38.2711085220178\n",
      "Epoch 13 \t Batch 500 \t Validation Loss: 37.98743567085266\n",
      "Epoch 13 \t Batch 520 \t Validation Loss: 37.954279059630174\n",
      "Epoch 13 \t Batch 540 \t Validation Loss: 37.724681773009124\n",
      "Epoch 13 \t Batch 560 \t Validation Loss: 37.53477458953857\n",
      "Epoch 13 \t Batch 580 \t Validation Loss: 37.34372043280766\n",
      "Epoch 13 \t Batch 600 \t Validation Loss: 37.5708532555898\n",
      "Epoch 13 Training Loss: 47.14025125118742 Validation Loss: 38.18605741897186\n",
      "Epoch 13 completed\n",
      "Epoch 14 \t Batch 20 \t Training Loss: 48.56925754547119\n",
      "Epoch 14 \t Batch 40 \t Training Loss: 47.862574100494385\n",
      "Epoch 14 \t Batch 60 \t Training Loss: 47.63776003519694\n",
      "Epoch 14 \t Batch 80 \t Training Loss: 47.209455251693726\n",
      "Epoch 14 \t Batch 100 \t Training Loss: 47.066571655273435\n",
      "Epoch 14 \t Batch 120 \t Training Loss: 46.83663330078125\n",
      "Epoch 14 \t Batch 140 \t Training Loss: 46.79712666102818\n",
      "Epoch 14 \t Batch 160 \t Training Loss: 46.88046255111694\n",
      "Epoch 14 \t Batch 180 \t Training Loss: 46.87876813676622\n",
      "Epoch 14 \t Batch 200 \t Training Loss: 46.95357624053955\n",
      "Epoch 14 \t Batch 220 \t Training Loss: 46.84306033741344\n",
      "Epoch 14 \t Batch 240 \t Training Loss: 46.82248600323995\n",
      "Epoch 14 \t Batch 260 \t Training Loss: 46.866937329218935\n",
      "Epoch 14 \t Batch 280 \t Training Loss: 46.851438876560756\n",
      "Epoch 14 \t Batch 300 \t Training Loss: 46.82309937795003\n",
      "Epoch 14 \t Batch 320 \t Training Loss: 46.86547441482544\n",
      "Epoch 14 \t Batch 340 \t Training Loss: 46.83683762269862\n",
      "Epoch 14 \t Batch 360 \t Training Loss: 46.99297718471951\n",
      "Epoch 14 \t Batch 380 \t Training Loss: 46.92628862481368\n",
      "Epoch 14 \t Batch 400 \t Training Loss: 46.96835376739502\n",
      "Epoch 14 \t Batch 420 \t Training Loss: 46.926763816106885\n",
      "Epoch 14 \t Batch 440 \t Training Loss: 46.97592537619851\n",
      "Epoch 14 \t Batch 460 \t Training Loss: 46.908514901866084\n",
      "Epoch 14 \t Batch 480 \t Training Loss: 46.93041701316834\n",
      "Epoch 14 \t Batch 500 \t Training Loss: 46.950228607177735\n",
      "Epoch 14 \t Batch 520 \t Training Loss: 46.92690778145423\n",
      "Epoch 14 \t Batch 540 \t Training Loss: 46.94577316708035\n",
      "Epoch 14 \t Batch 560 \t Training Loss: 46.946367611203875\n",
      "Epoch 14 \t Batch 580 \t Training Loss: 46.91175656154238\n",
      "Epoch 14 \t Batch 600 \t Training Loss: 46.954063447316486\n",
      "Epoch 14 \t Batch 620 \t Training Loss: 46.93832986893192\n",
      "Epoch 14 \t Batch 640 \t Training Loss: 46.93840488195419\n",
      "Epoch 14 \t Batch 660 \t Training Loss: 46.98367430369059\n",
      "Epoch 14 \t Batch 680 \t Training Loss: 46.96564202028162\n",
      "Epoch 14 \t Batch 700 \t Training Loss: 46.97660888671875\n",
      "Epoch 14 \t Batch 720 \t Training Loss: 46.96597961849636\n",
      "Epoch 14 \t Batch 740 \t Training Loss: 46.9764334549775\n",
      "Epoch 14 \t Batch 760 \t Training Loss: 46.97135848999024\n",
      "Epoch 14 \t Batch 780 \t Training Loss: 47.00361748719827\n",
      "Epoch 14 \t Batch 800 \t Training Loss: 46.99606620788574\n",
      "Epoch 14 \t Batch 820 \t Training Loss: 47.00890238226914\n",
      "Epoch 14 \t Batch 840 \t Training Loss: 47.02631890433175\n",
      "Epoch 14 \t Batch 860 \t Training Loss: 46.98781904176224\n",
      "Epoch 14 \t Batch 880 \t Training Loss: 46.9670991897583\n",
      "Epoch 14 \t Batch 900 \t Training Loss: 47.00195804595947\n",
      "Epoch 14 \t Batch 20 \t Validation Loss: 15.215744733810425\n",
      "Epoch 14 \t Batch 40 \t Validation Loss: 16.026506185531616\n",
      "Epoch 14 \t Batch 60 \t Validation Loss: 16.724460021654764\n",
      "Epoch 14 \t Batch 80 \t Validation Loss: 17.586047369241715\n",
      "Epoch 14 \t Batch 100 \t Validation Loss: 19.370938076972962\n",
      "Epoch 14 \t Batch 120 \t Validation Loss: 20.985276639461517\n",
      "Epoch 14 \t Batch 140 \t Validation Loss: 21.83500279358455\n",
      "Epoch 14 \t Batch 160 \t Validation Loss: 24.222317996621133\n",
      "Epoch 14 \t Batch 180 \t Validation Loss: 28.455344367027283\n",
      "Epoch 14 \t Batch 200 \t Validation Loss: 30.45065184354782\n",
      "Epoch 14 \t Batch 220 \t Validation Loss: 32.17896834070032\n",
      "Epoch 14 \t Batch 240 \t Validation Loss: 33.01382934053739\n",
      "Epoch 14 \t Batch 260 \t Validation Loss: 35.48063455911783\n",
      "Epoch 14 \t Batch 280 \t Validation Loss: 36.8313912987709\n",
      "Epoch 14 \t Batch 300 \t Validation Loss: 38.08158399422963\n",
      "Epoch 14 \t Batch 320 \t Validation Loss: 38.672095178067686\n",
      "Epoch 14 \t Batch 340 \t Validation Loss: 38.6818119764328\n",
      "Epoch 14 \t Batch 360 \t Validation Loss: 38.52374795675278\n",
      "Epoch 14 \t Batch 380 \t Validation Loss: 38.79704579679589\n",
      "Epoch 14 \t Batch 400 \t Validation Loss: 38.39395367026329\n",
      "Epoch 14 \t Batch 420 \t Validation Loss: 38.43197467327118\n",
      "Epoch 14 \t Batch 440 \t Validation Loss: 38.09603255336935\n",
      "Epoch 14 \t Batch 460 \t Validation Loss: 38.31318126243094\n",
      "Epoch 14 \t Batch 480 \t Validation Loss: 38.799961470564206\n",
      "Epoch 14 \t Batch 500 \t Validation Loss: 38.54881543636322\n",
      "Epoch 14 \t Batch 520 \t Validation Loss: 38.2791464979832\n",
      "Epoch 14 \t Batch 540 \t Validation Loss: 38.018525853863466\n",
      "Epoch 14 \t Batch 560 \t Validation Loss: 37.821726833922526\n",
      "Epoch 14 \t Batch 580 \t Validation Loss: 37.529640287366405\n",
      "Epoch 14 \t Batch 600 \t Validation Loss: 37.77045594930649\n",
      "Epoch 14 Training Loss: 47.0128824198649 Validation Loss: 38.37702912247026\n",
      "Epoch 14 completed\n",
      "Epoch 15 \t Batch 20 \t Training Loss: 47.58375415802002\n",
      "Epoch 15 \t Batch 40 \t Training Loss: 47.97548151016235\n",
      "Epoch 15 \t Batch 60 \t Training Loss: 47.5888650894165\n",
      "Epoch 15 \t Batch 80 \t Training Loss: 47.33087258338928\n",
      "Epoch 15 \t Batch 100 \t Training Loss: 47.74282646179199\n",
      "Epoch 15 \t Batch 120 \t Training Loss: 47.54939762751261\n",
      "Epoch 15 \t Batch 140 \t Training Loss: 47.38130356924874\n",
      "Epoch 15 \t Batch 160 \t Training Loss: 47.47585656642914\n",
      "Epoch 15 \t Batch 180 \t Training Loss: 47.521762148539224\n",
      "Epoch 15 \t Batch 200 \t Training Loss: 47.50373369216919\n",
      "Epoch 15 \t Batch 220 \t Training Loss: 47.470806642012164\n",
      "Epoch 15 \t Batch 240 \t Training Loss: 47.40462180773417\n",
      "Epoch 15 \t Batch 260 \t Training Loss: 47.35490342653715\n",
      "Epoch 15 \t Batch 280 \t Training Loss: 47.34222271783011\n",
      "Epoch 15 \t Batch 300 \t Training Loss: 47.29046712239583\n",
      "Epoch 15 \t Batch 320 \t Training Loss: 47.235888671875\n",
      "Epoch 15 \t Batch 340 \t Training Loss: 47.16908077913172\n",
      "Epoch 15 \t Batch 360 \t Training Loss: 47.13709816402859\n",
      "Epoch 15 \t Batch 380 \t Training Loss: 47.07569210654811\n",
      "Epoch 15 \t Batch 400 \t Training Loss: 47.02810886383057\n",
      "Epoch 15 \t Batch 420 \t Training Loss: 47.02486932845343\n",
      "Epoch 15 \t Batch 440 \t Training Loss: 46.9626239863309\n",
      "Epoch 15 \t Batch 460 \t Training Loss: 47.00628370202106\n",
      "Epoch 15 \t Batch 480 \t Training Loss: 47.029179279009504\n",
      "Epoch 15 \t Batch 500 \t Training Loss: 47.0149320602417\n",
      "Epoch 15 \t Batch 520 \t Training Loss: 46.986140456566446\n",
      "Epoch 15 \t Batch 540 \t Training Loss: 47.0036210378011\n",
      "Epoch 15 \t Batch 560 \t Training Loss: 47.00389633859907\n",
      "Epoch 15 \t Batch 580 \t Training Loss: 46.961469400340114\n",
      "Epoch 15 \t Batch 600 \t Training Loss: 46.982622362772624\n",
      "Epoch 15 \t Batch 620 \t Training Loss: 46.982863087807935\n",
      "Epoch 15 \t Batch 640 \t Training Loss: 46.98984123468399\n",
      "Epoch 15 \t Batch 660 \t Training Loss: 47.026537866303414\n",
      "Epoch 15 \t Batch 680 \t Training Loss: 47.00519866382375\n",
      "Epoch 15 \t Batch 700 \t Training Loss: 46.98011553628104\n",
      "Epoch 15 \t Batch 720 \t Training Loss: 47.00566758049859\n",
      "Epoch 15 \t Batch 740 \t Training Loss: 46.99661236324826\n",
      "Epoch 15 \t Batch 760 \t Training Loss: 46.992886257171634\n",
      "Epoch 15 \t Batch 780 \t Training Loss: 47.00970819913424\n",
      "Epoch 15 \t Batch 800 \t Training Loss: 46.99695761203766\n",
      "Epoch 15 \t Batch 820 \t Training Loss: 46.99399652248476\n",
      "Epoch 15 \t Batch 840 \t Training Loss: 46.969394524892174\n",
      "Epoch 15 \t Batch 860 \t Training Loss: 46.935405376345614\n",
      "Epoch 15 \t Batch 880 \t Training Loss: 46.927013011412186\n",
      "Epoch 15 \t Batch 900 \t Training Loss: 46.92074100494385\n",
      "Epoch 15 \t Batch 20 \t Validation Loss: 11.602879905700684\n",
      "Epoch 15 \t Batch 40 \t Validation Loss: 13.52751829624176\n",
      "Epoch 15 \t Batch 60 \t Validation Loss: 13.701709548632303\n",
      "Epoch 15 \t Batch 80 \t Validation Loss: 14.71027238368988\n",
      "Epoch 15 \t Batch 100 \t Validation Loss: 17.23435506820679\n",
      "Epoch 15 \t Batch 120 \t Validation Loss: 19.16212402979533\n",
      "Epoch 15 \t Batch 140 \t Validation Loss: 20.27842196055821\n",
      "Epoch 15 \t Batch 160 \t Validation Loss: 22.818666863441468\n",
      "Epoch 15 \t Batch 180 \t Validation Loss: 26.693706226348876\n",
      "Epoch 15 \t Batch 200 \t Validation Loss: 28.640069103240968\n",
      "Epoch 15 \t Batch 220 \t Validation Loss: 30.32214732603593\n",
      "Epoch 15 \t Batch 240 \t Validation Loss: 31.100094989935556\n",
      "Epoch 15 \t Batch 260 \t Validation Loss: 33.617315890238835\n",
      "Epoch 15 \t Batch 280 \t Validation Loss: 35.09847835132054\n",
      "Epoch 15 \t Batch 300 \t Validation Loss: 36.11405552546183\n",
      "Epoch 15 \t Batch 320 \t Validation Loss: 36.69329158961773\n",
      "Epoch 15 \t Batch 340 \t Validation Loss: 36.78299615803887\n",
      "Epoch 15 \t Batch 360 \t Validation Loss: 36.66675863530901\n",
      "Epoch 15 \t Batch 380 \t Validation Loss: 37.10888063280206\n",
      "Epoch 15 \t Batch 400 \t Validation Loss: 36.91491881608963\n",
      "Epoch 15 \t Batch 420 \t Validation Loss: 37.01765722320194\n",
      "Epoch 15 \t Batch 440 \t Validation Loss: 36.95784197937358\n",
      "Epoch 15 \t Batch 460 \t Validation Loss: 37.346544184892075\n",
      "Epoch 15 \t Batch 480 \t Validation Loss: 37.86709024707476\n",
      "Epoch 15 \t Batch 500 \t Validation Loss: 37.652711175918576\n",
      "Epoch 15 \t Batch 520 \t Validation Loss: 37.64421500976269\n",
      "Epoch 15 \t Batch 540 \t Validation Loss: 37.38326984158269\n",
      "Epoch 15 \t Batch 560 \t Validation Loss: 37.22169465848378\n",
      "Epoch 15 \t Batch 580 \t Validation Loss: 37.01661187369248\n",
      "Epoch 15 \t Batch 600 \t Validation Loss: 37.23792565822601\n",
      "Epoch 15 Training Loss: 46.90965880848459 Validation Loss: 37.8572237414199\n",
      "Epoch 15 completed\n",
      "Epoch 16 \t Batch 20 \t Training Loss: 46.32863006591797\n",
      "Epoch 16 \t Batch 40 \t Training Loss: 46.01318025588989\n",
      "Epoch 16 \t Batch 60 \t Training Loss: 46.75114574432373\n",
      "Epoch 16 \t Batch 80 \t Training Loss: 46.80824222564697\n",
      "Epoch 16 \t Batch 100 \t Training Loss: 46.893756790161135\n",
      "Epoch 16 \t Batch 120 \t Training Loss: 47.00868431727091\n",
      "Epoch 16 \t Batch 140 \t Training Loss: 47.27161001477923\n",
      "Epoch 16 \t Batch 160 \t Training Loss: 47.01457772254944\n",
      "Epoch 16 \t Batch 180 \t Training Loss: 47.02620502048069\n",
      "Epoch 16 \t Batch 200 \t Training Loss: 46.94290662765503\n",
      "Epoch 16 \t Batch 220 \t Training Loss: 46.89397820559415\n",
      "Epoch 16 \t Batch 240 \t Training Loss: 46.83520461718241\n",
      "Epoch 16 \t Batch 260 \t Training Loss: 46.896367880014274\n",
      "Epoch 16 \t Batch 280 \t Training Loss: 46.92096112115043\n",
      "Epoch 16 \t Batch 300 \t Training Loss: 46.83335383097331\n",
      "Epoch 16 \t Batch 320 \t Training Loss: 46.74685529470444\n",
      "Epoch 16 \t Batch 340 \t Training Loss: 46.732110820097084\n",
      "Epoch 16 \t Batch 360 \t Training Loss: 46.677676328023274\n",
      "Epoch 16 \t Batch 380 \t Training Loss: 46.714284284491285\n",
      "Epoch 16 \t Batch 400 \t Training Loss: 46.72212551116943\n",
      "Epoch 16 \t Batch 420 \t Training Loss: 46.72366757165818\n",
      "Epoch 16 \t Batch 440 \t Training Loss: 46.77593153173273\n",
      "Epoch 16 \t Batch 460 \t Training Loss: 46.79935894841733\n",
      "Epoch 16 \t Batch 480 \t Training Loss: 46.74975790182749\n",
      "Epoch 16 \t Batch 500 \t Training Loss: 46.74766165924072\n",
      "Epoch 16 \t Batch 520 \t Training Loss: 46.8804708480835\n",
      "Epoch 16 \t Batch 540 \t Training Loss: 46.866409831576874\n",
      "Epoch 16 \t Batch 560 \t Training Loss: 46.831902279172624\n",
      "Epoch 16 \t Batch 580 \t Training Loss: 46.83638144525988\n",
      "Epoch 16 \t Batch 600 \t Training Loss: 46.78807513554891\n",
      "Epoch 16 \t Batch 620 \t Training Loss: 46.828198660573655\n",
      "Epoch 16 \t Batch 640 \t Training Loss: 46.85429521203041\n",
      "Epoch 16 \t Batch 660 \t Training Loss: 46.83716674573494\n",
      "Epoch 16 \t Batch 680 \t Training Loss: 46.829607413796815\n",
      "Epoch 16 \t Batch 700 \t Training Loss: 46.86376913343157\n",
      "Epoch 16 \t Batch 720 \t Training Loss: 46.84893929693434\n",
      "Epoch 16 \t Batch 740 \t Training Loss: 46.86565688365214\n",
      "Epoch 16 \t Batch 760 \t Training Loss: 46.8281480839378\n",
      "Epoch 16 \t Batch 780 \t Training Loss: 46.84435280530881\n",
      "Epoch 16 \t Batch 800 \t Training Loss: 46.857043061256405\n",
      "Epoch 16 \t Batch 820 \t Training Loss: 46.84109651984238\n",
      "Epoch 16 \t Batch 840 \t Training Loss: 46.828846041361494\n",
      "Epoch 16 \t Batch 860 \t Training Loss: 46.787776019961335\n",
      "Epoch 16 \t Batch 880 \t Training Loss: 46.815528787266125\n",
      "Epoch 16 \t Batch 900 \t Training Loss: 46.80777754041884\n",
      "Epoch 16 \t Batch 20 \t Validation Loss: 9.603670930862426\n",
      "Epoch 16 \t Batch 40 \t Validation Loss: 11.949736952781677\n",
      "Epoch 16 \t Batch 60 \t Validation Loss: 12.070763564109802\n",
      "Epoch 16 \t Batch 80 \t Validation Loss: 12.967300307750701\n",
      "Epoch 16 \t Batch 100 \t Validation Loss: 15.656912965774536\n",
      "Epoch 16 \t Batch 120 \t Validation Loss: 17.822584128379823\n",
      "Epoch 16 \t Batch 140 \t Validation Loss: 19.078289556503297\n",
      "Epoch 16 \t Batch 160 \t Validation Loss: 21.462634760141373\n",
      "Epoch 16 \t Batch 180 \t Validation Loss: 25.189257510503133\n",
      "Epoch 16 \t Batch 200 \t Validation Loss: 26.98022054195404\n",
      "Epoch 16 \t Batch 220 \t Validation Loss: 28.56863011013378\n",
      "Epoch 16 \t Batch 240 \t Validation Loss: 29.374902927875517\n",
      "Epoch 16 \t Batch 260 \t Validation Loss: 31.7826946075146\n",
      "Epoch 16 \t Batch 280 \t Validation Loss: 33.21494094644274\n",
      "Epoch 16 \t Batch 300 \t Validation Loss: 34.19102981885274\n",
      "Epoch 16 \t Batch 320 \t Validation Loss: 34.75737486481667\n",
      "Epoch 16 \t Batch 340 \t Validation Loss: 34.871417539259966\n",
      "Epoch 16 \t Batch 360 \t Validation Loss: 34.71911239359114\n",
      "Epoch 16 \t Batch 380 \t Validation Loss: 35.15020502241034\n",
      "Epoch 16 \t Batch 400 \t Validation Loss: 35.01099443674087\n",
      "Epoch 16 \t Batch 420 \t Validation Loss: 35.153770671572005\n",
      "Epoch 16 \t Batch 440 \t Validation Loss: 35.08458219224757\n",
      "Epoch 16 \t Batch 460 \t Validation Loss: 35.47550136524698\n",
      "Epoch 16 \t Batch 480 \t Validation Loss: 36.03915480573972\n",
      "Epoch 16 \t Batch 500 \t Validation Loss: 35.81905534172058\n",
      "Epoch 16 \t Batch 520 \t Validation Loss: 35.77976612311143\n",
      "Epoch 16 \t Batch 540 \t Validation Loss: 35.60932104675858\n",
      "Epoch 16 \t Batch 560 \t Validation Loss: 35.49541258811951\n",
      "Epoch 16 \t Batch 580 \t Validation Loss: 35.31525031451521\n",
      "Epoch 16 \t Batch 600 \t Validation Loss: 35.61051966349284\n",
      "Epoch 16 Training Loss: 46.83054004032713 Validation Loss: 36.28032170642506\n",
      "Validation Loss Decreased(22653.77721309662--->22348.678171157837) Saving The Model\n",
      "Epoch 16 completed\n",
      "Epoch 17 \t Batch 20 \t Training Loss: 47.45388965606689\n",
      "Epoch 17 \t Batch 40 \t Training Loss: 47.040542697906496\n",
      "Epoch 17 \t Batch 60 \t Training Loss: 47.506265767415364\n",
      "Epoch 17 \t Batch 80 \t Training Loss: 47.16029586791992\n",
      "Epoch 17 \t Batch 100 \t Training Loss: 46.86786277770996\n",
      "Epoch 17 \t Batch 120 \t Training Loss: 46.99423694610596\n",
      "Epoch 17 \t Batch 140 \t Training Loss: 47.01807934897287\n",
      "Epoch 17 \t Batch 160 \t Training Loss: 47.20649931430817\n",
      "Epoch 17 \t Batch 180 \t Training Loss: 47.16090013715956\n",
      "Epoch 17 \t Batch 200 \t Training Loss: 47.07330434799194\n",
      "Epoch 17 \t Batch 220 \t Training Loss: 47.03559771451083\n",
      "Epoch 17 \t Batch 240 \t Training Loss: 47.09922029177348\n",
      "Epoch 17 \t Batch 260 \t Training Loss: 47.22487203157865\n",
      "Epoch 17 \t Batch 280 \t Training Loss: 47.291483879089355\n",
      "Epoch 17 \t Batch 300 \t Training Loss: 47.20505981445312\n",
      "Epoch 17 \t Batch 320 \t Training Loss: 47.20888885259628\n",
      "Epoch 17 \t Batch 340 \t Training Loss: 47.18186846340404\n",
      "Epoch 17 \t Batch 360 \t Training Loss: 47.104819997151694\n",
      "Epoch 17 \t Batch 380 \t Training Loss: 47.06614073703164\n",
      "Epoch 17 \t Batch 400 \t Training Loss: 47.092585706710814\n",
      "Epoch 17 \t Batch 420 \t Training Loss: 47.0526719956171\n",
      "Epoch 17 \t Batch 440 \t Training Loss: 46.97577650763772\n",
      "Epoch 17 \t Batch 460 \t Training Loss: 46.91006026060685\n",
      "Epoch 17 \t Batch 480 \t Training Loss: 46.844484917322795\n",
      "Epoch 17 \t Batch 500 \t Training Loss: 46.78920847320557\n",
      "Epoch 17 \t Batch 520 \t Training Loss: 46.756932207254266\n",
      "Epoch 17 \t Batch 540 \t Training Loss: 46.69820739604808\n",
      "Epoch 17 \t Batch 560 \t Training Loss: 46.72674970626831\n",
      "Epoch 17 \t Batch 580 \t Training Loss: 46.70832079525652\n",
      "Epoch 17 \t Batch 600 \t Training Loss: 46.73179110209147\n",
      "Epoch 17 \t Batch 620 \t Training Loss: 46.729173869471396\n",
      "Epoch 17 \t Batch 640 \t Training Loss: 46.76270050406456\n",
      "Epoch 17 \t Batch 660 \t Training Loss: 46.748048608953304\n",
      "Epoch 17 \t Batch 680 \t Training Loss: 46.75030486162971\n",
      "Epoch 17 \t Batch 700 \t Training Loss: 46.776263204302104\n",
      "Epoch 17 \t Batch 720 \t Training Loss: 46.73988790512085\n",
      "Epoch 17 \t Batch 740 \t Training Loss: 46.744012384156925\n",
      "Epoch 17 \t Batch 760 \t Training Loss: 46.74592288669787\n",
      "Epoch 17 \t Batch 780 \t Training Loss: 46.73599445147392\n",
      "Epoch 17 \t Batch 800 \t Training Loss: 46.71620279788971\n",
      "Epoch 17 \t Batch 820 \t Training Loss: 46.68623968450034\n",
      "Epoch 17 \t Batch 840 \t Training Loss: 46.7196833110991\n",
      "Epoch 17 \t Batch 860 \t Training Loss: 46.71842598582423\n",
      "Epoch 17 \t Batch 880 \t Training Loss: 46.700577419454405\n",
      "Epoch 17 \t Batch 900 \t Training Loss: 46.699113320244685\n",
      "Epoch 17 \t Batch 20 \t Validation Loss: 12.60048246383667\n",
      "Epoch 17 \t Batch 40 \t Validation Loss: 14.481976115703583\n",
      "Epoch 17 \t Batch 60 \t Validation Loss: 14.596797935167949\n",
      "Epoch 17 \t Batch 80 \t Validation Loss: 15.376106745004654\n",
      "Epoch 17 \t Batch 100 \t Validation Loss: 18.074648938179017\n",
      "Epoch 17 \t Batch 120 \t Validation Loss: 20.171039895216623\n",
      "Epoch 17 \t Batch 140 \t Validation Loss: 21.3147732428142\n",
      "Epoch 17 \t Batch 160 \t Validation Loss: 24.025907203555107\n",
      "Epoch 17 \t Batch 180 \t Validation Loss: 28.66349544260237\n",
      "Epoch 17 \t Batch 200 \t Validation Loss: 30.837543818950653\n",
      "Epoch 17 \t Batch 220 \t Validation Loss: 32.83817625696009\n",
      "Epoch 17 \t Batch 240 \t Validation Loss: 33.796685336033505\n",
      "Epoch 17 \t Batch 260 \t Validation Loss: 36.50487851179563\n",
      "Epoch 17 \t Batch 280 \t Validation Loss: 38.009099955218176\n",
      "Epoch 17 \t Batch 300 \t Validation Loss: 39.36417628765106\n",
      "Epoch 17 \t Batch 320 \t Validation Loss: 40.031660141050814\n",
      "Epoch 17 \t Batch 340 \t Validation Loss: 39.993168832274044\n",
      "Epoch 17 \t Batch 360 \t Validation Loss: 39.84791544940737\n",
      "Epoch 17 \t Batch 380 \t Validation Loss: 40.22205008080131\n",
      "Epoch 17 \t Batch 400 \t Validation Loss: 39.80378220677376\n",
      "Epoch 17 \t Batch 420 \t Validation Loss: 39.765019699505395\n",
      "Epoch 17 \t Batch 440 \t Validation Loss: 39.47466674826362\n",
      "Epoch 17 \t Batch 460 \t Validation Loss: 39.77622423690298\n",
      "Epoch 17 \t Batch 480 \t Validation Loss: 40.20910207927227\n",
      "Epoch 17 \t Batch 500 \t Validation Loss: 39.920920128822324\n",
      "Epoch 17 \t Batch 520 \t Validation Loss: 39.818963068265184\n",
      "Epoch 17 \t Batch 540 \t Validation Loss: 39.48722251874429\n",
      "Epoch 17 \t Batch 560 \t Validation Loss: 39.24480189170156\n",
      "Epoch 17 \t Batch 580 \t Validation Loss: 39.0086716314842\n",
      "Epoch 17 \t Batch 600 \t Validation Loss: 39.15624949375788\n",
      "Epoch 17 Training Loss: 46.70617399142899 Validation Loss: 39.71562775621167\n",
      "Epoch 17 completed\n",
      "Epoch 18 \t Batch 20 \t Training Loss: 45.557840538024905\n",
      "Epoch 18 \t Batch 40 \t Training Loss: 46.540651607513425\n",
      "Epoch 18 \t Batch 60 \t Training Loss: 46.751466814676924\n",
      "Epoch 18 \t Batch 80 \t Training Loss: 46.597207355499265\n",
      "Epoch 18 \t Batch 100 \t Training Loss: 46.587746925354004\n",
      "Epoch 18 \t Batch 120 \t Training Loss: 46.672496287028\n",
      "Epoch 18 \t Batch 140 \t Training Loss: 46.576480810982844\n",
      "Epoch 18 \t Batch 160 \t Training Loss: 46.61890096664429\n",
      "Epoch 18 \t Batch 180 \t Training Loss: 46.691310522291396\n",
      "Epoch 18 \t Batch 200 \t Training Loss: 46.59396484375\n",
      "Epoch 18 \t Batch 220 \t Training Loss: 46.61380039561879\n",
      "Epoch 18 \t Batch 240 \t Training Loss: 46.65021341641744\n",
      "Epoch 18 \t Batch 260 \t Training Loss: 46.6310212795551\n",
      "Epoch 18 \t Batch 280 \t Training Loss: 46.63111572265625\n",
      "Epoch 18 \t Batch 300 \t Training Loss: 46.65087445576986\n",
      "Epoch 18 \t Batch 320 \t Training Loss: 46.60154641866684\n",
      "Epoch 18 \t Batch 340 \t Training Loss: 46.658020905887376\n",
      "Epoch 18 \t Batch 360 \t Training Loss: 46.604360008239745\n",
      "Epoch 18 \t Batch 380 \t Training Loss: 46.562297851160956\n",
      "Epoch 18 \t Batch 400 \t Training Loss: 46.521889734268186\n",
      "Epoch 18 \t Batch 420 \t Training Loss: 46.52403687068394\n",
      "Epoch 18 \t Batch 440 \t Training Loss: 46.5079272963784\n",
      "Epoch 18 \t Batch 460 \t Training Loss: 46.529080341173255\n",
      "Epoch 18 \t Batch 480 \t Training Loss: 46.49656114578247\n",
      "Epoch 18 \t Batch 500 \t Training Loss: 46.55798140716553\n",
      "Epoch 18 \t Batch 520 \t Training Loss: 46.54795134617732\n",
      "Epoch 18 \t Batch 540 \t Training Loss: 46.586430641456886\n",
      "Epoch 18 \t Batch 560 \t Training Loss: 46.593251534870696\n",
      "Epoch 18 \t Batch 580 \t Training Loss: 46.57384881644413\n",
      "Epoch 18 \t Batch 600 \t Training Loss: 46.5507528368632\n",
      "Epoch 18 \t Batch 620 \t Training Loss: 46.521904939220796\n",
      "Epoch 18 \t Batch 640 \t Training Loss: 46.53666163086891\n",
      "Epoch 18 \t Batch 660 \t Training Loss: 46.530560025301845\n",
      "Epoch 18 \t Batch 680 \t Training Loss: 46.521345755633185\n",
      "Epoch 18 \t Batch 700 \t Training Loss: 46.57265213557652\n",
      "Epoch 18 \t Batch 720 \t Training Loss: 46.588720083236694\n",
      "Epoch 18 \t Batch 740 \t Training Loss: 46.5788288271105\n",
      "Epoch 18 \t Batch 760 \t Training Loss: 46.54931233054713\n",
      "Epoch 18 \t Batch 780 \t Training Loss: 46.57869137494992\n",
      "Epoch 18 \t Batch 800 \t Training Loss: 46.61282661914825\n",
      "Epoch 18 \t Batch 820 \t Training Loss: 46.62091341716487\n",
      "Epoch 18 \t Batch 840 \t Training Loss: 46.620098618098666\n",
      "Epoch 18 \t Batch 860 \t Training Loss: 46.6192811433659\n",
      "Epoch 18 \t Batch 880 \t Training Loss: 46.606667163155294\n",
      "Epoch 18 \t Batch 900 \t Training Loss: 46.58022812737359\n",
      "Epoch 18 \t Batch 20 \t Validation Loss: 15.210213708877564\n",
      "Epoch 18 \t Batch 40 \t Validation Loss: 16.035163927078248\n",
      "Epoch 18 \t Batch 60 \t Validation Loss: 16.660156838099162\n",
      "Epoch 18 \t Batch 80 \t Validation Loss: 17.30391010046005\n",
      "Epoch 18 \t Batch 100 \t Validation Loss: 19.742641000747682\n",
      "Epoch 18 \t Batch 120 \t Validation Loss: 21.891320252418517\n",
      "Epoch 18 \t Batch 140 \t Validation Loss: 22.8337804044996\n",
      "Epoch 18 \t Batch 160 \t Validation Loss: 24.807634550333024\n",
      "Epoch 18 \t Batch 180 \t Validation Loss: 28.430524640613132\n",
      "Epoch 18 \t Batch 200 \t Validation Loss: 30.089920306205748\n",
      "Epoch 18 \t Batch 220 \t Validation Loss: 31.513473662463102\n",
      "Epoch 18 \t Batch 240 \t Validation Loss: 32.13984087705612\n",
      "Epoch 18 \t Batch 260 \t Validation Loss: 34.4702268417065\n",
      "Epoch 18 \t Batch 280 \t Validation Loss: 35.769354285512655\n",
      "Epoch 18 \t Batch 300 \t Validation Loss: 36.72836422920227\n",
      "Epoch 18 \t Batch 320 \t Validation Loss: 37.21015250682831\n",
      "Epoch 18 \t Batch 340 \t Validation Loss: 37.18973435233621\n",
      "Epoch 18 \t Batch 360 \t Validation Loss: 36.95653467973073\n",
      "Epoch 18 \t Batch 380 \t Validation Loss: 37.30910611152649\n",
      "Epoch 18 \t Batch 400 \t Validation Loss: 36.98979686975479\n",
      "Epoch 18 \t Batch 420 \t Validation Loss: 37.059369625364035\n",
      "Epoch 18 \t Batch 440 \t Validation Loss: 36.84201914397153\n",
      "Epoch 18 \t Batch 460 \t Validation Loss: 37.198429308766904\n",
      "Epoch 18 \t Batch 480 \t Validation Loss: 37.696798370281854\n",
      "Epoch 18 \t Batch 500 \t Validation Loss: 37.42331500816345\n",
      "Epoch 18 \t Batch 520 \t Validation Loss: 37.29263371687669\n",
      "Epoch 18 \t Batch 540 \t Validation Loss: 37.08332368709423\n",
      "Epoch 18 \t Batch 560 \t Validation Loss: 36.92070006643023\n",
      "Epoch 18 \t Batch 580 \t Validation Loss: 36.689007170446985\n",
      "Epoch 18 \t Batch 600 \t Validation Loss: 36.971608521143594\n",
      "Epoch 18 Training Loss: 46.58192656422389 Validation Loss: 37.60762859319712\n",
      "Epoch 18 completed\n",
      "Epoch 19 \t Batch 20 \t Training Loss: 46.19078254699707\n",
      "Epoch 19 \t Batch 40 \t Training Loss: 46.60151853561401\n",
      "Epoch 19 \t Batch 60 \t Training Loss: 46.99073003133138\n",
      "Epoch 19 \t Batch 80 \t Training Loss: 46.87620625495911\n",
      "Epoch 19 \t Batch 100 \t Training Loss: 46.69394664764404\n",
      "Epoch 19 \t Batch 120 \t Training Loss: 46.57261527379354\n",
      "Epoch 19 \t Batch 140 \t Training Loss: 46.508653831481936\n",
      "Epoch 19 \t Batch 160 \t Training Loss: 46.57134828567505\n",
      "Epoch 19 \t Batch 180 \t Training Loss: 46.60291127098931\n",
      "Epoch 19 \t Batch 200 \t Training Loss: 46.57874576568604\n",
      "Epoch 19 \t Batch 220 \t Training Loss: 46.496560807661574\n",
      "Epoch 19 \t Batch 240 \t Training Loss: 46.59096663792928\n",
      "Epoch 19 \t Batch 260 \t Training Loss: 46.63757271399865\n",
      "Epoch 19 \t Batch 280 \t Training Loss: 46.52054155894688\n",
      "Epoch 19 \t Batch 300 \t Training Loss: 46.48233014424642\n",
      "Epoch 19 \t Batch 320 \t Training Loss: 46.42496664524079\n",
      "Epoch 19 \t Batch 340 \t Training Loss: 46.51964683532715\n",
      "Epoch 19 \t Batch 360 \t Training Loss: 46.47294249004788\n",
      "Epoch 19 \t Batch 380 \t Training Loss: 46.48961739791067\n",
      "Epoch 19 \t Batch 400 \t Training Loss: 46.52264529228211\n",
      "Epoch 19 \t Batch 420 \t Training Loss: 46.56175429026286\n",
      "Epoch 19 \t Batch 440 \t Training Loss: 46.59055854623968\n",
      "Epoch 19 \t Batch 460 \t Training Loss: 46.54963589543882\n",
      "Epoch 19 \t Batch 480 \t Training Loss: 46.522526295979816\n",
      "Epoch 19 \t Batch 500 \t Training Loss: 46.57681536865234\n",
      "Epoch 19 \t Batch 520 \t Training Loss: 46.48124781388503\n",
      "Epoch 19 \t Batch 540 \t Training Loss: 46.48473812385841\n",
      "Epoch 19 \t Batch 560 \t Training Loss: 46.4703533581325\n",
      "Epoch 19 \t Batch 580 \t Training Loss: 46.45815915732548\n",
      "Epoch 19 \t Batch 600 \t Training Loss: 46.4416841952006\n",
      "Epoch 19 \t Batch 620 \t Training Loss: 46.42276496887207\n",
      "Epoch 19 \t Batch 640 \t Training Loss: 46.44281182289124\n",
      "Epoch 19 \t Batch 660 \t Training Loss: 46.428252665201825\n",
      "Epoch 19 \t Batch 680 \t Training Loss: 46.421478019041174\n",
      "Epoch 19 \t Batch 700 \t Training Loss: 46.45047806876046\n",
      "Epoch 19 \t Batch 720 \t Training Loss: 46.3965734746721\n",
      "Epoch 19 \t Batch 740 \t Training Loss: 46.345197522962415\n",
      "Epoch 19 \t Batch 760 \t Training Loss: 46.360109424591066\n",
      "Epoch 19 \t Batch 780 \t Training Loss: 46.38095265902006\n",
      "Epoch 19 \t Batch 800 \t Training Loss: 46.38444951057434\n",
      "Epoch 19 \t Batch 820 \t Training Loss: 46.4171759023899\n",
      "Epoch 19 \t Batch 840 \t Training Loss: 46.43198935190836\n",
      "Epoch 19 \t Batch 860 \t Training Loss: 46.43877731589384\n",
      "Epoch 19 \t Batch 880 \t Training Loss: 46.49504519375888\n",
      "Epoch 19 \t Batch 900 \t Training Loss: 46.51704241858588\n",
      "Epoch 19 \t Batch 20 \t Validation Loss: 11.280342984199525\n",
      "Epoch 19 \t Batch 40 \t Validation Loss: 13.044713878631592\n",
      "Epoch 19 \t Batch 60 \t Validation Loss: 13.22143481572469\n",
      "Epoch 19 \t Batch 80 \t Validation Loss: 14.539341217279434\n",
      "Epoch 19 \t Batch 100 \t Validation Loss: 17.1728643655777\n",
      "Epoch 19 \t Batch 120 \t Validation Loss: 19.186704019705456\n",
      "Epoch 19 \t Batch 140 \t Validation Loss: 20.350819087028505\n",
      "Epoch 19 \t Batch 160 \t Validation Loss: 22.9948681384325\n",
      "Epoch 19 \t Batch 180 \t Validation Loss: 27.244410591655306\n",
      "Epoch 19 \t Batch 200 \t Validation Loss: 29.417943460941316\n",
      "Epoch 19 \t Batch 220 \t Validation Loss: 31.227719205076045\n",
      "Epoch 19 \t Batch 240 \t Validation Loss: 32.11293555299441\n",
      "Epoch 19 \t Batch 260 \t Validation Loss: 34.65872521217053\n",
      "Epoch 19 \t Batch 280 \t Validation Loss: 36.120144120284486\n",
      "Epoch 19 \t Batch 300 \t Validation Loss: 37.34079793135325\n",
      "Epoch 19 \t Batch 320 \t Validation Loss: 37.95994952172041\n",
      "Epoch 19 \t Batch 340 \t Validation Loss: 38.020011541422676\n",
      "Epoch 19 \t Batch 360 \t Validation Loss: 37.89628296295802\n",
      "Epoch 19 \t Batch 380 \t Validation Loss: 38.23597314608725\n",
      "Epoch 19 \t Batch 400 \t Validation Loss: 37.8978372991085\n",
      "Epoch 19 \t Batch 420 \t Validation Loss: 38.01156553881509\n",
      "Epoch 19 \t Batch 440 \t Validation Loss: 37.756318780508906\n",
      "Epoch 19 \t Batch 460 \t Validation Loss: 38.03479068486587\n",
      "Epoch 19 \t Batch 480 \t Validation Loss: 38.55137125949065\n",
      "Epoch 19 \t Batch 500 \t Validation Loss: 38.331084322929385\n",
      "Epoch 19 \t Batch 520 \t Validation Loss: 38.137593792952025\n",
      "Epoch 19 \t Batch 540 \t Validation Loss: 37.82473631170061\n",
      "Epoch 19 \t Batch 560 \t Validation Loss: 37.621946913003924\n",
      "Epoch 19 \t Batch 580 \t Validation Loss: 37.34907497455334\n",
      "Epoch 19 \t Batch 600 \t Validation Loss: 37.53002625068029\n",
      "Epoch 19 Training Loss: 46.4901932827649 Validation Loss: 38.12460023009932\n",
      "Epoch 19 completed\n",
      "Epoch 20 \t Batch 20 \t Training Loss: 45.11801548004151\n",
      "Epoch 20 \t Batch 40 \t Training Loss: 45.676652908325195\n",
      "Epoch 20 \t Batch 60 \t Training Loss: 45.609696515401204\n",
      "Epoch 20 \t Batch 80 \t Training Loss: 45.80045175552368\n",
      "Epoch 20 \t Batch 100 \t Training Loss: 45.98208549499512\n",
      "Epoch 20 \t Batch 120 \t Training Loss: 46.16971441904704\n",
      "Epoch 20 \t Batch 140 \t Training Loss: 46.31212605067662\n",
      "Epoch 20 \t Batch 160 \t Training Loss: 46.424910926818846\n",
      "Epoch 20 \t Batch 180 \t Training Loss: 46.24282824198405\n",
      "Epoch 20 \t Batch 200 \t Training Loss: 46.30822406768799\n",
      "Epoch 20 \t Batch 220 \t Training Loss: 46.42409775473855\n",
      "Epoch 20 \t Batch 240 \t Training Loss: 46.36835349400838\n",
      "Epoch 20 \t Batch 260 \t Training Loss: 46.36348457336426\n",
      "Epoch 20 \t Batch 280 \t Training Loss: 46.38354813711984\n",
      "Epoch 20 \t Batch 300 \t Training Loss: 46.375942408243816\n",
      "Epoch 20 \t Batch 320 \t Training Loss: 46.538416135311124\n",
      "Epoch 20 \t Batch 340 \t Training Loss: 46.478901088939\n",
      "Epoch 20 \t Batch 360 \t Training Loss: 46.34101268980238\n",
      "Epoch 20 \t Batch 380 \t Training Loss: 46.358282751786085\n",
      "Epoch 20 \t Batch 400 \t Training Loss: 46.36138418197632\n",
      "Epoch 20 \t Batch 420 \t Training Loss: 46.33520917438325\n",
      "Epoch 20 \t Batch 440 \t Training Loss: 46.40604869669134\n",
      "Epoch 20 \t Batch 460 \t Training Loss: 46.389339355800466\n",
      "Epoch 20 \t Batch 480 \t Training Loss: 46.42421499888102\n",
      "Epoch 20 \t Batch 500 \t Training Loss: 46.40480578613281\n",
      "Epoch 20 \t Batch 520 \t Training Loss: 46.40635874087994\n",
      "Epoch 20 \t Batch 540 \t Training Loss: 46.37914984667743\n",
      "Epoch 20 \t Batch 560 \t Training Loss: 46.40140508924212\n",
      "Epoch 20 \t Batch 580 \t Training Loss: 46.39718478630329\n",
      "Epoch 20 \t Batch 600 \t Training Loss: 46.424752159118654\n",
      "Epoch 20 \t Batch 620 \t Training Loss: 46.47458242600964\n",
      "Epoch 20 \t Batch 640 \t Training Loss: 46.43985247015953\n",
      "Epoch 20 \t Batch 660 \t Training Loss: 46.42831542275169\n",
      "Epoch 20 \t Batch 680 \t Training Loss: 46.42012451396269\n",
      "Epoch 20 \t Batch 700 \t Training Loss: 46.38756106240409\n",
      "Epoch 20 \t Batch 720 \t Training Loss: 46.37910479969449\n",
      "Epoch 20 \t Batch 740 \t Training Loss: 46.36989239357613\n",
      "Epoch 20 \t Batch 760 \t Training Loss: 46.40568093249672\n",
      "Epoch 20 \t Batch 780 \t Training Loss: 46.3701780465933\n",
      "Epoch 20 \t Batch 800 \t Training Loss: 46.374562020301816\n",
      "Epoch 20 \t Batch 820 \t Training Loss: 46.35341560084645\n",
      "Epoch 20 \t Batch 840 \t Training Loss: 46.337692174457366\n",
      "Epoch 20 \t Batch 860 \t Training Loss: 46.33965199936268\n",
      "Epoch 20 \t Batch 880 \t Training Loss: 46.363947972384366\n",
      "Epoch 20 \t Batch 900 \t Training Loss: 46.411619673834906\n",
      "Epoch 20 \t Batch 20 \t Validation Loss: 8.95707700252533\n",
      "Epoch 20 \t Batch 40 \t Validation Loss: 11.53014178276062\n",
      "Epoch 20 \t Batch 60 \t Validation Loss: 11.62181885242462\n",
      "Epoch 20 \t Batch 80 \t Validation Loss: 12.891241347789764\n",
      "Epoch 20 \t Batch 100 \t Validation Loss: 15.644203424453735\n",
      "Epoch 20 \t Batch 120 \t Validation Loss: 17.732098007202147\n",
      "Epoch 20 \t Batch 140 \t Validation Loss: 19.073261029379708\n",
      "Epoch 20 \t Batch 160 \t Validation Loss: 21.61381824016571\n",
      "Epoch 20 \t Batch 180 \t Validation Loss: 24.794318935606213\n",
      "Epoch 20 \t Batch 200 \t Validation Loss: 26.49371744632721\n",
      "Epoch 20 \t Batch 220 \t Validation Loss: 27.822709668766368\n",
      "Epoch 20 \t Batch 240 \t Validation Loss: 28.469578103224435\n",
      "Epoch 20 \t Batch 260 \t Validation Loss: 30.74035047384409\n",
      "Epoch 20 \t Batch 280 \t Validation Loss: 32.04241713796343\n",
      "Epoch 20 \t Batch 300 \t Validation Loss: 32.893673470815024\n",
      "Epoch 20 \t Batch 320 \t Validation Loss: 33.44713746905327\n",
      "Epoch 20 \t Batch 340 \t Validation Loss: 33.63209957235\n",
      "Epoch 20 \t Batch 360 \t Validation Loss: 33.582764593760174\n",
      "Epoch 20 \t Batch 380 \t Validation Loss: 34.13235748692563\n",
      "Epoch 20 \t Batch 400 \t Validation Loss: 34.17325579643249\n",
      "Epoch 20 \t Batch 420 \t Validation Loss: 34.432310065769016\n",
      "Epoch 20 \t Batch 440 \t Validation Loss: 34.60417738611048\n",
      "Epoch 20 \t Batch 460 \t Validation Loss: 35.1920563303906\n",
      "Epoch 20 \t Batch 480 \t Validation Loss: 35.799387194712956\n",
      "Epoch 20 \t Batch 500 \t Validation Loss: 35.66396413993836\n",
      "Epoch 20 \t Batch 520 \t Validation Loss: 35.8857390495447\n",
      "Epoch 20 \t Batch 540 \t Validation Loss: 35.71314199059098\n",
      "Epoch 20 \t Batch 560 \t Validation Loss: 35.62611236061369\n",
      "Epoch 20 \t Batch 580 \t Validation Loss: 35.52744938258467\n",
      "Epoch 20 \t Batch 600 \t Validation Loss: 35.79308829466502\n",
      "Epoch 20 Training Loss: 46.384432569064906 Validation Loss: 36.48282049383436\n",
      "Epoch 20 completed\n",
      "Epoch 21 \t Batch 20 \t Training Loss: 45.7396842956543\n",
      "Epoch 21 \t Batch 40 \t Training Loss: 46.67625532150269\n",
      "Epoch 21 \t Batch 60 \t Training Loss: 46.95173276265462\n",
      "Epoch 21 \t Batch 80 \t Training Loss: 46.727354001998904\n",
      "Epoch 21 \t Batch 100 \t Training Loss: 46.66815822601318\n",
      "Epoch 21 \t Batch 120 \t Training Loss: 46.493897438049316\n",
      "Epoch 21 \t Batch 140 \t Training Loss: 46.38075294494629\n",
      "Epoch 21 \t Batch 160 \t Training Loss: 46.287281608581544\n",
      "Epoch 21 \t Batch 180 \t Training Loss: 46.0288492626614\n",
      "Epoch 21 \t Batch 200 \t Training Loss: 46.10953535079956\n",
      "Epoch 21 \t Batch 220 \t Training Loss: 46.26894071752375\n",
      "Epoch 21 \t Batch 240 \t Training Loss: 46.29823300043742\n",
      "Epoch 21 \t Batch 260 \t Training Loss: 46.39833994645339\n",
      "Epoch 21 \t Batch 280 \t Training Loss: 46.29300395420619\n",
      "Epoch 21 \t Batch 300 \t Training Loss: 46.25465821584066\n",
      "Epoch 21 \t Batch 320 \t Training Loss: 46.24140553474426\n",
      "Epoch 21 \t Batch 340 \t Training Loss: 46.26858579972211\n",
      "Epoch 21 \t Batch 360 \t Training Loss: 46.28505224651761\n",
      "Epoch 21 \t Batch 380 \t Training Loss: 46.25850523898476\n",
      "Epoch 21 \t Batch 400 \t Training Loss: 46.316136569976806\n",
      "Epoch 21 \t Batch 420 \t Training Loss: 46.30066517421177\n",
      "Epoch 21 \t Batch 440 \t Training Loss: 46.30432870171287\n",
      "Epoch 21 \t Batch 460 \t Training Loss: 46.34919016464897\n",
      "Epoch 21 \t Batch 480 \t Training Loss: 46.38457119464874\n",
      "Epoch 21 \t Batch 500 \t Training Loss: 46.401193740844725\n",
      "Epoch 21 \t Batch 520 \t Training Loss: 46.396040909106915\n",
      "Epoch 21 \t Batch 540 \t Training Loss: 46.34926810794406\n",
      "Epoch 21 \t Batch 560 \t Training Loss: 46.3378920350756\n",
      "Epoch 21 \t Batch 580 \t Training Loss: 46.28431745726487\n",
      "Epoch 21 \t Batch 600 \t Training Loss: 46.29541652679443\n",
      "Epoch 21 \t Batch 620 \t Training Loss: 46.31650749944871\n",
      "Epoch 21 \t Batch 640 \t Training Loss: 46.304426395893096\n",
      "Epoch 21 \t Batch 660 \t Training Loss: 46.26432806650798\n",
      "Epoch 21 \t Batch 680 \t Training Loss: 46.2793207953958\n",
      "Epoch 21 \t Batch 700 \t Training Loss: 46.25289754050119\n",
      "Epoch 21 \t Batch 720 \t Training Loss: 46.24013038741218\n",
      "Epoch 21 \t Batch 740 \t Training Loss: 46.25259438592035\n",
      "Epoch 21 \t Batch 760 \t Training Loss: 46.21065770199424\n",
      "Epoch 21 \t Batch 780 \t Training Loss: 46.22094643421662\n",
      "Epoch 21 \t Batch 800 \t Training Loss: 46.20662979602814\n",
      "Epoch 21 \t Batch 820 \t Training Loss: 46.23003579116449\n",
      "Epoch 21 \t Batch 840 \t Training Loss: 46.24585321971348\n",
      "Epoch 21 \t Batch 860 \t Training Loss: 46.268063177064406\n",
      "Epoch 21 \t Batch 880 \t Training Loss: 46.28381405743686\n",
      "Epoch 21 \t Batch 900 \t Training Loss: 46.29464241451687\n",
      "Epoch 21 \t Batch 20 \t Validation Loss: 13.938181495666504\n",
      "Epoch 21 \t Batch 40 \t Validation Loss: 15.373282194137573\n",
      "Epoch 21 \t Batch 60 \t Validation Loss: 15.678298060099284\n",
      "Epoch 21 \t Batch 80 \t Validation Loss: 16.495676147937775\n",
      "Epoch 21 \t Batch 100 \t Validation Loss: 18.814217576980592\n",
      "Epoch 21 \t Batch 120 \t Validation Loss: 20.682750884691874\n",
      "Epoch 21 \t Batch 140 \t Validation Loss: 21.662289476394655\n",
      "Epoch 21 \t Batch 160 \t Validation Loss: 24.14225689768791\n",
      "Epoch 21 \t Batch 180 \t Validation Loss: 28.298392862743803\n",
      "Epoch 21 \t Batch 200 \t Validation Loss: 30.367495474815367\n",
      "Epoch 21 \t Batch 220 \t Validation Loss: 32.12880353494124\n",
      "Epoch 21 \t Batch 240 \t Validation Loss: 32.94769543011983\n",
      "Epoch 21 \t Batch 260 \t Validation Loss: 35.537319462115946\n",
      "Epoch 21 \t Batch 280 \t Validation Loss: 36.94951931067875\n",
      "Epoch 21 \t Batch 300 \t Validation Loss: 38.138181301752724\n",
      "Epoch 21 \t Batch 320 \t Validation Loss: 38.77033915221691\n",
      "Epoch 21 \t Batch 340 \t Validation Loss: 38.767210682700664\n",
      "Epoch 21 \t Batch 360 \t Validation Loss: 38.56883748372396\n",
      "Epoch 21 \t Batch 380 \t Validation Loss: 38.929757946415954\n",
      "Epoch 21 \t Batch 400 \t Validation Loss: 38.53266139030457\n",
      "Epoch 21 \t Batch 420 \t Validation Loss: 38.57910705293928\n",
      "Epoch 21 \t Batch 440 \t Validation Loss: 38.295689435438675\n",
      "Epoch 21 \t Batch 460 \t Validation Loss: 38.664138740041984\n",
      "Epoch 21 \t Batch 480 \t Validation Loss: 39.148065853118894\n",
      "Epoch 21 \t Batch 500 \t Validation Loss: 38.90934340286255\n",
      "Epoch 21 \t Batch 520 \t Validation Loss: 38.80394188440763\n",
      "Epoch 21 \t Batch 540 \t Validation Loss: 38.459862309915046\n",
      "Epoch 21 \t Batch 560 \t Validation Loss: 38.18433894259589\n",
      "Epoch 21 \t Batch 580 \t Validation Loss: 37.845416603417235\n",
      "Epoch 21 \t Batch 600 \t Validation Loss: 38.02358450730642\n",
      "Epoch 21 Training Loss: 46.29598110868731 Validation Loss: 38.62159795420511\n",
      "Epoch 21 completed\n",
      "Epoch 22 \t Batch 20 \t Training Loss: 46.27627391815186\n",
      "Epoch 22 \t Batch 40 \t Training Loss: 45.431701278686525\n",
      "Epoch 22 \t Batch 60 \t Training Loss: 45.62398287455241\n",
      "Epoch 22 \t Batch 80 \t Training Loss: 45.68307619094848\n",
      "Epoch 22 \t Batch 100 \t Training Loss: 45.70000011444092\n",
      "Epoch 22 \t Batch 120 \t Training Loss: 45.989049371083574\n",
      "Epoch 22 \t Batch 140 \t Training Loss: 45.94820954459054\n",
      "Epoch 22 \t Batch 160 \t Training Loss: 45.827954459190366\n",
      "Epoch 22 \t Batch 180 \t Training Loss: 45.85008559756809\n",
      "Epoch 22 \t Batch 200 \t Training Loss: 45.85847013473511\n",
      "Epoch 22 \t Batch 220 \t Training Loss: 45.80773429870605\n",
      "Epoch 22 \t Batch 240 \t Training Loss: 45.75680700937907\n",
      "Epoch 22 \t Batch 260 \t Training Loss: 45.67962096287654\n",
      "Epoch 22 \t Batch 280 \t Training Loss: 45.68099196297782\n",
      "Epoch 22 \t Batch 300 \t Training Loss: 45.67915040334066\n",
      "Epoch 22 \t Batch 320 \t Training Loss: 45.663620030879976\n",
      "Epoch 22 \t Batch 340 \t Training Loss: 45.70615774042466\n",
      "Epoch 22 \t Batch 360 \t Training Loss: 45.75345679389106\n",
      "Epoch 22 \t Batch 380 \t Training Loss: 45.749165635359915\n",
      "Epoch 22 \t Batch 400 \t Training Loss: 45.82678037643433\n",
      "Epoch 22 \t Batch 420 \t Training Loss: 45.929074814206075\n",
      "Epoch 22 \t Batch 440 \t Training Loss: 46.03119627345692\n",
      "Epoch 22 \t Batch 460 \t Training Loss: 46.05738875347635\n",
      "Epoch 22 \t Batch 480 \t Training Loss: 46.08298831780751\n",
      "Epoch 22 \t Batch 500 \t Training Loss: 46.049197883605956\n",
      "Epoch 22 \t Batch 520 \t Training Loss: 46.042562360029955\n",
      "Epoch 22 \t Batch 540 \t Training Loss: 46.08380842562075\n",
      "Epoch 22 \t Batch 560 \t Training Loss: 46.10070682253156\n",
      "Epoch 22 \t Batch 580 \t Training Loss: 46.08171991808661\n",
      "Epoch 22 \t Batch 600 \t Training Loss: 46.0704203859965\n",
      "Epoch 22 \t Batch 620 \t Training Loss: 46.08552505124\n",
      "Epoch 22 \t Batch 640 \t Training Loss: 46.10325684547424\n",
      "Epoch 22 \t Batch 660 \t Training Loss: 46.106053444833464\n",
      "Epoch 22 \t Batch 680 \t Training Loss: 46.13395251666798\n",
      "Epoch 22 \t Batch 700 \t Training Loss: 46.17755141122001\n",
      "Epoch 22 \t Batch 720 \t Training Loss: 46.1493199772305\n",
      "Epoch 22 \t Batch 740 \t Training Loss: 46.164745949410104\n",
      "Epoch 22 \t Batch 760 \t Training Loss: 46.19561969355533\n",
      "Epoch 22 \t Batch 780 \t Training Loss: 46.17231603280092\n",
      "Epoch 22 \t Batch 800 \t Training Loss: 46.169441089630126\n",
      "Epoch 22 \t Batch 820 \t Training Loss: 46.18940748819491\n",
      "Epoch 22 \t Batch 840 \t Training Loss: 46.18681861786615\n",
      "Epoch 22 \t Batch 860 \t Training Loss: 46.15186311145162\n",
      "Epoch 22 \t Batch 880 \t Training Loss: 46.19568363536488\n",
      "Epoch 22 \t Batch 900 \t Training Loss: 46.19307059817844\n",
      "Epoch 22 \t Batch 20 \t Validation Loss: 12.044588589668274\n",
      "Epoch 22 \t Batch 40 \t Validation Loss: 14.24204649925232\n",
      "Epoch 22 \t Batch 60 \t Validation Loss: 13.87773863474528\n",
      "Epoch 22 \t Batch 80 \t Validation Loss: 14.630500680208206\n",
      "Epoch 22 \t Batch 100 \t Validation Loss: 17.238768334388734\n",
      "Epoch 22 \t Batch 120 \t Validation Loss: 19.170290784041086\n",
      "Epoch 22 \t Batch 140 \t Validation Loss: 20.3964346579143\n",
      "Epoch 22 \t Batch 160 \t Validation Loss: 23.301772555708887\n",
      "Epoch 22 \t Batch 180 \t Validation Loss: 27.735532376501297\n",
      "Epoch 22 \t Batch 200 \t Validation Loss: 29.964399116039274\n",
      "Epoch 22 \t Batch 220 \t Validation Loss: 31.87857496738434\n",
      "Epoch 22 \t Batch 240 \t Validation Loss: 32.78945081432661\n",
      "Epoch 22 \t Batch 260 \t Validation Loss: 35.44183108073015\n",
      "Epoch 22 \t Batch 280 \t Validation Loss: 36.94085751090731\n",
      "Epoch 22 \t Batch 300 \t Validation Loss: 38.24056677341461\n",
      "Epoch 22 \t Batch 320 \t Validation Loss: 38.95309682935476\n",
      "Epoch 22 \t Batch 340 \t Validation Loss: 39.01770187686471\n",
      "Epoch 22 \t Batch 360 \t Validation Loss: 38.94312726524141\n",
      "Epoch 22 \t Batch 380 \t Validation Loss: 39.39024411628121\n",
      "Epoch 22 \t Batch 400 \t Validation Loss: 39.08225268244743\n",
      "Epoch 22 \t Batch 420 \t Validation Loss: 39.16563068912143\n",
      "Epoch 22 \t Batch 440 \t Validation Loss: 38.9792498274283\n",
      "Epoch 22 \t Batch 460 \t Validation Loss: 39.3514627446299\n",
      "Epoch 22 \t Batch 480 \t Validation Loss: 39.83942469656468\n",
      "Epoch 22 \t Batch 500 \t Validation Loss: 39.63395798397064\n",
      "Epoch 22 \t Batch 520 \t Validation Loss: 39.62383117217284\n",
      "Epoch 22 \t Batch 540 \t Validation Loss: 39.234941093126935\n",
      "Epoch 22 \t Batch 560 \t Validation Loss: 38.970773953199384\n",
      "Epoch 22 \t Batch 580 \t Validation Loss: 38.6919180417883\n",
      "Epoch 22 \t Batch 600 \t Validation Loss: 38.81572025696437\n",
      "Epoch 22 Training Loss: 46.19175804246343 Validation Loss: 39.42546102985159\n",
      "Epoch 22 completed\n",
      "Epoch 23 \t Batch 20 \t Training Loss: 45.45328330993652\n",
      "Epoch 23 \t Batch 40 \t Training Loss: 45.27888736724854\n",
      "Epoch 23 \t Batch 60 \t Training Loss: 45.63523089090983\n",
      "Epoch 23 \t Batch 80 \t Training Loss: 45.79944534301758\n",
      "Epoch 23 \t Batch 100 \t Training Loss: 45.87881252288818\n",
      "Epoch 23 \t Batch 120 \t Training Loss: 46.14182319641113\n",
      "Epoch 23 \t Batch 140 \t Training Loss: 46.03847201211112\n",
      "Epoch 23 \t Batch 160 \t Training Loss: 46.07353813648224\n",
      "Epoch 23 \t Batch 180 \t Training Loss: 46.01469006008572\n",
      "Epoch 23 \t Batch 200 \t Training Loss: 46.13620971679688\n",
      "Epoch 23 \t Batch 220 \t Training Loss: 46.20347395810214\n",
      "Epoch 23 \t Batch 240 \t Training Loss: 46.11609463691711\n",
      "Epoch 23 \t Batch 260 \t Training Loss: 46.07541892711933\n",
      "Epoch 23 \t Batch 280 \t Training Loss: 45.96850626809256\n",
      "Epoch 23 \t Batch 300 \t Training Loss: 45.97665479024251\n",
      "Epoch 23 \t Batch 320 \t Training Loss: 45.901017463207246\n",
      "Epoch 23 \t Batch 340 \t Training Loss: 45.93859806060791\n",
      "Epoch 23 \t Batch 360 \t Training Loss: 45.93706748750475\n",
      "Epoch 23 \t Batch 380 \t Training Loss: 45.96315871790836\n",
      "Epoch 23 \t Batch 400 \t Training Loss: 45.95246195793152\n",
      "Epoch 23 \t Batch 420 \t Training Loss: 45.8758334114438\n",
      "Epoch 23 \t Batch 440 \t Training Loss: 45.86960658160123\n",
      "Epoch 23 \t Batch 460 \t Training Loss: 45.938383036074434\n",
      "Epoch 23 \t Batch 480 \t Training Loss: 45.926948253313704\n",
      "Epoch 23 \t Batch 500 \t Training Loss: 45.95570285797119\n",
      "Epoch 23 \t Batch 520 \t Training Loss: 46.01433013035701\n",
      "Epoch 23 \t Batch 540 \t Training Loss: 46.04579909289325\n",
      "Epoch 23 \t Batch 560 \t Training Loss: 46.11704887662615\n",
      "Epoch 23 \t Batch 580 \t Training Loss: 46.10906383909028\n",
      "Epoch 23 \t Batch 600 \t Training Loss: 46.073820457458496\n",
      "Epoch 23 \t Batch 620 \t Training Loss: 46.069293237501576\n",
      "Epoch 23 \t Batch 640 \t Training Loss: 46.05927101969719\n",
      "Epoch 23 \t Batch 660 \t Training Loss: 46.062070887016525\n",
      "Epoch 23 \t Batch 680 \t Training Loss: 46.022029618655935\n",
      "Epoch 23 \t Batch 700 \t Training Loss: 46.05850679670061\n",
      "Epoch 23 \t Batch 720 \t Training Loss: 46.03299201859368\n",
      "Epoch 23 \t Batch 740 \t Training Loss: 46.04115259840682\n",
      "Epoch 23 \t Batch 760 \t Training Loss: 46.02633962129292\n",
      "Epoch 23 \t Batch 780 \t Training Loss: 46.04890097104586\n",
      "Epoch 23 \t Batch 800 \t Training Loss: 46.0435217666626\n",
      "Epoch 23 \t Batch 820 \t Training Loss: 46.038845276251074\n",
      "Epoch 23 \t Batch 840 \t Training Loss: 46.0370543162028\n",
      "Epoch 23 \t Batch 860 \t Training Loss: 46.08135977900305\n",
      "Epoch 23 \t Batch 880 \t Training Loss: 46.07436285018921\n",
      "Epoch 23 \t Batch 900 \t Training Loss: 46.07308343675402\n",
      "Epoch 23 \t Batch 20 \t Validation Loss: 15.062212276458741\n",
      "Epoch 23 \t Batch 40 \t Validation Loss: 17.868003678321838\n",
      "Epoch 23 \t Batch 60 \t Validation Loss: 17.56980725924174\n",
      "Epoch 23 \t Batch 80 \t Validation Loss: 18.256342458724976\n",
      "Epoch 23 \t Batch 100 \t Validation Loss: 20.656454334259035\n",
      "Epoch 23 \t Batch 120 \t Validation Loss: 22.613660430908205\n",
      "Epoch 23 \t Batch 140 \t Validation Loss: 23.528221947806223\n",
      "Epoch 23 \t Batch 160 \t Validation Loss: 25.455347144603728\n",
      "Epoch 23 \t Batch 180 \t Validation Loss: 28.752695449193318\n",
      "Epoch 23 \t Batch 200 \t Validation Loss: 30.44804859638214\n",
      "Epoch 23 \t Batch 220 \t Validation Loss: 31.714164686203002\n",
      "Epoch 23 \t Batch 240 \t Validation Loss: 32.21553854147593\n",
      "Epoch 23 \t Batch 260 \t Validation Loss: 34.50941695800194\n",
      "Epoch 23 \t Batch 280 \t Validation Loss: 35.76091694150652\n",
      "Epoch 23 \t Batch 300 \t Validation Loss: 36.568799114227296\n",
      "Epoch 23 \t Batch 320 \t Validation Loss: 36.983460107445715\n",
      "Epoch 23 \t Batch 340 \t Validation Loss: 37.012463286343745\n",
      "Epoch 23 \t Batch 360 \t Validation Loss: 36.77412233617571\n",
      "Epoch 23 \t Batch 380 \t Validation Loss: 37.081992658815885\n",
      "Epoch 23 \t Batch 400 \t Validation Loss: 36.82837910413742\n",
      "Epoch 23 \t Batch 420 \t Validation Loss: 36.948962556748164\n",
      "Epoch 23 \t Batch 440 \t Validation Loss: 36.77130162065679\n",
      "Epoch 23 \t Batch 460 \t Validation Loss: 37.12736238396686\n",
      "Epoch 23 \t Batch 480 \t Validation Loss: 37.626093077659604\n",
      "Epoch 23 \t Batch 500 \t Validation Loss: 37.415423160552976\n",
      "Epoch 23 \t Batch 520 \t Validation Loss: 37.312036855404195\n",
      "Epoch 23 \t Batch 540 \t Validation Loss: 37.05774051878188\n",
      "Epoch 23 \t Batch 560 \t Validation Loss: 36.8954133272171\n",
      "Epoch 23 \t Batch 580 \t Validation Loss: 36.73351099080053\n",
      "Epoch 23 \t Batch 600 \t Validation Loss: 36.96760241508484\n",
      "Epoch 23 Training Loss: 46.08048648480631 Validation Loss: 37.72380946518539\n",
      "Epoch 23 completed\n",
      "Epoch 24 \t Batch 20 \t Training Loss: 46.44069404602051\n",
      "Epoch 24 \t Batch 40 \t Training Loss: 46.402531433105466\n",
      "Epoch 24 \t Batch 60 \t Training Loss: 46.32384916941325\n",
      "Epoch 24 \t Batch 80 \t Training Loss: 46.67042908668518\n",
      "Epoch 24 \t Batch 100 \t Training Loss: 46.51261611938477\n",
      "Epoch 24 \t Batch 120 \t Training Loss: 46.29661429723104\n",
      "Epoch 24 \t Batch 140 \t Training Loss: 46.03428282056536\n",
      "Epoch 24 \t Batch 160 \t Training Loss: 46.005877780914304\n",
      "Epoch 24 \t Batch 180 \t Training Loss: 45.96627538469102\n",
      "Epoch 24 \t Batch 200 \t Training Loss: 46.04693002700806\n",
      "Epoch 24 \t Batch 220 \t Training Loss: 46.072246915643866\n",
      "Epoch 24 \t Batch 240 \t Training Loss: 45.99367771148682\n",
      "Epoch 24 \t Batch 260 \t Training Loss: 45.90496711730957\n",
      "Epoch 24 \t Batch 280 \t Training Loss: 45.904368087223595\n",
      "Epoch 24 \t Batch 300 \t Training Loss: 45.90411954243978\n",
      "Epoch 24 \t Batch 320 \t Training Loss: 45.87804127931595\n",
      "Epoch 24 \t Batch 340 \t Training Loss: 45.91466938467587\n",
      "Epoch 24 \t Batch 360 \t Training Loss: 45.88262159559462\n",
      "Epoch 24 \t Batch 380 \t Training Loss: 45.87767212516383\n",
      "Epoch 24 \t Batch 400 \t Training Loss: 45.91852272033692\n",
      "Epoch 24 \t Batch 420 \t Training Loss: 45.91863797505697\n",
      "Epoch 24 \t Batch 440 \t Training Loss: 45.90493359999223\n",
      "Epoch 24 \t Batch 460 \t Training Loss: 45.98434256678042\n",
      "Epoch 24 \t Batch 480 \t Training Loss: 45.96593321164449\n",
      "Epoch 24 \t Batch 500 \t Training Loss: 45.97235224914551\n",
      "Epoch 24 \t Batch 520 \t Training Loss: 45.969778838524455\n",
      "Epoch 24 \t Batch 540 \t Training Loss: 45.979004019278065\n",
      "Epoch 24 \t Batch 560 \t Training Loss: 45.982418046678816\n",
      "Epoch 24 \t Batch 580 \t Training Loss: 45.963948407666436\n",
      "Epoch 24 \t Batch 600 \t Training Loss: 45.93305924097697\n",
      "Epoch 24 \t Batch 620 \t Training Loss: 45.957852037491335\n",
      "Epoch 24 \t Batch 640 \t Training Loss: 45.94942646026611\n",
      "Epoch 24 \t Batch 660 \t Training Loss: 45.94499227928393\n",
      "Epoch 24 \t Batch 680 \t Training Loss: 45.956686221852024\n",
      "Epoch 24 \t Batch 700 \t Training Loss: 45.9742947605678\n",
      "Epoch 24 \t Batch 720 \t Training Loss: 46.02110899289449\n",
      "Epoch 24 \t Batch 740 \t Training Loss: 46.019230966310246\n",
      "Epoch 24 \t Batch 760 \t Training Loss: 46.04235514088681\n",
      "Epoch 24 \t Batch 780 \t Training Loss: 46.016692420763846\n",
      "Epoch 24 \t Batch 800 \t Training Loss: 46.016011095047\n",
      "Epoch 24 \t Batch 820 \t Training Loss: 45.99126092399039\n",
      "Epoch 24 \t Batch 840 \t Training Loss: 45.96635882059733\n",
      "Epoch 24 \t Batch 860 \t Training Loss: 45.95955367420995\n",
      "Epoch 24 \t Batch 880 \t Training Loss: 45.97386926737698\n",
      "Epoch 24 \t Batch 900 \t Training Loss: 45.96629107157389\n",
      "Epoch 24 \t Batch 20 \t Validation Loss: 9.747194981575012\n",
      "Epoch 24 \t Batch 40 \t Validation Loss: 11.657671523094177\n",
      "Epoch 24 \t Batch 60 \t Validation Loss: 11.936340721448262\n",
      "Epoch 24 \t Batch 80 \t Validation Loss: 13.136823534965515\n",
      "Epoch 24 \t Batch 100 \t Validation Loss: 15.703958702087402\n",
      "Epoch 24 \t Batch 120 \t Validation Loss: 17.75999787648519\n",
      "Epoch 24 \t Batch 140 \t Validation Loss: 18.99023844855172\n",
      "Epoch 24 \t Batch 160 \t Validation Loss: 21.57475565671921\n",
      "Epoch 24 \t Batch 180 \t Validation Loss: 25.623384751213923\n",
      "Epoch 24 \t Batch 200 \t Validation Loss: 27.75029535293579\n",
      "Epoch 24 \t Batch 220 \t Validation Loss: 29.43258698203347\n",
      "Epoch 24 \t Batch 240 \t Validation Loss: 30.267085536321005\n",
      "Epoch 24 \t Batch 260 \t Validation Loss: 32.78769226074219\n",
      "Epoch 24 \t Batch 280 \t Validation Loss: 34.26021272795541\n",
      "Epoch 24 \t Batch 300 \t Validation Loss: 35.36151932398478\n",
      "Epoch 24 \t Batch 320 \t Validation Loss: 35.9638552069664\n",
      "Epoch 24 \t Batch 340 \t Validation Loss: 36.06993727964513\n",
      "Epoch 24 \t Batch 360 \t Validation Loss: 35.953125418557065\n",
      "Epoch 24 \t Batch 380 \t Validation Loss: 36.34869996622989\n",
      "Epoch 24 \t Batch 400 \t Validation Loss: 36.10831614971161\n",
      "Epoch 24 \t Batch 420 \t Validation Loss: 36.25422135761806\n",
      "Epoch 24 \t Batch 440 \t Validation Loss: 36.05287034294822\n",
      "Epoch 24 \t Batch 460 \t Validation Loss: 36.43424046972524\n",
      "Epoch 24 \t Batch 480 \t Validation Loss: 37.0125843723615\n",
      "Epoch 24 \t Batch 500 \t Validation Loss: 36.827770999908445\n",
      "Epoch 24 \t Batch 520 \t Validation Loss: 36.68194234554584\n",
      "Epoch 24 \t Batch 540 \t Validation Loss: 36.42427965446755\n",
      "Epoch 24 \t Batch 560 \t Validation Loss: 36.23730090005057\n",
      "Epoch 24 \t Batch 580 \t Validation Loss: 36.00504321065442\n",
      "Epoch 24 \t Batch 600 \t Validation Loss: 36.24224093119303\n",
      "Epoch 24 Training Loss: 45.95484368959716 Validation Loss: 36.878387832022334\n",
      "Epoch 24 completed\n",
      "Epoch 25 \t Batch 20 \t Training Loss: 44.865485763549806\n",
      "Epoch 25 \t Batch 40 \t Training Loss: 45.110157299041745\n",
      "Epoch 25 \t Batch 60 \t Training Loss: 45.5260425567627\n",
      "Epoch 25 \t Batch 80 \t Training Loss: 45.340082502365114\n",
      "Epoch 25 \t Batch 100 \t Training Loss: 45.42640285491943\n",
      "Epoch 25 \t Batch 120 \t Training Loss: 45.55136674245198\n",
      "Epoch 25 \t Batch 140 \t Training Loss: 45.62488307952881\n",
      "Epoch 25 \t Batch 160 \t Training Loss: 45.676510548591615\n",
      "Epoch 25 \t Batch 180 \t Training Loss: 45.877013715108234\n",
      "Epoch 25 \t Batch 200 \t Training Loss: 46.12201061248779\n",
      "Epoch 25 \t Batch 220 \t Training Loss: 46.182706676829945\n",
      "Epoch 25 \t Batch 240 \t Training Loss: 46.27595370610555\n",
      "Epoch 25 \t Batch 260 \t Training Loss: 46.251867969219504\n",
      "Epoch 25 \t Batch 280 \t Training Loss: 46.1477903502328\n",
      "Epoch 25 \t Batch 300 \t Training Loss: 46.03769193013509\n",
      "Epoch 25 \t Batch 320 \t Training Loss: 46.12610557079315\n",
      "Epoch 25 \t Batch 340 \t Training Loss: 46.08063881817986\n",
      "Epoch 25 \t Batch 360 \t Training Loss: 46.092085191938615\n",
      "Epoch 25 \t Batch 380 \t Training Loss: 46.030637209038986\n",
      "Epoch 25 \t Batch 400 \t Training Loss: 46.0258952999115\n",
      "Epoch 25 \t Batch 420 \t Training Loss: 46.00226624806722\n",
      "Epoch 25 \t Batch 440 \t Training Loss: 45.990732444416395\n",
      "Epoch 25 \t Batch 460 \t Training Loss: 45.97756814542024\n",
      "Epoch 25 \t Batch 480 \t Training Loss: 46.00920826594035\n",
      "Epoch 25 \t Batch 500 \t Training Loss: 45.9266569442749\n",
      "Epoch 25 \t Batch 520 \t Training Loss: 45.85859231948852\n",
      "Epoch 25 \t Batch 540 \t Training Loss: 45.838331837124294\n",
      "Epoch 25 \t Batch 560 \t Training Loss: 45.8004269191197\n",
      "Epoch 25 \t Batch 580 \t Training Loss: 45.852978693205735\n",
      "Epoch 25 \t Batch 600 \t Training Loss: 45.8718514696757\n",
      "Epoch 25 \t Batch 620 \t Training Loss: 45.9109106433007\n",
      "Epoch 25 \t Batch 640 \t Training Loss: 45.922268921136855\n",
      "Epoch 25 \t Batch 660 \t Training Loss: 45.87731987924287\n",
      "Epoch 25 \t Batch 680 \t Training Loss: 45.89408982220818\n",
      "Epoch 25 \t Batch 700 \t Training Loss: 45.918851165771486\n",
      "Epoch 25 \t Batch 720 \t Training Loss: 45.91168481508891\n",
      "Epoch 25 \t Batch 740 \t Training Loss: 45.936037867778055\n",
      "Epoch 25 \t Batch 760 \t Training Loss: 45.937338999698035\n",
      "Epoch 25 \t Batch 780 \t Training Loss: 45.895863694411055\n",
      "Epoch 25 \t Batch 800 \t Training Loss: 45.85917059421539\n",
      "Epoch 25 \t Batch 820 \t Training Loss: 45.84157986524628\n",
      "Epoch 25 \t Batch 840 \t Training Loss: 45.84926387241909\n",
      "Epoch 25 \t Batch 860 \t Training Loss: 45.83805751800537\n",
      "Epoch 25 \t Batch 880 \t Training Loss: 45.820598242499614\n",
      "Epoch 25 \t Batch 900 \t Training Loss: 45.82972114139133\n",
      "Epoch 25 \t Batch 20 \t Validation Loss: 8.061253190040588\n",
      "Epoch 25 \t Batch 40 \t Validation Loss: 10.197573149204255\n",
      "Epoch 25 \t Batch 60 \t Validation Loss: 10.46232775847117\n",
      "Epoch 25 \t Batch 80 \t Validation Loss: 11.81990668773651\n",
      "Epoch 25 \t Batch 100 \t Validation Loss: 14.888692111968995\n",
      "Epoch 25 \t Batch 120 \t Validation Loss: 17.220945493380228\n",
      "Epoch 25 \t Batch 140 \t Validation Loss: 18.597257961545672\n",
      "Epoch 25 \t Batch 160 \t Validation Loss: 21.079645103216173\n",
      "Epoch 25 \t Batch 180 \t Validation Loss: 24.613693968454996\n",
      "Epoch 25 \t Batch 200 \t Validation Loss: 26.59767339706421\n",
      "Epoch 25 \t Batch 220 \t Validation Loss: 28.190245775742966\n",
      "Epoch 25 \t Batch 240 \t Validation Loss: 28.924624943733214\n",
      "Epoch 25 \t Batch 260 \t Validation Loss: 31.4454263320336\n",
      "Epoch 25 \t Batch 280 \t Validation Loss: 32.893146766935075\n",
      "Epoch 25 \t Batch 300 \t Validation Loss: 33.73544645945231\n",
      "Epoch 25 \t Batch 320 \t Validation Loss: 34.287613624334334\n",
      "Epoch 25 \t Batch 340 \t Validation Loss: 34.423979523602654\n",
      "Epoch 25 \t Batch 360 \t Validation Loss: 34.27111575603485\n",
      "Epoch 25 \t Batch 380 \t Validation Loss: 34.76885417386105\n",
      "Epoch 25 \t Batch 400 \t Validation Loss: 34.63572031736374\n",
      "Epoch 25 \t Batch 420 \t Validation Loss: 34.83896032287961\n",
      "Epoch 25 \t Batch 440 \t Validation Loss: 34.810533755475824\n",
      "Epoch 25 \t Batch 460 \t Validation Loss: 35.307493261668995\n",
      "Epoch 25 \t Batch 480 \t Validation Loss: 35.870304328203204\n",
      "Epoch 25 \t Batch 500 \t Validation Loss: 35.69977218818664\n",
      "Epoch 25 \t Batch 520 \t Validation Loss: 35.76931169399848\n",
      "Epoch 25 \t Batch 540 \t Validation Loss: 35.57543290632742\n",
      "Epoch 25 \t Batch 560 \t Validation Loss: 35.43626446894237\n",
      "Epoch 25 \t Batch 580 \t Validation Loss: 35.2624326492178\n",
      "Epoch 25 \t Batch 600 \t Validation Loss: 35.567896196047464\n",
      "Epoch 25 Training Loss: 45.85219657590111 Validation Loss: 36.22072746382131\n",
      "Validation Loss Decreased(22348.678171157837--->22311.96811771393) Saving The Model\n",
      "Epoch 25 completed\n",
      "Epoch 26 \t Batch 20 \t Training Loss: 45.12115592956543\n",
      "Epoch 26 \t Batch 40 \t Training Loss: 46.18641309738159\n",
      "Epoch 26 \t Batch 60 \t Training Loss: 46.101907602945964\n",
      "Epoch 26 \t Batch 80 \t Training Loss: 46.05417065620422\n",
      "Epoch 26 \t Batch 100 \t Training Loss: 46.02261936187744\n",
      "Epoch 26 \t Batch 120 \t Training Loss: 45.80677858988444\n",
      "Epoch 26 \t Batch 140 \t Training Loss: 45.6960207257952\n",
      "Epoch 26 \t Batch 160 \t Training Loss: 45.45405809879303\n",
      "Epoch 26 \t Batch 180 \t Training Loss: 45.48000823126899\n",
      "Epoch 26 \t Batch 200 \t Training Loss: 45.70823120117188\n",
      "Epoch 26 \t Batch 220 \t Training Loss: 45.66338709050959\n",
      "Epoch 26 \t Batch 240 \t Training Loss: 45.65056339899699\n",
      "Epoch 26 \t Batch 260 \t Training Loss: 45.63858246436486\n",
      "Epoch 26 \t Batch 280 \t Training Loss: 45.707089519500734\n",
      "Epoch 26 \t Batch 300 \t Training Loss: 45.67710700988769\n",
      "Epoch 26 \t Batch 320 \t Training Loss: 45.74427289962769\n",
      "Epoch 26 \t Batch 340 \t Training Loss: 45.75588698667639\n",
      "Epoch 26 \t Batch 360 \t Training Loss: 45.651342148251004\n",
      "Epoch 26 \t Batch 380 \t Training Loss: 45.72991978494745\n",
      "Epoch 26 \t Batch 400 \t Training Loss: 45.715484371185305\n",
      "Epoch 26 \t Batch 420 \t Training Loss: 45.69443545568557\n",
      "Epoch 26 \t Batch 440 \t Training Loss: 45.68936405181885\n",
      "Epoch 26 \t Batch 460 \t Training Loss: 45.7005277633667\n",
      "Epoch 26 \t Batch 480 \t Training Loss: 45.649947293599446\n",
      "Epoch 26 \t Batch 500 \t Training Loss: 45.67481164550781\n",
      "Epoch 26 \t Batch 520 \t Training Loss: 45.652031795795146\n",
      "Epoch 26 \t Batch 540 \t Training Loss: 45.59300850762261\n",
      "Epoch 26 \t Batch 560 \t Training Loss: 45.641647931507656\n",
      "Epoch 26 \t Batch 580 \t Training Loss: 45.614826800905426\n",
      "Epoch 26 \t Batch 600 \t Training Loss: 45.66100554784139\n",
      "Epoch 26 \t Batch 620 \t Training Loss: 45.65444600505214\n",
      "Epoch 26 \t Batch 640 \t Training Loss: 45.64651524424553\n",
      "Epoch 26 \t Batch 660 \t Training Loss: 45.651391023578064\n",
      "Epoch 26 \t Batch 680 \t Training Loss: 45.64757849749397\n",
      "Epoch 26 \t Batch 700 \t Training Loss: 45.63193894522531\n",
      "Epoch 26 \t Batch 720 \t Training Loss: 45.643771717283464\n",
      "Epoch 26 \t Batch 740 \t Training Loss: 45.67936876142347\n",
      "Epoch 26 \t Batch 760 \t Training Loss: 45.657450921911945\n",
      "Epoch 26 \t Batch 780 \t Training Loss: 45.69477301377516\n",
      "Epoch 26 \t Batch 800 \t Training Loss: 45.70514181137085\n",
      "Epoch 26 \t Batch 820 \t Training Loss: 45.72085305190668\n",
      "Epoch 26 \t Batch 840 \t Training Loss: 45.74361780257452\n",
      "Epoch 26 \t Batch 860 \t Training Loss: 45.773931272639786\n",
      "Epoch 26 \t Batch 880 \t Training Loss: 45.770228351246224\n",
      "Epoch 26 \t Batch 900 \t Training Loss: 45.75194452921549\n",
      "Epoch 26 \t Batch 20 \t Validation Loss: 13.062174677848816\n",
      "Epoch 26 \t Batch 40 \t Validation Loss: 15.732184958457946\n",
      "Epoch 26 \t Batch 60 \t Validation Loss: 15.530223321914672\n",
      "Epoch 26 \t Batch 80 \t Validation Loss: 16.340300607681275\n",
      "Epoch 26 \t Batch 100 \t Validation Loss: 18.751420612335206\n",
      "Epoch 26 \t Batch 120 \t Validation Loss: 20.665748739242552\n",
      "Epoch 26 \t Batch 140 \t Validation Loss: 21.64986412865775\n",
      "Epoch 26 \t Batch 160 \t Validation Loss: 23.868091940879822\n",
      "Epoch 26 \t Batch 180 \t Validation Loss: 27.34351092444526\n",
      "Epoch 26 \t Batch 200 \t Validation Loss: 29.071661348342897\n",
      "Epoch 26 \t Batch 220 \t Validation Loss: 30.56525108164007\n",
      "Epoch 26 \t Batch 240 \t Validation Loss: 31.223098973433178\n",
      "Epoch 26 \t Batch 260 \t Validation Loss: 33.62838752819942\n",
      "Epoch 26 \t Batch 280 \t Validation Loss: 34.96998002529144\n",
      "Epoch 26 \t Batch 300 \t Validation Loss: 35.832118565241494\n",
      "Epoch 26 \t Batch 320 \t Validation Loss: 36.34017207026481\n",
      "Epoch 26 \t Batch 340 \t Validation Loss: 36.382032310261444\n",
      "Epoch 26 \t Batch 360 \t Validation Loss: 36.1801627529992\n",
      "Epoch 26 \t Batch 380 \t Validation Loss: 36.54094145925421\n",
      "Epoch 26 \t Batch 400 \t Validation Loss: 36.27472578048706\n",
      "Epoch 26 \t Batch 420 \t Validation Loss: 36.36675411406017\n",
      "Epoch 26 \t Batch 440 \t Validation Loss: 36.1560350634835\n",
      "Epoch 26 \t Batch 460 \t Validation Loss: 36.49696328121683\n",
      "Epoch 26 \t Batch 480 \t Validation Loss: 37.0218409538269\n",
      "Epoch 26 \t Batch 500 \t Validation Loss: 36.80806004714966\n",
      "Epoch 26 \t Batch 520 \t Validation Loss: 36.70575192158039\n",
      "Epoch 26 \t Batch 540 \t Validation Loss: 36.456153873161036\n",
      "Epoch 26 \t Batch 560 \t Validation Loss: 36.29544657639095\n",
      "Epoch 26 \t Batch 580 \t Validation Loss: 36.09231851840841\n",
      "Epoch 26 \t Batch 600 \t Validation Loss: 36.32885845502218\n",
      "Epoch 26 Training Loss: 45.76101403771986 Validation Loss: 36.99618429642219\n",
      "Epoch 26 completed\n",
      "Epoch 27 \t Batch 20 \t Training Loss: 45.28469886779785\n",
      "Epoch 27 \t Batch 40 \t Training Loss: 44.51192092895508\n",
      "Epoch 27 \t Batch 60 \t Training Loss: 44.333614095052084\n",
      "Epoch 27 \t Batch 80 \t Training Loss: 44.4994637966156\n",
      "Epoch 27 \t Batch 100 \t Training Loss: 44.455934295654295\n",
      "Epoch 27 \t Batch 120 \t Training Loss: 44.59923963546753\n",
      "Epoch 27 \t Batch 140 \t Training Loss: 44.816705921718054\n",
      "Epoch 27 \t Batch 160 \t Training Loss: 44.84730892181396\n",
      "Epoch 27 \t Batch 180 \t Training Loss: 44.7688158247206\n",
      "Epoch 27 \t Batch 200 \t Training Loss: 44.780976638793945\n",
      "Epoch 27 \t Batch 220 \t Training Loss: 44.793137515674935\n",
      "Epoch 27 \t Batch 240 \t Training Loss: 44.89121685028076\n",
      "Epoch 27 \t Batch 260 \t Training Loss: 44.89556234799898\n",
      "Epoch 27 \t Batch 280 \t Training Loss: 45.0029894556318\n",
      "Epoch 27 \t Batch 300 \t Training Loss: 45.03497032165527\n",
      "Epoch 27 \t Batch 320 \t Training Loss: 45.180845725536344\n",
      "Epoch 27 \t Batch 340 \t Training Loss: 45.22804341035731\n",
      "Epoch 27 \t Batch 360 \t Training Loss: 45.26298893822564\n",
      "Epoch 27 \t Batch 380 \t Training Loss: 45.25318808304636\n",
      "Epoch 27 \t Batch 400 \t Training Loss: 45.270707082748416\n",
      "Epoch 27 \t Batch 420 \t Training Loss: 45.29931001209077\n",
      "Epoch 27 \t Batch 440 \t Training Loss: 45.280463218688965\n",
      "Epoch 27 \t Batch 460 \t Training Loss: 45.31040390678074\n",
      "Epoch 27 \t Batch 480 \t Training Loss: 45.339745076497394\n",
      "Epoch 27 \t Batch 500 \t Training Loss: 45.388989608764646\n",
      "Epoch 27 \t Batch 520 \t Training Loss: 45.396107526925896\n",
      "Epoch 27 \t Batch 540 \t Training Loss: 45.40126787821452\n",
      "Epoch 27 \t Batch 560 \t Training Loss: 45.432086079461236\n",
      "Epoch 27 \t Batch 580 \t Training Loss: 45.47759932813973\n",
      "Epoch 27 \t Batch 600 \t Training Loss: 45.48535243988037\n",
      "Epoch 27 \t Batch 620 \t Training Loss: 45.44964984770744\n",
      "Epoch 27 \t Batch 640 \t Training Loss: 45.47699570655823\n",
      "Epoch 27 \t Batch 660 \t Training Loss: 45.540630959019516\n",
      "Epoch 27 \t Batch 680 \t Training Loss: 45.520824696035945\n",
      "Epoch 27 \t Batch 700 \t Training Loss: 45.539576639447894\n",
      "Epoch 27 \t Batch 720 \t Training Loss: 45.50213027000427\n",
      "Epoch 27 \t Batch 740 \t Training Loss: 45.53300179661931\n",
      "Epoch 27 \t Batch 760 \t Training Loss: 45.53827359550878\n",
      "Epoch 27 \t Batch 780 \t Training Loss: 45.58155953822992\n",
      "Epoch 27 \t Batch 800 \t Training Loss: 45.57614831924438\n",
      "Epoch 27 \t Batch 820 \t Training Loss: 45.57736102778737\n",
      "Epoch 27 \t Batch 840 \t Training Loss: 45.61673815590995\n",
      "Epoch 27 \t Batch 860 \t Training Loss: 45.62614835694779\n",
      "Epoch 27 \t Batch 880 \t Training Loss: 45.62629365054044\n",
      "Epoch 27 \t Batch 900 \t Training Loss: 45.59465418073866\n",
      "Epoch 27 \t Batch 20 \t Validation Loss: 8.410358834266663\n",
      "Epoch 27 \t Batch 40 \t Validation Loss: 10.491805863380431\n",
      "Epoch 27 \t Batch 60 \t Validation Loss: 10.784727644920348\n",
      "Epoch 27 \t Batch 80 \t Validation Loss: 12.196657061576843\n",
      "Epoch 27 \t Batch 100 \t Validation Loss: 15.202346076965332\n",
      "Epoch 27 \t Batch 120 \t Validation Loss: 17.5134635925293\n",
      "Epoch 27 \t Batch 140 \t Validation Loss: 18.99025288990566\n",
      "Epoch 27 \t Batch 160 \t Validation Loss: 22.06143581867218\n",
      "Epoch 27 \t Batch 180 \t Validation Loss: 26.878118382559883\n",
      "Epoch 27 \t Batch 200 \t Validation Loss: 29.21378128528595\n",
      "Epoch 27 \t Batch 220 \t Validation Loss: 31.416538381576537\n",
      "Epoch 27 \t Batch 240 \t Validation Loss: 32.50691995223363\n",
      "Epoch 27 \t Batch 260 \t Validation Loss: 35.371535165493306\n",
      "Epoch 27 \t Batch 280 \t Validation Loss: 37.00029241357531\n",
      "Epoch 27 \t Batch 300 \t Validation Loss: 38.35746768633525\n",
      "Epoch 27 \t Batch 320 \t Validation Loss: 39.10491417348385\n",
      "Epoch 27 \t Batch 340 \t Validation Loss: 39.13949163941776\n",
      "Epoch 27 \t Batch 360 \t Validation Loss: 39.092051815986636\n",
      "Epoch 27 \t Batch 380 \t Validation Loss: 39.554343793266696\n",
      "Epoch 27 \t Batch 400 \t Validation Loss: 39.16380417108536\n",
      "Epoch 27 \t Batch 420 \t Validation Loss: 39.18459184964498\n",
      "Epoch 27 \t Batch 440 \t Validation Loss: 38.91619010188363\n",
      "Epoch 27 \t Batch 460 \t Validation Loss: 39.26865610661714\n",
      "Epoch 27 \t Batch 480 \t Validation Loss: 39.755994739135105\n",
      "Epoch 27 \t Batch 500 \t Validation Loss: 39.52584127998352\n",
      "Epoch 27 \t Batch 520 \t Validation Loss: 39.47256471927349\n",
      "Epoch 27 \t Batch 540 \t Validation Loss: 39.20361953311496\n",
      "Epoch 27 \t Batch 560 \t Validation Loss: 39.03756150858743\n",
      "Epoch 27 \t Batch 580 \t Validation Loss: 38.806264206458785\n",
      "Epoch 27 \t Batch 600 \t Validation Loss: 39.03485080401103\n",
      "Epoch 27 Training Loss: 45.630510789877455 Validation Loss: 39.6611167424685\n",
      "Epoch 27 completed\n",
      "Epoch 28 \t Batch 20 \t Training Loss: 44.406371116638184\n",
      "Epoch 28 \t Batch 40 \t Training Loss: 45.17716979980469\n",
      "Epoch 28 \t Batch 60 \t Training Loss: 44.75487766265869\n",
      "Epoch 28 \t Batch 80 \t Training Loss: 44.637997484207155\n",
      "Epoch 28 \t Batch 100 \t Training Loss: 45.0620837020874\n",
      "Epoch 28 \t Batch 120 \t Training Loss: 45.03999678293864\n",
      "Epoch 28 \t Batch 140 \t Training Loss: 45.12419422694615\n",
      "Epoch 28 \t Batch 160 \t Training Loss: 45.03675673007965\n",
      "Epoch 28 \t Batch 180 \t Training Loss: 45.17274083031548\n",
      "Epoch 28 \t Batch 200 \t Training Loss: 45.27417314529419\n",
      "Epoch 28 \t Batch 220 \t Training Loss: 45.19897176569158\n",
      "Epoch 28 \t Batch 240 \t Training Loss: 45.37487764358521\n",
      "Epoch 28 \t Batch 260 \t Training Loss: 45.42639952439528\n",
      "Epoch 28 \t Batch 280 \t Training Loss: 45.439224924360005\n",
      "Epoch 28 \t Batch 300 \t Training Loss: 45.34034178415934\n",
      "Epoch 28 \t Batch 320 \t Training Loss: 45.3698646903038\n",
      "Epoch 28 \t Batch 340 \t Training Loss: 45.405304246790266\n",
      "Epoch 28 \t Batch 360 \t Training Loss: 45.35040940178765\n",
      "Epoch 28 \t Batch 380 \t Training Loss: 45.40188531373676\n",
      "Epoch 28 \t Batch 400 \t Training Loss: 45.32418763160705\n",
      "Epoch 28 \t Batch 420 \t Training Loss: 45.383694221859884\n",
      "Epoch 28 \t Batch 440 \t Training Loss: 45.448166916587134\n",
      "Epoch 28 \t Batch 460 \t Training Loss: 45.48639794225278\n",
      "Epoch 28 \t Batch 480 \t Training Loss: 45.51989296277364\n",
      "Epoch 28 \t Batch 500 \t Training Loss: 45.5008637008667\n",
      "Epoch 28 \t Batch 520 \t Training Loss: 45.55957765579224\n",
      "Epoch 28 \t Batch 540 \t Training Loss: 45.52914945107919\n",
      "Epoch 28 \t Batch 560 \t Training Loss: 45.50164689336504\n",
      "Epoch 28 \t Batch 580 \t Training Loss: 45.56098938645988\n",
      "Epoch 28 \t Batch 600 \t Training Loss: 45.56965075174968\n",
      "Epoch 28 \t Batch 620 \t Training Loss: 45.573157618122714\n",
      "Epoch 28 \t Batch 640 \t Training Loss: 45.525322151184085\n",
      "Epoch 28 \t Batch 660 \t Training Loss: 45.57949482888886\n",
      "Epoch 28 \t Batch 680 \t Training Loss: 45.57172587338616\n",
      "Epoch 28 \t Batch 700 \t Training Loss: 45.57132142203195\n",
      "Epoch 28 \t Batch 720 \t Training Loss: 45.562598896026614\n",
      "Epoch 28 \t Batch 740 \t Training Loss: 45.5535982080408\n",
      "Epoch 28 \t Batch 760 \t Training Loss: 45.52026671861348\n",
      "Epoch 28 \t Batch 780 \t Training Loss: 45.521341749338006\n",
      "Epoch 28 \t Batch 800 \t Training Loss: 45.50608801841736\n",
      "Epoch 28 \t Batch 820 \t Training Loss: 45.49113981200428\n",
      "Epoch 28 \t Batch 840 \t Training Loss: 45.505971545264835\n",
      "Epoch 28 \t Batch 860 \t Training Loss: 45.52622221126113\n",
      "Epoch 28 \t Batch 880 \t Training Loss: 45.527908372879025\n",
      "Epoch 28 \t Batch 900 \t Training Loss: 45.54005345662435\n",
      "Epoch 28 \t Batch 20 \t Validation Loss: 10.275845575332642\n",
      "Epoch 28 \t Batch 40 \t Validation Loss: 13.83314424753189\n",
      "Epoch 28 \t Batch 60 \t Validation Loss: 13.344336986541748\n",
      "Epoch 28 \t Batch 80 \t Validation Loss: 14.47294944524765\n",
      "Epoch 28 \t Batch 100 \t Validation Loss: 17.053709535598756\n",
      "Epoch 28 \t Batch 120 \t Validation Loss: 19.162752016385397\n",
      "Epoch 28 \t Batch 140 \t Validation Loss: 20.380950689315796\n",
      "Epoch 28 \t Batch 160 \t Validation Loss: 22.96682763695717\n",
      "Epoch 28 \t Batch 180 \t Validation Loss: 26.94597300423516\n",
      "Epoch 28 \t Batch 200 \t Validation Loss: 28.989897327423094\n",
      "Epoch 28 \t Batch 220 \t Validation Loss: 30.78305231441151\n",
      "Epoch 28 \t Batch 240 \t Validation Loss: 31.60528029203415\n",
      "Epoch 28 \t Batch 260 \t Validation Loss: 34.23152081783001\n",
      "Epoch 28 \t Batch 280 \t Validation Loss: 35.74137281690325\n",
      "Epoch 28 \t Batch 300 \t Validation Loss: 36.76674840927124\n",
      "Epoch 28 \t Batch 320 \t Validation Loss: 37.346007004380226\n",
      "Epoch 28 \t Batch 340 \t Validation Loss: 37.40323191530564\n",
      "Epoch 28 \t Batch 360 \t Validation Loss: 37.282738412751094\n",
      "Epoch 28 \t Batch 380 \t Validation Loss: 37.72343716119465\n",
      "Epoch 28 \t Batch 400 \t Validation Loss: 37.46283500909805\n",
      "Epoch 28 \t Batch 420 \t Validation Loss: 37.54022970880781\n",
      "Epoch 28 \t Batch 440 \t Validation Loss: 37.39024058038538\n",
      "Epoch 28 \t Batch 460 \t Validation Loss: 37.77501448133717\n",
      "Epoch 28 \t Batch 480 \t Validation Loss: 38.293187175194426\n",
      "Epoch 28 \t Batch 500 \t Validation Loss: 38.091264837265015\n",
      "Epoch 28 \t Batch 520 \t Validation Loss: 38.05652510019449\n",
      "Epoch 28 \t Batch 540 \t Validation Loss: 37.78604763348897\n",
      "Epoch 28 \t Batch 560 \t Validation Loss: 37.61505892787661\n",
      "Epoch 28 \t Batch 580 \t Validation Loss: 37.469172485943496\n",
      "Epoch 28 \t Batch 600 \t Validation Loss: 37.69334348837535\n",
      "Epoch 28 Training Loss: 45.51899566151194 Validation Loss: 38.33165391853878\n",
      "Epoch 28 completed\n",
      "Epoch 29 \t Batch 20 \t Training Loss: 46.37671794891357\n",
      "Epoch 29 \t Batch 40 \t Training Loss: 45.78996887207031\n",
      "Epoch 29 \t Batch 60 \t Training Loss: 45.24264380137126\n",
      "Epoch 29 \t Batch 80 \t Training Loss: 45.19328918457031\n",
      "Epoch 29 \t Batch 100 \t Training Loss: 45.10997623443603\n",
      "Epoch 29 \t Batch 120 \t Training Loss: 44.997119998931886\n",
      "Epoch 29 \t Batch 140 \t Training Loss: 44.98238002232143\n",
      "Epoch 29 \t Batch 160 \t Training Loss: 44.99713761806488\n",
      "Epoch 29 \t Batch 180 \t Training Loss: 45.09413710700141\n",
      "Epoch 29 \t Batch 200 \t Training Loss: 45.20113437652588\n",
      "Epoch 29 \t Batch 220 \t Training Loss: 45.142854031649506\n",
      "Epoch 29 \t Batch 240 \t Training Loss: 45.20028196970622\n",
      "Epoch 29 \t Batch 260 \t Training Loss: 45.02050327887902\n",
      "Epoch 29 \t Batch 280 \t Training Loss: 44.99031382969448\n",
      "Epoch 29 \t Batch 300 \t Training Loss: 45.005179201761884\n",
      "Epoch 29 \t Batch 320 \t Training Loss: 45.020706391334535\n",
      "Epoch 29 \t Batch 340 \t Training Loss: 44.978762750064625\n",
      "Epoch 29 \t Batch 360 \t Training Loss: 44.98974968592326\n",
      "Epoch 29 \t Batch 380 \t Training Loss: 45.01617213801334\n",
      "Epoch 29 \t Batch 400 \t Training Loss: 45.071927795410154\n",
      "Epoch 29 \t Batch 420 \t Training Loss: 45.14491481781006\n",
      "Epoch 29 \t Batch 440 \t Training Loss: 45.19884532581676\n",
      "Epoch 29 \t Batch 460 \t Training Loss: 45.19882703864056\n",
      "Epoch 29 \t Batch 480 \t Training Loss: 45.17522712548574\n",
      "Epoch 29 \t Batch 500 \t Training Loss: 45.18469452667237\n",
      "Epoch 29 \t Batch 520 \t Training Loss: 45.2480669094966\n",
      "Epoch 29 \t Batch 540 \t Training Loss: 45.30560929333722\n",
      "Epoch 29 \t Batch 560 \t Training Loss: 45.26824792453221\n",
      "Epoch 29 \t Batch 580 \t Training Loss: 45.29568714273387\n",
      "Epoch 29 \t Batch 600 \t Training Loss: 45.30018844604492\n",
      "Epoch 29 \t Batch 620 \t Training Loss: 45.31559489465529\n",
      "Epoch 29 \t Batch 640 \t Training Loss: 45.30285924077034\n",
      "Epoch 29 \t Batch 660 \t Training Loss: 45.31560159741026\n",
      "Epoch 29 \t Batch 680 \t Training Loss: 45.263974537568934\n",
      "Epoch 29 \t Batch 700 \t Training Loss: 45.2550267464774\n",
      "Epoch 29 \t Batch 720 \t Training Loss: 45.228421513239546\n",
      "Epoch 29 \t Batch 740 \t Training Loss: 45.19920783687282\n",
      "Epoch 29 \t Batch 760 \t Training Loss: 45.2124119507639\n",
      "Epoch 29 \t Batch 780 \t Training Loss: 45.24901555379232\n",
      "Epoch 29 \t Batch 800 \t Training Loss: 45.29266056537628\n",
      "Epoch 29 \t Batch 820 \t Training Loss: 45.305002500952746\n",
      "Epoch 29 \t Batch 840 \t Training Loss: 45.37053052357265\n",
      "Epoch 29 \t Batch 860 \t Training Loss: 45.36465098358864\n",
      "Epoch 29 \t Batch 880 \t Training Loss: 45.381109432740644\n",
      "Epoch 29 \t Batch 900 \t Training Loss: 45.39958894941542\n",
      "Epoch 29 \t Batch 20 \t Validation Loss: 10.350541400909425\n",
      "Epoch 29 \t Batch 40 \t Validation Loss: 13.30581032037735\n",
      "Epoch 29 \t Batch 60 \t Validation Loss: 12.904080692927042\n",
      "Epoch 29 \t Batch 80 \t Validation Loss: 14.128240299224853\n",
      "Epoch 29 \t Batch 100 \t Validation Loss: 16.87118305206299\n",
      "Epoch 29 \t Batch 120 \t Validation Loss: 18.85107185045878\n",
      "Epoch 29 \t Batch 140 \t Validation Loss: 20.163757692064557\n",
      "Epoch 29 \t Batch 160 \t Validation Loss: 22.998942160606383\n",
      "Epoch 29 \t Batch 180 \t Validation Loss: 27.086570347679984\n",
      "Epoch 29 \t Batch 200 \t Validation Loss: 29.18039596557617\n",
      "Epoch 29 \t Batch 220 \t Validation Loss: 30.972924622622404\n",
      "Epoch 29 \t Batch 240 \t Validation Loss: 31.856766080856325\n",
      "Epoch 29 \t Batch 260 \t Validation Loss: 34.47443659122174\n",
      "Epoch 29 \t Batch 280 \t Validation Loss: 35.95727706636701\n",
      "Epoch 29 \t Batch 300 \t Validation Loss: 37.06352959950765\n",
      "Epoch 29 \t Batch 320 \t Validation Loss: 37.70202484130859\n",
      "Epoch 29 \t Batch 340 \t Validation Loss: 37.76939735412598\n",
      "Epoch 29 \t Batch 360 \t Validation Loss: 37.654009675979616\n",
      "Epoch 29 \t Batch 380 \t Validation Loss: 38.19877976869282\n",
      "Epoch 29 \t Batch 400 \t Validation Loss: 38.105014171600345\n",
      "Epoch 29 \t Batch 420 \t Validation Loss: 38.25704997380574\n",
      "Epoch 29 \t Batch 440 \t Validation Loss: 38.29047637852756\n",
      "Epoch 29 \t Batch 460 \t Validation Loss: 38.883470842112665\n",
      "Epoch 29 \t Batch 480 \t Validation Loss: 39.41668599446614\n",
      "Epoch 29 \t Batch 500 \t Validation Loss: 39.21110328674317\n",
      "Epoch 29 \t Batch 520 \t Validation Loss: 39.400122140004086\n",
      "Epoch 29 \t Batch 540 \t Validation Loss: 39.09499160625317\n",
      "Epoch 29 \t Batch 560 \t Validation Loss: 38.83797410896846\n",
      "Epoch 29 \t Batch 580 \t Validation Loss: 38.569769316706164\n",
      "Epoch 29 \t Batch 600 \t Validation Loss: 38.73454430580139\n",
      "Epoch 29 Training Loss: 45.41185746770129 Validation Loss: 39.31989138466971\n",
      "Epoch 29 completed\n",
      "Epoch 30 \t Batch 20 \t Training Loss: 44.18536319732666\n",
      "Epoch 30 \t Batch 40 \t Training Loss: 45.13701581954956\n",
      "Epoch 30 \t Batch 60 \t Training Loss: 44.95321203867594\n",
      "Epoch 30 \t Batch 80 \t Training Loss: 44.971147537231445\n",
      "Epoch 30 \t Batch 100 \t Training Loss: 45.06964584350586\n",
      "Epoch 30 \t Batch 120 \t Training Loss: 45.363170115153\n",
      "Epoch 30 \t Batch 140 \t Training Loss: 45.43766746520996\n",
      "Epoch 30 \t Batch 160 \t Training Loss: 45.256062507629395\n",
      "Epoch 30 \t Batch 180 \t Training Loss: 45.36744374169244\n",
      "Epoch 30 \t Batch 200 \t Training Loss: 45.30639043807983\n",
      "Epoch 30 \t Batch 220 \t Training Loss: 45.332058247652924\n",
      "Epoch 30 \t Batch 240 \t Training Loss: 45.41878547668457\n",
      "Epoch 30 \t Batch 260 \t Training Loss: 45.38733428074763\n",
      "Epoch 30 \t Batch 280 \t Training Loss: 45.395319271087644\n",
      "Epoch 30 \t Batch 300 \t Training Loss: 45.38838925679525\n",
      "Epoch 30 \t Batch 320 \t Training Loss: 45.27338690757752\n",
      "Epoch 30 \t Batch 340 \t Training Loss: 45.26986790825339\n",
      "Epoch 30 \t Batch 360 \t Training Loss: 45.35842517217\n",
      "Epoch 30 \t Batch 380 \t Training Loss: 45.28260828319349\n",
      "Epoch 30 \t Batch 400 \t Training Loss: 45.259611434936524\n",
      "Epoch 30 \t Batch 420 \t Training Loss: 45.2413059143793\n",
      "Epoch 30 \t Batch 440 \t Training Loss: 45.19715592644432\n",
      "Epoch 30 \t Batch 460 \t Training Loss: 45.192890648219894\n",
      "Epoch 30 \t Batch 480 \t Training Loss: 45.21704191366832\n",
      "Epoch 30 \t Batch 500 \t Training Loss: 45.199155952453616\n",
      "Epoch 30 \t Batch 520 \t Training Loss: 45.2239830824045\n",
      "Epoch 30 \t Batch 540 \t Training Loss: 45.23705087591101\n",
      "Epoch 30 \t Batch 560 \t Training Loss: 45.25581312860761\n",
      "Epoch 30 \t Batch 580 \t Training Loss: 45.253823418452825\n",
      "Epoch 30 \t Batch 600 \t Training Loss: 45.27507470448812\n",
      "Epoch 30 \t Batch 620 \t Training Loss: 45.21462364812051\n",
      "Epoch 30 \t Batch 640 \t Training Loss: 45.23110079169273\n",
      "Epoch 30 \t Batch 660 \t Training Loss: 45.251551639672485\n",
      "Epoch 30 \t Batch 680 \t Training Loss: 45.25078849231495\n",
      "Epoch 30 \t Batch 700 \t Training Loss: 45.21841517312186\n",
      "Epoch 30 \t Batch 720 \t Training Loss: 45.230283938513864\n",
      "Epoch 30 \t Batch 740 \t Training Loss: 45.258037675393595\n",
      "Epoch 30 \t Batch 760 \t Training Loss: 45.22979519994635\n",
      "Epoch 30 \t Batch 780 \t Training Loss: 45.20339318788969\n",
      "Epoch 30 \t Batch 800 \t Training Loss: 45.21239785671234\n",
      "Epoch 30 \t Batch 820 \t Training Loss: 45.27536591320503\n",
      "Epoch 30 \t Batch 840 \t Training Loss: 45.2631111508324\n",
      "Epoch 30 \t Batch 860 \t Training Loss: 45.28039166650107\n",
      "Epoch 30 \t Batch 880 \t Training Loss: 45.271453111821955\n",
      "Epoch 30 \t Batch 900 \t Training Loss: 45.30322900984022\n",
      "Epoch 30 \t Batch 20 \t Validation Loss: 12.589426040649414\n",
      "Epoch 30 \t Batch 40 \t Validation Loss: 14.886458349227905\n",
      "Epoch 30 \t Batch 60 \t Validation Loss: 14.550999641418457\n",
      "Epoch 30 \t Batch 80 \t Validation Loss: 15.323034274578095\n",
      "Epoch 30 \t Batch 100 \t Validation Loss: 17.86853434562683\n",
      "Epoch 30 \t Batch 120 \t Validation Loss: 19.867692025502524\n",
      "Epoch 30 \t Batch 140 \t Validation Loss: 21.020560673304967\n",
      "Epoch 30 \t Batch 160 \t Validation Loss: 23.724608182907104\n",
      "Epoch 30 \t Batch 180 \t Validation Loss: 27.858343415790134\n",
      "Epoch 30 \t Batch 200 \t Validation Loss: 29.94347686290741\n",
      "Epoch 30 \t Batch 220 \t Validation Loss: 31.794056211818347\n",
      "Epoch 30 \t Batch 240 \t Validation Loss: 32.658172857761386\n",
      "Epoch 30 \t Batch 260 \t Validation Loss: 35.30560336846572\n",
      "Epoch 30 \t Batch 280 \t Validation Loss: 36.79256150041308\n",
      "Epoch 30 \t Batch 300 \t Validation Loss: 37.92416979789734\n",
      "Epoch 30 \t Batch 320 \t Validation Loss: 38.5850426375866\n",
      "Epoch 30 \t Batch 340 \t Validation Loss: 38.62638901542215\n",
      "Epoch 30 \t Batch 360 \t Validation Loss: 38.49193550745646\n",
      "Epoch 30 \t Batch 380 \t Validation Loss: 38.99340637608579\n",
      "Epoch 30 \t Batch 400 \t Validation Loss: 38.78285115718842\n",
      "Epoch 30 \t Batch 420 \t Validation Loss: 38.91809782527742\n",
      "Epoch 30 \t Batch 440 \t Validation Loss: 38.93086238124154\n",
      "Epoch 30 \t Batch 460 \t Validation Loss: 39.3850660883862\n",
      "Epoch 30 \t Batch 480 \t Validation Loss: 39.86978644728661\n",
      "Epoch 30 \t Batch 500 \t Validation Loss: 39.62152372550965\n",
      "Epoch 30 \t Batch 520 \t Validation Loss: 39.71463328324831\n",
      "Epoch 30 \t Batch 540 \t Validation Loss: 39.369343254301285\n",
      "Epoch 30 \t Batch 560 \t Validation Loss: 39.10447783640453\n",
      "Epoch 30 \t Batch 580 \t Validation Loss: 38.804023484526006\n",
      "Epoch 30 \t Batch 600 \t Validation Loss: 38.959884696006775\n",
      "Epoch 30 Training Loss: 45.29521938394694 Validation Loss: 39.52052310999338\n",
      "Epoch 30 completed\n",
      "Epoch 31 \t Batch 20 \t Training Loss: 44.10502033233642\n",
      "Epoch 31 \t Batch 40 \t Training Loss: 43.962451457977295\n",
      "Epoch 31 \t Batch 60 \t Training Loss: 44.435822359720866\n",
      "Epoch 31 \t Batch 80 \t Training Loss: 44.568011093139646\n",
      "Epoch 31 \t Batch 100 \t Training Loss: 44.63325214385986\n",
      "Epoch 31 \t Batch 120 \t Training Loss: 44.66475887298584\n",
      "Epoch 31 \t Batch 140 \t Training Loss: 44.92366202218192\n",
      "Epoch 31 \t Batch 160 \t Training Loss: 44.91157593727112\n",
      "Epoch 31 \t Batch 180 \t Training Loss: 44.976277626885306\n",
      "Epoch 31 \t Batch 200 \t Training Loss: 45.14860204696655\n",
      "Epoch 31 \t Batch 220 \t Training Loss: 45.2519398949363\n",
      "Epoch 31 \t Batch 240 \t Training Loss: 45.20930264790853\n",
      "Epoch 31 \t Batch 260 \t Training Loss: 45.15580743642954\n",
      "Epoch 31 \t Batch 280 \t Training Loss: 45.13494873046875\n",
      "Epoch 31 \t Batch 300 \t Training Loss: 45.13826769510905\n",
      "Epoch 31 \t Batch 320 \t Training Loss: 45.06678067445755\n",
      "Epoch 31 \t Batch 340 \t Training Loss: 45.039132084566006\n",
      "Epoch 31 \t Batch 360 \t Training Loss: 45.040826596154105\n",
      "Epoch 31 \t Batch 380 \t Training Loss: 45.02911135021009\n",
      "Epoch 31 \t Batch 400 \t Training Loss: 44.97005265235901\n",
      "Epoch 31 \t Batch 420 \t Training Loss: 45.01852162679037\n",
      "Epoch 31 \t Batch 440 \t Training Loss: 44.998996353149415\n",
      "Epoch 31 \t Batch 460 \t Training Loss: 45.03606819484545\n",
      "Epoch 31 \t Batch 480 \t Training Loss: 45.067883451779686\n",
      "Epoch 31 \t Batch 500 \t Training Loss: 45.054884582519534\n",
      "Epoch 31 \t Batch 520 \t Training Loss: 45.10279899010291\n",
      "Epoch 31 \t Batch 540 \t Training Loss: 45.11222837942618\n",
      "Epoch 31 \t Batch 560 \t Training Loss: 45.08786027772086\n",
      "Epoch 31 \t Batch 580 \t Training Loss: 45.07293283528295\n",
      "Epoch 31 \t Batch 600 \t Training Loss: 45.04659777959188\n",
      "Epoch 31 \t Batch 620 \t Training Loss: 45.05727708878056\n",
      "Epoch 31 \t Batch 640 \t Training Loss: 45.07748310565948\n",
      "Epoch 31 \t Batch 660 \t Training Loss: 45.07967421213786\n",
      "Epoch 31 \t Batch 680 \t Training Loss: 45.121661494759955\n",
      "Epoch 31 \t Batch 700 \t Training Loss: 45.1867060470581\n",
      "Epoch 31 \t Batch 720 \t Training Loss: 45.16806011199951\n",
      "Epoch 31 \t Batch 740 \t Training Loss: 45.14766791060164\n",
      "Epoch 31 \t Batch 760 \t Training Loss: 45.168927734776545\n",
      "Epoch 31 \t Batch 780 \t Training Loss: 45.148951882582445\n",
      "Epoch 31 \t Batch 800 \t Training Loss: 45.156739978790284\n",
      "Epoch 31 \t Batch 820 \t Training Loss: 45.15553031084014\n",
      "Epoch 31 \t Batch 840 \t Training Loss: 45.128795723688036\n",
      "Epoch 31 \t Batch 860 \t Training Loss: 45.16431150214617\n",
      "Epoch 31 \t Batch 880 \t Training Loss: 45.14423433650624\n",
      "Epoch 31 \t Batch 900 \t Training Loss: 45.155930252075194\n",
      "Epoch 31 \t Batch 20 \t Validation Loss: 16.809062671661376\n",
      "Epoch 31 \t Batch 40 \t Validation Loss: 19.29957504272461\n",
      "Epoch 31 \t Batch 60 \t Validation Loss: 18.72009406089783\n",
      "Epoch 31 \t Batch 80 \t Validation Loss: 19.487989759445192\n",
      "Epoch 31 \t Batch 100 \t Validation Loss: 21.256100540161132\n",
      "Epoch 31 \t Batch 120 \t Validation Loss: 22.82890039285024\n",
      "Epoch 31 \t Batch 140 \t Validation Loss: 23.547100618907383\n",
      "Epoch 31 \t Batch 160 \t Validation Loss: 25.70288994908333\n",
      "Epoch 31 \t Batch 180 \t Validation Loss: 29.377222956551446\n",
      "Epoch 31 \t Batch 200 \t Validation Loss: 31.236261706352234\n",
      "Epoch 31 \t Batch 220 \t Validation Loss: 32.76635365486145\n",
      "Epoch 31 \t Batch 240 \t Validation Loss: 33.431781315803526\n",
      "Epoch 31 \t Batch 260 \t Validation Loss: 35.82910949633672\n",
      "Epoch 31 \t Batch 280 \t Validation Loss: 37.106309519495284\n",
      "Epoch 31 \t Batch 300 \t Validation Loss: 38.15663186709086\n",
      "Epoch 31 \t Batch 320 \t Validation Loss: 38.67689200341702\n",
      "Epoch 31 \t Batch 340 \t Validation Loss: 38.66843352878795\n",
      "Epoch 31 \t Batch 360 \t Validation Loss: 38.458104983965555\n",
      "Epoch 31 \t Batch 380 \t Validation Loss: 38.78001547612642\n",
      "Epoch 31 \t Batch 400 \t Validation Loss: 38.43313351869583\n",
      "Epoch 31 \t Batch 420 \t Validation Loss: 38.51062738554818\n",
      "Epoch 31 \t Batch 440 \t Validation Loss: 38.25334485877644\n",
      "Epoch 31 \t Batch 460 \t Validation Loss: 38.59464544835298\n",
      "Epoch 31 \t Batch 480 \t Validation Loss: 39.10869598587354\n",
      "Epoch 31 \t Batch 500 \t Validation Loss: 38.868084062576294\n",
      "Epoch 31 \t Batch 520 \t Validation Loss: 38.72098242502946\n",
      "Epoch 31 \t Batch 540 \t Validation Loss: 38.40424165902314\n",
      "Epoch 31 \t Batch 560 \t Validation Loss: 38.158464685508186\n",
      "Epoch 31 \t Batch 580 \t Validation Loss: 37.84059479812096\n",
      "Epoch 31 \t Batch 600 \t Validation Loss: 38.03586610794068\n",
      "Epoch 31 Training Loss: 45.156311846351414 Validation Loss: 38.6366204658112\n",
      "Epoch 31 completed\n",
      "Epoch 32 \t Batch 20 \t Training Loss: 45.18420810699463\n",
      "Epoch 32 \t Batch 40 \t Training Loss: 45.20529193878174\n",
      "Epoch 32 \t Batch 60 \t Training Loss: 45.0359489440918\n",
      "Epoch 32 \t Batch 80 \t Training Loss: 45.05520229339599\n",
      "Epoch 32 \t Batch 100 \t Training Loss: 45.21867607116699\n",
      "Epoch 32 \t Batch 120 \t Training Loss: 45.11951084136963\n",
      "Epoch 32 \t Batch 140 \t Training Loss: 45.024800709315706\n",
      "Epoch 32 \t Batch 160 \t Training Loss: 45.0695719242096\n",
      "Epoch 32 \t Batch 180 \t Training Loss: 45.138502735561794\n",
      "Epoch 32 \t Batch 200 \t Training Loss: 45.13669054031372\n",
      "Epoch 32 \t Batch 220 \t Training Loss: 45.12457048242742\n",
      "Epoch 32 \t Batch 240 \t Training Loss: 45.02486904462179\n",
      "Epoch 32 \t Batch 260 \t Training Loss: 44.977387237548825\n",
      "Epoch 32 \t Batch 280 \t Training Loss: 44.97917308807373\n",
      "Epoch 32 \t Batch 300 \t Training Loss: 45.03662158966065\n",
      "Epoch 32 \t Batch 320 \t Training Loss: 45.002354371547696\n",
      "Epoch 32 \t Batch 340 \t Training Loss: 44.97308879179113\n",
      "Epoch 32 \t Batch 360 \t Training Loss: 44.98234957589044\n",
      "Epoch 32 \t Batch 380 \t Training Loss: 44.92373225563451\n",
      "Epoch 32 \t Batch 400 \t Training Loss: 44.91149452209473\n",
      "Epoch 32 \t Batch 420 \t Training Loss: 44.911764626275925\n",
      "Epoch 32 \t Batch 440 \t Training Loss: 44.883010907606646\n",
      "Epoch 32 \t Batch 460 \t Training Loss: 44.87384551504384\n",
      "Epoch 32 \t Batch 480 \t Training Loss: 44.90712258021037\n",
      "Epoch 32 \t Batch 500 \t Training Loss: 44.94800316619873\n",
      "Epoch 32 \t Batch 520 \t Training Loss: 44.95490556863638\n",
      "Epoch 32 \t Batch 540 \t Training Loss: 44.915171206438984\n",
      "Epoch 32 \t Batch 560 \t Training Loss: 44.89520562716893\n",
      "Epoch 32 \t Batch 580 \t Training Loss: 44.85052736216578\n",
      "Epoch 32 \t Batch 600 \t Training Loss: 44.845604826609296\n",
      "Epoch 32 \t Batch 620 \t Training Loss: 44.88058316630702\n",
      "Epoch 32 \t Batch 640 \t Training Loss: 44.91564145684242\n",
      "Epoch 32 \t Batch 660 \t Training Loss: 44.9322014837554\n",
      "Epoch 32 \t Batch 680 \t Training Loss: 44.92200803195729\n",
      "Epoch 32 \t Batch 700 \t Training Loss: 44.98460519518171\n",
      "Epoch 32 \t Batch 720 \t Training Loss: 45.00122128062778\n",
      "Epoch 32 \t Batch 740 \t Training Loss: 44.99843697934537\n",
      "Epoch 32 \t Batch 760 \t Training Loss: 45.000912104154885\n",
      "Epoch 32 \t Batch 780 \t Training Loss: 45.059851485032304\n",
      "Epoch 32 \t Batch 800 \t Training Loss: 45.08539116859436\n",
      "Epoch 32 \t Batch 820 \t Training Loss: 45.0810086506169\n",
      "Epoch 32 \t Batch 840 \t Training Loss: 45.05795520600819\n",
      "Epoch 32 \t Batch 860 \t Training Loss: 45.05648630275283\n",
      "Epoch 32 \t Batch 880 \t Training Loss: 45.033546564795756\n",
      "Epoch 32 \t Batch 900 \t Training Loss: 45.03114362504747\n",
      "Epoch 32 \t Batch 20 \t Validation Loss: 8.560287618637085\n",
      "Epoch 32 \t Batch 40 \t Validation Loss: 10.83880832195282\n",
      "Epoch 32 \t Batch 60 \t Validation Loss: 10.965637636184692\n",
      "Epoch 32 \t Batch 80 \t Validation Loss: 12.100291657447816\n",
      "Epoch 32 \t Batch 100 \t Validation Loss: 15.0596648979187\n",
      "Epoch 32 \t Batch 120 \t Validation Loss: 17.36895568370819\n",
      "Epoch 32 \t Batch 140 \t Validation Loss: 18.82618513788496\n",
      "Epoch 32 \t Batch 160 \t Validation Loss: 21.755270391702652\n",
      "Epoch 32 \t Batch 180 \t Validation Loss: 26.118624777264067\n",
      "Epoch 32 \t Batch 200 \t Validation Loss: 28.357364573478698\n",
      "Epoch 32 \t Batch 220 \t Validation Loss: 30.417990012602374\n",
      "Epoch 32 \t Batch 240 \t Validation Loss: 31.413975775241852\n",
      "Epoch 32 \t Batch 260 \t Validation Loss: 34.206136439396786\n",
      "Epoch 32 \t Batch 280 \t Validation Loss: 35.79461460113525\n",
      "Epoch 32 \t Batch 300 \t Validation Loss: 36.9992094039917\n",
      "Epoch 32 \t Batch 320 \t Validation Loss: 37.73509885370731\n",
      "Epoch 32 \t Batch 340 \t Validation Loss: 37.839670122371004\n",
      "Epoch 32 \t Batch 360 \t Validation Loss: 37.75286559263865\n",
      "Epoch 32 \t Batch 380 \t Validation Loss: 38.20945857951516\n",
      "Epoch 32 \t Batch 400 \t Validation Loss: 37.89586115121841\n",
      "Epoch 32 \t Batch 420 \t Validation Loss: 37.973679381325134\n",
      "Epoch 32 \t Batch 440 \t Validation Loss: 37.757668388973585\n",
      "Epoch 32 \t Batch 460 \t Validation Loss: 38.13689263592595\n",
      "Epoch 32 \t Batch 480 \t Validation Loss: 38.663600506385166\n",
      "Epoch 32 \t Batch 500 \t Validation Loss: 38.46861226081848\n",
      "Epoch 32 \t Batch 520 \t Validation Loss: 38.39752562779647\n",
      "Epoch 32 \t Batch 540 \t Validation Loss: 38.07270300653246\n",
      "Epoch 32 \t Batch 560 \t Validation Loss: 37.83748773336411\n",
      "Epoch 32 \t Batch 580 \t Validation Loss: 37.549033202796146\n",
      "Epoch 32 \t Batch 600 \t Validation Loss: 37.739695785840354\n",
      "Epoch 32 Training Loss: 45.0376331949182 Validation Loss: 38.37995584444566\n",
      "Epoch 32 completed\n",
      "Epoch 33 \t Batch 20 \t Training Loss: 43.227863502502444\n",
      "Epoch 33 \t Batch 40 \t Training Loss: 43.84828519821167\n",
      "Epoch 33 \t Batch 60 \t Training Loss: 44.24809379577637\n",
      "Epoch 33 \t Batch 80 \t Training Loss: 44.611957502365115\n",
      "Epoch 33 \t Batch 100 \t Training Loss: 44.56668663024902\n",
      "Epoch 33 \t Batch 120 \t Training Loss: 44.834083557128906\n",
      "Epoch 33 \t Batch 140 \t Training Loss: 44.73583592006138\n",
      "Epoch 33 \t Batch 160 \t Training Loss: 44.83326168060303\n",
      "Epoch 33 \t Batch 180 \t Training Loss: 44.76498629252116\n",
      "Epoch 33 \t Batch 200 \t Training Loss: 44.86451194763183\n",
      "Epoch 33 \t Batch 220 \t Training Loss: 44.85368659279563\n",
      "Epoch 33 \t Batch 240 \t Training Loss: 44.835305802027385\n",
      "Epoch 33 \t Batch 260 \t Training Loss: 44.851178418673\n",
      "Epoch 33 \t Batch 280 \t Training Loss: 44.76987939562116\n",
      "Epoch 33 \t Batch 300 \t Training Loss: 44.73841252644857\n",
      "Epoch 33 \t Batch 320 \t Training Loss: 44.749643778800966\n",
      "Epoch 33 \t Batch 340 \t Training Loss: 44.69789446662454\n",
      "Epoch 33 \t Batch 360 \t Training Loss: 44.712583255767825\n",
      "Epoch 33 \t Batch 380 \t Training Loss: 44.83537569547954\n",
      "Epoch 33 \t Batch 400 \t Training Loss: 44.80036725997925\n",
      "Epoch 33 \t Batch 420 \t Training Loss: 44.773471641540525\n",
      "Epoch 33 \t Batch 440 \t Training Loss: 44.82655311064287\n",
      "Epoch 33 \t Batch 460 \t Training Loss: 44.87593177297841\n",
      "Epoch 33 \t Batch 480 \t Training Loss: 44.8657717148463\n",
      "Epoch 33 \t Batch 500 \t Training Loss: 44.86275845336914\n",
      "Epoch 33 \t Batch 520 \t Training Loss: 44.88319552494929\n",
      "Epoch 33 \t Batch 540 \t Training Loss: 44.90088796262388\n",
      "Epoch 33 \t Batch 560 \t Training Loss: 44.870552621568955\n",
      "Epoch 33 \t Batch 580 \t Training Loss: 44.869607629447145\n",
      "Epoch 33 \t Batch 600 \t Training Loss: 44.86763864517212\n",
      "Epoch 33 \t Batch 620 \t Training Loss: 44.8866004513156\n",
      "Epoch 33 \t Batch 640 \t Training Loss: 44.882190144062044\n",
      "Epoch 33 \t Batch 660 \t Training Loss: 44.91370559461189\n",
      "Epoch 33 \t Batch 680 \t Training Loss: 44.91330623065724\n",
      "Epoch 33 \t Batch 700 \t Training Loss: 44.88069989340646\n",
      "Epoch 33 \t Batch 720 \t Training Loss: 44.89925434324476\n",
      "Epoch 33 \t Batch 740 \t Training Loss: 44.942064408998235\n",
      "Epoch 33 \t Batch 760 \t Training Loss: 44.92584063379388\n",
      "Epoch 33 \t Batch 780 \t Training Loss: 44.94764465429844\n",
      "Epoch 33 \t Batch 800 \t Training Loss: 44.932681069374084\n",
      "Epoch 33 \t Batch 820 \t Training Loss: 44.91995185293803\n",
      "Epoch 33 \t Batch 840 \t Training Loss: 44.890543519882925\n",
      "Epoch 33 \t Batch 860 \t Training Loss: 44.88232287473457\n",
      "Epoch 33 \t Batch 880 \t Training Loss: 44.891744847731154\n",
      "Epoch 33 \t Batch 900 \t Training Loss: 44.88665040334066\n",
      "Epoch 33 \t Batch 20 \t Validation Loss: 11.330526208877563\n",
      "Epoch 33 \t Batch 40 \t Validation Loss: 13.757210409641266\n",
      "Epoch 33 \t Batch 60 \t Validation Loss: 13.584269658724468\n",
      "Epoch 33 \t Batch 80 \t Validation Loss: 14.666587698459626\n",
      "Epoch 33 \t Batch 100 \t Validation Loss: 17.228480958938597\n",
      "Epoch 33 \t Batch 120 \t Validation Loss: 19.23826580842336\n",
      "Epoch 33 \t Batch 140 \t Validation Loss: 20.37459534917559\n",
      "Epoch 33 \t Batch 160 \t Validation Loss: 22.83081447482109\n",
      "Epoch 33 \t Batch 180 \t Validation Loss: 26.57627002927992\n",
      "Epoch 33 \t Batch 200 \t Validation Loss: 28.533486747741698\n",
      "Epoch 33 \t Batch 220 \t Validation Loss: 30.158914678747003\n",
      "Epoch 33 \t Batch 240 \t Validation Loss: 30.965138149261474\n",
      "Epoch 33 \t Batch 260 \t Validation Loss: 33.45088720321655\n",
      "Epoch 33 \t Batch 280 \t Validation Loss: 34.86399030004229\n",
      "Epoch 33 \t Batch 300 \t Validation Loss: 35.866800746917725\n",
      "Epoch 33 \t Batch 320 \t Validation Loss: 36.498741149902344\n",
      "Epoch 33 \t Batch 340 \t Validation Loss: 36.632807423086724\n",
      "Epoch 33 \t Batch 360 \t Validation Loss: 36.491104226642186\n",
      "Epoch 33 \t Batch 380 \t Validation Loss: 36.88293807381078\n",
      "Epoch 33 \t Batch 400 \t Validation Loss: 36.64676869392395\n",
      "Epoch 33 \t Batch 420 \t Validation Loss: 36.83419046856108\n",
      "Epoch 33 \t Batch 440 \t Validation Loss: 36.66216764883561\n",
      "Epoch 33 \t Batch 460 \t Validation Loss: 36.986732474617334\n",
      "Epoch 33 \t Batch 480 \t Validation Loss: 37.56370807488759\n",
      "Epoch 33 \t Batch 500 \t Validation Loss: 37.39322476959229\n",
      "Epoch 33 \t Batch 520 \t Validation Loss: 37.231393472964946\n",
      "Epoch 33 \t Batch 540 \t Validation Loss: 36.94378494333338\n",
      "Epoch 33 \t Batch 560 \t Validation Loss: 36.74396470274244\n",
      "Epoch 33 \t Batch 580 \t Validation Loss: 36.50669307379887\n",
      "Epoch 33 \t Batch 600 \t Validation Loss: 36.72679247538249\n",
      "Epoch 33 Training Loss: 44.91828463771741 Validation Loss: 37.376817006569404\n",
      "Epoch 33 completed\n",
      "Epoch 34 \t Batch 20 \t Training Loss: 44.84053630828858\n",
      "Epoch 34 \t Batch 40 \t Training Loss: 44.70417804718018\n",
      "Epoch 34 \t Batch 60 \t Training Loss: 44.575455538431804\n",
      "Epoch 34 \t Batch 80 \t Training Loss: 44.371832084655765\n",
      "Epoch 34 \t Batch 100 \t Training Loss: 44.307764854431156\n",
      "Epoch 34 \t Batch 120 \t Training Loss: 44.56354157129923\n",
      "Epoch 34 \t Batch 140 \t Training Loss: 44.414960397992814\n",
      "Epoch 34 \t Batch 160 \t Training Loss: 44.36793401241302\n",
      "Epoch 34 \t Batch 180 \t Training Loss: 44.318036524454754\n",
      "Epoch 34 \t Batch 200 \t Training Loss: 44.378873596191404\n",
      "Epoch 34 \t Batch 220 \t Training Loss: 44.41198959350586\n",
      "Epoch 34 \t Batch 240 \t Training Loss: 44.52773407300313\n",
      "Epoch 34 \t Batch 260 \t Training Loss: 44.56737110431378\n",
      "Epoch 34 \t Batch 280 \t Training Loss: 44.72601902825492\n",
      "Epoch 34 \t Batch 300 \t Training Loss: 44.72479951222738\n",
      "Epoch 34 \t Batch 320 \t Training Loss: 44.693291866779326\n",
      "Epoch 34 \t Batch 340 \t Training Loss: 44.804755940156824\n",
      "Epoch 34 \t Batch 360 \t Training Loss: 44.86227398978339\n",
      "Epoch 34 \t Batch 380 \t Training Loss: 44.89792310815108\n",
      "Epoch 34 \t Batch 400 \t Training Loss: 44.80088352203369\n",
      "Epoch 34 \t Batch 420 \t Training Loss: 44.82125443049839\n",
      "Epoch 34 \t Batch 440 \t Training Loss: 44.8084302035245\n",
      "Epoch 34 \t Batch 460 \t Training Loss: 44.8319498559703\n",
      "Epoch 34 \t Batch 480 \t Training Loss: 44.80359763304393\n",
      "Epoch 34 \t Batch 500 \t Training Loss: 44.85320233917236\n",
      "Epoch 34 \t Batch 520 \t Training Loss: 44.861034752772404\n",
      "Epoch 34 \t Batch 540 \t Training Loss: 44.820265261332196\n",
      "Epoch 34 \t Batch 560 \t Training Loss: 44.812836102076936\n",
      "Epoch 34 \t Batch 580 \t Training Loss: 44.77107894502837\n",
      "Epoch 34 \t Batch 600 \t Training Loss: 44.77646702448527\n",
      "Epoch 34 \t Batch 620 \t Training Loss: 44.79296639965427\n",
      "Epoch 34 \t Batch 640 \t Training Loss: 44.74310779571533\n",
      "Epoch 34 \t Batch 660 \t Training Loss: 44.76754070628773\n",
      "Epoch 34 \t Batch 680 \t Training Loss: 44.78626087974099\n",
      "Epoch 34 \t Batch 700 \t Training Loss: 44.79725275312151\n",
      "Epoch 34 \t Batch 720 \t Training Loss: 44.812815782758925\n",
      "Epoch 34 \t Batch 740 \t Training Loss: 44.817590687725996\n",
      "Epoch 34 \t Batch 760 \t Training Loss: 44.807956419493024\n",
      "Epoch 34 \t Batch 780 \t Training Loss: 44.78694223746275\n",
      "Epoch 34 \t Batch 800 \t Training Loss: 44.807973575592044\n",
      "Epoch 34 \t Batch 820 \t Training Loss: 44.761613883041754\n",
      "Epoch 34 \t Batch 840 \t Training Loss: 44.76883383251372\n",
      "Epoch 34 \t Batch 860 \t Training Loss: 44.75518774875375\n",
      "Epoch 34 \t Batch 880 \t Training Loss: 44.72355964400551\n",
      "Epoch 34 \t Batch 900 \t Training Loss: 44.726411471896704\n",
      "Epoch 34 \t Batch 20 \t Validation Loss: 12.337753129005431\n",
      "Epoch 34 \t Batch 40 \t Validation Loss: 15.913236808776855\n",
      "Epoch 34 \t Batch 60 \t Validation Loss: 15.312246624628703\n",
      "Epoch 34 \t Batch 80 \t Validation Loss: 16.27989199757576\n",
      "Epoch 34 \t Batch 100 \t Validation Loss: 19.126795296669005\n",
      "Epoch 34 \t Batch 120 \t Validation Loss: 21.069008847077686\n",
      "Epoch 34 \t Batch 140 \t Validation Loss: 22.12886576311929\n",
      "Epoch 34 \t Batch 160 \t Validation Loss: 24.547397699952125\n",
      "Epoch 34 \t Batch 180 \t Validation Loss: 28.532422312100728\n",
      "Epoch 34 \t Batch 200 \t Validation Loss: 30.425737545490264\n",
      "Epoch 34 \t Batch 220 \t Validation Loss: 32.19252313483845\n",
      "Epoch 34 \t Batch 240 \t Validation Loss: 33.00709149638812\n",
      "Epoch 34 \t Batch 260 \t Validation Loss: 35.6321247449288\n",
      "Epoch 34 \t Batch 280 \t Validation Loss: 37.08936826331275\n",
      "Epoch 34 \t Batch 300 \t Validation Loss: 38.14467099030813\n",
      "Epoch 34 \t Batch 320 \t Validation Loss: 38.696283645927906\n",
      "Epoch 34 \t Batch 340 \t Validation Loss: 38.688243654195\n",
      "Epoch 34 \t Batch 360 \t Validation Loss: 38.49723607301712\n",
      "Epoch 34 \t Batch 380 \t Validation Loss: 38.9326017894243\n",
      "Epoch 34 \t Batch 400 \t Validation Loss: 38.64893628239631\n",
      "Epoch 34 \t Batch 420 \t Validation Loss: 38.69849509398143\n",
      "Epoch 34 \t Batch 440 \t Validation Loss: 38.53887580633163\n",
      "Epoch 34 \t Batch 460 \t Validation Loss: 38.93990911919138\n",
      "Epoch 34 \t Batch 480 \t Validation Loss: 39.42492458919684\n",
      "Epoch 34 \t Batch 500 \t Validation Loss: 39.17882925891876\n",
      "Epoch 34 \t Batch 520 \t Validation Loss: 39.12527106816952\n",
      "Epoch 34 \t Batch 540 \t Validation Loss: 38.85193040017729\n",
      "Epoch 34 \t Batch 560 \t Validation Loss: 38.62835138610431\n",
      "Epoch 34 \t Batch 580 \t Validation Loss: 38.35469833982402\n",
      "Epoch 34 \t Batch 600 \t Validation Loss: 38.589419820308684\n",
      "Epoch 34 Training Loss: 44.77326738197385 Validation Loss: 39.19875410083053\n",
      "Epoch 34 completed\n",
      "Epoch 35 \t Batch 20 \t Training Loss: 45.233140754699704\n",
      "Epoch 35 \t Batch 40 \t Training Loss: 44.5647759437561\n",
      "Epoch 35 \t Batch 60 \t Training Loss: 44.34903831481934\n",
      "Epoch 35 \t Batch 80 \t Training Loss: 44.10142073631287\n",
      "Epoch 35 \t Batch 100 \t Training Loss: 44.00417091369629\n",
      "Epoch 35 \t Batch 120 \t Training Loss: 44.180243174235024\n",
      "Epoch 35 \t Batch 140 \t Training Loss: 43.981321116856165\n",
      "Epoch 35 \t Batch 160 \t Training Loss: 43.991076421737674\n",
      "Epoch 35 \t Batch 180 \t Training Loss: 43.89083819919162\n",
      "Epoch 35 \t Batch 200 \t Training Loss: 43.93779127120972\n",
      "Epoch 35 \t Batch 220 \t Training Loss: 44.02478920329701\n",
      "Epoch 35 \t Batch 240 \t Training Loss: 44.01098947525024\n",
      "Epoch 35 \t Batch 260 \t Training Loss: 43.959561685415416\n",
      "Epoch 35 \t Batch 280 \t Training Loss: 44.16859115873064\n",
      "Epoch 35 \t Batch 300 \t Training Loss: 44.24170495351156\n",
      "Epoch 35 \t Batch 320 \t Training Loss: 44.279767143726346\n",
      "Epoch 35 \t Batch 340 \t Training Loss: 44.28487746294807\n",
      "Epoch 35 \t Batch 360 \t Training Loss: 44.27854707505968\n",
      "Epoch 35 \t Batch 380 \t Training Loss: 44.29362191651997\n",
      "Epoch 35 \t Batch 400 \t Training Loss: 44.298058624267576\n",
      "Epoch 35 \t Batch 420 \t Training Loss: 44.38205031440371\n",
      "Epoch 35 \t Batch 440 \t Training Loss: 44.447954567995936\n",
      "Epoch 35 \t Batch 460 \t Training Loss: 44.48179074163022\n",
      "Epoch 35 \t Batch 480 \t Training Loss: 44.51802963415782\n",
      "Epoch 35 \t Batch 500 \t Training Loss: 44.52992448425293\n",
      "Epoch 35 \t Batch 520 \t Training Loss: 44.46336247370793\n",
      "Epoch 35 \t Batch 540 \t Training Loss: 44.502071098045064\n",
      "Epoch 35 \t Batch 560 \t Training Loss: 44.49542435918536\n",
      "Epoch 35 \t Batch 580 \t Training Loss: 44.55254358094314\n",
      "Epoch 35 \t Batch 600 \t Training Loss: 44.522849051157635\n",
      "Epoch 35 \t Batch 620 \t Training Loss: 44.51175585716001\n",
      "Epoch 35 \t Batch 640 \t Training Loss: 44.5483922958374\n",
      "Epoch 35 \t Batch 660 \t Training Loss: 44.53677407467004\n",
      "Epoch 35 \t Batch 680 \t Training Loss: 44.54685196035049\n",
      "Epoch 35 \t Batch 700 \t Training Loss: 44.5521766444615\n",
      "Epoch 35 \t Batch 720 \t Training Loss: 44.600401655832925\n",
      "Epoch 35 \t Batch 740 \t Training Loss: 44.631300575668746\n",
      "Epoch 35 \t Batch 760 \t Training Loss: 44.64671749315764\n",
      "Epoch 35 \t Batch 780 \t Training Loss: 44.647798660473946\n",
      "Epoch 35 \t Batch 800 \t Training Loss: 44.62470311164856\n",
      "Epoch 35 \t Batch 820 \t Training Loss: 44.629378193180735\n",
      "Epoch 35 \t Batch 840 \t Training Loss: 44.622624647049676\n",
      "Epoch 35 \t Batch 860 \t Training Loss: 44.60861653172692\n",
      "Epoch 35 \t Batch 880 \t Training Loss: 44.60399979678067\n",
      "Epoch 35 \t Batch 900 \t Training Loss: 44.6353705130683\n",
      "Epoch 35 \t Batch 20 \t Validation Loss: 9.676967525482178\n",
      "Epoch 35 \t Batch 40 \t Validation Loss: 12.688648998737335\n",
      "Epoch 35 \t Batch 60 \t Validation Loss: 12.529308009147645\n",
      "Epoch 35 \t Batch 80 \t Validation Loss: 14.018453395366668\n",
      "Epoch 35 \t Batch 100 \t Validation Loss: 16.87546977043152\n",
      "Epoch 35 \t Batch 120 \t Validation Loss: 19.05117311477661\n",
      "Epoch 35 \t Batch 140 \t Validation Loss: 20.32946241923741\n",
      "Epoch 35 \t Batch 160 \t Validation Loss: 23.09444363117218\n",
      "Epoch 35 \t Batch 180 \t Validation Loss: 27.546873993343777\n",
      "Epoch 35 \t Batch 200 \t Validation Loss: 29.794536666870115\n",
      "Epoch 35 \t Batch 220 \t Validation Loss: 31.745513369820333\n",
      "Epoch 35 \t Batch 240 \t Validation Loss: 32.69628001054128\n",
      "Epoch 35 \t Batch 260 \t Validation Loss: 35.43485816075252\n",
      "Epoch 35 \t Batch 280 \t Validation Loss: 36.9747551713671\n",
      "Epoch 35 \t Batch 300 \t Validation Loss: 38.208388589223226\n",
      "Epoch 35 \t Batch 320 \t Validation Loss: 38.89219718873501\n",
      "Epoch 35 \t Batch 340 \t Validation Loss: 38.94304788533379\n",
      "Epoch 35 \t Batch 360 \t Validation Loss: 38.842429184913634\n",
      "Epoch 35 \t Batch 380 \t Validation Loss: 39.22050343312715\n",
      "Epoch 35 \t Batch 400 \t Validation Loss: 38.82898471117019\n",
      "Epoch 35 \t Batch 420 \t Validation Loss: 38.8614479519072\n",
      "Epoch 35 \t Batch 440 \t Validation Loss: 38.57124854001132\n",
      "Epoch 35 \t Batch 460 \t Validation Loss: 38.8638524470122\n",
      "Epoch 35 \t Batch 480 \t Validation Loss: 39.36335147619248\n",
      "Epoch 35 \t Batch 500 \t Validation Loss: 39.14529195404053\n",
      "Epoch 35 \t Batch 520 \t Validation Loss: 39.02743738247798\n",
      "Epoch 35 \t Batch 540 \t Validation Loss: 38.73276009736238\n",
      "Epoch 35 \t Batch 560 \t Validation Loss: 38.58716960634504\n",
      "Epoch 35 \t Batch 580 \t Validation Loss: 38.422401441376785\n",
      "Epoch 35 \t Batch 600 \t Validation Loss: 38.619444653193156\n",
      "Epoch 35 Training Loss: 44.64407628599144 Validation Loss: 39.24642031533377\n",
      "Epoch 35 completed\n",
      "Epoch 36 \t Batch 20 \t Training Loss: 43.232429885864256\n",
      "Epoch 36 \t Batch 40 \t Training Loss: 44.4449405670166\n",
      "Epoch 36 \t Batch 60 \t Training Loss: 44.58480453491211\n",
      "Epoch 36 \t Batch 80 \t Training Loss: 44.4045590877533\n",
      "Epoch 36 \t Batch 100 \t Training Loss: 44.45663177490234\n",
      "Epoch 36 \t Batch 120 \t Training Loss: 44.33662687937419\n",
      "Epoch 36 \t Batch 140 \t Training Loss: 44.4161646434239\n",
      "Epoch 36 \t Batch 160 \t Training Loss: 44.21252565383911\n",
      "Epoch 36 \t Batch 180 \t Training Loss: 44.25279432932536\n",
      "Epoch 36 \t Batch 200 \t Training Loss: 44.31036405563354\n",
      "Epoch 36 \t Batch 220 \t Training Loss: 44.37526345686479\n",
      "Epoch 36 \t Batch 240 \t Training Loss: 44.2827722231547\n",
      "Epoch 36 \t Batch 260 \t Training Loss: 44.383742097707895\n",
      "Epoch 36 \t Batch 280 \t Training Loss: 44.44152371542794\n",
      "Epoch 36 \t Batch 300 \t Training Loss: 44.40672682444254\n",
      "Epoch 36 \t Batch 320 \t Training Loss: 44.43757290840149\n",
      "Epoch 36 \t Batch 340 \t Training Loss: 44.36081676483154\n",
      "Epoch 36 \t Batch 360 \t Training Loss: 44.240080780453155\n",
      "Epoch 36 \t Batch 380 \t Training Loss: 44.24260802018015\n",
      "Epoch 36 \t Batch 400 \t Training Loss: 44.2591166973114\n",
      "Epoch 36 \t Batch 420 \t Training Loss: 44.290596907479426\n",
      "Epoch 36 \t Batch 440 \t Training Loss: 44.30454146645286\n",
      "Epoch 36 \t Batch 460 \t Training Loss: 44.37089427450429\n",
      "Epoch 36 \t Batch 480 \t Training Loss: 44.377755268414816\n",
      "Epoch 36 \t Batch 500 \t Training Loss: 44.39718840789795\n",
      "Epoch 36 \t Batch 520 \t Training Loss: 44.354611206054685\n",
      "Epoch 36 \t Batch 540 \t Training Loss: 44.40826322061044\n",
      "Epoch 36 \t Batch 560 \t Training Loss: 44.4390592779432\n",
      "Epoch 36 \t Batch 580 \t Training Loss: 44.48056150633713\n",
      "Epoch 36 \t Batch 600 \t Training Loss: 44.46966364542643\n",
      "Epoch 36 \t Batch 620 \t Training Loss: 44.448420709179295\n",
      "Epoch 36 \t Batch 640 \t Training Loss: 44.44529929757118\n",
      "Epoch 36 \t Batch 660 \t Training Loss: 44.46790233958851\n",
      "Epoch 36 \t Batch 680 \t Training Loss: 44.47376338173361\n",
      "Epoch 36 \t Batch 700 \t Training Loss: 44.47826053074428\n",
      "Epoch 36 \t Batch 720 \t Training Loss: 44.509847990671794\n",
      "Epoch 36 \t Batch 740 \t Training Loss: 44.553147228344066\n",
      "Epoch 36 \t Batch 760 \t Training Loss: 44.53789399297614\n",
      "Epoch 36 \t Batch 780 \t Training Loss: 44.547350022731685\n",
      "Epoch 36 \t Batch 800 \t Training Loss: 44.53224627494812\n",
      "Epoch 36 \t Batch 820 \t Training Loss: 44.54902042295875\n",
      "Epoch 36 \t Batch 840 \t Training Loss: 44.53157031649635\n",
      "Epoch 36 \t Batch 860 \t Training Loss: 44.51071426036746\n",
      "Epoch 36 \t Batch 880 \t Training Loss: 44.50844515453685\n",
      "Epoch 36 \t Batch 900 \t Training Loss: 44.534172706604004\n",
      "Epoch 36 \t Batch 20 \t Validation Loss: 10.500446796417236\n",
      "Epoch 36 \t Batch 40 \t Validation Loss: 13.513489747047425\n",
      "Epoch 36 \t Batch 60 \t Validation Loss: 13.233303658167522\n",
      "Epoch 36 \t Batch 80 \t Validation Loss: 14.569114208221436\n",
      "Epoch 36 \t Batch 100 \t Validation Loss: 17.727467136383055\n",
      "Epoch 36 \t Batch 120 \t Validation Loss: 19.9215300877889\n",
      "Epoch 36 \t Batch 140 \t Validation Loss: 21.217088781084332\n",
      "Epoch 36 \t Batch 160 \t Validation Loss: 23.80469173192978\n",
      "Epoch 36 \t Batch 180 \t Validation Loss: 27.961720673243203\n",
      "Epoch 36 \t Batch 200 \t Validation Loss: 30.125159859657288\n",
      "Epoch 36 \t Batch 220 \t Validation Loss: 31.946065959063443\n",
      "Epoch 36 \t Batch 240 \t Validation Loss: 32.81476885875066\n",
      "Epoch 36 \t Batch 260 \t Validation Loss: 35.490750921689546\n",
      "Epoch 36 \t Batch 280 \t Validation Loss: 36.99465077263968\n",
      "Epoch 36 \t Batch 300 \t Validation Loss: 38.08317754745483\n",
      "Epoch 36 \t Batch 320 \t Validation Loss: 38.69447064399719\n",
      "Epoch 36 \t Batch 340 \t Validation Loss: 38.75944810755112\n",
      "Epoch 36 \t Batch 360 \t Validation Loss: 38.623748472001814\n",
      "Epoch 36 \t Batch 380 \t Validation Loss: 39.051941259283765\n",
      "Epoch 36 \t Batch 400 \t Validation Loss: 38.758993668556215\n",
      "Epoch 36 \t Batch 420 \t Validation Loss: 38.84214567911057\n",
      "Epoch 36 \t Batch 440 \t Validation Loss: 38.65822345126759\n",
      "Epoch 36 \t Batch 460 \t Validation Loss: 39.06619618042656\n",
      "Epoch 36 \t Batch 480 \t Validation Loss: 39.563770325978595\n",
      "Epoch 36 \t Batch 500 \t Validation Loss: 39.35375814437866\n",
      "Epoch 36 \t Batch 520 \t Validation Loss: 39.37226038712721\n",
      "Epoch 36 \t Batch 540 \t Validation Loss: 39.132146220737035\n",
      "Epoch 36 \t Batch 560 \t Validation Loss: 38.98344380174365\n",
      "Epoch 36 \t Batch 580 \t Validation Loss: 38.81439878529516\n",
      "Epoch 36 \t Batch 600 \t Validation Loss: 39.04606773376465\n",
      "Epoch 36 Training Loss: 44.49898411741434 Validation Loss: 39.687130429527976\n",
      "Epoch 36 completed\n",
      "Epoch 37 \t Batch 20 \t Training Loss: 42.61399536132812\n",
      "Epoch 37 \t Batch 40 \t Training Loss: 43.82942724227905\n",
      "Epoch 37 \t Batch 60 \t Training Loss: 44.0794153213501\n",
      "Epoch 37 \t Batch 80 \t Training Loss: 44.24471197128296\n",
      "Epoch 37 \t Batch 100 \t Training Loss: 43.92380096435547\n",
      "Epoch 37 \t Batch 120 \t Training Loss: 43.96834456125895\n",
      "Epoch 37 \t Batch 140 \t Training Loss: 44.05550662449428\n",
      "Epoch 37 \t Batch 160 \t Training Loss: 44.073528265953065\n",
      "Epoch 37 \t Batch 180 \t Training Loss: 44.0191712697347\n",
      "Epoch 37 \t Batch 200 \t Training Loss: 44.10448114395142\n",
      "Epoch 37 \t Batch 220 \t Training Loss: 44.07065103704279\n",
      "Epoch 37 \t Batch 240 \t Training Loss: 44.14712611834208\n",
      "Epoch 37 \t Batch 260 \t Training Loss: 44.196764373779295\n",
      "Epoch 37 \t Batch 280 \t Training Loss: 44.337725911821636\n",
      "Epoch 37 \t Batch 300 \t Training Loss: 44.27581703186035\n",
      "Epoch 37 \t Batch 320 \t Training Loss: 44.20471478700638\n",
      "Epoch 37 \t Batch 340 \t Training Loss: 44.16406101899989\n",
      "Epoch 37 \t Batch 360 \t Training Loss: 44.15749864578247\n",
      "Epoch 37 \t Batch 380 \t Training Loss: 44.18552869495593\n",
      "Epoch 37 \t Batch 400 \t Training Loss: 44.15311650276184\n",
      "Epoch 37 \t Batch 420 \t Training Loss: 44.2242861248198\n",
      "Epoch 37 \t Batch 440 \t Training Loss: 44.21194711164995\n",
      "Epoch 37 \t Batch 460 \t Training Loss: 44.21493196072786\n",
      "Epoch 37 \t Batch 480 \t Training Loss: 44.21475818157196\n",
      "Epoch 37 \t Batch 500 \t Training Loss: 44.16475371551514\n",
      "Epoch 37 \t Batch 520 \t Training Loss: 44.14848670959473\n",
      "Epoch 37 \t Batch 540 \t Training Loss: 44.1988559016475\n",
      "Epoch 37 \t Batch 560 \t Training Loss: 44.24437163216727\n",
      "Epoch 37 \t Batch 580 \t Training Loss: 44.2483535503519\n",
      "Epoch 37 \t Batch 600 \t Training Loss: 44.3053377087911\n",
      "Epoch 37 \t Batch 620 \t Training Loss: 44.34140983089324\n",
      "Epoch 37 \t Batch 640 \t Training Loss: 44.34809759259224\n",
      "Epoch 37 \t Batch 660 \t Training Loss: 44.3663910779086\n",
      "Epoch 37 \t Batch 680 \t Training Loss: 44.38827809165506\n",
      "Epoch 37 \t Batch 700 \t Training Loss: 44.4049138205392\n",
      "Epoch 37 \t Batch 720 \t Training Loss: 44.41291183895535\n",
      "Epoch 37 \t Batch 740 \t Training Loss: 44.39605795370566\n",
      "Epoch 37 \t Batch 760 \t Training Loss: 44.38002893548263\n",
      "Epoch 37 \t Batch 780 \t Training Loss: 44.37577522962521\n",
      "Epoch 37 \t Batch 800 \t Training Loss: 44.37845517158508\n",
      "Epoch 37 \t Batch 820 \t Training Loss: 44.36467073952279\n",
      "Epoch 37 \t Batch 840 \t Training Loss: 44.36070520764306\n",
      "Epoch 37 \t Batch 860 \t Training Loss: 44.39591501369033\n",
      "Epoch 37 \t Batch 880 \t Training Loss: 44.4052668094635\n",
      "Epoch 37 \t Batch 900 \t Training Loss: 44.39531467013889\n",
      "Epoch 37 \t Batch 20 \t Validation Loss: 8.439761209487916\n",
      "Epoch 37 \t Batch 40 \t Validation Loss: 11.909129023551941\n",
      "Epoch 37 \t Batch 60 \t Validation Loss: 11.669390074412028\n",
      "Epoch 37 \t Batch 80 \t Validation Loss: 13.099495857954025\n",
      "Epoch 37 \t Batch 100 \t Validation Loss: 15.945389494895934\n",
      "Epoch 37 \t Batch 120 \t Validation Loss: 18.154841808478036\n",
      "Epoch 37 \t Batch 140 \t Validation Loss: 19.482628280775888\n",
      "Epoch 37 \t Batch 160 \t Validation Loss: 22.4347246915102\n",
      "Epoch 37 \t Batch 180 \t Validation Loss: 26.969709261258444\n",
      "Epoch 37 \t Batch 200 \t Validation Loss: 29.341018488407133\n",
      "Epoch 37 \t Batch 220 \t Validation Loss: 31.358463003418663\n",
      "Epoch 37 \t Batch 240 \t Validation Loss: 32.33537910183271\n",
      "Epoch 37 \t Batch 260 \t Validation Loss: 35.06853594963367\n",
      "Epoch 37 \t Batch 280 \t Validation Loss: 36.61514566455568\n",
      "Epoch 37 \t Batch 300 \t Validation Loss: 37.887948629061384\n",
      "Epoch 37 \t Batch 320 \t Validation Loss: 38.5847309038043\n",
      "Epoch 37 \t Batch 340 \t Validation Loss: 38.67127520477071\n",
      "Epoch 37 \t Batch 360 \t Validation Loss: 38.62669253216849\n",
      "Epoch 37 \t Batch 380 \t Validation Loss: 39.03233674576408\n",
      "Epoch 37 \t Batch 400 \t Validation Loss: 38.70432093501091\n",
      "Epoch 37 \t Batch 420 \t Validation Loss: 38.794159720057536\n",
      "Epoch 37 \t Batch 440 \t Validation Loss: 38.520409032431516\n",
      "Epoch 37 \t Batch 460 \t Validation Loss: 38.813604876269466\n",
      "Epoch 37 \t Batch 480 \t Validation Loss: 39.34574571748575\n",
      "Epoch 37 \t Batch 500 \t Validation Loss: 39.13506389522553\n",
      "Epoch 37 \t Batch 520 \t Validation Loss: 38.98563888164667\n",
      "Epoch 37 \t Batch 540 \t Validation Loss: 38.700216762224834\n",
      "Epoch 37 \t Batch 560 \t Validation Loss: 38.524887377023695\n",
      "Epoch 37 \t Batch 580 \t Validation Loss: 38.275946925426354\n",
      "Epoch 37 \t Batch 600 \t Validation Loss: 38.48099436044693\n",
      "Epoch 37 Training Loss: 44.36061310846127 Validation Loss: 39.12099448659203\n",
      "Epoch 37 completed\n",
      "Epoch 38 \t Batch 20 \t Training Loss: 43.57896461486816\n",
      "Epoch 38 \t Batch 40 \t Training Loss: 43.75715847015381\n",
      "Epoch 38 \t Batch 60 \t Training Loss: 43.776122601826984\n",
      "Epoch 38 \t Batch 80 \t Training Loss: 44.294681787490845\n",
      "Epoch 38 \t Batch 100 \t Training Loss: 44.06821537017822\n",
      "Epoch 38 \t Batch 120 \t Training Loss: 44.232816791534425\n",
      "Epoch 38 \t Batch 140 \t Training Loss: 44.11637712206159\n",
      "Epoch 38 \t Batch 160 \t Training Loss: 43.91057047843933\n",
      "Epoch 38 \t Batch 180 \t Training Loss: 43.870139863755966\n",
      "Epoch 38 \t Batch 200 \t Training Loss: 43.80363832473755\n",
      "Epoch 38 \t Batch 220 \t Training Loss: 43.90323959697377\n",
      "Epoch 38 \t Batch 240 \t Training Loss: 43.939081033070885\n",
      "Epoch 38 \t Batch 260 \t Training Loss: 44.13163904043344\n",
      "Epoch 38 \t Batch 280 \t Training Loss: 44.086938285827635\n",
      "Epoch 38 \t Batch 300 \t Training Loss: 44.14910294850667\n",
      "Epoch 38 \t Batch 320 \t Training Loss: 44.126083731651306\n",
      "Epoch 38 \t Batch 340 \t Training Loss: 44.13620002970976\n",
      "Epoch 38 \t Batch 360 \t Training Loss: 44.18788763682048\n",
      "Epoch 38 \t Batch 380 \t Training Loss: 44.19198846315083\n",
      "Epoch 38 \t Batch 400 \t Training Loss: 44.16188877105713\n",
      "Epoch 38 \t Batch 420 \t Training Loss: 44.13871747879755\n",
      "Epoch 38 \t Batch 440 \t Training Loss: 44.16824913024902\n",
      "Epoch 38 \t Batch 460 \t Training Loss: 44.13663530764372\n",
      "Epoch 38 \t Batch 480 \t Training Loss: 44.1304615577062\n",
      "Epoch 38 \t Batch 500 \t Training Loss: 44.16927247619629\n",
      "Epoch 38 \t Batch 520 \t Training Loss: 44.14440977389996\n",
      "Epoch 38 \t Batch 540 \t Training Loss: 44.15700969696045\n",
      "Epoch 38 \t Batch 560 \t Training Loss: 44.193048620224\n",
      "Epoch 38 \t Batch 580 \t Training Loss: 44.16733557602455\n",
      "Epoch 38 \t Batch 600 \t Training Loss: 44.18983641306559\n",
      "Epoch 38 \t Batch 620 \t Training Loss: 44.1899654757592\n",
      "Epoch 38 \t Batch 640 \t Training Loss: 44.17840085029602\n",
      "Epoch 38 \t Batch 660 \t Training Loss: 44.219994400486804\n",
      "Epoch 38 \t Batch 680 \t Training Loss: 44.163890518861656\n",
      "Epoch 38 \t Batch 700 \t Training Loss: 44.12935003553118\n",
      "Epoch 38 \t Batch 720 \t Training Loss: 44.12858017285665\n",
      "Epoch 38 \t Batch 740 \t Training Loss: 44.109600577483306\n",
      "Epoch 38 \t Batch 760 \t Training Loss: 44.14989926187616\n",
      "Epoch 38 \t Batch 780 \t Training Loss: 44.155053554437096\n",
      "Epoch 38 \t Batch 800 \t Training Loss: 44.164272074699404\n",
      "Epoch 38 \t Batch 820 \t Training Loss: 44.17088897053788\n",
      "Epoch 38 \t Batch 840 \t Training Loss: 44.184961464291526\n",
      "Epoch 38 \t Batch 860 \t Training Loss: 44.19132866970328\n",
      "Epoch 38 \t Batch 880 \t Training Loss: 44.20412963953885\n",
      "Epoch 38 \t Batch 900 \t Training Loss: 44.19712412092421\n",
      "Epoch 38 \t Batch 20 \t Validation Loss: 10.69015438556671\n",
      "Epoch 38 \t Batch 40 \t Validation Loss: 14.35707186460495\n",
      "Epoch 38 \t Batch 60 \t Validation Loss: 13.68812500635783\n",
      "Epoch 38 \t Batch 80 \t Validation Loss: 14.866204404830933\n",
      "Epoch 38 \t Batch 100 \t Validation Loss: 17.800422420501707\n",
      "Epoch 38 \t Batch 120 \t Validation Loss: 19.822385772069296\n",
      "Epoch 38 \t Batch 140 \t Validation Loss: 21.088968999045235\n",
      "Epoch 38 \t Batch 160 \t Validation Loss: 23.64062043428421\n",
      "Epoch 38 \t Batch 180 \t Validation Loss: 27.30194443066915\n",
      "Epoch 38 \t Batch 200 \t Validation Loss: 29.095612988471984\n",
      "Epoch 38 \t Batch 220 \t Validation Loss: 30.70383838740262\n",
      "Epoch 38 \t Batch 240 \t Validation Loss: 31.468877152601877\n",
      "Epoch 38 \t Batch 260 \t Validation Loss: 33.96435927244333\n",
      "Epoch 38 \t Batch 280 \t Validation Loss: 35.3770557982581\n",
      "Epoch 38 \t Batch 300 \t Validation Loss: 36.31195153872172\n",
      "Epoch 38 \t Batch 320 \t Validation Loss: 36.89494217038155\n",
      "Epoch 38 \t Batch 340 \t Validation Loss: 36.97247792412253\n",
      "Epoch 38 \t Batch 360 \t Validation Loss: 36.8747910340627\n",
      "Epoch 38 \t Batch 380 \t Validation Loss: 37.373300547348826\n",
      "Epoch 38 \t Batch 400 \t Validation Loss: 37.23454966545105\n",
      "Epoch 38 \t Batch 420 \t Validation Loss: 37.35656404495239\n",
      "Epoch 38 \t Batch 440 \t Validation Loss: 37.3004479451613\n",
      "Epoch 38 \t Batch 460 \t Validation Loss: 37.79090494487597\n",
      "Epoch 38 \t Batch 480 \t Validation Loss: 38.34820113579432\n",
      "Epoch 38 \t Batch 500 \t Validation Loss: 38.15219686126709\n",
      "Epoch 38 \t Batch 520 \t Validation Loss: 38.254714976824246\n",
      "Epoch 38 \t Batch 540 \t Validation Loss: 38.023694850780345\n",
      "Epoch 38 \t Batch 560 \t Validation Loss: 37.8682384456907\n",
      "Epoch 38 \t Batch 580 \t Validation Loss: 37.714118112366776\n",
      "Epoch 38 \t Batch 600 \t Validation Loss: 37.953063071568806\n",
      "Epoch 38 Training Loss: 44.22415700265599 Validation Loss: 38.60176989320037\n",
      "Epoch 38 completed\n",
      "Epoch 39 \t Batch 20 \t Training Loss: 42.87435188293457\n",
      "Epoch 39 \t Batch 40 \t Training Loss: 42.631411743164065\n",
      "Epoch 39 \t Batch 60 \t Training Loss: 43.078627332051596\n",
      "Epoch 39 \t Batch 80 \t Training Loss: 43.468982744216916\n",
      "Epoch 39 \t Batch 100 \t Training Loss: 43.40961051940918\n",
      "Epoch 39 \t Batch 120 \t Training Loss: 43.4894110361735\n",
      "Epoch 39 \t Batch 140 \t Training Loss: 43.59315749577114\n",
      "Epoch 39 \t Batch 160 \t Training Loss: 43.598058748245236\n",
      "Epoch 39 \t Batch 180 \t Training Loss: 43.696402888827855\n",
      "Epoch 39 \t Batch 200 \t Training Loss: 43.827472019195554\n",
      "Epoch 39 \t Batch 220 \t Training Loss: 43.702962736649944\n",
      "Epoch 39 \t Batch 240 \t Training Loss: 43.67289633750916\n",
      "Epoch 39 \t Batch 260 \t Training Loss: 43.74943455916185\n",
      "Epoch 39 \t Batch 280 \t Training Loss: 43.692195538112095\n",
      "Epoch 39 \t Batch 300 \t Training Loss: 43.690467071533206\n",
      "Epoch 39 \t Batch 320 \t Training Loss: 43.79338397979736\n",
      "Epoch 39 \t Batch 340 \t Training Loss: 43.77230070899515\n",
      "Epoch 39 \t Batch 360 \t Training Loss: 43.807706907060414\n",
      "Epoch 39 \t Batch 380 \t Training Loss: 43.77998479541979\n",
      "Epoch 39 \t Batch 400 \t Training Loss: 43.75849006652832\n",
      "Epoch 39 \t Batch 420 \t Training Loss: 43.792325782775876\n",
      "Epoch 39 \t Batch 440 \t Training Loss: 43.77911722009832\n",
      "Epoch 39 \t Batch 460 \t Training Loss: 43.79569162285846\n",
      "Epoch 39 \t Batch 480 \t Training Loss: 43.80764451821645\n",
      "Epoch 39 \t Batch 500 \t Training Loss: 43.85144929504394\n",
      "Epoch 39 \t Batch 520 \t Training Loss: 43.87024692388681\n",
      "Epoch 39 \t Batch 540 \t Training Loss: 43.885429325810186\n",
      "Epoch 39 \t Batch 560 \t Training Loss: 43.8439303125654\n",
      "Epoch 39 \t Batch 580 \t Training Loss: 43.89850464524894\n",
      "Epoch 39 \t Batch 600 \t Training Loss: 43.90078223546346\n",
      "Epoch 39 \t Batch 620 \t Training Loss: 43.91156249507781\n",
      "Epoch 39 \t Batch 640 \t Training Loss: 43.95141307115555\n",
      "Epoch 39 \t Batch 660 \t Training Loss: 43.98007597489791\n",
      "Epoch 39 \t Batch 680 \t Training Loss: 43.95700340831981\n",
      "Epoch 39 \t Batch 700 \t Training Loss: 43.95114426204137\n",
      "Epoch 39 \t Batch 720 \t Training Loss: 43.965132066938615\n",
      "Epoch 39 \t Batch 740 \t Training Loss: 44.01229211961901\n",
      "Epoch 39 \t Batch 760 \t Training Loss: 44.062608377557055\n",
      "Epoch 39 \t Batch 780 \t Training Loss: 44.04831021137726\n",
      "Epoch 39 \t Batch 800 \t Training Loss: 44.06760029315949\n",
      "Epoch 39 \t Batch 820 \t Training Loss: 44.0661406400727\n",
      "Epoch 39 \t Batch 840 \t Training Loss: 44.03605937503633\n",
      "Epoch 39 \t Batch 860 \t Training Loss: 44.002574423856515\n",
      "Epoch 39 \t Batch 880 \t Training Loss: 43.993101848255506\n",
      "Epoch 39 \t Batch 900 \t Training Loss: 44.02248632642958\n",
      "Epoch 39 \t Batch 20 \t Validation Loss: 11.472377347946168\n",
      "Epoch 39 \t Batch 40 \t Validation Loss: 13.471479260921479\n",
      "Epoch 39 \t Batch 60 \t Validation Loss: 13.417550905545552\n",
      "Epoch 39 \t Batch 80 \t Validation Loss: 14.611031025648117\n",
      "Epoch 39 \t Batch 100 \t Validation Loss: 17.840107607841492\n",
      "Epoch 39 \t Batch 120 \t Validation Loss: 20.13480521440506\n",
      "Epoch 39 \t Batch 140 \t Validation Loss: 21.30959439618247\n",
      "Epoch 39 \t Batch 160 \t Validation Loss: 23.640438637137414\n",
      "Epoch 39 \t Batch 180 \t Validation Loss: 27.105016085836624\n",
      "Epoch 39 \t Batch 200 \t Validation Loss: 28.835074684619904\n",
      "Epoch 39 \t Batch 220 \t Validation Loss: 30.260986165566877\n",
      "Epoch 39 \t Batch 240 \t Validation Loss: 30.925932655731838\n",
      "Epoch 39 \t Batch 260 \t Validation Loss: 33.35299129669483\n",
      "Epoch 39 \t Batch 280 \t Validation Loss: 34.749359803540365\n",
      "Epoch 39 \t Batch 300 \t Validation Loss: 35.580428649584455\n",
      "Epoch 39 \t Batch 320 \t Validation Loss: 36.07472651451826\n",
      "Epoch 39 \t Batch 340 \t Validation Loss: 36.1583749925389\n",
      "Epoch 39 \t Batch 360 \t Validation Loss: 35.982080728477904\n",
      "Epoch 39 \t Batch 380 \t Validation Loss: 36.45301882969706\n",
      "Epoch 39 \t Batch 400 \t Validation Loss: 36.30385115981102\n",
      "Epoch 39 \t Batch 420 \t Validation Loss: 36.43857825937725\n",
      "Epoch 39 \t Batch 440 \t Validation Loss: 36.36737853722139\n",
      "Epoch 39 \t Batch 460 \t Validation Loss: 36.832607056783594\n",
      "Epoch 39 \t Batch 480 \t Validation Loss: 37.36241968770822\n",
      "Epoch 39 \t Batch 500 \t Validation Loss: 37.17148227214813\n",
      "Epoch 39 \t Batch 520 \t Validation Loss: 37.200095329834866\n",
      "Epoch 39 \t Batch 540 \t Validation Loss: 36.98161225053999\n",
      "Epoch 39 \t Batch 560 \t Validation Loss: 36.842826907123836\n",
      "Epoch 39 \t Batch 580 \t Validation Loss: 36.61730752402338\n",
      "Epoch 39 \t Batch 600 \t Validation Loss: 36.88485848983129\n",
      "Epoch 39 Training Loss: 44.054723071948004 Validation Loss: 37.53821315006776\n",
      "Epoch 39 completed\n",
      "Epoch 40 \t Batch 20 \t Training Loss: 43.43146190643311\n",
      "Epoch 40 \t Batch 40 \t Training Loss: 43.284648418426514\n",
      "Epoch 40 \t Batch 60 \t Training Loss: 43.23681151072184\n",
      "Epoch 40 \t Batch 80 \t Training Loss: 43.247577047348024\n",
      "Epoch 40 \t Batch 100 \t Training Loss: 43.60116310119629\n",
      "Epoch 40 \t Batch 120 \t Training Loss: 43.69331248601278\n",
      "Epoch 40 \t Batch 140 \t Training Loss: 43.714129502432684\n",
      "Epoch 40 \t Batch 160 \t Training Loss: 43.70970823764801\n",
      "Epoch 40 \t Batch 180 \t Training Loss: 43.77487097846137\n",
      "Epoch 40 \t Batch 200 \t Training Loss: 43.77188106536865\n",
      "Epoch 40 \t Batch 220 \t Training Loss: 43.92221965789795\n",
      "Epoch 40 \t Batch 240 \t Training Loss: 43.913772424062095\n",
      "Epoch 40 \t Batch 260 \t Training Loss: 43.90843574817364\n",
      "Epoch 40 \t Batch 280 \t Training Loss: 43.86374953133719\n",
      "Epoch 40 \t Batch 300 \t Training Loss: 43.75332204182943\n",
      "Epoch 40 \t Batch 320 \t Training Loss: 43.72411202192306\n",
      "Epoch 40 \t Batch 340 \t Training Loss: 43.756103212693155\n",
      "Epoch 40 \t Batch 360 \t Training Loss: 43.7114600499471\n",
      "Epoch 40 \t Batch 380 \t Training Loss: 43.70352103584691\n",
      "Epoch 40 \t Batch 400 \t Training Loss: 43.697616834640506\n",
      "Epoch 40 \t Batch 420 \t Training Loss: 43.67766657329741\n",
      "Epoch 40 \t Batch 440 \t Training Loss: 43.685570907592776\n",
      "Epoch 40 \t Batch 460 \t Training Loss: 43.69522363413935\n",
      "Epoch 40 \t Batch 480 \t Training Loss: 43.72567150592804\n",
      "Epoch 40 \t Batch 500 \t Training Loss: 43.73581672668457\n",
      "Epoch 40 \t Batch 520 \t Training Loss: 43.7977662893442\n",
      "Epoch 40 \t Batch 540 \t Training Loss: 43.760842612937644\n",
      "Epoch 40 \t Batch 560 \t Training Loss: 43.78965873718262\n",
      "Epoch 40 \t Batch 580 \t Training Loss: 43.79995294110528\n",
      "Epoch 40 \t Batch 600 \t Training Loss: 43.81922912597656\n",
      "Epoch 40 \t Batch 620 \t Training Loss: 43.808380126953125\n",
      "Epoch 40 \t Batch 640 \t Training Loss: 43.81386926174164\n",
      "Epoch 40 \t Batch 660 \t Training Loss: 43.793690484942815\n",
      "Epoch 40 \t Batch 680 \t Training Loss: 43.830189833921544\n",
      "Epoch 40 \t Batch 700 \t Training Loss: 43.81967189243861\n",
      "Epoch 40 \t Batch 720 \t Training Loss: 43.80335570971171\n",
      "Epoch 40 \t Batch 740 \t Training Loss: 43.80437421540956\n",
      "Epoch 40 \t Batch 760 \t Training Loss: 43.83457659671181\n",
      "Epoch 40 \t Batch 780 \t Training Loss: 43.85799233363225\n",
      "Epoch 40 \t Batch 800 \t Training Loss: 43.83597725868225\n",
      "Epoch 40 \t Batch 820 \t Training Loss: 43.83483538743926\n",
      "Epoch 40 \t Batch 840 \t Training Loss: 43.84828515279861\n",
      "Epoch 40 \t Batch 860 \t Training Loss: 43.850603449621865\n",
      "Epoch 40 \t Batch 880 \t Training Loss: 43.871061693538316\n",
      "Epoch 40 \t Batch 900 \t Training Loss: 43.84116639455159\n",
      "Epoch 40 \t Batch 20 \t Validation Loss: 11.743961763381957\n",
      "Epoch 40 \t Batch 40 \t Validation Loss: 14.559731364250183\n",
      "Epoch 40 \t Batch 60 \t Validation Loss: 14.015671332677206\n",
      "Epoch 40 \t Batch 80 \t Validation Loss: 15.091951960325241\n",
      "Epoch 40 \t Batch 100 \t Validation Loss: 18.028441042900084\n",
      "Epoch 40 \t Batch 120 \t Validation Loss: 20.0346208691597\n",
      "Epoch 40 \t Batch 140 \t Validation Loss: 21.164589878490993\n",
      "Epoch 40 \t Batch 160 \t Validation Loss: 23.62775748670101\n",
      "Epoch 40 \t Batch 180 \t Validation Loss: 27.154457222090826\n",
      "Epoch 40 \t Batch 200 \t Validation Loss: 28.904190151691438\n",
      "Epoch 40 \t Batch 220 \t Validation Loss: 30.330479858138343\n",
      "Epoch 40 \t Batch 240 \t Validation Loss: 30.991711419820785\n",
      "Epoch 40 \t Batch 260 \t Validation Loss: 33.368628646777225\n",
      "Epoch 40 \t Batch 280 \t Validation Loss: 34.73264161007745\n",
      "Epoch 40 \t Batch 300 \t Validation Loss: 35.66539728323619\n",
      "Epoch 40 \t Batch 320 \t Validation Loss: 36.26662755161524\n",
      "Epoch 40 \t Batch 340 \t Validation Loss: 36.38932065543006\n",
      "Epoch 40 \t Batch 360 \t Validation Loss: 36.29229778846105\n",
      "Epoch 40 \t Batch 380 \t Validation Loss: 36.80454204584423\n",
      "Epoch 40 \t Batch 400 \t Validation Loss: 36.82175971150398\n",
      "Epoch 40 \t Batch 420 \t Validation Loss: 37.051645861353194\n",
      "Epoch 40 \t Batch 440 \t Validation Loss: 37.21212974136526\n",
      "Epoch 40 \t Batch 460 \t Validation Loss: 37.82345539279606\n",
      "Epoch 40 \t Batch 480 \t Validation Loss: 38.40066617031892\n",
      "Epoch 40 \t Batch 500 \t Validation Loss: 38.24025679111481\n",
      "Epoch 40 \t Batch 520 \t Validation Loss: 38.451472558425024\n",
      "Epoch 40 \t Batch 540 \t Validation Loss: 38.26425989733802\n",
      "Epoch 40 \t Batch 560 \t Validation Loss: 38.11455086384501\n",
      "Epoch 40 \t Batch 580 \t Validation Loss: 37.88673358210202\n",
      "Epoch 40 \t Batch 600 \t Validation Loss: 38.16277318080267\n",
      "Epoch 40 Training Loss: 43.88536074273329 Validation Loss: 38.81340829195914\n",
      "Epoch 40 completed\n",
      "Epoch 41 \t Batch 20 \t Training Loss: 41.99471168518066\n",
      "Epoch 41 \t Batch 40 \t Training Loss: 42.5056113243103\n",
      "Epoch 41 \t Batch 60 \t Training Loss: 42.95675888061523\n",
      "Epoch 41 \t Batch 80 \t Training Loss: 42.80278868675232\n",
      "Epoch 41 \t Batch 100 \t Training Loss: 42.94194896697998\n",
      "Epoch 41 \t Batch 120 \t Training Loss: 42.93169600168864\n",
      "Epoch 41 \t Batch 140 \t Training Loss: 42.92266927446638\n",
      "Epoch 41 \t Batch 160 \t Training Loss: 43.074381828308105\n",
      "Epoch 41 \t Batch 180 \t Training Loss: 43.19373029073079\n",
      "Epoch 41 \t Batch 200 \t Training Loss: 43.26955707550049\n",
      "Epoch 41 \t Batch 220 \t Training Loss: 43.34280343489213\n",
      "Epoch 41 \t Batch 240 \t Training Loss: 43.360924243927\n",
      "Epoch 41 \t Batch 260 \t Training Loss: 43.379904835040755\n",
      "Epoch 41 \t Batch 280 \t Training Loss: 43.359871115003315\n",
      "Epoch 41 \t Batch 300 \t Training Loss: 43.3328779220581\n",
      "Epoch 41 \t Batch 320 \t Training Loss: 43.27742810249329\n",
      "Epoch 41 \t Batch 340 \t Training Loss: 43.25838428946103\n",
      "Epoch 41 \t Batch 360 \t Training Loss: 43.30806698269314\n",
      "Epoch 41 \t Batch 380 \t Training Loss: 43.38237422140021\n",
      "Epoch 41 \t Batch 400 \t Training Loss: 43.39148240089416\n",
      "Epoch 41 \t Batch 420 \t Training Loss: 43.40420401436942\n",
      "Epoch 41 \t Batch 440 \t Training Loss: 43.380435501445426\n",
      "Epoch 41 \t Batch 460 \t Training Loss: 43.407820369886316\n",
      "Epoch 41 \t Batch 480 \t Training Loss: 43.40089892546336\n",
      "Epoch 41 \t Batch 500 \t Training Loss: 43.411451805114744\n",
      "Epoch 41 \t Batch 520 \t Training Loss: 43.38333076330332\n",
      "Epoch 41 \t Batch 540 \t Training Loss: 43.38342919526277\n",
      "Epoch 41 \t Batch 560 \t Training Loss: 43.38781500543867\n",
      "Epoch 41 \t Batch 580 \t Training Loss: 43.38894127155172\n",
      "Epoch 41 \t Batch 600 \t Training Loss: 43.42701383590698\n",
      "Epoch 41 \t Batch 620 \t Training Loss: 43.472430702947804\n",
      "Epoch 41 \t Batch 640 \t Training Loss: 43.48083748221397\n",
      "Epoch 41 \t Batch 660 \t Training Loss: 43.46271442644524\n",
      "Epoch 41 \t Batch 680 \t Training Loss: 43.46820995106417\n",
      "Epoch 41 \t Batch 700 \t Training Loss: 43.489806197030205\n",
      "Epoch 41 \t Batch 720 \t Training Loss: 43.54870556195577\n",
      "Epoch 41 \t Batch 740 \t Training Loss: 43.5972186165887\n",
      "Epoch 41 \t Batch 760 \t Training Loss: 43.609665664873624\n",
      "Epoch 41 \t Batch 780 \t Training Loss: 43.61583151695056\n",
      "Epoch 41 \t Batch 800 \t Training Loss: 43.63251107215881\n",
      "Epoch 41 \t Batch 820 \t Training Loss: 43.67314058629478\n",
      "Epoch 41 \t Batch 840 \t Training Loss: 43.69296135675339\n",
      "Epoch 41 \t Batch 860 \t Training Loss: 43.71906562627748\n",
      "Epoch 41 \t Batch 880 \t Training Loss: 43.70280508128079\n",
      "Epoch 41 \t Batch 900 \t Training Loss: 43.69822129143609\n",
      "Epoch 41 \t Batch 20 \t Validation Loss: 8.505490064620972\n",
      "Epoch 41 \t Batch 40 \t Validation Loss: 11.97997088432312\n",
      "Epoch 41 \t Batch 60 \t Validation Loss: 11.754560724894207\n",
      "Epoch 41 \t Batch 80 \t Validation Loss: 12.94156373143196\n",
      "Epoch 41 \t Batch 100 \t Validation Loss: 15.863258481025696\n",
      "Epoch 41 \t Batch 120 \t Validation Loss: 18.02841415007909\n",
      "Epoch 41 \t Batch 140 \t Validation Loss: 19.42391997405461\n",
      "Epoch 41 \t Batch 160 \t Validation Loss: 22.129299768805502\n",
      "Epoch 41 \t Batch 180 \t Validation Loss: 25.940968187650046\n",
      "Epoch 41 \t Batch 200 \t Validation Loss: 28.133363373279572\n",
      "Epoch 41 \t Batch 220 \t Validation Loss: 29.805771838534962\n",
      "Epoch 41 \t Batch 240 \t Validation Loss: 30.59393464922905\n",
      "Epoch 41 \t Batch 260 \t Validation Loss: 33.26466272977682\n",
      "Epoch 41 \t Batch 280 \t Validation Loss: 34.776589926651546\n",
      "Epoch 41 \t Batch 300 \t Validation Loss: 35.74817169348399\n",
      "Epoch 41 \t Batch 320 \t Validation Loss: 36.332740946114065\n",
      "Epoch 41 \t Batch 340 \t Validation Loss: 36.45216447746053\n",
      "Epoch 41 \t Batch 360 \t Validation Loss: 36.35527749194039\n",
      "Epoch 41 \t Batch 380 \t Validation Loss: 36.93339025221373\n",
      "Epoch 41 \t Batch 400 \t Validation Loss: 36.90161868214607\n",
      "Epoch 41 \t Batch 420 \t Validation Loss: 37.09134856973375\n",
      "Epoch 41 \t Batch 440 \t Validation Loss: 37.20775276855989\n",
      "Epoch 41 \t Batch 460 \t Validation Loss: 37.77202050063921\n",
      "Epoch 41 \t Batch 480 \t Validation Loss: 38.323977393905324\n",
      "Epoch 41 \t Batch 500 \t Validation Loss: 38.122680636405946\n",
      "Epoch 41 \t Batch 520 \t Validation Loss: 38.379403544389284\n",
      "Epoch 41 \t Batch 540 \t Validation Loss: 38.21590720370964\n",
      "Epoch 41 \t Batch 560 \t Validation Loss: 38.13528137973377\n",
      "Epoch 41 \t Batch 580 \t Validation Loss: 38.09425428735799\n",
      "Epoch 41 \t Batch 600 \t Validation Loss: 38.380541582902275\n",
      "Epoch 41 Training Loss: 43.712683911671945 Validation Loss: 39.09634677852903\n",
      "Epoch 41 completed\n",
      "Epoch 42 \t Batch 20 \t Training Loss: 43.57921867370605\n",
      "Epoch 42 \t Batch 40 \t Training Loss: 43.13663129806518\n",
      "Epoch 42 \t Batch 60 \t Training Loss: 43.578192710876465\n",
      "Epoch 42 \t Batch 80 \t Training Loss: 43.678460931777956\n",
      "Epoch 42 \t Batch 100 \t Training Loss: 43.56103569030762\n",
      "Epoch 42 \t Batch 120 \t Training Loss: 43.162902641296384\n",
      "Epoch 42 \t Batch 140 \t Training Loss: 43.16231441497803\n",
      "Epoch 42 \t Batch 160 \t Training Loss: 43.32900717258453\n",
      "Epoch 42 \t Batch 180 \t Training Loss: 43.11220230526394\n",
      "Epoch 42 \t Batch 200 \t Training Loss: 43.11531511306762\n",
      "Epoch 42 \t Batch 220 \t Training Loss: 43.186812175403944\n",
      "Epoch 42 \t Batch 240 \t Training Loss: 43.20284627278646\n",
      "Epoch 42 \t Batch 260 \t Training Loss: 43.194409047640285\n",
      "Epoch 42 \t Batch 280 \t Training Loss: 43.166133117675784\n",
      "Epoch 42 \t Batch 300 \t Training Loss: 43.20712895711263\n",
      "Epoch 42 \t Batch 320 \t Training Loss: 43.215808963775636\n",
      "Epoch 42 \t Batch 340 \t Training Loss: 43.178127883462345\n",
      "Epoch 42 \t Batch 360 \t Training Loss: 43.15479146109687\n",
      "Epoch 42 \t Batch 380 \t Training Loss: 43.28378430416709\n",
      "Epoch 42 \t Batch 400 \t Training Loss: 43.31515439987183\n",
      "Epoch 42 \t Batch 420 \t Training Loss: 43.36467464083717\n",
      "Epoch 42 \t Batch 440 \t Training Loss: 43.3561637618325\n",
      "Epoch 42 \t Batch 460 \t Training Loss: 43.399657299207604\n",
      "Epoch 42 \t Batch 480 \t Training Loss: 43.392935609817506\n",
      "Epoch 42 \t Batch 500 \t Training Loss: 43.374828010559085\n",
      "Epoch 42 \t Batch 520 \t Training Loss: 43.408776628054106\n",
      "Epoch 42 \t Batch 540 \t Training Loss: 43.37393018228036\n",
      "Epoch 42 \t Batch 560 \t Training Loss: 43.38345294679914\n",
      "Epoch 42 \t Batch 580 \t Training Loss: 43.36223804868501\n",
      "Epoch 42 \t Batch 600 \t Training Loss: 43.3800714302063\n",
      "Epoch 42 \t Batch 620 \t Training Loss: 43.39924489913448\n",
      "Epoch 42 \t Batch 640 \t Training Loss: 43.38995127677917\n",
      "Epoch 42 \t Batch 660 \t Training Loss: 43.38371710921779\n",
      "Epoch 42 \t Batch 680 \t Training Loss: 43.375815082998834\n",
      "Epoch 42 \t Batch 700 \t Training Loss: 43.36149657658168\n",
      "Epoch 42 \t Batch 720 \t Training Loss: 43.38019352489047\n",
      "Epoch 42 \t Batch 740 \t Training Loss: 43.41514261091078\n",
      "Epoch 42 \t Batch 760 \t Training Loss: 43.428617060811895\n",
      "Epoch 42 \t Batch 780 \t Training Loss: 43.422011443896174\n",
      "Epoch 42 \t Batch 800 \t Training Loss: 43.43681235313416\n",
      "Epoch 42 \t Batch 820 \t Training Loss: 43.46177195572272\n",
      "Epoch 42 \t Batch 840 \t Training Loss: 43.46570463180542\n",
      "Epoch 42 \t Batch 860 \t Training Loss: 43.47693165623865\n",
      "Epoch 42 \t Batch 880 \t Training Loss: 43.48072842684659\n",
      "Epoch 42 \t Batch 900 \t Training Loss: 43.49615292867025\n",
      "Epoch 42 \t Batch 20 \t Validation Loss: 10.973095965385436\n",
      "Epoch 42 \t Batch 40 \t Validation Loss: 14.96607768535614\n",
      "Epoch 42 \t Batch 60 \t Validation Loss: 14.260585927963257\n",
      "Epoch 42 \t Batch 80 \t Validation Loss: 15.558702397346497\n",
      "Epoch 42 \t Batch 100 \t Validation Loss: 18.84152256011963\n",
      "Epoch 42 \t Batch 120 \t Validation Loss: 21.026949747403464\n",
      "Epoch 42 \t Batch 140 \t Validation Loss: 22.189483281544277\n",
      "Epoch 42 \t Batch 160 \t Validation Loss: 24.8611978828907\n",
      "Epoch 42 \t Batch 180 \t Validation Loss: 28.602858622868855\n",
      "Epoch 42 \t Batch 200 \t Validation Loss: 30.522561812400816\n",
      "Epoch 42 \t Batch 220 \t Validation Loss: 32.19039628722451\n",
      "Epoch 42 \t Batch 240 \t Validation Loss: 32.915715873241425\n",
      "Epoch 42 \t Batch 260 \t Validation Loss: 35.46233134269714\n",
      "Epoch 42 \t Batch 280 \t Validation Loss: 36.88139178412301\n",
      "Epoch 42 \t Batch 300 \t Validation Loss: 37.85392615000407\n",
      "Epoch 42 \t Batch 320 \t Validation Loss: 38.43553176224232\n",
      "Epoch 42 \t Batch 340 \t Validation Loss: 38.46921264143551\n",
      "Epoch 42 \t Batch 360 \t Validation Loss: 38.3781718439526\n",
      "Epoch 42 \t Batch 380 \t Validation Loss: 38.86488750608344\n",
      "Epoch 42 \t Batch 400 \t Validation Loss: 38.65820469141006\n",
      "Epoch 42 \t Batch 420 \t Validation Loss: 38.80193598156884\n",
      "Epoch 42 \t Batch 440 \t Validation Loss: 38.72558191039345\n",
      "Epoch 42 \t Batch 460 \t Validation Loss: 39.196430272641386\n",
      "Epoch 42 \t Batch 480 \t Validation Loss: 39.708446423212685\n",
      "Epoch 42 \t Batch 500 \t Validation Loss: 39.50371796798706\n",
      "Epoch 42 \t Batch 520 \t Validation Loss: 39.64384197455186\n",
      "Epoch 42 \t Batch 540 \t Validation Loss: 39.332303252043545\n",
      "Epoch 42 \t Batch 560 \t Validation Loss: 39.09006849697658\n",
      "Epoch 42 \t Batch 580 \t Validation Loss: 38.80061419256802\n",
      "Epoch 42 \t Batch 600 \t Validation Loss: 38.96562411308289\n",
      "Epoch 42 Training Loss: 43.5336398267174 Validation Loss: 39.568097250802175\n",
      "Epoch 42 completed\n",
      "Epoch 43 \t Batch 20 \t Training Loss: 43.57883167266846\n",
      "Epoch 43 \t Batch 40 \t Training Loss: 42.7899263381958\n",
      "Epoch 43 \t Batch 60 \t Training Loss: 43.053015518188474\n",
      "Epoch 43 \t Batch 80 \t Training Loss: 43.261254501342776\n",
      "Epoch 43 \t Batch 100 \t Training Loss: 43.429855270385744\n",
      "Epoch 43 \t Batch 120 \t Training Loss: 43.256375058492026\n",
      "Epoch 43 \t Batch 140 \t Training Loss: 43.25697784423828\n",
      "Epoch 43 \t Batch 160 \t Training Loss: 43.17204923629761\n",
      "Epoch 43 \t Batch 180 \t Training Loss: 43.17306209140354\n",
      "Epoch 43 \t Batch 200 \t Training Loss: 43.131738109588625\n",
      "Epoch 43 \t Batch 220 \t Training Loss: 43.253072478554465\n",
      "Epoch 43 \t Batch 240 \t Training Loss: 43.2055771668752\n",
      "Epoch 43 \t Batch 260 \t Training Loss: 43.2654586498554\n",
      "Epoch 43 \t Batch 280 \t Training Loss: 43.255046708243235\n",
      "Epoch 43 \t Batch 300 \t Training Loss: 43.1641242090861\n",
      "Epoch 43 \t Batch 320 \t Training Loss: 43.11536209583282\n",
      "Epoch 43 \t Batch 340 \t Training Loss: 43.126727496876434\n",
      "Epoch 43 \t Batch 360 \t Training Loss: 43.192514197031656\n",
      "Epoch 43 \t Batch 380 \t Training Loss: 43.28959358616879\n",
      "Epoch 43 \t Batch 400 \t Training Loss: 43.29707416534424\n",
      "Epoch 43 \t Batch 420 \t Training Loss: 43.31904754638672\n",
      "Epoch 43 \t Batch 440 \t Training Loss: 43.3458005731756\n",
      "Epoch 43 \t Batch 460 \t Training Loss: 43.3846821328868\n",
      "Epoch 43 \t Batch 480 \t Training Loss: 43.36935990651448\n",
      "Epoch 43 \t Batch 500 \t Training Loss: 43.40172021484375\n",
      "Epoch 43 \t Batch 520 \t Training Loss: 43.36149939757127\n",
      "Epoch 43 \t Batch 540 \t Training Loss: 43.405602850737395\n",
      "Epoch 43 \t Batch 560 \t Training Loss: 43.43633904457092\n",
      "Epoch 43 \t Batch 580 \t Training Loss: 43.41391312829379\n",
      "Epoch 43 \t Batch 600 \t Training Loss: 43.372788174947104\n",
      "Epoch 43 \t Batch 620 \t Training Loss: 43.31824441109934\n",
      "Epoch 43 \t Batch 640 \t Training Loss: 43.280034857988355\n",
      "Epoch 43 \t Batch 660 \t Training Loss: 43.28433224071156\n",
      "Epoch 43 \t Batch 680 \t Training Loss: 43.34461350721472\n",
      "Epoch 43 \t Batch 700 \t Training Loss: 43.34403094700404\n",
      "Epoch 43 \t Batch 720 \t Training Loss: 43.35710645251804\n",
      "Epoch 43 \t Batch 740 \t Training Loss: 43.38274617066254\n",
      "Epoch 43 \t Batch 760 \t Training Loss: 43.383583505530105\n",
      "Epoch 43 \t Batch 780 \t Training Loss: 43.38838839408679\n",
      "Epoch 43 \t Batch 800 \t Training Loss: 43.37947683811188\n",
      "Epoch 43 \t Batch 820 \t Training Loss: 43.3375168404928\n",
      "Epoch 43 \t Batch 840 \t Training Loss: 43.359040192195344\n",
      "Epoch 43 \t Batch 860 \t Training Loss: 43.33242400856905\n",
      "Epoch 43 \t Batch 880 \t Training Loss: 43.31182761192322\n",
      "Epoch 43 \t Batch 900 \t Training Loss: 43.33572745853\n",
      "Epoch 43 \t Batch 20 \t Validation Loss: 9.610188245773315\n",
      "Epoch 43 \t Batch 40 \t Validation Loss: 12.579866790771485\n",
      "Epoch 43 \t Batch 60 \t Validation Loss: 12.493864957491557\n",
      "Epoch 43 \t Batch 80 \t Validation Loss: 14.064160495996475\n",
      "Epoch 43 \t Batch 100 \t Validation Loss: 17.31943510532379\n",
      "Epoch 43 \t Batch 120 \t Validation Loss: 19.443241441249846\n",
      "Epoch 43 \t Batch 140 \t Validation Loss: 20.910846304893493\n",
      "Epoch 43 \t Batch 160 \t Validation Loss: 23.860554626584054\n",
      "Epoch 43 \t Batch 180 \t Validation Loss: 27.25700427426232\n",
      "Epoch 43 \t Batch 200 \t Validation Loss: 29.370899012088774\n",
      "Epoch 43 \t Batch 220 \t Validation Loss: 30.798949252475392\n",
      "Epoch 43 \t Batch 240 \t Validation Loss: 31.414500031868617\n",
      "Epoch 43 \t Batch 260 \t Validation Loss: 33.89790425483997\n",
      "Epoch 43 \t Batch 280 \t Validation Loss: 35.26460713488715\n",
      "Epoch 43 \t Batch 300 \t Validation Loss: 36.16405052661896\n",
      "Epoch 43 \t Batch 320 \t Validation Loss: 36.75224268883467\n",
      "Epoch 43 \t Batch 340 \t Validation Loss: 36.95405660937814\n",
      "Epoch 43 \t Batch 360 \t Validation Loss: 36.95207300053703\n",
      "Epoch 43 \t Batch 380 \t Validation Loss: 37.684925258787054\n",
      "Epoch 43 \t Batch 400 \t Validation Loss: 37.98697113394737\n",
      "Epoch 43 \t Batch 420 \t Validation Loss: 38.31622413340069\n",
      "Epoch 43 \t Batch 440 \t Validation Loss: 38.88397849581458\n",
      "Epoch 43 \t Batch 460 \t Validation Loss: 39.798835094078726\n",
      "Epoch 43 \t Batch 480 \t Validation Loss: 40.397655593355495\n",
      "Epoch 43 \t Batch 500 \t Validation Loss: 40.27440408611297\n",
      "Epoch 43 \t Batch 520 \t Validation Loss: 40.98162621259689\n",
      "Epoch 43 \t Batch 540 \t Validation Loss: 40.77428324222565\n",
      "Epoch 43 \t Batch 560 \t Validation Loss: 40.635236664329256\n",
      "Epoch 43 \t Batch 580 \t Validation Loss: 40.52225118094477\n",
      "Epoch 43 \t Batch 600 \t Validation Loss: 40.732679407596585\n",
      "Epoch 43 Training Loss: 43.31725512206099 Validation Loss: 41.37437479139923\n",
      "Epoch 43 completed\n",
      "Epoch 44 \t Batch 20 \t Training Loss: 44.15455951690674\n",
      "Epoch 44 \t Batch 40 \t Training Loss: 43.479371547698975\n",
      "Epoch 44 \t Batch 60 \t Training Loss: 43.227454821268715\n",
      "Epoch 44 \t Batch 80 \t Training Loss: 43.172547340393066\n",
      "Epoch 44 \t Batch 100 \t Training Loss: 42.94407062530517\n",
      "Epoch 44 \t Batch 120 \t Training Loss: 42.89141556421916\n",
      "Epoch 44 \t Batch 140 \t Training Loss: 42.888038635253906\n",
      "Epoch 44 \t Batch 160 \t Training Loss: 42.898037505149844\n",
      "Epoch 44 \t Batch 180 \t Training Loss: 42.981317117479115\n",
      "Epoch 44 \t Batch 200 \t Training Loss: 42.85566175460816\n",
      "Epoch 44 \t Batch 220 \t Training Loss: 42.74015300057151\n",
      "Epoch 44 \t Batch 240 \t Training Loss: 42.7094695409139\n",
      "Epoch 44 \t Batch 260 \t Training Loss: 42.65707324101375\n",
      "Epoch 44 \t Batch 280 \t Training Loss: 42.644744750431606\n",
      "Epoch 44 \t Batch 300 \t Training Loss: 42.61682422637939\n",
      "Epoch 44 \t Batch 320 \t Training Loss: 42.6979917049408\n",
      "Epoch 44 \t Batch 340 \t Training Loss: 42.754701356326834\n",
      "Epoch 44 \t Batch 360 \t Training Loss: 42.742327721913654\n",
      "Epoch 44 \t Batch 380 \t Training Loss: 42.68548342052259\n",
      "Epoch 44 \t Batch 400 \t Training Loss: 42.68074360847473\n",
      "Epoch 44 \t Batch 420 \t Training Loss: 42.72591361999512\n",
      "Epoch 44 \t Batch 440 \t Training Loss: 42.68556345159357\n",
      "Epoch 44 \t Batch 460 \t Training Loss: 42.681469875833265\n",
      "Epoch 44 \t Batch 480 \t Training Loss: 42.72438244024912\n",
      "Epoch 44 \t Batch 500 \t Training Loss: 42.770925071716306\n",
      "Epoch 44 \t Batch 520 \t Training Loss: 42.762739518972545\n",
      "Epoch 44 \t Batch 540 \t Training Loss: 42.799300292686176\n",
      "Epoch 44 \t Batch 560 \t Training Loss: 42.816658408301215\n",
      "Epoch 44 \t Batch 580 \t Training Loss: 42.89953706675562\n",
      "Epoch 44 \t Batch 600 \t Training Loss: 42.92564859390259\n",
      "Epoch 44 \t Batch 620 \t Training Loss: 42.95661449432373\n",
      "Epoch 44 \t Batch 640 \t Training Loss: 42.980571472644804\n",
      "Epoch 44 \t Batch 660 \t Training Loss: 42.97403306672067\n",
      "Epoch 44 \t Batch 680 \t Training Loss: 42.97896628660314\n",
      "Epoch 44 \t Batch 700 \t Training Loss: 42.96325331551688\n",
      "Epoch 44 \t Batch 720 \t Training Loss: 42.98625277413262\n",
      "Epoch 44 \t Batch 740 \t Training Loss: 42.981239963222194\n",
      "Epoch 44 \t Batch 760 \t Training Loss: 42.98047066236797\n",
      "Epoch 44 \t Batch 780 \t Training Loss: 42.985133503644896\n",
      "Epoch 44 \t Batch 800 \t Training Loss: 43.020019397735595\n",
      "Epoch 44 \t Batch 820 \t Training Loss: 43.08583655473662\n",
      "Epoch 44 \t Batch 840 \t Training Loss: 43.08751545860654\n",
      "Epoch 44 \t Batch 860 \t Training Loss: 43.08995288139166\n",
      "Epoch 44 \t Batch 880 \t Training Loss: 43.11170934763822\n",
      "Epoch 44 \t Batch 900 \t Training Loss: 43.1059534157647\n",
      "Epoch 44 \t Batch 20 \t Validation Loss: 12.996430802345277\n",
      "Epoch 44 \t Batch 40 \t Validation Loss: 17.937626683712004\n",
      "Epoch 44 \t Batch 60 \t Validation Loss: 16.25617520014445\n",
      "Epoch 44 \t Batch 80 \t Validation Loss: 17.065791612863542\n",
      "Epoch 44 \t Batch 100 \t Validation Loss: 19.454634079933168\n",
      "Epoch 44 \t Batch 120 \t Validation Loss: 21.24718288977941\n",
      "Epoch 44 \t Batch 140 \t Validation Loss: 22.214021570341927\n",
      "Epoch 44 \t Batch 160 \t Validation Loss: 24.43634739816189\n",
      "Epoch 44 \t Batch 180 \t Validation Loss: 27.781013290087383\n",
      "Epoch 44 \t Batch 200 \t Validation Loss: 29.525177738666535\n",
      "Epoch 44 \t Batch 220 \t Validation Loss: 30.997785423018716\n",
      "Epoch 44 \t Batch 240 \t Validation Loss: 31.632241052389144\n",
      "Epoch 44 \t Batch 260 \t Validation Loss: 34.096651181807886\n",
      "Epoch 44 \t Batch 280 \t Validation Loss: 35.508114649568284\n",
      "Epoch 44 \t Batch 300 \t Validation Loss: 36.25584095478058\n",
      "Epoch 44 \t Batch 320 \t Validation Loss: 36.72638805359602\n",
      "Epoch 44 \t Batch 340 \t Validation Loss: 36.77814585040598\n",
      "Epoch 44 \t Batch 360 \t Validation Loss: 36.61458430157767\n",
      "Epoch 44 \t Batch 380 \t Validation Loss: 37.042032637094195\n",
      "Epoch 44 \t Batch 400 \t Validation Loss: 36.84493893504143\n",
      "Epoch 44 \t Batch 420 \t Validation Loss: 36.95572976044246\n",
      "Epoch 44 \t Batch 440 \t Validation Loss: 36.816811135682194\n",
      "Epoch 44 \t Batch 460 \t Validation Loss: 37.22590027166449\n",
      "Epoch 44 \t Batch 480 \t Validation Loss: 37.77382546563943\n",
      "Epoch 44 \t Batch 500 \t Validation Loss: 37.55633600521087\n",
      "Epoch 44 \t Batch 520 \t Validation Loss: 37.51988365008281\n",
      "Epoch 44 \t Batch 540 \t Validation Loss: 37.32199649016062\n",
      "Epoch 44 \t Batch 560 \t Validation Loss: 37.182434980358394\n",
      "Epoch 44 \t Batch 580 \t Validation Loss: 37.027452852808196\n",
      "Epoch 44 \t Batch 600 \t Validation Loss: 37.305531310240426\n",
      "Epoch 44 Training Loss: 43.134545150473954 Validation Loss: 37.96714747958369\n",
      "Epoch 44 completed\n",
      "Epoch 45 \t Batch 20 \t Training Loss: 41.45420875549316\n",
      "Epoch 45 \t Batch 40 \t Training Loss: 42.4981014251709\n",
      "Epoch 45 \t Batch 60 \t Training Loss: 42.29157911936442\n",
      "Epoch 45 \t Batch 80 \t Training Loss: 42.259422063827515\n",
      "Epoch 45 \t Batch 100 \t Training Loss: 42.34988307952881\n",
      "Epoch 45 \t Batch 120 \t Training Loss: 42.28829533259074\n",
      "Epoch 45 \t Batch 140 \t Training Loss: 42.37200036730085\n",
      "Epoch 45 \t Batch 160 \t Training Loss: 42.20590724945068\n",
      "Epoch 45 \t Batch 180 \t Training Loss: 42.37491885291205\n",
      "Epoch 45 \t Batch 200 \t Training Loss: 42.53472511291504\n",
      "Epoch 45 \t Batch 220 \t Training Loss: 42.46438364549117\n",
      "Epoch 45 \t Batch 240 \t Training Loss: 42.44659938812256\n",
      "Epoch 45 \t Batch 260 \t Training Loss: 42.582673542316144\n",
      "Epoch 45 \t Batch 280 \t Training Loss: 42.55302200317383\n",
      "Epoch 45 \t Batch 300 \t Training Loss: 42.57121083577474\n",
      "Epoch 45 \t Batch 320 \t Training Loss: 42.60324054956436\n",
      "Epoch 45 \t Batch 340 \t Training Loss: 42.60214563257554\n",
      "Epoch 45 \t Batch 360 \t Training Loss: 42.64283583958944\n",
      "Epoch 45 \t Batch 380 \t Training Loss: 42.627388713234346\n",
      "Epoch 45 \t Batch 400 \t Training Loss: 42.63946557998657\n",
      "Epoch 45 \t Batch 420 \t Training Loss: 42.64092423575265\n",
      "Epoch 45 \t Batch 440 \t Training Loss: 42.69101992520419\n",
      "Epoch 45 \t Batch 460 \t Training Loss: 42.68705534727677\n",
      "Epoch 45 \t Batch 480 \t Training Loss: 42.65701123873393\n",
      "Epoch 45 \t Batch 500 \t Training Loss: 42.706534873962404\n",
      "Epoch 45 \t Batch 520 \t Training Loss: 42.654571925676784\n",
      "Epoch 45 \t Batch 540 \t Training Loss: 42.73412454039962\n",
      "Epoch 45 \t Batch 560 \t Training Loss: 42.7036418403898\n",
      "Epoch 45 \t Batch 580 \t Training Loss: 42.686236634747736\n",
      "Epoch 45 \t Batch 600 \t Training Loss: 42.71586567878723\n",
      "Epoch 45 \t Batch 620 \t Training Loss: 42.753559004875925\n",
      "Epoch 45 \t Batch 640 \t Training Loss: 42.78582211434841\n",
      "Epoch 45 \t Batch 660 \t Training Loss: 42.78824083443844\n",
      "Epoch 45 \t Batch 680 \t Training Loss: 42.804433331770056\n",
      "Epoch 45 \t Batch 700 \t Training Loss: 42.859082562582834\n",
      "Epoch 45 \t Batch 720 \t Training Loss: 42.85405377017127\n",
      "Epoch 45 \t Batch 740 \t Training Loss: 42.839404969602015\n",
      "Epoch 45 \t Batch 760 \t Training Loss: 42.82973889551665\n",
      "Epoch 45 \t Batch 780 \t Training Loss: 42.82097466542171\n",
      "Epoch 45 \t Batch 800 \t Training Loss: 42.85342265367508\n",
      "Epoch 45 \t Batch 820 \t Training Loss: 42.87744078054661\n",
      "Epoch 45 \t Batch 840 \t Training Loss: 42.896343133563086\n",
      "Epoch 45 \t Batch 860 \t Training Loss: 42.92215379005255\n",
      "Epoch 45 \t Batch 880 \t Training Loss: 42.92989559823816\n",
      "Epoch 45 \t Batch 900 \t Training Loss: 42.90557703018189\n",
      "Epoch 45 \t Batch 20 \t Validation Loss: 6.910051786899567\n",
      "Epoch 45 \t Batch 40 \t Validation Loss: 9.610142177343368\n",
      "Epoch 45 \t Batch 60 \t Validation Loss: 9.780411128203074\n",
      "Epoch 45 \t Batch 80 \t Validation Loss: 11.364904603362083\n",
      "Epoch 45 \t Batch 100 \t Validation Loss: 14.874905383586883\n",
      "Epoch 45 \t Batch 120 \t Validation Loss: 17.34262209534645\n",
      "Epoch 45 \t Batch 140 \t Validation Loss: 18.848718811784472\n",
      "Epoch 45 \t Batch 160 \t Validation Loss: 21.62819527834654\n",
      "Epoch 45 \t Batch 180 \t Validation Loss: 25.56628841691547\n",
      "Epoch 45 \t Batch 200 \t Validation Loss: 27.728153866529464\n",
      "Epoch 45 \t Batch 220 \t Validation Loss: 29.48243867592378\n",
      "Epoch 45 \t Batch 240 \t Validation Loss: 30.335883393883705\n",
      "Epoch 45 \t Batch 260 \t Validation Loss: 32.95359627191837\n",
      "Epoch 45 \t Batch 280 \t Validation Loss: 34.47363199761936\n",
      "Epoch 45 \t Batch 300 \t Validation Loss: 35.485752222538\n",
      "Epoch 45 \t Batch 320 \t Validation Loss: 36.13230998143554\n",
      "Epoch 45 \t Batch 340 \t Validation Loss: 36.304196443978476\n",
      "Epoch 45 \t Batch 360 \t Validation Loss: 36.212388861841625\n",
      "Epoch 45 \t Batch 380 \t Validation Loss: 36.7332645133922\n",
      "Epoch 45 \t Batch 400 \t Validation Loss: 36.5924633616209\n",
      "Epoch 45 \t Batch 420 \t Validation Loss: 36.76727997405188\n",
      "Epoch 45 \t Batch 440 \t Validation Loss: 36.69764220443639\n",
      "Epoch 45 \t Batch 460 \t Validation Loss: 37.14722596821578\n",
      "Epoch 45 \t Batch 480 \t Validation Loss: 37.71093096286059\n",
      "Epoch 45 \t Batch 500 \t Validation Loss: 37.5383333697319\n",
      "Epoch 45 \t Batch 520 \t Validation Loss: 37.597560060482756\n",
      "Epoch 45 \t Batch 540 \t Validation Loss: 37.38365853583371\n",
      "Epoch 45 \t Batch 560 \t Validation Loss: 37.255711725354196\n",
      "Epoch 45 \t Batch 580 \t Validation Loss: 37.06409801902442\n",
      "Epoch 45 \t Batch 600 \t Validation Loss: 37.33184872587522\n",
      "Epoch 45 Training Loss: 42.91413651219891 Validation Loss: 38.006780286500984\n",
      "Epoch 45 completed\n",
      "Epoch 46 \t Batch 20 \t Training Loss: 43.04875087738037\n",
      "Epoch 46 \t Batch 40 \t Training Loss: 42.383648014068605\n",
      "Epoch 46 \t Batch 60 \t Training Loss: 42.322364489237465\n",
      "Epoch 46 \t Batch 80 \t Training Loss: 42.86040554046631\n",
      "Epoch 46 \t Batch 100 \t Training Loss: 42.708520126342776\n",
      "Epoch 46 \t Batch 120 \t Training Loss: 42.530024687449135\n",
      "Epoch 46 \t Batch 140 \t Training Loss: 42.4834290095738\n",
      "Epoch 46 \t Batch 160 \t Training Loss: 42.37101495265961\n",
      "Epoch 46 \t Batch 180 \t Training Loss: 42.48172906239827\n",
      "Epoch 46 \t Batch 200 \t Training Loss: 42.43951210021973\n",
      "Epoch 46 \t Batch 220 \t Training Loss: 42.39043482000177\n",
      "Epoch 46 \t Batch 240 \t Training Loss: 42.41568587621053\n",
      "Epoch 46 \t Batch 260 \t Training Loss: 42.44881266080416\n",
      "Epoch 46 \t Batch 280 \t Training Loss: 42.49170019967215\n",
      "Epoch 46 \t Batch 300 \t Training Loss: 42.474129867553714\n",
      "Epoch 46 \t Batch 320 \t Training Loss: 42.4797483086586\n",
      "Epoch 46 \t Batch 340 \t Training Loss: 42.529584054385914\n",
      "Epoch 46 \t Batch 360 \t Training Loss: 42.524873903062606\n",
      "Epoch 46 \t Batch 380 \t Training Loss: 42.59131873281378\n",
      "Epoch 46 \t Batch 400 \t Training Loss: 42.56876987457275\n",
      "Epoch 46 \t Batch 420 \t Training Loss: 42.56440694899786\n",
      "Epoch 46 \t Batch 440 \t Training Loss: 42.58607886054299\n",
      "Epoch 46 \t Batch 460 \t Training Loss: 42.64595009347667\n",
      "Epoch 46 \t Batch 480 \t Training Loss: 42.62426666418711\n",
      "Epoch 46 \t Batch 500 \t Training Loss: 42.55529029083252\n",
      "Epoch 46 \t Batch 520 \t Training Loss: 42.54564696091872\n",
      "Epoch 46 \t Batch 540 \t Training Loss: 42.589432504442\n",
      "Epoch 46 \t Batch 560 \t Training Loss: 42.638622583661764\n",
      "Epoch 46 \t Batch 580 \t Training Loss: 42.595879153547614\n",
      "Epoch 46 \t Batch 600 \t Training Loss: 42.6395320892334\n",
      "Epoch 46 \t Batch 620 \t Training Loss: 42.64134271683231\n",
      "Epoch 46 \t Batch 640 \t Training Loss: 42.65934107899666\n",
      "Epoch 46 \t Batch 660 \t Training Loss: 42.62993503339363\n",
      "Epoch 46 \t Batch 680 \t Training Loss: 42.61969562979306\n",
      "Epoch 46 \t Batch 700 \t Training Loss: 42.650552389962336\n",
      "Epoch 46 \t Batch 720 \t Training Loss: 42.66325116687351\n",
      "Epoch 46 \t Batch 740 \t Training Loss: 42.70013348862931\n",
      "Epoch 46 \t Batch 760 \t Training Loss: 42.68701473537244\n",
      "Epoch 46 \t Batch 780 \t Training Loss: 42.706294421660594\n",
      "Epoch 46 \t Batch 800 \t Training Loss: 42.7273025226593\n",
      "Epoch 46 \t Batch 820 \t Training Loss: 42.70806462357684\n",
      "Epoch 46 \t Batch 840 \t Training Loss: 42.71284480321975\n",
      "Epoch 46 \t Batch 860 \t Training Loss: 42.72015639903933\n",
      "Epoch 46 \t Batch 880 \t Training Loss: 42.70181801535866\n",
      "Epoch 46 \t Batch 900 \t Training Loss: 42.702018864949544\n",
      "Epoch 46 \t Batch 20 \t Validation Loss: 12.061127209663391\n",
      "Epoch 46 \t Batch 40 \t Validation Loss: 16.05019083023071\n",
      "Epoch 46 \t Batch 60 \t Validation Loss: 15.096008117993673\n",
      "Epoch 46 \t Batch 80 \t Validation Loss: 16.482614105939867\n",
      "Epoch 46 \t Batch 100 \t Validation Loss: 19.523457503318788\n",
      "Epoch 46 \t Batch 120 \t Validation Loss: 21.586291642983756\n",
      "Epoch 46 \t Batch 140 \t Validation Loss: 22.631315990856717\n",
      "Epoch 46 \t Batch 160 \t Validation Loss: 24.94549694955349\n",
      "Epoch 46 \t Batch 180 \t Validation Loss: 28.396561582883198\n",
      "Epoch 46 \t Batch 200 \t Validation Loss: 30.241430218219758\n",
      "Epoch 46 \t Batch 220 \t Validation Loss: 31.720181640711697\n",
      "Epoch 46 \t Batch 240 \t Validation Loss: 32.358482577403386\n",
      "Epoch 46 \t Batch 260 \t Validation Loss: 34.77898762226105\n",
      "Epoch 46 \t Batch 280 \t Validation Loss: 36.110785847050806\n",
      "Epoch 46 \t Batch 300 \t Validation Loss: 36.958686257998146\n",
      "Epoch 46 \t Batch 320 \t Validation Loss: 37.46150213629007\n",
      "Epoch 46 \t Batch 340 \t Validation Loss: 37.511220075102415\n",
      "Epoch 46 \t Batch 360 \t Validation Loss: 37.33506188525094\n",
      "Epoch 46 \t Batch 380 \t Validation Loss: 37.78153817151722\n",
      "Epoch 46 \t Batch 400 \t Validation Loss: 37.5618601000309\n",
      "Epoch 46 \t Batch 420 \t Validation Loss: 37.673171671231586\n",
      "Epoch 46 \t Batch 440 \t Validation Loss: 37.55617024790157\n",
      "Epoch 46 \t Batch 460 \t Validation Loss: 38.04566535638726\n",
      "Epoch 46 \t Batch 480 \t Validation Loss: 38.56287754873435\n",
      "Epoch 46 \t Batch 500 \t Validation Loss: 38.362567368507385\n",
      "Epoch 46 \t Batch 520 \t Validation Loss: 38.45705350087239\n",
      "Epoch 46 \t Batch 540 \t Validation Loss: 38.23040833384903\n",
      "Epoch 46 \t Batch 560 \t Validation Loss: 38.09831924864224\n",
      "Epoch 46 \t Batch 580 \t Validation Loss: 37.89632520100166\n",
      "Epoch 46 \t Batch 600 \t Validation Loss: 38.13624568065008\n",
      "Epoch 46 Training Loss: 42.720011325282904 Validation Loss: 38.823715771173504\n",
      "Epoch 46 completed\n",
      "Epoch 47 \t Batch 20 \t Training Loss: 40.66376075744629\n",
      "Epoch 47 \t Batch 40 \t Training Loss: 41.93276081085205\n",
      "Epoch 47 \t Batch 60 \t Training Loss: 41.74216988881429\n",
      "Epoch 47 \t Batch 80 \t Training Loss: 42.033637285232544\n",
      "Epoch 47 \t Batch 100 \t Training Loss: 42.1242594909668\n",
      "Epoch 47 \t Batch 120 \t Training Loss: 42.292340723673504\n",
      "Epoch 47 \t Batch 140 \t Training Loss: 42.368206923348566\n",
      "Epoch 47 \t Batch 160 \t Training Loss: 42.37522282600403\n",
      "Epoch 47 \t Batch 180 \t Training Loss: 42.41237345801459\n",
      "Epoch 47 \t Batch 200 \t Training Loss: 42.35058145523071\n",
      "Epoch 47 \t Batch 220 \t Training Loss: 42.3608979138461\n",
      "Epoch 47 \t Batch 240 \t Training Loss: 42.29920323689779\n",
      "Epoch 47 \t Batch 260 \t Training Loss: 42.31255893707275\n",
      "Epoch 47 \t Batch 280 \t Training Loss: 42.35584693636213\n",
      "Epoch 47 \t Batch 300 \t Training Loss: 42.39512531280518\n",
      "Epoch 47 \t Batch 320 \t Training Loss: 42.45534597635269\n",
      "Epoch 47 \t Batch 340 \t Training Loss: 42.4736188103171\n",
      "Epoch 47 \t Batch 360 \t Training Loss: 42.461923906538225\n",
      "Epoch 47 \t Batch 380 \t Training Loss: 42.522224827816615\n",
      "Epoch 47 \t Batch 400 \t Training Loss: 42.442111539840695\n",
      "Epoch 47 \t Batch 420 \t Training Loss: 42.449634197780064\n",
      "Epoch 47 \t Batch 440 \t Training Loss: 42.450074421275744\n",
      "Epoch 47 \t Batch 460 \t Training Loss: 42.42216166620669\n",
      "Epoch 47 \t Batch 480 \t Training Loss: 42.39331658681234\n",
      "Epoch 47 \t Batch 500 \t Training Loss: 42.402095817565915\n",
      "Epoch 47 \t Batch 520 \t Training Loss: 42.464615455040565\n",
      "Epoch 47 \t Batch 540 \t Training Loss: 42.40920935736762\n",
      "Epoch 47 \t Batch 560 \t Training Loss: 42.46928061076573\n",
      "Epoch 47 \t Batch 580 \t Training Loss: 42.45247500189419\n",
      "Epoch 47 \t Batch 600 \t Training Loss: 42.50842237472534\n",
      "Epoch 47 \t Batch 620 \t Training Loss: 42.48073019212292\n",
      "Epoch 47 \t Batch 640 \t Training Loss: 42.450271332263945\n",
      "Epoch 47 \t Batch 660 \t Training Loss: 42.41605857502331\n",
      "Epoch 47 \t Batch 680 \t Training Loss: 42.41045306149651\n",
      "Epoch 47 \t Batch 700 \t Training Loss: 42.41766298021589\n",
      "Epoch 47 \t Batch 720 \t Training Loss: 42.41716186205546\n",
      "Epoch 47 \t Batch 740 \t Training Loss: 42.40909203194283\n",
      "Epoch 47 \t Batch 760 \t Training Loss: 42.46077424601505\n",
      "Epoch 47 \t Batch 780 \t Training Loss: 42.46827324598264\n",
      "Epoch 47 \t Batch 800 \t Training Loss: 42.47112487792969\n",
      "Epoch 47 \t Batch 820 \t Training Loss: 42.47140421983672\n",
      "Epoch 47 \t Batch 840 \t Training Loss: 42.46998567581177\n",
      "Epoch 47 \t Batch 860 \t Training Loss: 42.49372048488883\n",
      "Epoch 47 \t Batch 880 \t Training Loss: 42.49173564043912\n",
      "Epoch 47 \t Batch 900 \t Training Loss: 42.50016484154595\n",
      "Epoch 47 \t Batch 20 \t Validation Loss: 9.402243661880494\n",
      "Epoch 47 \t Batch 40 \t Validation Loss: 13.115966737270355\n",
      "Epoch 47 \t Batch 60 \t Validation Loss: 12.58293788433075\n",
      "Epoch 47 \t Batch 80 \t Validation Loss: 14.013655644655227\n",
      "Epoch 47 \t Batch 100 \t Validation Loss: 17.0144628572464\n",
      "Epoch 47 \t Batch 120 \t Validation Loss: 19.161519857247672\n",
      "Epoch 47 \t Batch 140 \t Validation Loss: 20.5035776921681\n",
      "Epoch 47 \t Batch 160 \t Validation Loss: 23.488891038298608\n",
      "Epoch 47 \t Batch 180 \t Validation Loss: 28.367518676651848\n",
      "Epoch 47 \t Batch 200 \t Validation Loss: 30.786869361400605\n",
      "Epoch 47 \t Batch 220 \t Validation Loss: 33.01622259616852\n",
      "Epoch 47 \t Batch 240 \t Validation Loss: 34.05096405545871\n",
      "Epoch 47 \t Batch 260 \t Validation Loss: 36.87742040524116\n",
      "Epoch 47 \t Batch 280 \t Validation Loss: 38.45755116428648\n",
      "Epoch 47 \t Batch 300 \t Validation Loss: 39.87450552463532\n",
      "Epoch 47 \t Batch 320 \t Validation Loss: 40.61303882449865\n",
      "Epoch 47 \t Batch 340 \t Validation Loss: 40.62699045153225\n",
      "Epoch 47 \t Batch 360 \t Validation Loss: 40.55304854578442\n",
      "Epoch 47 \t Batch 380 \t Validation Loss: 40.99491618683464\n",
      "Epoch 47 \t Batch 400 \t Validation Loss: 40.57962033629418\n",
      "Epoch 47 \t Batch 420 \t Validation Loss: 40.55589450768062\n",
      "Epoch 47 \t Batch 440 \t Validation Loss: 40.260663241689855\n",
      "Epoch 47 \t Batch 460 \t Validation Loss: 40.55311669992364\n",
      "Epoch 47 \t Batch 480 \t Validation Loss: 41.04347322483857\n",
      "Epoch 47 \t Batch 500 \t Validation Loss: 40.77122689533233\n",
      "Epoch 47 \t Batch 520 \t Validation Loss: 40.67942649492851\n",
      "Epoch 47 \t Batch 540 \t Validation Loss: 40.37193684842851\n",
      "Epoch 47 \t Batch 560 \t Validation Loss: 40.18049381545612\n",
      "Epoch 47 \t Batch 580 \t Validation Loss: 39.940289179210005\n",
      "Epoch 47 \t Batch 600 \t Validation Loss: 40.123931980927786\n",
      "Epoch 47 Training Loss: 42.48391393330781 Validation Loss: 40.773460887469255\n",
      "Epoch 47 completed\n",
      "Epoch 48 \t Batch 20 \t Training Loss: 41.55732669830322\n",
      "Epoch 48 \t Batch 40 \t Training Loss: 40.83671817779541\n",
      "Epoch 48 \t Batch 60 \t Training Loss: 41.443721771240234\n",
      "Epoch 48 \t Batch 80 \t Training Loss: 41.72121777534485\n",
      "Epoch 48 \t Batch 100 \t Training Loss: 42.05356613159179\n",
      "Epoch 48 \t Batch 120 \t Training Loss: 42.07447884877523\n",
      "Epoch 48 \t Batch 140 \t Training Loss: 42.01021426064627\n",
      "Epoch 48 \t Batch 160 \t Training Loss: 42.13264791965484\n",
      "Epoch 48 \t Batch 180 \t Training Loss: 42.27720964219835\n",
      "Epoch 48 \t Batch 200 \t Training Loss: 42.277530670166016\n",
      "Epoch 48 \t Batch 220 \t Training Loss: 42.327895823392\n",
      "Epoch 48 \t Batch 240 \t Training Loss: 42.30608547528585\n",
      "Epoch 48 \t Batch 260 \t Training Loss: 42.25766283181997\n",
      "Epoch 48 \t Batch 280 \t Training Loss: 42.282557242257255\n",
      "Epoch 48 \t Batch 300 \t Training Loss: 42.216798044840495\n",
      "Epoch 48 \t Batch 320 \t Training Loss: 42.214988470077515\n",
      "Epoch 48 \t Batch 340 \t Training Loss: 42.25692136427936\n",
      "Epoch 48 \t Batch 360 \t Training Loss: 42.138335429297555\n",
      "Epoch 48 \t Batch 380 \t Training Loss: 42.10514504282098\n",
      "Epoch 48 \t Batch 400 \t Training Loss: 42.10118627548218\n",
      "Epoch 48 \t Batch 420 \t Training Loss: 42.12325145176479\n",
      "Epoch 48 \t Batch 440 \t Training Loss: 42.12169790267944\n",
      "Epoch 48 \t Batch 460 \t Training Loss: 42.20239358984906\n",
      "Epoch 48 \t Batch 480 \t Training Loss: 42.233085584640506\n",
      "Epoch 48 \t Batch 500 \t Training Loss: 42.22826586151123\n",
      "Epoch 48 \t Batch 520 \t Training Loss: 42.185980019202596\n",
      "Epoch 48 \t Batch 540 \t Training Loss: 42.118416969864455\n",
      "Epoch 48 \t Batch 560 \t Training Loss: 42.09679354258946\n",
      "Epoch 48 \t Batch 580 \t Training Loss: 42.067995972468935\n",
      "Epoch 48 \t Batch 600 \t Training Loss: 42.089739093780516\n",
      "Epoch 48 \t Batch 620 \t Training Loss: 42.07396366365494\n",
      "Epoch 48 \t Batch 640 \t Training Loss: 42.10278816819191\n",
      "Epoch 48 \t Batch 660 \t Training Loss: 42.103962846235795\n",
      "Epoch 48 \t Batch 680 \t Training Loss: 42.11117717518526\n",
      "Epoch 48 \t Batch 700 \t Training Loss: 42.128807923453195\n",
      "Epoch 48 \t Batch 720 \t Training Loss: 42.11337669160631\n",
      "Epoch 48 \t Batch 740 \t Training Loss: 42.10363844794196\n",
      "Epoch 48 \t Batch 760 \t Training Loss: 42.13894662355122\n",
      "Epoch 48 \t Batch 780 \t Training Loss: 42.165681643363754\n",
      "Epoch 48 \t Batch 800 \t Training Loss: 42.15932198524475\n",
      "Epoch 48 \t Batch 820 \t Training Loss: 42.16608182395377\n",
      "Epoch 48 \t Batch 840 \t Training Loss: 42.17858132407779\n",
      "Epoch 48 \t Batch 860 \t Training Loss: 42.18788460132688\n",
      "Epoch 48 \t Batch 880 \t Training Loss: 42.2169633995403\n",
      "Epoch 48 \t Batch 900 \t Training Loss: 42.224270549350315\n",
      "Epoch 48 \t Batch 20 \t Validation Loss: 11.32555594444275\n",
      "Epoch 48 \t Batch 40 \t Validation Loss: 15.59656822681427\n",
      "Epoch 48 \t Batch 60 \t Validation Loss: 14.378067922592162\n",
      "Epoch 48 \t Batch 80 \t Validation Loss: 15.35784563422203\n",
      "Epoch 48 \t Batch 100 \t Validation Loss: 18.1951105260849\n",
      "Epoch 48 \t Batch 120 \t Validation Loss: 20.198751000563302\n",
      "Epoch 48 \t Batch 140 \t Validation Loss: 21.337215733528136\n",
      "Epoch 48 \t Batch 160 \t Validation Loss: 23.94023834168911\n",
      "Epoch 48 \t Batch 180 \t Validation Loss: 27.957447441418967\n",
      "Epoch 48 \t Batch 200 \t Validation Loss: 30.049512717723847\n",
      "Epoch 48 \t Batch 220 \t Validation Loss: 31.790601121295584\n",
      "Epoch 48 \t Batch 240 \t Validation Loss: 32.58627411723137\n",
      "Epoch 48 \t Batch 260 \t Validation Loss: 35.21621808455541\n",
      "Epoch 48 \t Batch 280 \t Validation Loss: 36.7280340722629\n",
      "Epoch 48 \t Batch 300 \t Validation Loss: 37.75238839944204\n",
      "Epoch 48 \t Batch 320 \t Validation Loss: 38.33327457457781\n",
      "Epoch 48 \t Batch 340 \t Validation Loss: 38.38722808641546\n",
      "Epoch 48 \t Batch 360 \t Validation Loss: 38.24890014463001\n",
      "Epoch 48 \t Batch 380 \t Validation Loss: 38.728599449207906\n",
      "Epoch 48 \t Batch 400 \t Validation Loss: 38.481855503320695\n",
      "Epoch 48 \t Batch 420 \t Validation Loss: 38.60561616193681\n",
      "Epoch 48 \t Batch 440 \t Validation Loss: 38.47079532905058\n",
      "Epoch 48 \t Batch 460 \t Validation Loss: 38.946158385276796\n",
      "Epoch 48 \t Batch 480 \t Validation Loss: 39.45887269675732\n",
      "Epoch 48 \t Batch 500 \t Validation Loss: 39.2644770860672\n",
      "Epoch 48 \t Batch 520 \t Validation Loss: 39.35105462532777\n",
      "Epoch 48 \t Batch 540 \t Validation Loss: 39.08888064578728\n",
      "Epoch 48 \t Batch 560 \t Validation Loss: 38.930292004346846\n",
      "Epoch 48 \t Batch 580 \t Validation Loss: 38.69869066846782\n",
      "Epoch 48 \t Batch 600 \t Validation Loss: 38.94221235354741\n",
      "Epoch 48 Training Loss: 42.25805003458437 Validation Loss: 39.59260674343481\n",
      "Epoch 48 completed\n",
      "Epoch 49 \t Batch 20 \t Training Loss: 41.33659343719482\n",
      "Epoch 49 \t Batch 40 \t Training Loss: 41.311334228515626\n",
      "Epoch 49 \t Batch 60 \t Training Loss: 41.040312894185384\n",
      "Epoch 49 \t Batch 80 \t Training Loss: 41.29783296585083\n",
      "Epoch 49 \t Batch 100 \t Training Loss: 41.69053291320801\n",
      "Epoch 49 \t Batch 120 \t Training Loss: 41.524152247111004\n",
      "Epoch 49 \t Batch 140 \t Training Loss: 41.448207936968124\n",
      "Epoch 49 \t Batch 160 \t Training Loss: 41.49351224899292\n",
      "Epoch 49 \t Batch 180 \t Training Loss: 41.54364386664496\n",
      "Epoch 49 \t Batch 200 \t Training Loss: 41.53717611312866\n",
      "Epoch 49 \t Batch 220 \t Training Loss: 41.568161808360706\n",
      "Epoch 49 \t Batch 240 \t Training Loss: 41.57073615392049\n",
      "Epoch 49 \t Batch 260 \t Training Loss: 41.582047521151026\n",
      "Epoch 49 \t Batch 280 \t Training Loss: 41.55859911782401\n",
      "Epoch 49 \t Batch 300 \t Training Loss: 41.570178833007816\n",
      "Epoch 49 \t Batch 320 \t Training Loss: 41.644931292533876\n",
      "Epoch 49 \t Batch 340 \t Training Loss: 41.684420260261085\n",
      "Epoch 49 \t Batch 360 \t Training Loss: 41.67470930947198\n",
      "Epoch 49 \t Batch 380 \t Training Loss: 41.611530966507765\n",
      "Epoch 49 \t Batch 400 \t Training Loss: 41.63561015129089\n",
      "Epoch 49 \t Batch 420 \t Training Loss: 41.64800243377685\n",
      "Epoch 49 \t Batch 440 \t Training Loss: 41.66203739859841\n",
      "Epoch 49 \t Batch 460 \t Training Loss: 41.662621265908946\n",
      "Epoch 49 \t Batch 480 \t Training Loss: 41.64621988137563\n",
      "Epoch 49 \t Batch 500 \t Training Loss: 41.64329336547851\n",
      "Epoch 49 \t Batch 520 \t Training Loss: 41.720180856264555\n",
      "Epoch 49 \t Batch 540 \t Training Loss: 41.74281154208713\n",
      "Epoch 49 \t Batch 560 \t Training Loss: 41.748738758904594\n",
      "Epoch 49 \t Batch 580 \t Training Loss: 41.78282187231656\n",
      "Epoch 49 \t Batch 600 \t Training Loss: 41.818441727956134\n",
      "Epoch 49 \t Batch 620 \t Training Loss: 41.80400442307995\n",
      "Epoch 49 \t Batch 640 \t Training Loss: 41.81348739862442\n",
      "Epoch 49 \t Batch 660 \t Training Loss: 41.82604023326527\n",
      "Epoch 49 \t Batch 680 \t Training Loss: 41.85867987240062\n",
      "Epoch 49 \t Batch 700 \t Training Loss: 41.87321843283517\n",
      "Epoch 49 \t Batch 720 \t Training Loss: 41.889538531833225\n",
      "Epoch 49 \t Batch 740 \t Training Loss: 41.8758525384439\n",
      "Epoch 49 \t Batch 760 \t Training Loss: 41.8873170451114\n",
      "Epoch 49 \t Batch 780 \t Training Loss: 41.92100129738832\n",
      "Epoch 49 \t Batch 800 \t Training Loss: 41.910443634986876\n",
      "Epoch 49 \t Batch 820 \t Training Loss: 41.958494935384614\n",
      "Epoch 49 \t Batch 840 \t Training Loss: 41.969602026258194\n",
      "Epoch 49 \t Batch 860 \t Training Loss: 41.98579448433809\n",
      "Epoch 49 \t Batch 880 \t Training Loss: 42.02545681866732\n",
      "Epoch 49 \t Batch 900 \t Training Loss: 42.028924191792804\n",
      "Epoch 49 \t Batch 20 \t Validation Loss: 11.765645718574524\n",
      "Epoch 49 \t Batch 40 \t Validation Loss: 15.301756978034973\n",
      "Epoch 49 \t Batch 60 \t Validation Loss: 14.812883011500041\n",
      "Epoch 49 \t Batch 80 \t Validation Loss: 16.056238377094267\n",
      "Epoch 49 \t Batch 100 \t Validation Loss: 18.948501386642455\n",
      "Epoch 49 \t Batch 120 \t Validation Loss: 21.182646600405374\n",
      "Epoch 49 \t Batch 140 \t Validation Loss: 22.250354037966048\n",
      "Epoch 49 \t Batch 160 \t Validation Loss: 24.331437593698503\n",
      "Epoch 49 \t Batch 180 \t Validation Loss: 27.514227437973023\n",
      "Epoch 49 \t Batch 200 \t Validation Loss: 29.26067084789276\n",
      "Epoch 49 \t Batch 220 \t Validation Loss: 30.60810943516818\n",
      "Epoch 49 \t Batch 240 \t Validation Loss: 31.126449906826018\n",
      "Epoch 49 \t Batch 260 \t Validation Loss: 33.4563260445228\n",
      "Epoch 49 \t Batch 280 \t Validation Loss: 34.768944587026326\n",
      "Epoch 49 \t Batch 300 \t Validation Loss: 35.49521061261495\n",
      "Epoch 49 \t Batch 320 \t Validation Loss: 35.89909926056862\n",
      "Epoch 49 \t Batch 340 \t Validation Loss: 35.99671400294584\n",
      "Epoch 49 \t Batch 360 \t Validation Loss: 35.822911426756114\n",
      "Epoch 49 \t Batch 380 \t Validation Loss: 36.21339637354801\n",
      "Epoch 49 \t Batch 400 \t Validation Loss: 36.042358956336976\n",
      "Epoch 49 \t Batch 420 \t Validation Loss: 36.196264600753786\n",
      "Epoch 49 \t Batch 440 \t Validation Loss: 36.09020024429668\n",
      "Epoch 49 \t Batch 460 \t Validation Loss: 36.511238630958225\n",
      "Epoch 49 \t Batch 480 \t Validation Loss: 37.06535958250364\n",
      "Epoch 49 \t Batch 500 \t Validation Loss: 36.844363744735716\n",
      "Epoch 49 \t Batch 520 \t Validation Loss: 36.82541859516731\n",
      "Epoch 49 \t Batch 540 \t Validation Loss: 36.72791377350136\n",
      "Epoch 49 \t Batch 560 \t Validation Loss: 36.7359625186239\n",
      "Epoch 49 \t Batch 580 \t Validation Loss: 36.66958896538307\n",
      "Epoch 49 \t Batch 600 \t Validation Loss: 37.03378709952037\n",
      "Epoch 49 Training Loss: 42.03136892121127 Validation Loss: 37.772016700212056\n",
      "Epoch 49 completed\n",
      "Epoch 50 \t Batch 20 \t Training Loss: 40.8192777633667\n",
      "Epoch 50 \t Batch 40 \t Training Loss: 40.985804080963135\n",
      "Epoch 50 \t Batch 60 \t Training Loss: 41.33220335642497\n",
      "Epoch 50 \t Batch 80 \t Training Loss: 41.42875871658325\n",
      "Epoch 50 \t Batch 100 \t Training Loss: 41.024832496643064\n",
      "Epoch 50 \t Batch 120 \t Training Loss: 41.145267422993975\n",
      "Epoch 50 \t Batch 140 \t Training Loss: 40.99569489615304\n",
      "Epoch 50 \t Batch 160 \t Training Loss: 41.12655057907104\n",
      "Epoch 50 \t Batch 180 \t Training Loss: 41.09910903506809\n",
      "Epoch 50 \t Batch 200 \t Training Loss: 41.18649124145508\n",
      "Epoch 50 \t Batch 220 \t Training Loss: 41.209687666459516\n",
      "Epoch 50 \t Batch 240 \t Training Loss: 41.26346435546875\n",
      "Epoch 50 \t Batch 260 \t Training Loss: 41.21447246258075\n",
      "Epoch 50 \t Batch 280 \t Training Loss: 41.31515319006784\n",
      "Epoch 50 \t Batch 300 \t Training Loss: 41.341968320210775\n",
      "Epoch 50 \t Batch 320 \t Training Loss: 41.39233680963516\n",
      "Epoch 50 \t Batch 340 \t Training Loss: 41.44775453455308\n",
      "Epoch 50 \t Batch 360 \t Training Loss: 41.457084835900204\n",
      "Epoch 50 \t Batch 380 \t Training Loss: 41.50467133773001\n",
      "Epoch 50 \t Batch 400 \t Training Loss: 41.551374740600586\n",
      "Epoch 50 \t Batch 420 \t Training Loss: 41.585955338251026\n",
      "Epoch 50 \t Batch 440 \t Training Loss: 41.61924296292392\n",
      "Epoch 50 \t Batch 460 \t Training Loss: 41.69228746994682\n",
      "Epoch 50 \t Batch 480 \t Training Loss: 41.7290202220281\n",
      "Epoch 50 \t Batch 500 \t Training Loss: 41.72744640350342\n",
      "Epoch 50 \t Batch 520 \t Training Loss: 41.785292133918176\n",
      "Epoch 50 \t Batch 540 \t Training Loss: 41.75259578846119\n",
      "Epoch 50 \t Batch 560 \t Training Loss: 41.74947547231402\n",
      "Epoch 50 \t Batch 580 \t Training Loss: 41.70451477313864\n",
      "Epoch 50 \t Batch 600 \t Training Loss: 41.705448939005535\n",
      "Epoch 50 \t Batch 620 \t Training Loss: 41.72252846994708\n",
      "Epoch 50 \t Batch 640 \t Training Loss: 41.73567477464676\n",
      "Epoch 50 \t Batch 660 \t Training Loss: 41.741820300709115\n",
      "Epoch 50 \t Batch 680 \t Training Loss: 41.73448343837963\n",
      "Epoch 50 \t Batch 700 \t Training Loss: 41.7812098802839\n",
      "Epoch 50 \t Batch 720 \t Training Loss: 41.800558143191864\n",
      "Epoch 50 \t Batch 740 \t Training Loss: 41.77848014831543\n",
      "Epoch 50 \t Batch 760 \t Training Loss: 41.755773192957825\n",
      "Epoch 50 \t Batch 780 \t Training Loss: 41.74382089957213\n",
      "Epoch 50 \t Batch 800 \t Training Loss: 41.74952370166778\n",
      "Epoch 50 \t Batch 820 \t Training Loss: 41.752848248365446\n",
      "Epoch 50 \t Batch 840 \t Training Loss: 41.75461512066069\n",
      "Epoch 50 \t Batch 860 \t Training Loss: 41.74484717346901\n",
      "Epoch 50 \t Batch 880 \t Training Loss: 41.75689869793979\n",
      "Epoch 50 \t Batch 900 \t Training Loss: 41.76055426279704\n",
      "Epoch 50 \t Batch 20 \t Validation Loss: 10.316545104980468\n",
      "Epoch 50 \t Batch 40 \t Validation Loss: 13.973188769817352\n",
      "Epoch 50 \t Batch 60 \t Validation Loss: 13.445692078272502\n",
      "Epoch 50 \t Batch 80 \t Validation Loss: 14.764952164888381\n",
      "Epoch 50 \t Batch 100 \t Validation Loss: 18.111509976387023\n",
      "Epoch 50 \t Batch 120 \t Validation Loss: 20.607398370901745\n",
      "Epoch 50 \t Batch 140 \t Validation Loss: 21.92035710811615\n",
      "Epoch 50 \t Batch 160 \t Validation Loss: 24.52079919874668\n",
      "Epoch 50 \t Batch 180 \t Validation Loss: 28.878186943795946\n",
      "Epoch 50 \t Batch 200 \t Validation Loss: 30.97416957616806\n",
      "Epoch 50 \t Batch 220 \t Validation Loss: 32.90897044918754\n",
      "Epoch 50 \t Batch 240 \t Validation Loss: 33.769173961877826\n",
      "Epoch 50 \t Batch 260 \t Validation Loss: 36.4315080844439\n",
      "Epoch 50 \t Batch 280 \t Validation Loss: 37.955845061370304\n",
      "Epoch 50 \t Batch 300 \t Validation Loss: 39.138556650479636\n",
      "Epoch 50 \t Batch 320 \t Validation Loss: 39.75796604603529\n",
      "Epoch 50 \t Batch 340 \t Validation Loss: 39.767191363783446\n",
      "Epoch 50 \t Batch 360 \t Validation Loss: 39.62331434753206\n",
      "Epoch 50 \t Batch 380 \t Validation Loss: 40.01967205624831\n",
      "Epoch 50 \t Batch 400 \t Validation Loss: 39.63693898558617\n",
      "Epoch 50 \t Batch 420 \t Validation Loss: 39.664242180188495\n",
      "Epoch 50 \t Batch 440 \t Validation Loss: 39.38970893404701\n",
      "Epoch 50 \t Batch 460 \t Validation Loss: 39.70893404069154\n",
      "Epoch 50 \t Batch 480 \t Validation Loss: 40.2013892998298\n",
      "Epoch 50 \t Batch 500 \t Validation Loss: 39.94909596538544\n",
      "Epoch 50 \t Batch 520 \t Validation Loss: 39.83251515076711\n",
      "Epoch 50 \t Batch 540 \t Validation Loss: 39.537711626512035\n",
      "Epoch 50 \t Batch 560 \t Validation Loss: 39.32861252427101\n",
      "Epoch 50 \t Batch 580 \t Validation Loss: 39.035387656606474\n",
      "Epoch 50 \t Batch 600 \t Validation Loss: 39.2531459291776\n",
      "Epoch 50 Training Loss: 41.76597677582353 Validation Loss: 39.897194053445546\n",
      "Epoch 50 completed\n",
      "Epoch 51 \t Batch 20 \t Training Loss: 40.77551612854004\n",
      "Epoch 51 \t Batch 40 \t Training Loss: 40.45169506072998\n",
      "Epoch 51 \t Batch 60 \t Training Loss: 40.555836868286136\n",
      "Epoch 51 \t Batch 80 \t Training Loss: 40.66174364089966\n",
      "Epoch 51 \t Batch 100 \t Training Loss: 40.50196250915528\n",
      "Epoch 51 \t Batch 120 \t Training Loss: 40.56936318079631\n",
      "Epoch 51 \t Batch 140 \t Training Loss: 40.69546827588763\n",
      "Epoch 51 \t Batch 160 \t Training Loss: 40.86651244163513\n",
      "Epoch 51 \t Batch 180 \t Training Loss: 41.022893863254126\n",
      "Epoch 51 \t Batch 200 \t Training Loss: 41.077037239074706\n",
      "Epoch 51 \t Batch 220 \t Training Loss: 41.13149336034601\n",
      "Epoch 51 \t Batch 240 \t Training Loss: 41.14986173311869\n",
      "Epoch 51 \t Batch 260 \t Training Loss: 41.29514273129977\n",
      "Epoch 51 \t Batch 280 \t Training Loss: 41.319064658028736\n",
      "Epoch 51 \t Batch 300 \t Training Loss: 41.25651030222575\n",
      "Epoch 51 \t Batch 320 \t Training Loss: 41.19142663478851\n",
      "Epoch 51 \t Batch 340 \t Training Loss: 41.20653668571921\n",
      "Epoch 51 \t Batch 360 \t Training Loss: 41.23120831383599\n",
      "Epoch 51 \t Batch 380 \t Training Loss: 41.25876857356021\n",
      "Epoch 51 \t Batch 400 \t Training Loss: 41.28974991798401\n",
      "Epoch 51 \t Batch 420 \t Training Loss: 41.281003579639254\n",
      "Epoch 51 \t Batch 440 \t Training Loss: 41.298155819286\n",
      "Epoch 51 \t Batch 460 \t Training Loss: 41.29283715123716\n",
      "Epoch 51 \t Batch 480 \t Training Loss: 41.285545802116395\n",
      "Epoch 51 \t Batch 500 \t Training Loss: 41.323415924072265\n",
      "Epoch 51 \t Batch 520 \t Training Loss: 41.33626019404485\n",
      "Epoch 51 \t Batch 540 \t Training Loss: 41.365843511510775\n",
      "Epoch 51 \t Batch 560 \t Training Loss: 41.42802175794329\n",
      "Epoch 51 \t Batch 580 \t Training Loss: 41.401307816341\n",
      "Epoch 51 \t Batch 600 \t Training Loss: 41.4406235631307\n",
      "Epoch 51 \t Batch 620 \t Training Loss: 41.44629841466104\n",
      "Epoch 51 \t Batch 640 \t Training Loss: 41.44259054064751\n",
      "Epoch 51 \t Batch 660 \t Training Loss: 41.45855198484479\n",
      "Epoch 51 \t Batch 680 \t Training Loss: 41.481276944104366\n",
      "Epoch 51 \t Batch 700 \t Training Loss: 41.521092196873255\n",
      "Epoch 51 \t Batch 720 \t Training Loss: 41.49377777841356\n",
      "Epoch 51 \t Batch 740 \t Training Loss: 41.4477229350322\n",
      "Epoch 51 \t Batch 760 \t Training Loss: 41.45412155954461\n",
      "Epoch 51 \t Batch 780 \t Training Loss: 41.50136762765737\n",
      "Epoch 51 \t Batch 800 \t Training Loss: 41.47595596790314\n",
      "Epoch 51 \t Batch 820 \t Training Loss: 41.48973426818848\n",
      "Epoch 51 \t Batch 840 \t Training Loss: 41.48946417399815\n",
      "Epoch 51 \t Batch 860 \t Training Loss: 41.46866955868033\n",
      "Epoch 51 \t Batch 880 \t Training Loss: 41.46176606958563\n",
      "Epoch 51 \t Batch 900 \t Training Loss: 41.47502538469103\n",
      "Epoch 51 \t Batch 20 \t Validation Loss: 21.421341991424562\n",
      "Epoch 51 \t Batch 40 \t Validation Loss: 22.10112121105194\n",
      "Epoch 51 \t Batch 60 \t Validation Loss: 21.491588338216147\n",
      "Epoch 51 \t Batch 80 \t Validation Loss: 21.805455338954925\n",
      "Epoch 51 \t Batch 100 \t Validation Loss: 24.265923013687132\n",
      "Epoch 51 \t Batch 120 \t Validation Loss: 26.189870762825013\n",
      "Epoch 51 \t Batch 140 \t Validation Loss: 26.882100016730174\n",
      "Epoch 51 \t Batch 160 \t Validation Loss: 28.91462308764458\n",
      "Epoch 51 \t Batch 180 \t Validation Loss: 31.928604125976562\n",
      "Epoch 51 \t Batch 200 \t Validation Loss: 33.35683556556702\n",
      "Epoch 51 \t Batch 220 \t Validation Loss: 34.608429822054774\n",
      "Epoch 51 \t Batch 240 \t Validation Loss: 35.023265997568764\n",
      "Epoch 51 \t Batch 260 \t Validation Loss: 37.18428970850431\n",
      "Epoch 51 \t Batch 280 \t Validation Loss: 38.32821685586657\n",
      "Epoch 51 \t Batch 300 \t Validation Loss: 39.099248037338256\n",
      "Epoch 51 \t Batch 320 \t Validation Loss: 39.517505440115926\n",
      "Epoch 51 \t Batch 340 \t Validation Loss: 39.45006587926079\n",
      "Epoch 51 \t Batch 360 \t Validation Loss: 39.26846554809146\n",
      "Epoch 51 \t Batch 380 \t Validation Loss: 39.67880837540878\n",
      "Epoch 51 \t Batch 400 \t Validation Loss: 39.49580866575241\n",
      "Epoch 51 \t Batch 420 \t Validation Loss: 39.62272777784438\n",
      "Epoch 51 \t Batch 440 \t Validation Loss: 39.55071716958826\n",
      "Epoch 51 \t Batch 460 \t Validation Loss: 40.086911120622055\n",
      "Epoch 51 \t Batch 480 \t Validation Loss: 40.577165148655574\n",
      "Epoch 51 \t Batch 500 \t Validation Loss: 40.34861638450622\n",
      "Epoch 51 \t Batch 520 \t Validation Loss: 40.524448796418994\n",
      "Epoch 51 \t Batch 540 \t Validation Loss: 40.25503261530841\n",
      "Epoch 51 \t Batch 560 \t Validation Loss: 40.03279093163354\n",
      "Epoch 51 \t Batch 580 \t Validation Loss: 39.77037988037899\n",
      "Epoch 51 \t Batch 600 \t Validation Loss: 39.97860322475433\n",
      "Epoch 51 Training Loss: 41.48941006759261 Validation Loss: 40.64982538873499\n",
      "Epoch 51 completed\n",
      "Epoch 52 \t Batch 20 \t Training Loss: 40.56384010314942\n",
      "Epoch 52 \t Batch 40 \t Training Loss: 40.385818576812746\n",
      "Epoch 52 \t Batch 60 \t Training Loss: 40.34324131011963\n",
      "Epoch 52 \t Batch 80 \t Training Loss: 40.679745721817014\n",
      "Epoch 52 \t Batch 100 \t Training Loss: 40.560440406799316\n",
      "Epoch 52 \t Batch 120 \t Training Loss: 40.55633942286173\n",
      "Epoch 52 \t Batch 140 \t Training Loss: 40.59049876076835\n",
      "Epoch 52 \t Batch 160 \t Training Loss: 40.68717103004455\n",
      "Epoch 52 \t Batch 180 \t Training Loss: 40.80956499311659\n",
      "Epoch 52 \t Batch 200 \t Training Loss: 40.851050300598146\n",
      "Epoch 52 \t Batch 220 \t Training Loss: 40.74331215945157\n",
      "Epoch 52 \t Batch 240 \t Training Loss: 40.69921770095825\n",
      "Epoch 52 \t Batch 260 \t Training Loss: 40.672579002380374\n",
      "Epoch 52 \t Batch 280 \t Training Loss: 40.664890207563126\n",
      "Epoch 52 \t Batch 300 \t Training Loss: 40.70247787475586\n",
      "Epoch 52 \t Batch 320 \t Training Loss: 40.74161549806595\n",
      "Epoch 52 \t Batch 340 \t Training Loss: 40.715358711691465\n",
      "Epoch 52 \t Batch 360 \t Training Loss: 40.81518970065647\n",
      "Epoch 52 \t Batch 380 \t Training Loss: 40.86914333544279\n",
      "Epoch 52 \t Batch 400 \t Training Loss: 40.867806091308594\n",
      "Epoch 52 \t Batch 420 \t Training Loss: 40.91103372119722\n",
      "Epoch 52 \t Batch 440 \t Training Loss: 40.964085431532425\n",
      "Epoch 52 \t Batch 460 \t Training Loss: 40.96528040844461\n",
      "Epoch 52 \t Batch 480 \t Training Loss: 40.99427179495493\n",
      "Epoch 52 \t Batch 500 \t Training Loss: 40.946752952575686\n",
      "Epoch 52 \t Batch 520 \t Training Loss: 40.924753035031834\n",
      "Epoch 52 \t Batch 540 \t Training Loss: 40.92324997937238\n",
      "Epoch 52 \t Batch 560 \t Training Loss: 40.942097248349874\n",
      "Epoch 52 \t Batch 580 \t Training Loss: 40.97983043933737\n",
      "Epoch 52 \t Batch 600 \t Training Loss: 40.98856160481771\n",
      "Epoch 52 \t Batch 620 \t Training Loss: 41.00827616414716\n",
      "Epoch 52 \t Batch 640 \t Training Loss: 41.00352376103401\n",
      "Epoch 52 \t Batch 660 \t Training Loss: 41.05557427261815\n",
      "Epoch 52 \t Batch 680 \t Training Loss: 41.10082509096931\n",
      "Epoch 52 \t Batch 700 \t Training Loss: 41.09625446864537\n",
      "Epoch 52 \t Batch 720 \t Training Loss: 41.115872065226235\n",
      "Epoch 52 \t Batch 740 \t Training Loss: 41.11660520708239\n",
      "Epoch 52 \t Batch 760 \t Training Loss: 41.15440580468429\n",
      "Epoch 52 \t Batch 780 \t Training Loss: 41.176742084209735\n",
      "Epoch 52 \t Batch 800 \t Training Loss: 41.17190774917603\n",
      "Epoch 52 \t Batch 820 \t Training Loss: 41.16760756794999\n",
      "Epoch 52 \t Batch 840 \t Training Loss: 41.17930887767247\n",
      "Epoch 52 \t Batch 860 \t Training Loss: 41.177944644661835\n",
      "Epoch 52 \t Batch 880 \t Training Loss: 41.216642475128175\n",
      "Epoch 52 \t Batch 900 \t Training Loss: 41.22566659291585\n",
      "Epoch 52 \t Batch 20 \t Validation Loss: 11.853283143043518\n",
      "Epoch 52 \t Batch 40 \t Validation Loss: 16.284506464004515\n",
      "Epoch 52 \t Batch 60 \t Validation Loss: 15.103410895665487\n",
      "Epoch 52 \t Batch 80 \t Validation Loss: 16.223733288049697\n",
      "Epoch 52 \t Batch 100 \t Validation Loss: 19.109945874214173\n",
      "Epoch 52 \t Batch 120 \t Validation Loss: 21.161913446585338\n",
      "Epoch 52 \t Batch 140 \t Validation Loss: 22.30915245669229\n",
      "Epoch 52 \t Batch 160 \t Validation Loss: 25.09794103205204\n",
      "Epoch 52 \t Batch 180 \t Validation Loss: 29.56157501803504\n",
      "Epoch 52 \t Batch 200 \t Validation Loss: 31.788492848873137\n",
      "Epoch 52 \t Batch 220 \t Validation Loss: 33.753344204209064\n",
      "Epoch 52 \t Batch 240 \t Validation Loss: 34.6266936759154\n",
      "Epoch 52 \t Batch 260 \t Validation Loss: 37.374873232841495\n",
      "Epoch 52 \t Batch 280 \t Validation Loss: 38.880131965024134\n",
      "Epoch 52 \t Batch 300 \t Validation Loss: 40.114266228675845\n",
      "Epoch 52 \t Batch 320 \t Validation Loss: 40.75153215974569\n",
      "Epoch 52 \t Batch 340 \t Validation Loss: 40.731147349582\n",
      "Epoch 52 \t Batch 360 \t Validation Loss: 40.619423139095304\n",
      "Epoch 52 \t Batch 380 \t Validation Loss: 41.08672891290564\n",
      "Epoch 52 \t Batch 400 \t Validation Loss: 40.727476218938826\n",
      "Epoch 52 \t Batch 420 \t Validation Loss: 40.7490324031739\n",
      "Epoch 52 \t Batch 440 \t Validation Loss: 40.5133351596919\n",
      "Epoch 52 \t Batch 460 \t Validation Loss: 40.953139762256455\n",
      "Epoch 52 \t Batch 480 \t Validation Loss: 41.44046597580115\n",
      "Epoch 52 \t Batch 500 \t Validation Loss: 41.19265001010895\n",
      "Epoch 52 \t Batch 520 \t Validation Loss: 41.25957449491207\n",
      "Epoch 52 \t Batch 540 \t Validation Loss: 40.91236602377008\n",
      "Epoch 52 \t Batch 560 \t Validation Loss: 40.657313228505\n",
      "Epoch 52 \t Batch 580 \t Validation Loss: 40.354878238973946\n",
      "Epoch 52 \t Batch 600 \t Validation Loss: 40.52172018448512\n",
      "Epoch 52 Training Loss: 41.212977693281104 Validation Loss: 41.15029542554509\n",
      "Epoch 52 completed\n",
      "Epoch 53 \t Batch 20 \t Training Loss: 41.17679004669189\n",
      "Epoch 53 \t Batch 40 \t Training Loss: 40.46212396621704\n",
      "Epoch 53 \t Batch 60 \t Training Loss: 40.37963968912761\n",
      "Epoch 53 \t Batch 80 \t Training Loss: 40.176800060272214\n",
      "Epoch 53 \t Batch 100 \t Training Loss: 40.44065124511719\n",
      "Epoch 53 \t Batch 120 \t Training Loss: 40.57798948287964\n",
      "Epoch 53 \t Batch 140 \t Training Loss: 40.5496543611799\n",
      "Epoch 53 \t Batch 160 \t Training Loss: 40.54074261188507\n",
      "Epoch 53 \t Batch 180 \t Training Loss: 40.715962176852756\n",
      "Epoch 53 \t Batch 200 \t Training Loss: 40.80529594421387\n",
      "Epoch 53 \t Batch 220 \t Training Loss: 40.76507485129616\n",
      "Epoch 53 \t Batch 240 \t Training Loss: 40.83759228388468\n",
      "Epoch 53 \t Batch 260 \t Training Loss: 40.8740214567918\n",
      "Epoch 53 \t Batch 280 \t Training Loss: 40.90482503346035\n",
      "Epoch 53 \t Batch 300 \t Training Loss: 40.890737482706704\n",
      "Epoch 53 \t Batch 320 \t Training Loss: 40.863083112239835\n",
      "Epoch 53 \t Batch 340 \t Training Loss: 40.882531188516054\n",
      "Epoch 53 \t Batch 360 \t Training Loss: 40.939121108584935\n",
      "Epoch 53 \t Batch 380 \t Training Loss: 40.976821768911265\n",
      "Epoch 53 \t Batch 400 \t Training Loss: 40.95143225669861\n",
      "Epoch 53 \t Batch 420 \t Training Loss: 40.95688626425607\n",
      "Epoch 53 \t Batch 440 \t Training Loss: 40.945588025179774\n",
      "Epoch 53 \t Batch 460 \t Training Loss: 40.93095529805059\n",
      "Epoch 53 \t Batch 480 \t Training Loss: 40.98793942133586\n",
      "Epoch 53 \t Batch 500 \t Training Loss: 40.94114065551758\n",
      "Epoch 53 \t Batch 520 \t Training Loss: 40.95825634736281\n",
      "Epoch 53 \t Batch 540 \t Training Loss: 40.990986569722494\n",
      "Epoch 53 \t Batch 560 \t Training Loss: 40.94761231286185\n",
      "Epoch 53 \t Batch 580 \t Training Loss: 40.92602683100207\n",
      "Epoch 53 \t Batch 600 \t Training Loss: 40.96091082255045\n",
      "Epoch 53 \t Batch 620 \t Training Loss: 40.930484753270306\n",
      "Epoch 53 \t Batch 640 \t Training Loss: 40.942471808195116\n",
      "Epoch 53 \t Batch 660 \t Training Loss: 40.91509521368778\n",
      "Epoch 53 \t Batch 680 \t Training Loss: 40.92166276258581\n",
      "Epoch 53 \t Batch 700 \t Training Loss: 40.93909491402762\n",
      "Epoch 53 \t Batch 720 \t Training Loss: 40.92510321405199\n",
      "Epoch 53 \t Batch 740 \t Training Loss: 40.90667389534615\n",
      "Epoch 53 \t Batch 760 \t Training Loss: 40.93293790315327\n",
      "Epoch 53 \t Batch 780 \t Training Loss: 40.96564059624305\n",
      "Epoch 53 \t Batch 800 \t Training Loss: 40.96491750240326\n",
      "Epoch 53 \t Batch 820 \t Training Loss: 40.99013387633533\n",
      "Epoch 53 \t Batch 840 \t Training Loss: 41.001318704514276\n",
      "Epoch 53 \t Batch 860 \t Training Loss: 40.99360758759255\n",
      "Epoch 53 \t Batch 880 \t Training Loss: 40.951893576708706\n",
      "Epoch 53 \t Batch 900 \t Training Loss: 40.94791973114014\n",
      "Epoch 53 \t Batch 20 \t Validation Loss: 8.476747179031372\n",
      "Epoch 53 \t Batch 40 \t Validation Loss: 11.706823945045471\n",
      "Epoch 53 \t Batch 60 \t Validation Loss: 11.325074044863383\n",
      "Epoch 53 \t Batch 80 \t Validation Loss: 12.644293737411498\n",
      "Epoch 53 \t Batch 100 \t Validation Loss: 16.145504665374755\n",
      "Epoch 53 \t Batch 120 \t Validation Loss: 18.688374964396157\n",
      "Epoch 53 \t Batch 140 \t Validation Loss: 20.066532066890172\n",
      "Epoch 53 \t Batch 160 \t Validation Loss: 22.7271288394928\n",
      "Epoch 53 \t Batch 180 \t Validation Loss: 26.523863373862373\n",
      "Epoch 53 \t Batch 200 \t Validation Loss: 28.36590744495392\n",
      "Epoch 53 \t Batch 220 \t Validation Loss: 29.95217133868824\n",
      "Epoch 53 \t Batch 240 \t Validation Loss: 30.721131579081217\n",
      "Epoch 53 \t Batch 260 \t Validation Loss: 33.2295935520759\n",
      "Epoch 53 \t Batch 280 \t Validation Loss: 34.71193272726877\n",
      "Epoch 53 \t Batch 300 \t Validation Loss: 35.685089626312255\n",
      "Epoch 53 \t Batch 320 \t Validation Loss: 36.27749411761761\n",
      "Epoch 53 \t Batch 340 \t Validation Loss: 36.36487440501942\n",
      "Epoch 53 \t Batch 360 \t Validation Loss: 36.32963839901818\n",
      "Epoch 53 \t Batch 380 \t Validation Loss: 36.79282215268989\n",
      "Epoch 53 \t Batch 400 \t Validation Loss: 36.636470510959626\n",
      "Epoch 53 \t Batch 420 \t Validation Loss: 36.78651675496783\n",
      "Epoch 53 \t Batch 440 \t Validation Loss: 36.6912151705135\n",
      "Epoch 53 \t Batch 460 \t Validation Loss: 37.114653166480686\n",
      "Epoch 53 \t Batch 480 \t Validation Loss: 37.72917381723722\n",
      "Epoch 53 \t Batch 500 \t Validation Loss: 37.535032598495484\n",
      "Epoch 53 \t Batch 520 \t Validation Loss: 37.50802702353551\n",
      "Epoch 53 \t Batch 540 \t Validation Loss: 37.31390283902486\n",
      "Epoch 53 \t Batch 560 \t Validation Loss: 37.195691764354706\n",
      "Epoch 53 \t Batch 580 \t Validation Loss: 37.02839390491617\n",
      "Epoch 53 \t Batch 600 \t Validation Loss: 37.299837177594505\n",
      "Epoch 53 Training Loss: 40.98429378896399 Validation Loss: 38.00186648152091\n",
      "Epoch 53 completed\n",
      "Epoch 54 \t Batch 20 \t Training Loss: 40.0181001663208\n",
      "Epoch 54 \t Batch 40 \t Training Loss: 40.21593866348267\n",
      "Epoch 54 \t Batch 60 \t Training Loss: 39.94998976389567\n",
      "Epoch 54 \t Batch 80 \t Training Loss: 40.323738718032835\n",
      "Epoch 54 \t Batch 100 \t Training Loss: 40.29519702911377\n",
      "Epoch 54 \t Batch 120 \t Training Loss: 40.11601867675781\n",
      "Epoch 54 \t Batch 140 \t Training Loss: 40.24646472930908\n",
      "Epoch 54 \t Batch 160 \t Training Loss: 40.15591657161713\n",
      "Epoch 54 \t Batch 180 \t Training Loss: 40.189260228474936\n",
      "Epoch 54 \t Batch 200 \t Training Loss: 40.30326984405517\n",
      "Epoch 54 \t Batch 220 \t Training Loss: 40.23265275088224\n",
      "Epoch 54 \t Batch 240 \t Training Loss: 40.28362646102905\n",
      "Epoch 54 \t Batch 260 \t Training Loss: 40.18192713810847\n",
      "Epoch 54 \t Batch 280 \t Training Loss: 40.26629570552281\n",
      "Epoch 54 \t Batch 300 \t Training Loss: 40.28173332214355\n",
      "Epoch 54 \t Batch 320 \t Training Loss: 40.2503115773201\n",
      "Epoch 54 \t Batch 340 \t Training Loss: 40.178842830657956\n",
      "Epoch 54 \t Batch 360 \t Training Loss: 40.27176474995083\n",
      "Epoch 54 \t Batch 380 \t Training Loss: 40.324288453553855\n",
      "Epoch 54 \t Batch 400 \t Training Loss: 40.316811985969544\n",
      "Epoch 54 \t Batch 420 \t Training Loss: 40.24363207590012\n",
      "Epoch 54 \t Batch 440 \t Training Loss: 40.265519926764746\n",
      "Epoch 54 \t Batch 460 \t Training Loss: 40.32942943158357\n",
      "Epoch 54 \t Batch 480 \t Training Loss: 40.326249182224274\n",
      "Epoch 54 \t Batch 500 \t Training Loss: 40.33262898635864\n",
      "Epoch 54 \t Batch 520 \t Training Loss: 40.40258192282457\n",
      "Epoch 54 \t Batch 540 \t Training Loss: 40.397279530984385\n",
      "Epoch 54 \t Batch 560 \t Training Loss: 40.39152773788997\n",
      "Epoch 54 \t Batch 580 \t Training Loss: 40.41016092958122\n",
      "Epoch 54 \t Batch 600 \t Training Loss: 40.414565076828005\n",
      "Epoch 54 \t Batch 620 \t Training Loss: 40.45329241291169\n",
      "Epoch 54 \t Batch 640 \t Training Loss: 40.48636047542095\n",
      "Epoch 54 \t Batch 660 \t Training Loss: 40.51694805549853\n",
      "Epoch 54 \t Batch 680 \t Training Loss: 40.54316528825199\n",
      "Epoch 54 \t Batch 700 \t Training Loss: 40.562656530652724\n",
      "Epoch 54 \t Batch 720 \t Training Loss: 40.56212807496389\n",
      "Epoch 54 \t Batch 740 \t Training Loss: 40.57300398800824\n",
      "Epoch 54 \t Batch 760 \t Training Loss: 40.60221785746123\n",
      "Epoch 54 \t Batch 780 \t Training Loss: 40.60834731322068\n",
      "Epoch 54 \t Batch 800 \t Training Loss: 40.607377140522004\n",
      "Epoch 54 \t Batch 820 \t Training Loss: 40.61849367793013\n",
      "Epoch 54 \t Batch 840 \t Training Loss: 40.63251469702948\n",
      "Epoch 54 \t Batch 860 \t Training Loss: 40.64966859817505\n",
      "Epoch 54 \t Batch 880 \t Training Loss: 40.63190400600433\n",
      "Epoch 54 \t Batch 900 \t Training Loss: 40.655535263485376\n",
      "Epoch 54 \t Batch 20 \t Validation Loss: 11.076424050331116\n",
      "Epoch 54 \t Batch 40 \t Validation Loss: 14.798036289215087\n",
      "Epoch 54 \t Batch 60 \t Validation Loss: 14.002812520662944\n",
      "Epoch 54 \t Batch 80 \t Validation Loss: 15.098638665676116\n",
      "Epoch 54 \t Batch 100 \t Validation Loss: 18.28621949195862\n",
      "Epoch 54 \t Batch 120 \t Validation Loss: 20.618078939119975\n",
      "Epoch 54 \t Batch 140 \t Validation Loss: 21.82263113430568\n",
      "Epoch 54 \t Batch 160 \t Validation Loss: 24.32016471028328\n",
      "Epoch 54 \t Batch 180 \t Validation Loss: 27.943670892715453\n",
      "Epoch 54 \t Batch 200 \t Validation Loss: 29.940361504554748\n",
      "Epoch 54 \t Batch 220 \t Validation Loss: 31.4993850144473\n",
      "Epoch 54 \t Batch 240 \t Validation Loss: 32.16125634511312\n",
      "Epoch 54 \t Batch 260 \t Validation Loss: 34.69666844881498\n",
      "Epoch 54 \t Batch 280 \t Validation Loss: 36.11724514961243\n",
      "Epoch 54 \t Batch 300 \t Validation Loss: 36.98958990732829\n",
      "Epoch 54 \t Batch 320 \t Validation Loss: 37.50284964740276\n",
      "Epoch 54 \t Batch 340 \t Validation Loss: 37.59550180715673\n",
      "Epoch 54 \t Batch 360 \t Validation Loss: 37.51074781152937\n",
      "Epoch 54 \t Batch 380 \t Validation Loss: 38.037723167319044\n",
      "Epoch 54 \t Batch 400 \t Validation Loss: 37.927349727153775\n",
      "Epoch 54 \t Batch 420 \t Validation Loss: 38.10302667163667\n",
      "Epoch 54 \t Batch 440 \t Validation Loss: 38.13068646084179\n",
      "Epoch 54 \t Batch 460 \t Validation Loss: 38.6519054910411\n",
      "Epoch 54 \t Batch 480 \t Validation Loss: 39.21967137654622\n",
      "Epoch 54 \t Batch 500 \t Validation Loss: 38.985847942352294\n",
      "Epoch 54 \t Batch 520 \t Validation Loss: 39.11423231638395\n",
      "Epoch 54 \t Batch 540 \t Validation Loss: 38.92397696530377\n",
      "Epoch 54 \t Batch 560 \t Validation Loss: 38.7952605009079\n",
      "Epoch 54 \t Batch 580 \t Validation Loss: 38.616533993030416\n",
      "Epoch 54 \t Batch 600 \t Validation Loss: 38.889026215871176\n",
      "Epoch 54 Training Loss: 40.690126431547306 Validation Loss: 39.58507247714253\n",
      "Epoch 54 completed\n",
      "Epoch 55 \t Batch 20 \t Training Loss: 41.07147560119629\n",
      "Epoch 55 \t Batch 40 \t Training Loss: 40.508730030059816\n",
      "Epoch 55 \t Batch 60 \t Training Loss: 40.230036926269534\n",
      "Epoch 55 \t Batch 80 \t Training Loss: 39.812693214416505\n",
      "Epoch 55 \t Batch 100 \t Training Loss: 39.66807399749756\n",
      "Epoch 55 \t Batch 120 \t Training Loss: 39.50317093531291\n",
      "Epoch 55 \t Batch 140 \t Training Loss: 39.51677526746477\n",
      "Epoch 55 \t Batch 160 \t Training Loss: 39.673921370506285\n",
      "Epoch 55 \t Batch 180 \t Training Loss: 39.67520966000027\n",
      "Epoch 55 \t Batch 200 \t Training Loss: 39.69728910446167\n",
      "Epoch 55 \t Batch 220 \t Training Loss: 39.72155335166237\n",
      "Epoch 55 \t Batch 240 \t Training Loss: 39.754343096415205\n",
      "Epoch 55 \t Batch 260 \t Training Loss: 39.75210855924166\n",
      "Epoch 55 \t Batch 280 \t Training Loss: 39.81020729882376\n",
      "Epoch 55 \t Batch 300 \t Training Loss: 39.81506304423014\n",
      "Epoch 55 \t Batch 320 \t Training Loss: 39.90606554746628\n",
      "Epoch 55 \t Batch 340 \t Training Loss: 39.954649846694046\n",
      "Epoch 55 \t Batch 360 \t Training Loss: 39.96889575322469\n",
      "Epoch 55 \t Batch 380 \t Training Loss: 39.92620863663523\n",
      "Epoch 55 \t Batch 400 \t Training Loss: 40.031304922103885\n",
      "Epoch 55 \t Batch 420 \t Training Loss: 40.07001208350772\n",
      "Epoch 55 \t Batch 440 \t Training Loss: 40.117841026999734\n",
      "Epoch 55 \t Batch 460 \t Training Loss: 40.11921071591585\n",
      "Epoch 55 \t Batch 480 \t Training Loss: 40.06603844165802\n",
      "Epoch 55 \t Batch 500 \t Training Loss: 40.08446226501465\n",
      "Epoch 55 \t Batch 520 \t Training Loss: 40.06557832864615\n",
      "Epoch 55 \t Batch 540 \t Training Loss: 40.109544944763186\n",
      "Epoch 55 \t Batch 560 \t Training Loss: 40.14172152110508\n",
      "Epoch 55 \t Batch 580 \t Training Loss: 40.16628496235815\n",
      "Epoch 55 \t Batch 600 \t Training Loss: 40.23482641220093\n",
      "Epoch 55 \t Batch 620 \t Training Loss: 40.25347473390641\n",
      "Epoch 55 \t Batch 640 \t Training Loss: 40.28891971707344\n",
      "Epoch 55 \t Batch 660 \t Training Loss: 40.32709884643555\n",
      "Epoch 55 \t Batch 680 \t Training Loss: 40.32199383903952\n",
      "Epoch 55 \t Batch 700 \t Training Loss: 40.349628230503626\n",
      "Epoch 55 \t Batch 720 \t Training Loss: 40.34387211799621\n",
      "Epoch 55 \t Batch 740 \t Training Loss: 40.37184055946969\n",
      "Epoch 55 \t Batch 760 \t Training Loss: 40.39851037075645\n",
      "Epoch 55 \t Batch 780 \t Training Loss: 40.407633698292265\n",
      "Epoch 55 \t Batch 800 \t Training Loss: 40.380834350585936\n",
      "Epoch 55 \t Batch 820 \t Training Loss: 40.412346490999546\n",
      "Epoch 55 \t Batch 840 \t Training Loss: 40.4196404230027\n",
      "Epoch 55 \t Batch 860 \t Training Loss: 40.418429374694824\n",
      "Epoch 55 \t Batch 880 \t Training Loss: 40.43232640786604\n",
      "Epoch 55 \t Batch 900 \t Training Loss: 40.44013451470269\n",
      "Epoch 55 \t Batch 20 \t Validation Loss: 9.112659430503845\n",
      "Epoch 55 \t Batch 40 \t Validation Loss: 12.07335274219513\n",
      "Epoch 55 \t Batch 60 \t Validation Loss: 11.815354410807291\n",
      "Epoch 55 \t Batch 80 \t Validation Loss: 13.093014723062515\n",
      "Epoch 55 \t Batch 100 \t Validation Loss: 17.37420614719391\n",
      "Epoch 55 \t Batch 120 \t Validation Loss: 20.474101785818736\n",
      "Epoch 55 \t Batch 140 \t Validation Loss: 22.09095776762281\n",
      "Epoch 55 \t Batch 160 \t Validation Loss: 24.708542492985725\n",
      "Epoch 55 \t Batch 180 \t Validation Loss: 28.703237022293937\n",
      "Epoch 55 \t Batch 200 \t Validation Loss: 30.76765785932541\n",
      "Epoch 55 \t Batch 220 \t Validation Loss: 32.418211596662346\n",
      "Epoch 55 \t Batch 240 \t Validation Loss: 33.16173047025998\n",
      "Epoch 55 \t Batch 260 \t Validation Loss: 35.65102790135604\n",
      "Epoch 55 \t Batch 280 \t Validation Loss: 37.12135151965278\n",
      "Epoch 55 \t Batch 300 \t Validation Loss: 38.19595551013946\n",
      "Epoch 55 \t Batch 320 \t Validation Loss: 38.85094261020422\n",
      "Epoch 55 \t Batch 340 \t Validation Loss: 38.942154961473804\n",
      "Epoch 55 \t Batch 360 \t Validation Loss: 38.815355177720384\n",
      "Epoch 55 \t Batch 380 \t Validation Loss: 39.27920611406628\n",
      "Epoch 55 \t Batch 400 \t Validation Loss: 39.02760621905327\n",
      "Epoch 55 \t Batch 420 \t Validation Loss: 39.19031675543104\n",
      "Epoch 55 \t Batch 440 \t Validation Loss: 39.048073999448256\n",
      "Epoch 55 \t Batch 460 \t Validation Loss: 39.50480561152749\n",
      "Epoch 55 \t Batch 480 \t Validation Loss: 40.066217717528346\n",
      "Epoch 55 \t Batch 500 \t Validation Loss: 39.89558760547638\n",
      "Epoch 55 \t Batch 520 \t Validation Loss: 39.940566551685336\n",
      "Epoch 55 \t Batch 540 \t Validation Loss: 39.68280203519044\n",
      "Epoch 55 \t Batch 560 \t Validation Loss: 39.52842803427151\n",
      "Epoch 55 \t Batch 580 \t Validation Loss: 39.26882042144907\n",
      "Epoch 55 \t Batch 600 \t Validation Loss: 39.49554951429367\n",
      "Epoch 55 Training Loss: 40.424671547493475 Validation Loss: 40.18027284934923\n",
      "Epoch 55 completed\n",
      "Epoch 56 \t Batch 20 \t Training Loss: 39.94118385314941\n",
      "Epoch 56 \t Batch 40 \t Training Loss: 39.6674298286438\n",
      "Epoch 56 \t Batch 60 \t Training Loss: 39.637463887532554\n",
      "Epoch 56 \t Batch 80 \t Training Loss: 39.92663860321045\n",
      "Epoch 56 \t Batch 100 \t Training Loss: 40.07893653869629\n",
      "Epoch 56 \t Batch 120 \t Training Loss: 39.956178506215416\n",
      "Epoch 56 \t Batch 140 \t Training Loss: 40.06018159048898\n",
      "Epoch 56 \t Batch 160 \t Training Loss: 39.962624764442445\n",
      "Epoch 56 \t Batch 180 \t Training Loss: 39.90578998989529\n",
      "Epoch 56 \t Batch 200 \t Training Loss: 39.773671264648435\n",
      "Epoch 56 \t Batch 220 \t Training Loss: 39.75973004427823\n",
      "Epoch 56 \t Batch 240 \t Training Loss: 39.82914191881816\n",
      "Epoch 56 \t Batch 260 \t Training Loss: 39.72136143904466\n",
      "Epoch 56 \t Batch 280 \t Training Loss: 39.77655553817749\n",
      "Epoch 56 \t Batch 300 \t Training Loss: 39.77094310760498\n",
      "Epoch 56 \t Batch 320 \t Training Loss: 39.76870337724686\n",
      "Epoch 56 \t Batch 340 \t Training Loss: 39.750962077870085\n",
      "Epoch 56 \t Batch 360 \t Training Loss: 39.801329909430606\n",
      "Epoch 56 \t Batch 380 \t Training Loss: 39.83595891249807\n",
      "Epoch 56 \t Batch 400 \t Training Loss: 39.810541820526126\n",
      "Epoch 56 \t Batch 420 \t Training Loss: 39.80914428347633\n",
      "Epoch 56 \t Batch 440 \t Training Loss: 39.82090878053145\n",
      "Epoch 56 \t Batch 460 \t Training Loss: 39.87515004199484\n",
      "Epoch 56 \t Batch 480 \t Training Loss: 39.90435207684835\n",
      "Epoch 56 \t Batch 500 \t Training Loss: 39.9195299911499\n",
      "Epoch 56 \t Batch 520 \t Training Loss: 39.92413813517644\n",
      "Epoch 56 \t Batch 540 \t Training Loss: 39.91066967999494\n",
      "Epoch 56 \t Batch 560 \t Training Loss: 39.919773544584004\n",
      "Epoch 56 \t Batch 580 \t Training Loss: 39.91300509222623\n",
      "Epoch 56 \t Batch 600 \t Training Loss: 39.948027846018476\n",
      "Epoch 56 \t Batch 620 \t Training Loss: 39.95960406026533\n",
      "Epoch 56 \t Batch 640 \t Training Loss: 39.96958669424057\n",
      "Epoch 56 \t Batch 660 \t Training Loss: 39.963107259345776\n",
      "Epoch 56 \t Batch 680 \t Training Loss: 39.98038157855763\n",
      "Epoch 56 \t Batch 700 \t Training Loss: 39.99741181509835\n",
      "Epoch 56 \t Batch 720 \t Training Loss: 40.030478625827364\n",
      "Epoch 56 \t Batch 740 \t Training Loss: 40.04304480681548\n",
      "Epoch 56 \t Batch 760 \t Training Loss: 40.06007611124139\n",
      "Epoch 56 \t Batch 780 \t Training Loss: 40.120878923856296\n",
      "Epoch 56 \t Batch 800 \t Training Loss: 40.126380395889285\n",
      "Epoch 56 \t Batch 820 \t Training Loss: 40.144698668689266\n",
      "Epoch 56 \t Batch 840 \t Training Loss: 40.11867035911197\n",
      "Epoch 56 \t Batch 860 \t Training Loss: 40.11556910137797\n",
      "Epoch 56 \t Batch 880 \t Training Loss: 40.14327980388295\n",
      "Epoch 56 \t Batch 900 \t Training Loss: 40.148795229593915\n",
      "Epoch 56 \t Batch 20 \t Validation Loss: 10.078428196907044\n",
      "Epoch 56 \t Batch 40 \t Validation Loss: 13.043159937858581\n",
      "Epoch 56 \t Batch 60 \t Validation Loss: 12.574533176422118\n",
      "Epoch 56 \t Batch 80 \t Validation Loss: 14.00961280465126\n",
      "Epoch 56 \t Batch 100 \t Validation Loss: 17.231545205116273\n",
      "Epoch 56 \t Batch 120 \t Validation Loss: 19.575421488285066\n",
      "Epoch 56 \t Batch 140 \t Validation Loss: 20.87028076648712\n",
      "Epoch 56 \t Batch 160 \t Validation Loss: 23.48307418525219\n",
      "Epoch 56 \t Batch 180 \t Validation Loss: 26.98671963214874\n",
      "Epoch 56 \t Batch 200 \t Validation Loss: 28.931082832813264\n",
      "Epoch 56 \t Batch 220 \t Validation Loss: 30.450184893608093\n",
      "Epoch 56 \t Batch 240 \t Validation Loss: 31.099462018410364\n",
      "Epoch 56 \t Batch 260 \t Validation Loss: 33.582670496060295\n",
      "Epoch 56 \t Batch 280 \t Validation Loss: 35.01946257352829\n",
      "Epoch 56 \t Batch 300 \t Validation Loss: 35.92465416749319\n",
      "Epoch 56 \t Batch 320 \t Validation Loss: 36.40033893436193\n",
      "Epoch 56 \t Batch 340 \t Validation Loss: 36.5024797706043\n",
      "Epoch 56 \t Batch 360 \t Validation Loss: 36.441291458076904\n",
      "Epoch 56 \t Batch 380 \t Validation Loss: 37.00086191453432\n",
      "Epoch 56 \t Batch 400 \t Validation Loss: 36.94331618189812\n",
      "Epoch 56 \t Batch 420 \t Validation Loss: 37.18257738181523\n",
      "Epoch 56 \t Batch 440 \t Validation Loss: 37.26464491649107\n",
      "Epoch 56 \t Batch 460 \t Validation Loss: 37.869254965367524\n",
      "Epoch 56 \t Batch 480 \t Validation Loss: 38.43133678734303\n",
      "Epoch 56 \t Batch 500 \t Validation Loss: 38.248792719841006\n",
      "Epoch 56 \t Batch 520 \t Validation Loss: 38.48066648978453\n",
      "Epoch 56 \t Batch 540 \t Validation Loss: 38.29611174530453\n",
      "Epoch 56 \t Batch 560 \t Validation Loss: 38.188601097890306\n",
      "Epoch 56 \t Batch 580 \t Validation Loss: 38.0692020671121\n",
      "Epoch 56 \t Batch 600 \t Validation Loss: 38.32469855864843\n",
      "Epoch 56 Training Loss: 40.16222183160979 Validation Loss: 39.040250464693294\n",
      "Epoch 56 completed\n",
      "Epoch 57 \t Batch 20 \t Training Loss: 39.719845581054685\n",
      "Epoch 57 \t Batch 40 \t Training Loss: 39.24039192199707\n",
      "Epoch 57 \t Batch 60 \t Training Loss: 39.30072873433431\n",
      "Epoch 57 \t Batch 80 \t Training Loss: 39.169654083251956\n",
      "Epoch 57 \t Batch 100 \t Training Loss: 39.28494483947754\n",
      "Epoch 57 \t Batch 120 \t Training Loss: 39.47893873850504\n",
      "Epoch 57 \t Batch 140 \t Training Loss: 39.43924056461879\n",
      "Epoch 57 \t Batch 160 \t Training Loss: 39.4566978931427\n",
      "Epoch 57 \t Batch 180 \t Training Loss: 39.44049021402995\n",
      "Epoch 57 \t Batch 200 \t Training Loss: 39.483072433471676\n",
      "Epoch 57 \t Batch 220 \t Training Loss: 39.49561176300049\n",
      "Epoch 57 \t Batch 240 \t Training Loss: 39.59027895927429\n",
      "Epoch 57 \t Batch 260 \t Training Loss: 39.59152864309458\n",
      "Epoch 57 \t Batch 280 \t Training Loss: 39.57170471463885\n",
      "Epoch 57 \t Batch 300 \t Training Loss: 39.592051137288415\n",
      "Epoch 57 \t Batch 320 \t Training Loss: 39.587712502479555\n",
      "Epoch 57 \t Batch 340 \t Training Loss: 39.632593783210304\n",
      "Epoch 57 \t Batch 360 \t Training Loss: 39.59126257366604\n",
      "Epoch 57 \t Batch 380 \t Training Loss: 39.5687772951628\n",
      "Epoch 57 \t Batch 400 \t Training Loss: 39.5705055141449\n",
      "Epoch 57 \t Batch 420 \t Training Loss: 39.579930087498255\n",
      "Epoch 57 \t Batch 440 \t Training Loss: 39.55280955054543\n",
      "Epoch 57 \t Batch 460 \t Training Loss: 39.584332482711126\n",
      "Epoch 57 \t Batch 480 \t Training Loss: 39.62853381633759\n",
      "Epoch 57 \t Batch 500 \t Training Loss: 39.60344230651855\n",
      "Epoch 57 \t Batch 520 \t Training Loss: 39.65154584004329\n",
      "Epoch 57 \t Batch 540 \t Training Loss: 39.674850188361276\n",
      "Epoch 57 \t Batch 560 \t Training Loss: 39.689923075267245\n",
      "Epoch 57 \t Batch 580 \t Training Loss: 39.72023290436843\n",
      "Epoch 57 \t Batch 600 \t Training Loss: 39.73851641337077\n",
      "Epoch 57 \t Batch 620 \t Training Loss: 39.751118229281516\n",
      "Epoch 57 \t Batch 640 \t Training Loss: 39.755446249246596\n",
      "Epoch 57 \t Batch 660 \t Training Loss: 39.748693240772596\n",
      "Epoch 57 \t Batch 680 \t Training Loss: 39.7771395963781\n",
      "Epoch 57 \t Batch 700 \t Training Loss: 39.7672828238351\n",
      "Epoch 57 \t Batch 720 \t Training Loss: 39.7878331290351\n",
      "Epoch 57 \t Batch 740 \t Training Loss: 39.799467076482\n",
      "Epoch 57 \t Batch 760 \t Training Loss: 39.83324123683729\n",
      "Epoch 57 \t Batch 780 \t Training Loss: 39.832995400061975\n",
      "Epoch 57 \t Batch 800 \t Training Loss: 39.80537531852722\n",
      "Epoch 57 \t Batch 820 \t Training Loss: 39.82925913740949\n",
      "Epoch 57 \t Batch 840 \t Training Loss: 39.811871857870194\n",
      "Epoch 57 \t Batch 860 \t Training Loss: 39.84010801758877\n",
      "Epoch 57 \t Batch 880 \t Training Loss: 39.83592601039193\n",
      "Epoch 57 \t Batch 900 \t Training Loss: 39.84805732091268\n",
      "Epoch 57 \t Batch 20 \t Validation Loss: 13.664434051513672\n",
      "Epoch 57 \t Batch 40 \t Validation Loss: 18.967370438575745\n",
      "Epoch 57 \t Batch 60 \t Validation Loss: 17.641232307751974\n",
      "Epoch 57 \t Batch 80 \t Validation Loss: 18.948870080709458\n",
      "Epoch 57 \t Batch 100 \t Validation Loss: 21.735410838127137\n",
      "Epoch 57 \t Batch 120 \t Validation Loss: 23.595534908771516\n",
      "Epoch 57 \t Batch 140 \t Validation Loss: 24.460943967955455\n",
      "Epoch 57 \t Batch 160 \t Validation Loss: 26.602416607737542\n",
      "Epoch 57 \t Batch 180 \t Validation Loss: 30.004385982619393\n",
      "Epoch 57 \t Batch 200 \t Validation Loss: 31.7393439412117\n",
      "Epoch 57 \t Batch 220 \t Validation Loss: 33.15010769800706\n",
      "Epoch 57 \t Batch 240 \t Validation Loss: 33.727415698766706\n",
      "Epoch 57 \t Batch 260 \t Validation Loss: 36.13618755890773\n",
      "Epoch 57 \t Batch 280 \t Validation Loss: 37.47958255665643\n",
      "Epoch 57 \t Batch 300 \t Validation Loss: 38.31807731151581\n",
      "Epoch 57 \t Batch 320 \t Validation Loss: 38.77229919880629\n",
      "Epoch 57 \t Batch 340 \t Validation Loss: 38.772037319576036\n",
      "Epoch 57 \t Batch 360 \t Validation Loss: 38.61430694659551\n",
      "Epoch 57 \t Batch 380 \t Validation Loss: 38.99033585473111\n",
      "Epoch 57 \t Batch 400 \t Validation Loss: 38.75039355158806\n",
      "Epoch 57 \t Batch 420 \t Validation Loss: 38.84963405586424\n",
      "Epoch 57 \t Batch 440 \t Validation Loss: 38.713917367024855\n",
      "Epoch 57 \t Batch 460 \t Validation Loss: 39.06180436818496\n",
      "Epoch 57 \t Batch 480 \t Validation Loss: 39.60355315506458\n",
      "Epoch 57 \t Batch 500 \t Validation Loss: 39.34218331050873\n",
      "Epoch 57 \t Batch 520 \t Validation Loss: 39.284586568062124\n",
      "Epoch 57 \t Batch 540 \t Validation Loss: 39.10572768935451\n",
      "Epoch 57 \t Batch 560 \t Validation Loss: 39.00640613606998\n",
      "Epoch 57 \t Batch 580 \t Validation Loss: 38.857628962911406\n",
      "Epoch 57 \t Batch 600 \t Validation Loss: 39.1452227584521\n",
      "Epoch 57 Training Loss: 39.85638311879065 Validation Loss: 39.96676131579783\n",
      "Epoch 57 completed\n",
      "Epoch 58 \t Batch 20 \t Training Loss: 39.34881744384766\n",
      "Epoch 58 \t Batch 40 \t Training Loss: 39.23819446563721\n",
      "Epoch 58 \t Batch 60 \t Training Loss: 39.15437227884929\n",
      "Epoch 58 \t Batch 80 \t Training Loss: 38.9441915512085\n",
      "Epoch 58 \t Batch 100 \t Training Loss: 38.588027801513675\n",
      "Epoch 58 \t Batch 120 \t Training Loss: 38.51173152923584\n",
      "Epoch 58 \t Batch 140 \t Training Loss: 38.659186935424806\n",
      "Epoch 58 \t Batch 160 \t Training Loss: 38.94612321853638\n",
      "Epoch 58 \t Batch 180 \t Training Loss: 38.953154140048554\n",
      "Epoch 58 \t Batch 200 \t Training Loss: 39.000553588867184\n",
      "Epoch 58 \t Batch 220 \t Training Loss: 39.04731018759988\n",
      "Epoch 58 \t Batch 240 \t Training Loss: 39.17688749631246\n",
      "Epoch 58 \t Batch 260 \t Training Loss: 39.31172841879038\n",
      "Epoch 58 \t Batch 280 \t Training Loss: 39.23613977432251\n",
      "Epoch 58 \t Batch 300 \t Training Loss: 39.28177075703939\n",
      "Epoch 58 \t Batch 320 \t Training Loss: 39.31588851213455\n",
      "Epoch 58 \t Batch 340 \t Training Loss: 39.35166645050049\n",
      "Epoch 58 \t Batch 360 \t Training Loss: 39.37521188524034\n",
      "Epoch 58 \t Batch 380 \t Training Loss: 39.41260409104196\n",
      "Epoch 58 \t Batch 400 \t Training Loss: 39.41036814689636\n",
      "Epoch 58 \t Batch 420 \t Training Loss: 39.48790341331845\n",
      "Epoch 58 \t Batch 440 \t Training Loss: 39.50696109424938\n",
      "Epoch 58 \t Batch 460 \t Training Loss: 39.52411133310069\n",
      "Epoch 58 \t Batch 480 \t Training Loss: 39.53090079625448\n",
      "Epoch 58 \t Batch 500 \t Training Loss: 39.55835828399658\n",
      "Epoch 58 \t Batch 520 \t Training Loss: 39.554940583155705\n",
      "Epoch 58 \t Batch 540 \t Training Loss: 39.59147100095396\n",
      "Epoch 58 \t Batch 560 \t Training Loss: 39.614210857663835\n",
      "Epoch 58 \t Batch 580 \t Training Loss: 39.62923349183181\n",
      "Epoch 58 \t Batch 600 \t Training Loss: 39.61667854309082\n",
      "Epoch 58 \t Batch 620 \t Training Loss: 39.59440671243975\n",
      "Epoch 58 \t Batch 640 \t Training Loss: 39.605302369594575\n",
      "Epoch 58 \t Batch 660 \t Training Loss: 39.59882884748054\n",
      "Epoch 58 \t Batch 680 \t Training Loss: 39.59325205298031\n",
      "Epoch 58 \t Batch 700 \t Training Loss: 39.582532642909456\n",
      "Epoch 58 \t Batch 720 \t Training Loss: 39.56231005986532\n",
      "Epoch 58 \t Batch 740 \t Training Loss: 39.58239845069679\n",
      "Epoch 58 \t Batch 760 \t Training Loss: 39.60319702750758\n",
      "Epoch 58 \t Batch 780 \t Training Loss: 39.58277796720847\n",
      "Epoch 58 \t Batch 800 \t Training Loss: 39.55850392818451\n",
      "Epoch 58 \t Batch 820 \t Training Loss: 39.58863513295243\n",
      "Epoch 58 \t Batch 840 \t Training Loss: 39.59592640740531\n",
      "Epoch 58 \t Batch 860 \t Training Loss: 39.60560805742131\n",
      "Epoch 58 \t Batch 880 \t Training Loss: 39.622253743085\n",
      "Epoch 58 \t Batch 900 \t Training Loss: 39.60652358161079\n",
      "Epoch 58 \t Batch 20 \t Validation Loss: 16.072249317169188\n",
      "Epoch 58 \t Batch 40 \t Validation Loss: 18.05322164297104\n",
      "Epoch 58 \t Batch 60 \t Validation Loss: 17.53379417260488\n",
      "Epoch 58 \t Batch 80 \t Validation Loss: 18.59349326491356\n",
      "Epoch 58 \t Batch 100 \t Validation Loss: 21.449224333763123\n",
      "Epoch 58 \t Batch 120 \t Validation Loss: 23.40218457778295\n",
      "Epoch 58 \t Batch 140 \t Validation Loss: 24.28645600931985\n",
      "Epoch 58 \t Batch 160 \t Validation Loss: 26.357000824809074\n",
      "Epoch 58 \t Batch 180 \t Validation Loss: 28.988697981834413\n",
      "Epoch 58 \t Batch 200 \t Validation Loss: 30.324927775859834\n",
      "Epoch 58 \t Batch 220 \t Validation Loss: 31.371738344972783\n",
      "Epoch 58 \t Batch 240 \t Validation Loss: 31.73498336672783\n",
      "Epoch 58 \t Batch 260 \t Validation Loss: 33.75565291368044\n",
      "Epoch 58 \t Batch 280 \t Validation Loss: 34.859273152691976\n",
      "Epoch 58 \t Batch 300 \t Validation Loss: 35.56709632078807\n",
      "Epoch 58 \t Batch 320 \t Validation Loss: 36.00385576635599\n",
      "Epoch 58 \t Batch 340 \t Validation Loss: 36.07880457850064\n",
      "Epoch 58 \t Batch 360 \t Validation Loss: 35.99725974533293\n",
      "Epoch 58 \t Batch 380 \t Validation Loss: 36.5607404370057\n",
      "Epoch 58 \t Batch 400 \t Validation Loss: 36.54680622696876\n",
      "Epoch 58 \t Batch 420 \t Validation Loss: 36.84054548059191\n",
      "Epoch 58 \t Batch 440 \t Validation Loss: 36.951238670132376\n",
      "Epoch 58 \t Batch 460 \t Validation Loss: 37.67709899881612\n",
      "Epoch 58 \t Batch 480 \t Validation Loss: 38.263080603877704\n",
      "Epoch 58 \t Batch 500 \t Validation Loss: 38.103324610710146\n",
      "Epoch 58 \t Batch 520 \t Validation Loss: 38.493810973717615\n",
      "Epoch 58 \t Batch 540 \t Validation Loss: 38.30915613969167\n",
      "Epoch 58 \t Batch 560 \t Validation Loss: 38.18068266340664\n",
      "Epoch 58 \t Batch 580 \t Validation Loss: 37.98556311130524\n",
      "Epoch 58 \t Batch 600 \t Validation Loss: 38.25405777057012\n",
      "Epoch 58 Training Loss: 39.59258050616981 Validation Loss: 38.956823061813004\n",
      "Epoch 58 completed\n",
      "Epoch 59 \t Batch 20 \t Training Loss: 38.48313274383545\n",
      "Epoch 59 \t Batch 40 \t Training Loss: 38.97244987487793\n",
      "Epoch 59 \t Batch 60 \t Training Loss: 38.84396203358968\n",
      "Epoch 59 \t Batch 80 \t Training Loss: 38.78993997573853\n",
      "Epoch 59 \t Batch 100 \t Training Loss: 38.76039012908936\n",
      "Epoch 59 \t Batch 120 \t Training Loss: 38.70587689081828\n",
      "Epoch 59 \t Batch 140 \t Training Loss: 38.83523990086147\n",
      "Epoch 59 \t Batch 160 \t Training Loss: 38.87792546749115\n",
      "Epoch 59 \t Batch 180 \t Training Loss: 38.77574708726671\n",
      "Epoch 59 \t Batch 200 \t Training Loss: 38.715885677337646\n",
      "Epoch 59 \t Batch 220 \t Training Loss: 38.79317919991233\n",
      "Epoch 59 \t Batch 240 \t Training Loss: 38.76633602778117\n",
      "Epoch 59 \t Batch 260 \t Training Loss: 38.80440518305852\n",
      "Epoch 59 \t Batch 280 \t Training Loss: 38.74949016571045\n",
      "Epoch 59 \t Batch 300 \t Training Loss: 38.73898952484131\n",
      "Epoch 59 \t Batch 320 \t Training Loss: 38.76103872060776\n",
      "Epoch 59 \t Batch 340 \t Training Loss: 38.79407677930944\n",
      "Epoch 59 \t Batch 360 \t Training Loss: 38.85303343137105\n",
      "Epoch 59 \t Batch 380 \t Training Loss: 38.88998420614945\n",
      "Epoch 59 \t Batch 400 \t Training Loss: 38.88607085704803\n",
      "Epoch 59 \t Batch 420 \t Training Loss: 38.88270942597162\n",
      "Epoch 59 \t Batch 440 \t Training Loss: 38.88982788866217\n",
      "Epoch 59 \t Batch 460 \t Training Loss: 38.898248958587644\n",
      "Epoch 59 \t Batch 480 \t Training Loss: 38.96627801656723\n",
      "Epoch 59 \t Batch 500 \t Training Loss: 38.99122864151001\n",
      "Epoch 59 \t Batch 520 \t Training Loss: 39.00058827400208\n",
      "Epoch 59 \t Batch 540 \t Training Loss: 38.9910843566612\n",
      "Epoch 59 \t Batch 560 \t Training Loss: 39.02702797140394\n",
      "Epoch 59 \t Batch 580 \t Training Loss: 39.04182853369877\n",
      "Epoch 59 \t Batch 600 \t Training Loss: 39.052539644241335\n",
      "Epoch 59 \t Batch 620 \t Training Loss: 39.05981280111497\n",
      "Epoch 59 \t Batch 640 \t Training Loss: 39.086072251200676\n",
      "Epoch 59 \t Batch 660 \t Training Loss: 39.10303857687748\n",
      "Epoch 59 \t Batch 680 \t Training Loss: 39.13606264170478\n",
      "Epoch 59 \t Batch 700 \t Training Loss: 39.17247978482928\n",
      "Epoch 59 \t Batch 720 \t Training Loss: 39.18287666903602\n",
      "Epoch 59 \t Batch 740 \t Training Loss: 39.22179518261471\n",
      "Epoch 59 \t Batch 760 \t Training Loss: 39.2293097721903\n",
      "Epoch 59 \t Batch 780 \t Training Loss: 39.23428243734898\n",
      "Epoch 59 \t Batch 800 \t Training Loss: 39.22406363248825\n",
      "Epoch 59 \t Batch 820 \t Training Loss: 39.22698051871323\n",
      "Epoch 59 \t Batch 840 \t Training Loss: 39.257709137598674\n",
      "Epoch 59 \t Batch 860 \t Training Loss: 39.28712225403897\n",
      "Epoch 59 \t Batch 880 \t Training Loss: 39.300120412219655\n",
      "Epoch 59 \t Batch 900 \t Training Loss: 39.30986736509535\n",
      "Epoch 59 \t Batch 20 \t Validation Loss: 10.08425862789154\n",
      "Epoch 59 \t Batch 40 \t Validation Loss: 14.18137105703354\n",
      "Epoch 59 \t Batch 60 \t Validation Loss: 13.581883470217386\n",
      "Epoch 59 \t Batch 80 \t Validation Loss: 14.941117709875106\n",
      "Epoch 59 \t Batch 100 \t Validation Loss: 18.311486697196962\n",
      "Epoch 59 \t Batch 120 \t Validation Loss: 20.668104406197866\n",
      "Epoch 59 \t Batch 140 \t Validation Loss: 21.850431043761116\n",
      "Epoch 59 \t Batch 160 \t Validation Loss: 24.62438461482525\n",
      "Epoch 59 \t Batch 180 \t Validation Loss: 29.05318490929074\n",
      "Epoch 59 \t Batch 200 \t Validation Loss: 31.36292556524277\n",
      "Epoch 59 \t Batch 220 \t Validation Loss: 33.31885337612846\n",
      "Epoch 59 \t Batch 240 \t Validation Loss: 34.197015855709715\n",
      "Epoch 59 \t Batch 260 \t Validation Loss: 36.877616161566515\n",
      "Epoch 59 \t Batch 280 \t Validation Loss: 38.372164353302544\n",
      "Epoch 59 \t Batch 300 \t Validation Loss: 39.61160227616628\n",
      "Epoch 59 \t Batch 320 \t Validation Loss: 40.22180493026972\n",
      "Epoch 59 \t Batch 340 \t Validation Loss: 40.28222643487594\n",
      "Epoch 59 \t Batch 360 \t Validation Loss: 40.18768830961651\n",
      "Epoch 59 \t Batch 380 \t Validation Loss: 40.63953075032485\n",
      "Epoch 59 \t Batch 400 \t Validation Loss: 40.393418422937394\n",
      "Epoch 59 \t Batch 420 \t Validation Loss: 40.545738048780535\n",
      "Epoch 59 \t Batch 440 \t Validation Loss: 40.402013233574955\n",
      "Epoch 59 \t Batch 460 \t Validation Loss: 40.74203611975131\n",
      "Epoch 59 \t Batch 480 \t Validation Loss: 41.257396480441095\n",
      "Epoch 59 \t Batch 500 \t Validation Loss: 41.00640022754669\n",
      "Epoch 59 \t Batch 520 \t Validation Loss: 40.96933864171688\n",
      "Epoch 59 \t Batch 540 \t Validation Loss: 40.71319990599597\n",
      "Epoch 59 \t Batch 560 \t Validation Loss: 40.600823307888845\n",
      "Epoch 59 \t Batch 580 \t Validation Loss: 40.44353551617984\n",
      "Epoch 59 \t Batch 600 \t Validation Loss: 40.65910534302394\n",
      "Epoch 59 Training Loss: 39.30824351128731 Validation Loss: 41.3765267064045\n",
      "Epoch 59 completed\n",
      "Epoch 60 \t Batch 20 \t Training Loss: 37.53946800231934\n",
      "Epoch 60 \t Batch 40 \t Training Loss: 37.735879611969\n",
      "Epoch 60 \t Batch 60 \t Training Loss: 37.529745992024736\n",
      "Epoch 60 \t Batch 80 \t Training Loss: 37.75490612983704\n",
      "Epoch 60 \t Batch 100 \t Training Loss: 37.93235126495361\n",
      "Epoch 60 \t Batch 120 \t Training Loss: 38.10125910441081\n",
      "Epoch 60 \t Batch 140 \t Training Loss: 38.37685274396624\n",
      "Epoch 60 \t Batch 160 \t Training Loss: 38.227319192886355\n",
      "Epoch 60 \t Batch 180 \t Training Loss: 38.269108560350205\n",
      "Epoch 60 \t Batch 200 \t Training Loss: 38.356450958251955\n",
      "Epoch 60 \t Batch 220 \t Training Loss: 38.44064861644398\n",
      "Epoch 60 \t Batch 240 \t Training Loss: 38.509453264872235\n",
      "Epoch 60 \t Batch 260 \t Training Loss: 38.55905926044171\n",
      "Epoch 60 \t Batch 280 \t Training Loss: 38.6240416935512\n",
      "Epoch 60 \t Batch 300 \t Training Loss: 38.59027618408203\n",
      "Epoch 60 \t Batch 320 \t Training Loss: 38.68874170780182\n",
      "Epoch 60 \t Batch 340 \t Training Loss: 38.653402496786676\n",
      "Epoch 60 \t Batch 360 \t Training Loss: 38.769795417785645\n",
      "Epoch 60 \t Batch 380 \t Training Loss: 38.68953304290771\n",
      "Epoch 60 \t Batch 400 \t Training Loss: 38.69764143943787\n",
      "Epoch 60 \t Batch 420 \t Training Loss: 38.71273840949649\n",
      "Epoch 60 \t Batch 440 \t Training Loss: 38.747346834702924\n",
      "Epoch 60 \t Batch 460 \t Training Loss: 38.77693969892419\n",
      "Epoch 60 \t Batch 480 \t Training Loss: 38.77332491874695\n",
      "Epoch 60 \t Batch 500 \t Training Loss: 38.78864017486572\n",
      "Epoch 60 \t Batch 520 \t Training Loss: 38.82910812818087\n",
      "Epoch 60 \t Batch 540 \t Training Loss: 38.832550356123186\n",
      "Epoch 60 \t Batch 560 \t Training Loss: 38.81979796205248\n",
      "Epoch 60 \t Batch 580 \t Training Loss: 38.83161579329392\n",
      "Epoch 60 \t Batch 600 \t Training Loss: 38.80294058799743\n",
      "Epoch 60 \t Batch 620 \t Training Loss: 38.85268668205507\n",
      "Epoch 60 \t Batch 640 \t Training Loss: 38.85009599626064\n",
      "Epoch 60 \t Batch 660 \t Training Loss: 38.88980795253407\n",
      "Epoch 60 \t Batch 680 \t Training Loss: 38.919992387996004\n",
      "Epoch 60 \t Batch 700 \t Training Loss: 38.966806746891564\n",
      "Epoch 60 \t Batch 720 \t Training Loss: 38.96377136707306\n",
      "Epoch 60 \t Batch 740 \t Training Loss: 38.97685768282091\n",
      "Epoch 60 \t Batch 760 \t Training Loss: 38.95991012924596\n",
      "Epoch 60 \t Batch 780 \t Training Loss: 38.94459773088113\n",
      "Epoch 60 \t Batch 800 \t Training Loss: 38.94290956258774\n",
      "Epoch 60 \t Batch 820 \t Training Loss: 38.95519635968092\n",
      "Epoch 60 \t Batch 840 \t Training Loss: 38.96242024784996\n",
      "Epoch 60 \t Batch 860 \t Training Loss: 38.9716699755469\n",
      "Epoch 60 \t Batch 880 \t Training Loss: 39.00825617313385\n",
      "Epoch 60 \t Batch 900 \t Training Loss: 39.01546407699585\n",
      "Epoch 60 \t Batch 20 \t Validation Loss: 9.977477264404296\n",
      "Epoch 60 \t Batch 40 \t Validation Loss: 13.983873629570008\n",
      "Epoch 60 \t Batch 60 \t Validation Loss: 13.28948036034902\n",
      "Epoch 60 \t Batch 80 \t Validation Loss: 14.781545329093934\n",
      "Epoch 60 \t Batch 100 \t Validation Loss: 17.893413429260253\n",
      "Epoch 60 \t Batch 120 \t Validation Loss: 20.17193739414215\n",
      "Epoch 60 \t Batch 140 \t Validation Loss: 21.32941038267953\n",
      "Epoch 60 \t Batch 160 \t Validation Loss: 23.945932382345198\n",
      "Epoch 60 \t Batch 180 \t Validation Loss: 27.464101049635147\n",
      "Epoch 60 \t Batch 200 \t Validation Loss: 29.55290840148926\n",
      "Epoch 60 \t Batch 220 \t Validation Loss: 31.118792698600075\n",
      "Epoch 60 \t Batch 240 \t Validation Loss: 31.802821254730226\n",
      "Epoch 60 \t Batch 260 \t Validation Loss: 34.344940574352556\n",
      "Epoch 60 \t Batch 280 \t Validation Loss: 35.776081347465514\n",
      "Epoch 60 \t Batch 300 \t Validation Loss: 36.6199835173289\n",
      "Epoch 60 \t Batch 320 \t Validation Loss: 37.11145628988743\n",
      "Epoch 60 \t Batch 340 \t Validation Loss: 37.24750057949739\n",
      "Epoch 60 \t Batch 360 \t Validation Loss: 37.16371844344669\n",
      "Epoch 60 \t Batch 380 \t Validation Loss: 37.70344175288552\n",
      "Epoch 60 \t Batch 400 \t Validation Loss: 37.64043813943863\n",
      "Epoch 60 \t Batch 420 \t Validation Loss: 37.888179872149514\n",
      "Epoch 60 \t Batch 440 \t Validation Loss: 37.986610839583655\n",
      "Epoch 60 \t Batch 460 \t Validation Loss: 38.584111112097034\n",
      "Epoch 60 \t Batch 480 \t Validation Loss: 39.14166770577431\n",
      "Epoch 60 \t Batch 500 \t Validation Loss: 38.96124218559265\n",
      "Epoch 60 \t Batch 520 \t Validation Loss: 39.17313917783591\n",
      "Epoch 60 \t Batch 540 \t Validation Loss: 39.02067067711442\n",
      "Epoch 60 \t Batch 560 \t Validation Loss: 38.949862849712375\n",
      "Epoch 60 \t Batch 580 \t Validation Loss: 38.774366778340834\n",
      "Epoch 60 \t Batch 600 \t Validation Loss: 39.0967119773229\n",
      "Epoch 60 Training Loss: 39.03682053336141 Validation Loss: 39.799999176681816\n",
      "Epoch 60 completed\n",
      "Epoch 61 \t Batch 20 \t Training Loss: 37.887682342529295\n",
      "Epoch 61 \t Batch 40 \t Training Loss: 37.71320524215698\n",
      "Epoch 61 \t Batch 60 \t Training Loss: 37.80798892974853\n",
      "Epoch 61 \t Batch 80 \t Training Loss: 37.85116119384766\n",
      "Epoch 61 \t Batch 100 \t Training Loss: 38.0480541229248\n",
      "Epoch 61 \t Batch 120 \t Training Loss: 38.061901823679605\n",
      "Epoch 61 \t Batch 140 \t Training Loss: 38.12099279676165\n",
      "Epoch 61 \t Batch 160 \t Training Loss: 38.18435959815979\n",
      "Epoch 61 \t Batch 180 \t Training Loss: 38.190982500712074\n",
      "Epoch 61 \t Batch 200 \t Training Loss: 38.28180324554443\n",
      "Epoch 61 \t Batch 220 \t Training Loss: 38.25211547504772\n",
      "Epoch 61 \t Batch 240 \t Training Loss: 38.25074348449707\n",
      "Epoch 61 \t Batch 260 \t Training Loss: 38.31286455301138\n",
      "Epoch 61 \t Batch 280 \t Training Loss: 38.295031152452744\n",
      "Epoch 61 \t Batch 300 \t Training Loss: 38.31446483612061\n",
      "Epoch 61 \t Batch 320 \t Training Loss: 38.268828058242796\n",
      "Epoch 61 \t Batch 340 \t Training Loss: 38.361652710858515\n",
      "Epoch 61 \t Batch 360 \t Training Loss: 38.34752197265625\n",
      "Epoch 61 \t Batch 380 \t Training Loss: 38.390338039398195\n",
      "Epoch 61 \t Batch 400 \t Training Loss: 38.396915879249576\n",
      "Epoch 61 \t Batch 420 \t Training Loss: 38.41268863223848\n",
      "Epoch 61 \t Batch 440 \t Training Loss: 38.470392578298394\n",
      "Epoch 61 \t Batch 460 \t Training Loss: 38.47246677149897\n",
      "Epoch 61 \t Batch 480 \t Training Loss: 38.46648870706558\n",
      "Epoch 61 \t Batch 500 \t Training Loss: 38.52385322189331\n",
      "Epoch 61 \t Batch 520 \t Training Loss: 38.53992629784804\n",
      "Epoch 61 \t Batch 540 \t Training Loss: 38.5220108350118\n",
      "Epoch 61 \t Batch 560 \t Training Loss: 38.57587690012796\n",
      "Epoch 61 \t Batch 580 \t Training Loss: 38.63878237954501\n",
      "Epoch 61 \t Batch 600 \t Training Loss: 38.663684377670286\n",
      "Epoch 61 \t Batch 620 \t Training Loss: 38.69065363176407\n",
      "Epoch 61 \t Batch 640 \t Training Loss: 38.69259077012539\n",
      "Epoch 61 \t Batch 660 \t Training Loss: 38.65369365865534\n",
      "Epoch 61 \t Batch 680 \t Training Loss: 38.658352938820336\n",
      "Epoch 61 \t Batch 700 \t Training Loss: 38.655079209463935\n",
      "Epoch 61 \t Batch 720 \t Training Loss: 38.687030951182045\n",
      "Epoch 61 \t Batch 740 \t Training Loss: 38.69201251880543\n",
      "Epoch 61 \t Batch 760 \t Training Loss: 38.689647679579885\n",
      "Epoch 61 \t Batch 780 \t Training Loss: 38.6848368864793\n",
      "Epoch 61 \t Batch 800 \t Training Loss: 38.67998054504395\n",
      "Epoch 61 \t Batch 820 \t Training Loss: 38.67892222055575\n",
      "Epoch 61 \t Batch 840 \t Training Loss: 38.69132130032494\n",
      "Epoch 61 \t Batch 860 \t Training Loss: 38.699745195965434\n",
      "Epoch 61 \t Batch 880 \t Training Loss: 38.699493863365866\n",
      "Epoch 61 \t Batch 900 \t Training Loss: 38.719953265719944\n",
      "Epoch 61 \t Batch 20 \t Validation Loss: 10.99511775970459\n",
      "Epoch 61 \t Batch 40 \t Validation Loss: 14.429768300056457\n",
      "Epoch 61 \t Batch 60 \t Validation Loss: 13.873358138402303\n",
      "Epoch 61 \t Batch 80 \t Validation Loss: 15.21772872209549\n",
      "Epoch 61 \t Batch 100 \t Validation Loss: 18.33139344215393\n",
      "Epoch 61 \t Batch 120 \t Validation Loss: 20.472147409121195\n",
      "Epoch 61 \t Batch 140 \t Validation Loss: 21.696982908248902\n",
      "Epoch 61 \t Batch 160 \t Validation Loss: 24.3242007791996\n",
      "Epoch 61 \t Batch 180 \t Validation Loss: 28.21781899134318\n",
      "Epoch 61 \t Batch 200 \t Validation Loss: 30.20054488182068\n",
      "Epoch 61 \t Batch 220 \t Validation Loss: 31.904912974617698\n",
      "Epoch 61 \t Batch 240 \t Validation Loss: 32.688198399543765\n",
      "Epoch 61 \t Batch 260 \t Validation Loss: 35.24407534232506\n",
      "Epoch 61 \t Batch 280 \t Validation Loss: 36.73440205369677\n",
      "Epoch 61 \t Batch 300 \t Validation Loss: 37.76592908541362\n",
      "Epoch 61 \t Batch 320 \t Validation Loss: 38.368199172616\n",
      "Epoch 61 \t Batch 340 \t Validation Loss: 38.50238846891067\n",
      "Epoch 61 \t Batch 360 \t Validation Loss: 38.42422978878021\n",
      "Epoch 61 \t Batch 380 \t Validation Loss: 38.927846549686635\n",
      "Epoch 61 \t Batch 400 \t Validation Loss: 38.82313562631607\n",
      "Epoch 61 \t Batch 420 \t Validation Loss: 39.01672267686753\n",
      "Epoch 61 \t Batch 440 \t Validation Loss: 39.065621772679414\n",
      "Epoch 61 \t Batch 460 \t Validation Loss: 39.58790058260379\n",
      "Epoch 61 \t Batch 480 \t Validation Loss: 40.11607320110003\n",
      "Epoch 61 \t Batch 500 \t Validation Loss: 39.89673874092102\n",
      "Epoch 61 \t Batch 520 \t Validation Loss: 40.00687314730424\n",
      "Epoch 61 \t Batch 540 \t Validation Loss: 39.836646699905394\n",
      "Epoch 61 \t Batch 560 \t Validation Loss: 39.73427584341594\n",
      "Epoch 61 \t Batch 580 \t Validation Loss: 39.51738495169015\n",
      "Epoch 61 \t Batch 600 \t Validation Loss: 39.79799915154775\n",
      "Epoch 61 Training Loss: 38.72837054586463 Validation Loss: 40.50273175518234\n",
      "Epoch 61 completed\n",
      "Epoch 62 \t Batch 20 \t Training Loss: 37.143276405334475\n",
      "Epoch 62 \t Batch 40 \t Training Loss: 37.361898040771486\n",
      "Epoch 62 \t Batch 60 \t Training Loss: 37.39511324564616\n",
      "Epoch 62 \t Batch 80 \t Training Loss: 37.41728658676148\n",
      "Epoch 62 \t Batch 100 \t Training Loss: 37.4847452545166\n",
      "Epoch 62 \t Batch 120 \t Training Loss: 37.57565453847249\n",
      "Epoch 62 \t Batch 140 \t Training Loss: 37.65639724731445\n",
      "Epoch 62 \t Batch 160 \t Training Loss: 37.76095175743103\n",
      "Epoch 62 \t Batch 180 \t Training Loss: 37.740405866834855\n",
      "Epoch 62 \t Batch 200 \t Training Loss: 37.82989246368408\n",
      "Epoch 62 \t Batch 220 \t Training Loss: 37.81097412109375\n",
      "Epoch 62 \t Batch 240 \t Training Loss: 37.8140296459198\n",
      "Epoch 62 \t Batch 260 \t Training Loss: 37.841298822256235\n",
      "Epoch 62 \t Batch 280 \t Training Loss: 37.86027461460659\n",
      "Epoch 62 \t Batch 300 \t Training Loss: 37.86507572174072\n",
      "Epoch 62 \t Batch 320 \t Training Loss: 37.90588771104812\n",
      "Epoch 62 \t Batch 340 \t Training Loss: 37.99837484920726\n",
      "Epoch 62 \t Batch 360 \t Training Loss: 38.11033828523424\n",
      "Epoch 62 \t Batch 380 \t Training Loss: 38.12948976817884\n",
      "Epoch 62 \t Batch 400 \t Training Loss: 38.19056900978089\n",
      "Epoch 62 \t Batch 420 \t Training Loss: 38.18393633706229\n",
      "Epoch 62 \t Batch 440 \t Training Loss: 38.18580832047896\n",
      "Epoch 62 \t Batch 460 \t Training Loss: 38.18577214116635\n",
      "Epoch 62 \t Batch 480 \t Training Loss: 38.23542624314626\n",
      "Epoch 62 \t Batch 500 \t Training Loss: 38.205429267883304\n",
      "Epoch 62 \t Batch 520 \t Training Loss: 38.27023121026846\n",
      "Epoch 62 \t Batch 540 \t Training Loss: 38.26608180293331\n",
      "Epoch 62 \t Batch 560 \t Training Loss: 38.30870041166033\n",
      "Epoch 62 \t Batch 580 \t Training Loss: 38.32153413049106\n",
      "Epoch 62 \t Batch 600 \t Training Loss: 38.376684296925866\n",
      "Epoch 62 \t Batch 620 \t Training Loss: 38.43690548558389\n",
      "Epoch 62 \t Batch 640 \t Training Loss: 38.426195698976514\n",
      "Epoch 62 \t Batch 660 \t Training Loss: 38.43158798217773\n",
      "Epoch 62 \t Batch 680 \t Training Loss: 38.42904288909015\n",
      "Epoch 62 \t Batch 700 \t Training Loss: 38.43244904654367\n",
      "Epoch 62 \t Batch 720 \t Training Loss: 38.460431549284195\n",
      "Epoch 62 \t Batch 740 \t Training Loss: 38.49157009124756\n",
      "Epoch 62 \t Batch 760 \t Training Loss: 38.48073706375925\n",
      "Epoch 62 \t Batch 780 \t Training Loss: 38.45600101764386\n",
      "Epoch 62 \t Batch 800 \t Training Loss: 38.447755823135374\n",
      "Epoch 62 \t Batch 820 \t Training Loss: 38.4434988114892\n",
      "Epoch 62 \t Batch 840 \t Training Loss: 38.46878657113938\n",
      "Epoch 62 \t Batch 860 \t Training Loss: 38.487082663247755\n",
      "Epoch 62 \t Batch 880 \t Training Loss: 38.50428400039673\n",
      "Epoch 62 \t Batch 900 \t Training Loss: 38.48392414940728\n",
      "Epoch 62 \t Batch 20 \t Validation Loss: 10.457431507110595\n",
      "Epoch 62 \t Batch 40 \t Validation Loss: 12.955090129375458\n",
      "Epoch 62 \t Batch 60 \t Validation Loss: 12.871123846371969\n",
      "Epoch 62 \t Batch 80 \t Validation Loss: 14.240271538496017\n",
      "Epoch 62 \t Batch 100 \t Validation Loss: 17.43942527294159\n",
      "Epoch 62 \t Batch 120 \t Validation Loss: 19.780388247966766\n",
      "Epoch 62 \t Batch 140 \t Validation Loss: 21.071039693696157\n",
      "Epoch 62 \t Batch 160 \t Validation Loss: 23.787587121129036\n",
      "Epoch 62 \t Batch 180 \t Validation Loss: 27.840665168232388\n",
      "Epoch 62 \t Batch 200 \t Validation Loss: 30.01614844083786\n",
      "Epoch 62 \t Batch 220 \t Validation Loss: 31.796323141184722\n",
      "Epoch 62 \t Batch 240 \t Validation Loss: 32.611551461617154\n",
      "Epoch 62 \t Batch 260 \t Validation Loss: 35.228261645023636\n",
      "Epoch 62 \t Batch 280 \t Validation Loss: 36.70009863887515\n",
      "Epoch 62 \t Batch 300 \t Validation Loss: 37.784018867810566\n",
      "Epoch 62 \t Batch 320 \t Validation Loss: 38.412532563507554\n",
      "Epoch 62 \t Batch 340 \t Validation Loss: 38.50756695831523\n",
      "Epoch 62 \t Batch 360 \t Validation Loss: 38.41874268982146\n",
      "Epoch 62 \t Batch 380 \t Validation Loss: 38.89485822100389\n",
      "Epoch 62 \t Batch 400 \t Validation Loss: 38.6902231657505\n",
      "Epoch 62 \t Batch 420 \t Validation Loss: 38.85727361610957\n",
      "Epoch 62 \t Batch 440 \t Validation Loss: 38.77179012406956\n",
      "Epoch 62 \t Batch 460 \t Validation Loss: 39.21383374773938\n",
      "Epoch 62 \t Batch 480 \t Validation Loss: 39.78778944512208\n",
      "Epoch 62 \t Batch 500 \t Validation Loss: 39.58733891391754\n",
      "Epoch 62 \t Batch 520 \t Validation Loss: 39.63293351485179\n",
      "Epoch 62 \t Batch 540 \t Validation Loss: 39.4707629636482\n",
      "Epoch 62 \t Batch 560 \t Validation Loss: 39.38595579436847\n",
      "Epoch 62 \t Batch 580 \t Validation Loss: 39.260357466237295\n",
      "Epoch 62 \t Batch 600 \t Validation Loss: 39.53592723290126\n",
      "Epoch 62 Training Loss: 38.503814202228575 Validation Loss: 40.31161757645669\n",
      "Epoch 62 completed\n",
      "Epoch 63 \t Batch 20 \t Training Loss: 37.7137809753418\n",
      "Epoch 63 \t Batch 40 \t Training Loss: 36.90549502372742\n",
      "Epoch 63 \t Batch 60 \t Training Loss: 37.25669527053833\n",
      "Epoch 63 \t Batch 80 \t Training Loss: 37.439963555336\n",
      "Epoch 63 \t Batch 100 \t Training Loss: 37.58246141433716\n",
      "Epoch 63 \t Batch 120 \t Training Loss: 37.759003432591754\n",
      "Epoch 63 \t Batch 140 \t Training Loss: 37.718361377716064\n",
      "Epoch 63 \t Batch 160 \t Training Loss: 37.66666713953018\n",
      "Epoch 63 \t Batch 180 \t Training Loss: 37.58792273203532\n",
      "Epoch 63 \t Batch 200 \t Training Loss: 37.65660265922546\n",
      "Epoch 63 \t Batch 220 \t Training Loss: 37.73278691551902\n",
      "Epoch 63 \t Batch 240 \t Training Loss: 37.72277599175771\n",
      "Epoch 63 \t Batch 260 \t Training Loss: 37.75026373496422\n",
      "Epoch 63 \t Batch 280 \t Training Loss: 37.79392247200012\n",
      "Epoch 63 \t Batch 300 \t Training Loss: 37.88149282455444\n",
      "Epoch 63 \t Batch 320 \t Training Loss: 37.882984751462935\n",
      "Epoch 63 \t Batch 340 \t Training Loss: 37.88481178844676\n",
      "Epoch 63 \t Batch 360 \t Training Loss: 37.91296297179328\n",
      "Epoch 63 \t Batch 380 \t Training Loss: 37.92392024491963\n",
      "Epoch 63 \t Batch 400 \t Training Loss: 37.97084308147431\n",
      "Epoch 63 \t Batch 420 \t Training Loss: 37.98568768728347\n",
      "Epoch 63 \t Batch 440 \t Training Loss: 38.00354888655922\n",
      "Epoch 63 \t Batch 460 \t Training Loss: 38.00576371731965\n",
      "Epoch 63 \t Batch 480 \t Training Loss: 38.010888330141704\n",
      "Epoch 63 \t Batch 500 \t Training Loss: 37.990101249694824\n",
      "Epoch 63 \t Batch 520 \t Training Loss: 38.005875147306\n",
      "Epoch 63 \t Batch 540 \t Training Loss: 37.97600277088306\n",
      "Epoch 63 \t Batch 560 \t Training Loss: 37.944034235818044\n",
      "Epoch 63 \t Batch 580 \t Training Loss: 37.96488496846166\n",
      "Epoch 63 \t Batch 600 \t Training Loss: 38.013834120432534\n",
      "Epoch 63 \t Batch 620 \t Training Loss: 38.006918261128085\n",
      "Epoch 63 \t Batch 640 \t Training Loss: 38.01805031895638\n",
      "Epoch 63 \t Batch 660 \t Training Loss: 38.03239787130645\n",
      "Epoch 63 \t Batch 680 \t Training Loss: 38.090572424495925\n",
      "Epoch 63 \t Batch 700 \t Training Loss: 38.12743996756417\n",
      "Epoch 63 \t Batch 720 \t Training Loss: 38.13505406909519\n",
      "Epoch 63 \t Batch 740 \t Training Loss: 38.159101666630924\n",
      "Epoch 63 \t Batch 760 \t Training Loss: 38.15224856828389\n",
      "Epoch 63 \t Batch 780 \t Training Loss: 38.16670341002636\n",
      "Epoch 63 \t Batch 800 \t Training Loss: 38.18735721588135\n",
      "Epoch 63 \t Batch 820 \t Training Loss: 38.17267832407137\n",
      "Epoch 63 \t Batch 840 \t Training Loss: 38.198149830954414\n",
      "Epoch 63 \t Batch 860 \t Training Loss: 38.18167584663214\n",
      "Epoch 63 \t Batch 880 \t Training Loss: 38.18459896391089\n",
      "Epoch 63 \t Batch 900 \t Training Loss: 38.19190623601278\n",
      "Epoch 63 \t Batch 20 \t Validation Loss: 10.6241375207901\n",
      "Epoch 63 \t Batch 40 \t Validation Loss: 15.490515625476837\n",
      "Epoch 63 \t Batch 60 \t Validation Loss: 14.258976777394613\n",
      "Epoch 63 \t Batch 80 \t Validation Loss: 15.398309445381164\n",
      "Epoch 63 \t Batch 100 \t Validation Loss: 18.61101713180542\n",
      "Epoch 63 \t Batch 120 \t Validation Loss: 20.89014433224996\n",
      "Epoch 63 \t Batch 140 \t Validation Loss: 22.008028439113072\n",
      "Epoch 63 \t Batch 160 \t Validation Loss: 24.50493711233139\n",
      "Epoch 63 \t Batch 180 \t Validation Loss: 28.26003270149231\n",
      "Epoch 63 \t Batch 200 \t Validation Loss: 30.220249848365782\n",
      "Epoch 63 \t Batch 220 \t Validation Loss: 31.846825274554167\n",
      "Epoch 63 \t Batch 240 \t Validation Loss: 32.57508643468221\n",
      "Epoch 63 \t Batch 260 \t Validation Loss: 35.07399919583247\n",
      "Epoch 63 \t Batch 280 \t Validation Loss: 36.48365972042084\n",
      "Epoch 63 \t Batch 300 \t Validation Loss: 37.49442907015482\n",
      "Epoch 63 \t Batch 320 \t Validation Loss: 38.07938468456268\n",
      "Epoch 63 \t Batch 340 \t Validation Loss: 38.147502310135785\n",
      "Epoch 63 \t Batch 360 \t Validation Loss: 38.04397249221802\n",
      "Epoch 63 \t Batch 380 \t Validation Loss: 38.49459221488551\n",
      "Epoch 63 \t Batch 400 \t Validation Loss: 38.28520067691803\n",
      "Epoch 63 \t Batch 420 \t Validation Loss: 38.476340023676556\n",
      "Epoch 63 \t Batch 440 \t Validation Loss: 38.38665049943057\n",
      "Epoch 63 \t Batch 460 \t Validation Loss: 38.81939149317534\n",
      "Epoch 63 \t Batch 480 \t Validation Loss: 39.36675543189049\n",
      "Epoch 63 \t Batch 500 \t Validation Loss: 39.15952416801453\n",
      "Epoch 63 \t Batch 520 \t Validation Loss: 39.211534344233\n",
      "Epoch 63 \t Batch 540 \t Validation Loss: 39.000195250687774\n",
      "Epoch 63 \t Batch 560 \t Validation Loss: 38.87236090898514\n",
      "Epoch 63 \t Batch 580 \t Validation Loss: 38.70365502423254\n",
      "Epoch 63 \t Batch 600 \t Validation Loss: 38.9545077308019\n",
      "Epoch 63 Training Loss: 38.19332189570336 Validation Loss: 39.70176772173349\n",
      "Epoch 63 completed\n",
      "Epoch 64 \t Batch 20 \t Training Loss: 37.367930030822755\n",
      "Epoch 64 \t Batch 40 \t Training Loss: 37.42224588394165\n",
      "Epoch 64 \t Batch 60 \t Training Loss: 37.43286425272624\n",
      "Epoch 64 \t Batch 80 \t Training Loss: 37.22495837211609\n",
      "Epoch 64 \t Batch 100 \t Training Loss: 37.067184391021726\n",
      "Epoch 64 \t Batch 120 \t Training Loss: 37.00689295132955\n",
      "Epoch 64 \t Batch 140 \t Training Loss: 36.98593164171491\n",
      "Epoch 64 \t Batch 160 \t Training Loss: 37.10922181606293\n",
      "Epoch 64 \t Batch 180 \t Training Loss: 37.10062149895562\n",
      "Epoch 64 \t Batch 200 \t Training Loss: 37.19661993026733\n",
      "Epoch 64 \t Batch 220 \t Training Loss: 37.26616720719771\n",
      "Epoch 64 \t Batch 240 \t Training Loss: 37.27182690302531\n",
      "Epoch 64 \t Batch 260 \t Training Loss: 37.333951201805704\n",
      "Epoch 64 \t Batch 280 \t Training Loss: 37.245372486114505\n",
      "Epoch 64 \t Batch 300 \t Training Loss: 37.31472723642985\n",
      "Epoch 64 \t Batch 320 \t Training Loss: 37.364522671699525\n",
      "Epoch 64 \t Batch 340 \t Training Loss: 37.3717773213106\n",
      "Epoch 64 \t Batch 360 \t Training Loss: 37.38297980626424\n",
      "Epoch 64 \t Batch 380 \t Training Loss: 37.40868230116995\n",
      "Epoch 64 \t Batch 400 \t Training Loss: 37.37975853919983\n",
      "Epoch 64 \t Batch 420 \t Training Loss: 37.447350883483885\n",
      "Epoch 64 \t Batch 440 \t Training Loss: 37.48210220336914\n",
      "Epoch 64 \t Batch 460 \t Training Loss: 37.52094729050346\n",
      "Epoch 64 \t Batch 480 \t Training Loss: 37.55348274707794\n",
      "Epoch 64 \t Batch 500 \t Training Loss: 37.54298827362061\n",
      "Epoch 64 \t Batch 520 \t Training Loss: 37.540839158571686\n",
      "Epoch 64 \t Batch 540 \t Training Loss: 37.524537573920355\n",
      "Epoch 64 \t Batch 560 \t Training Loss: 37.57169502803257\n",
      "Epoch 64 \t Batch 580 \t Training Loss: 37.58952826138201\n",
      "Epoch 64 \t Batch 600 \t Training Loss: 37.58175999323527\n",
      "Epoch 64 \t Batch 620 \t Training Loss: 37.5822050340714\n",
      "Epoch 64 \t Batch 640 \t Training Loss: 37.632152223587035\n",
      "Epoch 64 \t Batch 660 \t Training Loss: 37.63074439077666\n",
      "Epoch 64 \t Batch 680 \t Training Loss: 37.668120978860294\n",
      "Epoch 64 \t Batch 700 \t Training Loss: 37.736092420305525\n",
      "Epoch 64 \t Batch 720 \t Training Loss: 37.764602247873945\n",
      "Epoch 64 \t Batch 740 \t Training Loss: 37.799587930215374\n",
      "Epoch 64 \t Batch 760 \t Training Loss: 37.78840010291652\n",
      "Epoch 64 \t Batch 780 \t Training Loss: 37.836995716584035\n",
      "Epoch 64 \t Batch 800 \t Training Loss: 37.83158562660217\n",
      "Epoch 64 \t Batch 820 \t Training Loss: 37.835113641692374\n",
      "Epoch 64 \t Batch 840 \t Training Loss: 37.84660222189767\n",
      "Epoch 64 \t Batch 860 \t Training Loss: 37.85885096261668\n",
      "Epoch 64 \t Batch 880 \t Training Loss: 37.876798434691\n",
      "Epoch 64 \t Batch 900 \t Training Loss: 37.86741204579671\n",
      "Epoch 64 \t Batch 20 \t Validation Loss: 11.999933338165283\n",
      "Epoch 64 \t Batch 40 \t Validation Loss: 16.982469832897188\n",
      "Epoch 64 \t Batch 60 \t Validation Loss: 15.844706670443218\n",
      "Epoch 64 \t Batch 80 \t Validation Loss: 17.19030606150627\n",
      "Epoch 64 \t Batch 100 \t Validation Loss: 19.883631625175475\n",
      "Epoch 64 \t Batch 120 \t Validation Loss: 21.90813072125117\n",
      "Epoch 64 \t Batch 140 \t Validation Loss: 22.8439920936312\n",
      "Epoch 64 \t Batch 160 \t Validation Loss: 24.844077321887017\n",
      "Epoch 64 \t Batch 180 \t Validation Loss: 27.68767307334476\n",
      "Epoch 64 \t Batch 200 \t Validation Loss: 29.204521448612212\n",
      "Epoch 64 \t Batch 220 \t Validation Loss: 30.39412946050817\n",
      "Epoch 64 \t Batch 240 \t Validation Loss: 30.8199082950751\n",
      "Epoch 64 \t Batch 260 \t Validation Loss: 32.980770483383765\n",
      "Epoch 64 \t Batch 280 \t Validation Loss: 34.19035101788385\n",
      "Epoch 64 \t Batch 300 \t Validation Loss: 34.95211425940196\n",
      "Epoch 64 \t Batch 320 \t Validation Loss: 35.344532646238804\n",
      "Epoch 64 \t Batch 340 \t Validation Loss: 35.48507212049821\n",
      "Epoch 64 \t Batch 360 \t Validation Loss: 35.36353474325604\n",
      "Epoch 64 \t Batch 380 \t Validation Loss: 35.76531351114574\n",
      "Epoch 64 \t Batch 400 \t Validation Loss: 35.606260343790055\n",
      "Epoch 64 \t Batch 420 \t Validation Loss: 35.81367896965572\n",
      "Epoch 64 \t Batch 440 \t Validation Loss: 35.73782824711366\n",
      "Epoch 64 \t Batch 460 \t Validation Loss: 36.22459227624147\n",
      "Epoch 64 \t Batch 480 \t Validation Loss: 36.8335778961579\n",
      "Epoch 64 \t Batch 500 \t Validation Loss: 36.64241126346588\n",
      "Epoch 64 \t Batch 520 \t Validation Loss: 36.62214715022307\n",
      "Epoch 64 \t Batch 540 \t Validation Loss: 36.52799536122216\n",
      "Epoch 64 \t Batch 560 \t Validation Loss: 36.48317541820662\n",
      "Epoch 64 \t Batch 580 \t Validation Loss: 36.321860396450965\n",
      "Epoch 64 \t Batch 600 \t Validation Loss: 36.68904798905055\n",
      "Epoch 64 Training Loss: 37.87624453978294 Validation Loss: 37.43980413675308\n",
      "Epoch 64 completed\n",
      "Epoch 65 \t Batch 20 \t Training Loss: 36.701954460144044\n",
      "Epoch 65 \t Batch 40 \t Training Loss: 36.500359773635864\n",
      "Epoch 65 \t Batch 60 \t Training Loss: 36.47113564809163\n",
      "Epoch 65 \t Batch 80 \t Training Loss: 36.55704574584961\n",
      "Epoch 65 \t Batch 100 \t Training Loss: 36.605939025878904\n",
      "Epoch 65 \t Batch 120 \t Training Loss: 36.67355747222901\n",
      "Epoch 65 \t Batch 140 \t Training Loss: 36.617390169416154\n",
      "Epoch 65 \t Batch 160 \t Training Loss: 36.75322841405868\n",
      "Epoch 65 \t Batch 180 \t Training Loss: 36.83920831680298\n",
      "Epoch 65 \t Batch 200 \t Training Loss: 36.89315291404724\n",
      "Epoch 65 \t Batch 220 \t Training Loss: 36.882670948722144\n",
      "Epoch 65 \t Batch 240 \t Training Loss: 36.95757699807485\n",
      "Epoch 65 \t Batch 260 \t Training Loss: 36.93227307979877\n",
      "Epoch 65 \t Batch 280 \t Training Loss: 36.93468862942287\n",
      "Epoch 65 \t Batch 300 \t Training Loss: 37.020657812754315\n",
      "Epoch 65 \t Batch 320 \t Training Loss: 37.064676243066785\n",
      "Epoch 65 \t Batch 340 \t Training Loss: 37.124985801472384\n",
      "Epoch 65 \t Batch 360 \t Training Loss: 37.16583661503262\n",
      "Epoch 65 \t Batch 380 \t Training Loss: 37.17698198117708\n",
      "Epoch 65 \t Batch 400 \t Training Loss: 37.15985769748688\n",
      "Epoch 65 \t Batch 420 \t Training Loss: 37.18334707986741\n",
      "Epoch 65 \t Batch 440 \t Training Loss: 37.225541951439595\n",
      "Epoch 65 \t Batch 460 \t Training Loss: 37.250304806750755\n",
      "Epoch 65 \t Batch 480 \t Training Loss: 37.27850353320439\n",
      "Epoch 65 \t Batch 500 \t Training Loss: 37.2796005935669\n",
      "Epoch 65 \t Batch 520 \t Training Loss: 37.30693189180814\n",
      "Epoch 65 \t Batch 540 \t Training Loss: 37.33030979015209\n",
      "Epoch 65 \t Batch 560 \t Training Loss: 37.32607640538897\n",
      "Epoch 65 \t Batch 580 \t Training Loss: 37.3315971506053\n",
      "Epoch 65 \t Batch 600 \t Training Loss: 37.37310928980509\n",
      "Epoch 65 \t Batch 620 \t Training Loss: 37.39847765891783\n",
      "Epoch 65 \t Batch 640 \t Training Loss: 37.401043742895126\n",
      "Epoch 65 \t Batch 660 \t Training Loss: 37.45079244555849\n",
      "Epoch 65 \t Batch 680 \t Training Loss: 37.47004027086146\n",
      "Epoch 65 \t Batch 700 \t Training Loss: 37.472646160125734\n",
      "Epoch 65 \t Batch 720 \t Training Loss: 37.48504988087548\n",
      "Epoch 65 \t Batch 740 \t Training Loss: 37.502192932850605\n",
      "Epoch 65 \t Batch 760 \t Training Loss: 37.51822189782795\n",
      "Epoch 65 \t Batch 780 \t Training Loss: 37.49639322085258\n",
      "Epoch 65 \t Batch 800 \t Training Loss: 37.49511631250382\n",
      "Epoch 65 \t Batch 820 \t Training Loss: 37.50500280566332\n",
      "Epoch 65 \t Batch 840 \t Training Loss: 37.51697194462731\n",
      "Epoch 65 \t Batch 860 \t Training Loss: 37.54650956087334\n",
      "Epoch 65 \t Batch 880 \t Training Loss: 37.56183443502946\n",
      "Epoch 65 \t Batch 900 \t Training Loss: 37.57549338658651\n",
      "Epoch 65 \t Batch 20 \t Validation Loss: 8.462266778945922\n",
      "Epoch 65 \t Batch 40 \t Validation Loss: 12.173001354932785\n",
      "Epoch 65 \t Batch 60 \t Validation Loss: 11.682749724388122\n",
      "Epoch 65 \t Batch 80 \t Validation Loss: 13.08633314371109\n",
      "Epoch 65 \t Batch 100 \t Validation Loss: 16.56021092414856\n",
      "Epoch 65 \t Batch 120 \t Validation Loss: 19.04883329073588\n",
      "Epoch 65 \t Batch 140 \t Validation Loss: 20.378710065569198\n",
      "Epoch 65 \t Batch 160 \t Validation Loss: 23.00018744468689\n",
      "Epoch 65 \t Batch 180 \t Validation Loss: 26.76802519692315\n",
      "Epoch 65 \t Batch 200 \t Validation Loss: 28.724762020111083\n",
      "Epoch 65 \t Batch 220 \t Validation Loss: 30.383751409704036\n",
      "Epoch 65 \t Batch 240 \t Validation Loss: 31.14430412451426\n",
      "Epoch 65 \t Batch 260 \t Validation Loss: 33.62997621756334\n",
      "Epoch 65 \t Batch 280 \t Validation Loss: 35.0874345438821\n",
      "Epoch 65 \t Batch 300 \t Validation Loss: 36.15003547032674\n",
      "Epoch 65 \t Batch 320 \t Validation Loss: 36.73723841011524\n",
      "Epoch 65 \t Batch 340 \t Validation Loss: 36.84520021606894\n",
      "Epoch 65 \t Batch 360 \t Validation Loss: 36.811920311715866\n",
      "Epoch 65 \t Batch 380 \t Validation Loss: 37.334708467282745\n",
      "Epoch 65 \t Batch 400 \t Validation Loss: 37.18925458669663\n",
      "Epoch 65 \t Batch 420 \t Validation Loss: 37.406008309409735\n",
      "Epoch 65 \t Batch 440 \t Validation Loss: 37.36281973882155\n",
      "Epoch 65 \t Batch 460 \t Validation Loss: 37.86195164763409\n",
      "Epoch 65 \t Batch 480 \t Validation Loss: 38.44648771484693\n",
      "Epoch 65 \t Batch 500 \t Validation Loss: 38.24728656959534\n",
      "Epoch 65 \t Batch 520 \t Validation Loss: 38.31968161876385\n",
      "Epoch 65 \t Batch 540 \t Validation Loss: 38.180615191989475\n",
      "Epoch 65 \t Batch 560 \t Validation Loss: 38.12699627876282\n",
      "Epoch 65 \t Batch 580 \t Validation Loss: 38.01177859141909\n",
      "Epoch 65 \t Batch 600 \t Validation Loss: 38.338067013422645\n",
      "Epoch 65 Training Loss: 37.58061870337833 Validation Loss: 39.06923742418165\n",
      "Epoch 65 completed\n",
      "Epoch 66 \t Batch 20 \t Training Loss: 36.70959606170654\n",
      "Epoch 66 \t Batch 40 \t Training Loss: 36.693295288085935\n",
      "Epoch 66 \t Batch 60 \t Training Loss: 36.69667835235596\n",
      "Epoch 66 \t Batch 80 \t Training Loss: 36.96449303627014\n",
      "Epoch 66 \t Batch 100 \t Training Loss: 37.05481250762939\n",
      "Epoch 66 \t Batch 120 \t Training Loss: 37.038020833333334\n",
      "Epoch 66 \t Batch 140 \t Training Loss: 36.96697390420096\n",
      "Epoch 66 \t Batch 160 \t Training Loss: 36.88755884170532\n",
      "Epoch 66 \t Batch 180 \t Training Loss: 36.9695322672526\n",
      "Epoch 66 \t Batch 200 \t Training Loss: 37.05540736198425\n",
      "Epoch 66 \t Batch 220 \t Training Loss: 36.98022650805387\n",
      "Epoch 66 \t Batch 240 \t Training Loss: 36.998026172320046\n",
      "Epoch 66 \t Batch 260 \t Training Loss: 36.97350672941941\n",
      "Epoch 66 \t Batch 280 \t Training Loss: 37.01077502795628\n",
      "Epoch 66 \t Batch 300 \t Training Loss: 36.98898416519165\n",
      "Epoch 66 \t Batch 320 \t Training Loss: 37.011361819505694\n",
      "Epoch 66 \t Batch 340 \t Training Loss: 37.05330483492683\n",
      "Epoch 66 \t Batch 360 \t Training Loss: 37.055338303248085\n",
      "Epoch 66 \t Batch 380 \t Training Loss: 37.08849798503675\n",
      "Epoch 66 \t Batch 400 \t Training Loss: 37.10238260269165\n",
      "Epoch 66 \t Batch 420 \t Training Loss: 37.09998514084589\n",
      "Epoch 66 \t Batch 440 \t Training Loss: 37.113983834873544\n",
      "Epoch 66 \t Batch 460 \t Training Loss: 37.11860835863197\n",
      "Epoch 66 \t Batch 480 \t Training Loss: 37.110296932856244\n",
      "Epoch 66 \t Batch 500 \t Training Loss: 37.109782897949216\n",
      "Epoch 66 \t Batch 520 \t Training Loss: 37.09628502038809\n",
      "Epoch 66 \t Batch 540 \t Training Loss: 37.11576581177888\n",
      "Epoch 66 \t Batch 560 \t Training Loss: 37.130605820247105\n",
      "Epoch 66 \t Batch 580 \t Training Loss: 37.128608677305024\n",
      "Epoch 66 \t Batch 600 \t Training Loss: 37.12007935523987\n",
      "Epoch 66 \t Batch 620 \t Training Loss: 37.19610013346518\n",
      "Epoch 66 \t Batch 640 \t Training Loss: 37.230028381943704\n",
      "Epoch 66 \t Batch 660 \t Training Loss: 37.266584040901876\n",
      "Epoch 66 \t Batch 680 \t Training Loss: 37.25762289552127\n",
      "Epoch 66 \t Batch 700 \t Training Loss: 37.25636719839913\n",
      "Epoch 66 \t Batch 720 \t Training Loss: 37.30044933954875\n",
      "Epoch 66 \t Batch 740 \t Training Loss: 37.330356046315785\n",
      "Epoch 66 \t Batch 760 \t Training Loss: 37.34877061341938\n",
      "Epoch 66 \t Batch 780 \t Training Loss: 37.37203038044465\n",
      "Epoch 66 \t Batch 800 \t Training Loss: 37.36862261295319\n",
      "Epoch 66 \t Batch 820 \t Training Loss: 37.39390266232374\n",
      "Epoch 66 \t Batch 840 \t Training Loss: 37.38294648670015\n",
      "Epoch 66 \t Batch 860 \t Training Loss: 37.36153175664503\n",
      "Epoch 66 \t Batch 880 \t Training Loss: 37.370845489068465\n",
      "Epoch 66 \t Batch 900 \t Training Loss: 37.372774092356366\n",
      "Epoch 66 \t Batch 20 \t Validation Loss: 14.02758526802063\n",
      "Epoch 66 \t Batch 40 \t Validation Loss: 18.220757234096528\n",
      "Epoch 66 \t Batch 60 \t Validation Loss: 17.060128855705262\n",
      "Epoch 66 \t Batch 80 \t Validation Loss: 18.591024857759475\n",
      "Epoch 66 \t Batch 100 \t Validation Loss: 21.516551671028136\n",
      "Epoch 66 \t Batch 120 \t Validation Loss: 23.57504816452662\n",
      "Epoch 66 \t Batch 140 \t Validation Loss: 24.496470672743662\n",
      "Epoch 66 \t Batch 160 \t Validation Loss: 26.46376542150974\n",
      "Epoch 66 \t Batch 180 \t Validation Loss: 29.904953821500143\n",
      "Epoch 66 \t Batch 200 \t Validation Loss: 31.781050274372102\n",
      "Epoch 66 \t Batch 220 \t Validation Loss: 33.23871930946003\n",
      "Epoch 66 \t Batch 240 \t Validation Loss: 33.81960574189822\n",
      "Epoch 66 \t Batch 260 \t Validation Loss: 36.2107154754492\n",
      "Epoch 66 \t Batch 280 \t Validation Loss: 37.49404069185257\n",
      "Epoch 66 \t Batch 300 \t Validation Loss: 38.35105910460154\n",
      "Epoch 66 \t Batch 320 \t Validation Loss: 38.791019181907174\n",
      "Epoch 66 \t Batch 340 \t Validation Loss: 38.81037058970507\n",
      "Epoch 66 \t Batch 360 \t Validation Loss: 38.608848385016124\n",
      "Epoch 66 \t Batch 380 \t Validation Loss: 38.94891235326466\n",
      "Epoch 66 \t Batch 400 \t Validation Loss: 38.69738965392113\n",
      "Epoch 66 \t Batch 420 \t Validation Loss: 38.85064409233275\n",
      "Epoch 66 \t Batch 440 \t Validation Loss: 38.715177175131714\n",
      "Epoch 66 \t Batch 460 \t Validation Loss: 38.965325325468314\n",
      "Epoch 66 \t Batch 480 \t Validation Loss: 39.51262214084466\n",
      "Epoch 66 \t Batch 500 \t Validation Loss: 39.230953013420105\n",
      "Epoch 66 \t Batch 520 \t Validation Loss: 39.068549287319186\n",
      "Epoch 66 \t Batch 540 \t Validation Loss: 38.880694324881944\n",
      "Epoch 66 \t Batch 560 \t Validation Loss: 38.82158149225371\n",
      "Epoch 66 \t Batch 580 \t Validation Loss: 38.68901719635931\n",
      "Epoch 66 \t Batch 600 \t Validation Loss: 38.96620553572973\n",
      "Epoch 66 Training Loss: 37.38531248910086 Validation Loss: 39.715153978242505\n",
      "Epoch 66 completed\n",
      "Epoch 67 \t Batch 20 \t Training Loss: 36.446351146698\n",
      "Epoch 67 \t Batch 40 \t Training Loss: 36.35047254562378\n",
      "Epoch 67 \t Batch 60 \t Training Loss: 36.5027411142985\n",
      "Epoch 67 \t Batch 80 \t Training Loss: 36.51312727928162\n",
      "Epoch 67 \t Batch 100 \t Training Loss: 36.42943656921387\n",
      "Epoch 67 \t Batch 120 \t Training Loss: 36.46913347244263\n",
      "Epoch 67 \t Batch 140 \t Training Loss: 36.61387923104422\n",
      "Epoch 67 \t Batch 160 \t Training Loss: 36.75705749988556\n",
      "Epoch 67 \t Batch 180 \t Training Loss: 36.789115227593314\n",
      "Epoch 67 \t Batch 200 \t Training Loss: 36.82432466506958\n",
      "Epoch 67 \t Batch 220 \t Training Loss: 36.78587329170921\n",
      "Epoch 67 \t Batch 240 \t Training Loss: 36.79996035893758\n",
      "Epoch 67 \t Batch 260 \t Training Loss: 36.76209890659039\n",
      "Epoch 67 \t Batch 280 \t Training Loss: 36.74720725331988\n",
      "Epoch 67 \t Batch 300 \t Training Loss: 36.74477430343628\n",
      "Epoch 67 \t Batch 320 \t Training Loss: 36.68266009688377\n",
      "Epoch 67 \t Batch 340 \t Training Loss: 36.61927383647245\n",
      "Epoch 67 \t Batch 360 \t Training Loss: 36.669669500986735\n",
      "Epoch 67 \t Batch 380 \t Training Loss: 36.68028884686922\n",
      "Epoch 67 \t Batch 400 \t Training Loss: 36.74673051834107\n",
      "Epoch 67 \t Batch 420 \t Training Loss: 36.772607031322664\n",
      "Epoch 67 \t Batch 440 \t Training Loss: 36.77321275364269\n",
      "Epoch 67 \t Batch 460 \t Training Loss: 36.8061838398809\n",
      "Epoch 67 \t Batch 480 \t Training Loss: 36.85264736016591\n",
      "Epoch 67 \t Batch 500 \t Training Loss: 36.88192184448242\n",
      "Epoch 67 \t Batch 520 \t Training Loss: 36.876884482457086\n",
      "Epoch 67 \t Batch 540 \t Training Loss: 36.88078782823351\n",
      "Epoch 67 \t Batch 560 \t Training Loss: 36.924669878823416\n",
      "Epoch 67 \t Batch 580 \t Training Loss: 36.91721150628452\n",
      "Epoch 67 \t Batch 600 \t Training Loss: 36.90824878692627\n",
      "Epoch 67 \t Batch 620 \t Training Loss: 36.92484835963096\n",
      "Epoch 67 \t Batch 640 \t Training Loss: 36.93932660222053\n",
      "Epoch 67 \t Batch 660 \t Training Loss: 36.95043119372743\n",
      "Epoch 67 \t Batch 680 \t Training Loss: 36.95720052158131\n",
      "Epoch 67 \t Batch 700 \t Training Loss: 36.97935606820243\n",
      "Epoch 67 \t Batch 720 \t Training Loss: 36.994533734851416\n",
      "Epoch 67 \t Batch 740 \t Training Loss: 36.996729721894134\n",
      "Epoch 67 \t Batch 760 \t Training Loss: 36.999673647629585\n",
      "Epoch 67 \t Batch 780 \t Training Loss: 37.000302833165875\n",
      "Epoch 67 \t Batch 800 \t Training Loss: 36.989911150932315\n",
      "Epoch 67 \t Batch 820 \t Training Loss: 36.987987062407704\n",
      "Epoch 67 \t Batch 840 \t Training Loss: 36.99663758050828\n",
      "Epoch 67 \t Batch 860 \t Training Loss: 37.016429315611376\n",
      "Epoch 67 \t Batch 880 \t Training Loss: 37.02673478126526\n",
      "Epoch 67 \t Batch 900 \t Training Loss: 37.044313765631784\n",
      "Epoch 67 \t Batch 20 \t Validation Loss: 11.885992813110352\n",
      "Epoch 67 \t Batch 40 \t Validation Loss: 15.946495282649995\n",
      "Epoch 67 \t Batch 60 \t Validation Loss: 15.027557174364725\n",
      "Epoch 67 \t Batch 80 \t Validation Loss: 16.519542515277863\n",
      "Epoch 67 \t Batch 100 \t Validation Loss: 20.42574942588806\n",
      "Epoch 67 \t Batch 120 \t Validation Loss: 22.785399015744527\n",
      "Epoch 67 \t Batch 140 \t Validation Loss: 23.87106351171221\n",
      "Epoch 67 \t Batch 160 \t Validation Loss: 26.038197642564775\n",
      "Epoch 67 \t Batch 180 \t Validation Loss: 29.256669500139026\n",
      "Epoch 67 \t Batch 200 \t Validation Loss: 30.942717256546022\n",
      "Epoch 67 \t Batch 220 \t Validation Loss: 32.297889761491255\n",
      "Epoch 67 \t Batch 240 \t Validation Loss: 32.799398092428845\n",
      "Epoch 67 \t Batch 260 \t Validation Loss: 35.10959393061125\n",
      "Epoch 67 \t Batch 280 \t Validation Loss: 36.39442913872855\n",
      "Epoch 67 \t Batch 300 \t Validation Loss: 37.20440384546916\n",
      "Epoch 67 \t Batch 320 \t Validation Loss: 37.685219895839694\n",
      "Epoch 67 \t Batch 340 \t Validation Loss: 37.72236294465907\n",
      "Epoch 67 \t Batch 360 \t Validation Loss: 37.620326365364924\n",
      "Epoch 67 \t Batch 380 \t Validation Loss: 38.09025383497539\n",
      "Epoch 67 \t Batch 400 \t Validation Loss: 37.91456415176392\n",
      "Epoch 67 \t Batch 420 \t Validation Loss: 38.07947142237709\n",
      "Epoch 67 \t Batch 440 \t Validation Loss: 37.999323274872516\n",
      "Epoch 67 \t Batch 460 \t Validation Loss: 38.49596858024597\n",
      "Epoch 67 \t Batch 480 \t Validation Loss: 39.06975712577502\n",
      "Epoch 67 \t Batch 500 \t Validation Loss: 38.8601274471283\n",
      "Epoch 67 \t Batch 520 \t Validation Loss: 39.004408144950865\n",
      "Epoch 67 \t Batch 540 \t Validation Loss: 38.886416362833096\n",
      "Epoch 67 \t Batch 560 \t Validation Loss: 38.86410995040621\n",
      "Epoch 67 \t Batch 580 \t Validation Loss: 38.798897251589544\n",
      "Epoch 67 \t Batch 600 \t Validation Loss: 39.12179276943207\n",
      "Epoch 67 Training Loss: 37.06438832506619 Validation Loss: 39.91195115021297\n",
      "Epoch 67 completed\n",
      "Epoch 68 \t Batch 20 \t Training Loss: 35.272306251525876\n",
      "Epoch 68 \t Batch 40 \t Training Loss: 35.8920024394989\n",
      "Epoch 68 \t Batch 60 \t Training Loss: 35.60572443008423\n",
      "Epoch 68 \t Batch 80 \t Training Loss: 35.46955301761627\n",
      "Epoch 68 \t Batch 100 \t Training Loss: 35.816890201568604\n",
      "Epoch 68 \t Batch 120 \t Training Loss: 35.93068510691325\n",
      "Epoch 68 \t Batch 140 \t Training Loss: 35.73271444865635\n",
      "Epoch 68 \t Batch 160 \t Training Loss: 35.90601426362991\n",
      "Epoch 68 \t Batch 180 \t Training Loss: 36.062799061669246\n",
      "Epoch 68 \t Batch 200 \t Training Loss: 36.08775985717774\n",
      "Epoch 68 \t Batch 220 \t Training Loss: 36.09844699339433\n",
      "Epoch 68 \t Batch 240 \t Training Loss: 36.112670707702634\n",
      "Epoch 68 \t Batch 260 \t Training Loss: 36.19625954261193\n",
      "Epoch 68 \t Batch 280 \t Training Loss: 36.18481822013855\n",
      "Epoch 68 \t Batch 300 \t Training Loss: 36.27424576441447\n",
      "Epoch 68 \t Batch 320 \t Training Loss: 36.19928151369095\n",
      "Epoch 68 \t Batch 340 \t Training Loss: 36.227887131186094\n",
      "Epoch 68 \t Batch 360 \t Training Loss: 36.215692329406735\n",
      "Epoch 68 \t Batch 380 \t Training Loss: 36.17209731152183\n",
      "Epoch 68 \t Batch 400 \t Training Loss: 36.175029335021975\n",
      "Epoch 68 \t Batch 420 \t Training Loss: 36.13717623211089\n",
      "Epoch 68 \t Batch 440 \t Training Loss: 36.2028321612965\n",
      "Epoch 68 \t Batch 460 \t Training Loss: 36.250560727326764\n",
      "Epoch 68 \t Batch 480 \t Training Loss: 36.253694947560625\n",
      "Epoch 68 \t Batch 500 \t Training Loss: 36.31207727813721\n",
      "Epoch 68 \t Batch 520 \t Training Loss: 36.32122992735643\n",
      "Epoch 68 \t Batch 540 \t Training Loss: 36.35085594389174\n",
      "Epoch 68 \t Batch 560 \t Training Loss: 36.36693321977343\n",
      "Epoch 68 \t Batch 580 \t Training Loss: 36.39838250258873\n",
      "Epoch 68 \t Batch 600 \t Training Loss: 36.419264758427936\n",
      "Epoch 68 \t Batch 620 \t Training Loss: 36.481426931196644\n",
      "Epoch 68 \t Batch 640 \t Training Loss: 36.49151245653629\n",
      "Epoch 68 \t Batch 660 \t Training Loss: 36.52268515789147\n",
      "Epoch 68 \t Batch 680 \t Training Loss: 36.56474074475906\n",
      "Epoch 68 \t Batch 700 \t Training Loss: 36.55456438064575\n",
      "Epoch 68 \t Batch 720 \t Training Loss: 36.5546638197369\n",
      "Epoch 68 \t Batch 740 \t Training Loss: 36.56609972360972\n",
      "Epoch 68 \t Batch 760 \t Training Loss: 36.58749761581421\n",
      "Epoch 68 \t Batch 780 \t Training Loss: 36.592217768155614\n",
      "Epoch 68 \t Batch 800 \t Training Loss: 36.602587513923645\n",
      "Epoch 68 \t Batch 820 \t Training Loss: 36.62996117196432\n",
      "Epoch 68 \t Batch 840 \t Training Loss: 36.64229001998901\n",
      "Epoch 68 \t Batch 860 \t Training Loss: 36.6469446936319\n",
      "Epoch 68 \t Batch 880 \t Training Loss: 36.657852883772414\n",
      "Epoch 68 \t Batch 900 \t Training Loss: 36.695306345621745\n",
      "Epoch 68 \t Batch 20 \t Validation Loss: 12.36768798828125\n",
      "Epoch 68 \t Batch 40 \t Validation Loss: 17.41004921197891\n",
      "Epoch 68 \t Batch 60 \t Validation Loss: 16.044613043467205\n",
      "Epoch 68 \t Batch 80 \t Validation Loss: 16.933533370494843\n",
      "Epoch 68 \t Batch 100 \t Validation Loss: 20.144693269729615\n",
      "Epoch 68 \t Batch 120 \t Validation Loss: 22.51863876183828\n",
      "Epoch 68 \t Batch 140 \t Validation Loss: 23.67624705859593\n",
      "Epoch 68 \t Batch 160 \t Validation Loss: 26.226476973295213\n",
      "Epoch 68 \t Batch 180 \t Validation Loss: 29.944490104251436\n",
      "Epoch 68 \t Batch 200 \t Validation Loss: 31.87012409210205\n",
      "Epoch 68 \t Batch 220 \t Validation Loss: 33.38763641010631\n",
      "Epoch 68 \t Batch 240 \t Validation Loss: 34.05148277282715\n",
      "Epoch 68 \t Batch 260 \t Validation Loss: 36.4692418391888\n",
      "Epoch 68 \t Batch 280 \t Validation Loss: 37.84574988228934\n",
      "Epoch 68 \t Batch 300 \t Validation Loss: 38.89901622772217\n",
      "Epoch 68 \t Batch 320 \t Validation Loss: 39.50001692175865\n",
      "Epoch 68 \t Batch 340 \t Validation Loss: 39.55012856651755\n",
      "Epoch 68 \t Batch 360 \t Validation Loss: 39.47904234992133\n",
      "Epoch 68 \t Batch 380 \t Validation Loss: 39.98340553484465\n",
      "Epoch 68 \t Batch 400 \t Validation Loss: 39.84821956157684\n",
      "Epoch 68 \t Batch 420 \t Validation Loss: 40.02149059658959\n",
      "Epoch 68 \t Batch 440 \t Validation Loss: 40.02397484779358\n",
      "Epoch 68 \t Batch 460 \t Validation Loss: 40.52708803674449\n",
      "Epoch 68 \t Batch 480 \t Validation Loss: 41.07846374511719\n",
      "Epoch 68 \t Batch 500 \t Validation Loss: 40.86102787399292\n",
      "Epoch 68 \t Batch 520 \t Validation Loss: 41.07020984062782\n",
      "Epoch 68 \t Batch 540 \t Validation Loss: 40.88456973323115\n",
      "Epoch 68 \t Batch 560 \t Validation Loss: 40.80169164112636\n",
      "Epoch 68 \t Batch 580 \t Validation Loss: 40.65965909957886\n",
      "Epoch 68 \t Batch 600 \t Validation Loss: 40.91837848981221\n",
      "Epoch 68 Training Loss: 36.73727286368966 Validation Loss: 41.629537207739695\n",
      "Epoch 68 completed\n",
      "Epoch 69 \t Batch 20 \t Training Loss: 36.085153865814206\n",
      "Epoch 69 \t Batch 40 \t Training Loss: 35.957510471343994\n",
      "Epoch 69 \t Batch 60 \t Training Loss: 35.62824061711629\n",
      "Epoch 69 \t Batch 80 \t Training Loss: 35.58063223361969\n",
      "Epoch 69 \t Batch 100 \t Training Loss: 35.50905918121338\n",
      "Epoch 69 \t Batch 120 \t Training Loss: 35.69062716166179\n",
      "Epoch 69 \t Batch 140 \t Training Loss: 35.690826184409005\n",
      "Epoch 69 \t Batch 160 \t Training Loss: 35.62419666051865\n",
      "Epoch 69 \t Batch 180 \t Training Loss: 35.796057309044734\n",
      "Epoch 69 \t Batch 200 \t Training Loss: 35.77571595191956\n",
      "Epoch 69 \t Batch 220 \t Training Loss: 35.82097459272905\n",
      "Epoch 69 \t Batch 240 \t Training Loss: 35.76315928300222\n",
      "Epoch 69 \t Batch 260 \t Training Loss: 35.690748339432936\n",
      "Epoch 69 \t Batch 280 \t Training Loss: 35.77094368934631\n",
      "Epoch 69 \t Batch 300 \t Training Loss: 35.93203148523966\n",
      "Epoch 69 \t Batch 320 \t Training Loss: 35.929290145635605\n",
      "Epoch 69 \t Batch 340 \t Training Loss: 36.03019654330085\n",
      "Epoch 69 \t Batch 360 \t Training Loss: 36.057191144095526\n",
      "Epoch 69 \t Batch 380 \t Training Loss: 36.12607732572054\n",
      "Epoch 69 \t Batch 400 \t Training Loss: 36.11059404373169\n",
      "Epoch 69 \t Batch 420 \t Training Loss: 36.09891038168044\n",
      "Epoch 69 \t Batch 440 \t Training Loss: 36.12042009180242\n",
      "Epoch 69 \t Batch 460 \t Training Loss: 36.13981103482454\n",
      "Epoch 69 \t Batch 480 \t Training Loss: 36.10867130756378\n",
      "Epoch 69 \t Batch 500 \t Training Loss: 36.134251640319825\n",
      "Epoch 69 \t Batch 520 \t Training Loss: 36.159717115989096\n",
      "Epoch 69 \t Batch 540 \t Training Loss: 36.15924856751053\n",
      "Epoch 69 \t Batch 560 \t Training Loss: 36.16836604731424\n",
      "Epoch 69 \t Batch 580 \t Training Loss: 36.19173033155244\n",
      "Epoch 69 \t Batch 600 \t Training Loss: 36.23721351941427\n",
      "Epoch 69 \t Batch 620 \t Training Loss: 36.210780715942384\n",
      "Epoch 69 \t Batch 640 \t Training Loss: 36.2063392162323\n",
      "Epoch 69 \t Batch 660 \t Training Loss: 36.233590657783274\n",
      "Epoch 69 \t Batch 680 \t Training Loss: 36.277338959189024\n",
      "Epoch 69 \t Batch 700 \t Training Loss: 36.27224476405552\n",
      "Epoch 69 \t Batch 720 \t Training Loss: 36.29470871554481\n",
      "Epoch 69 \t Batch 740 \t Training Loss: 36.31043734421601\n",
      "Epoch 69 \t Batch 760 \t Training Loss: 36.347336897097136\n",
      "Epoch 69 \t Batch 780 \t Training Loss: 36.38449564469166\n",
      "Epoch 69 \t Batch 800 \t Training Loss: 36.4104056596756\n",
      "Epoch 69 \t Batch 820 \t Training Loss: 36.41308989175936\n",
      "Epoch 69 \t Batch 840 \t Training Loss: 36.443221341995965\n",
      "Epoch 69 \t Batch 860 \t Training Loss: 36.45848089706066\n",
      "Epoch 69 \t Batch 880 \t Training Loss: 36.46741560155695\n",
      "Epoch 69 \t Batch 900 \t Training Loss: 36.507278175354\n",
      "Epoch 69 \t Batch 20 \t Validation Loss: 9.052552509307862\n",
      "Epoch 69 \t Batch 40 \t Validation Loss: 12.33555473089218\n",
      "Epoch 69 \t Batch 60 \t Validation Loss: 11.745841725667317\n",
      "Epoch 69 \t Batch 80 \t Validation Loss: 12.962868928909302\n",
      "Epoch 69 \t Batch 100 \t Validation Loss: 16.355582580566406\n",
      "Epoch 69 \t Batch 120 \t Validation Loss: 18.774198293685913\n",
      "Epoch 69 \t Batch 140 \t Validation Loss: 20.168631730760847\n",
      "Epoch 69 \t Batch 160 \t Validation Loss: 22.957404947280885\n",
      "Epoch 69 \t Batch 180 \t Validation Loss: 27.066050730811224\n",
      "Epoch 69 \t Batch 200 \t Validation Loss: 29.265838708877563\n",
      "Epoch 69 \t Batch 220 \t Validation Loss: 30.97012758255005\n",
      "Epoch 69 \t Batch 240 \t Validation Loss: 31.75302987098694\n",
      "Epoch 69 \t Batch 260 \t Validation Loss: 34.40259233988248\n",
      "Epoch 69 \t Batch 280 \t Validation Loss: 35.95289731366294\n",
      "Epoch 69 \t Batch 300 \t Validation Loss: 37.050284465154014\n",
      "Epoch 69 \t Batch 320 \t Validation Loss: 37.67860127389431\n",
      "Epoch 69 \t Batch 340 \t Validation Loss: 37.81336547346676\n",
      "Epoch 69 \t Batch 360 \t Validation Loss: 37.75920957194434\n",
      "Epoch 69 \t Batch 380 \t Validation Loss: 38.27402416781375\n",
      "Epoch 69 \t Batch 400 \t Validation Loss: 38.171188628673555\n",
      "Epoch 69 \t Batch 420 \t Validation Loss: 38.37913625126793\n",
      "Epoch 69 \t Batch 440 \t Validation Loss: 38.377838639779526\n",
      "Epoch 69 \t Batch 460 \t Validation Loss: 38.89209702325904\n",
      "Epoch 69 \t Batch 480 \t Validation Loss: 39.50709974169731\n",
      "Epoch 69 \t Batch 500 \t Validation Loss: 39.32834068107605\n",
      "Epoch 69 \t Batch 520 \t Validation Loss: 39.48046327187465\n",
      "Epoch 69 \t Batch 540 \t Validation Loss: 39.28141769303216\n",
      "Epoch 69 \t Batch 560 \t Validation Loss: 39.18004846743175\n",
      "Epoch 69 \t Batch 580 \t Validation Loss: 39.08824688648355\n",
      "Epoch 69 \t Batch 600 \t Validation Loss: 39.36183689594269\n",
      "Epoch 69 Training Loss: 36.52198295218864 Validation Loss: 40.096013138820595\n",
      "Epoch 69 completed\n",
      "Epoch 70 \t Batch 20 \t Training Loss: 34.968574142456056\n",
      "Epoch 70 \t Batch 40 \t Training Loss: 35.15692892074585\n",
      "Epoch 70 \t Batch 60 \t Training Loss: 35.14763758977254\n",
      "Epoch 70 \t Batch 80 \t Training Loss: 35.374972915649415\n",
      "Epoch 70 \t Batch 100 \t Training Loss: 35.40689228057861\n",
      "Epoch 70 \t Batch 120 \t Training Loss: 35.388367891311646\n",
      "Epoch 70 \t Batch 140 \t Training Loss: 35.47536196027483\n",
      "Epoch 70 \t Batch 160 \t Training Loss: 35.45034248828888\n",
      "Epoch 70 \t Batch 180 \t Training Loss: 35.43252158694797\n",
      "Epoch 70 \t Batch 200 \t Training Loss: 35.4050767326355\n",
      "Epoch 70 \t Batch 220 \t Training Loss: 35.50091888254339\n",
      "Epoch 70 \t Batch 240 \t Training Loss: 35.46512811978658\n",
      "Epoch 70 \t Batch 260 \t Training Loss: 35.58467703599196\n",
      "Epoch 70 \t Batch 280 \t Training Loss: 35.63344312395368\n",
      "Epoch 70 \t Batch 300 \t Training Loss: 35.685966911315916\n",
      "Epoch 70 \t Batch 320 \t Training Loss: 35.718051087856296\n",
      "Epoch 70 \t Batch 340 \t Training Loss: 35.70101543314317\n",
      "Epoch 70 \t Batch 360 \t Training Loss: 35.688995636834036\n",
      "Epoch 70 \t Batch 380 \t Training Loss: 35.735207798606474\n",
      "Epoch 70 \t Batch 400 \t Training Loss: 35.814365315437314\n",
      "Epoch 70 \t Batch 420 \t Training Loss: 35.83068191891625\n",
      "Epoch 70 \t Batch 440 \t Training Loss: 35.849045567079024\n",
      "Epoch 70 \t Batch 460 \t Training Loss: 35.896752751391865\n",
      "Epoch 70 \t Batch 480 \t Training Loss: 35.91680411895116\n",
      "Epoch 70 \t Batch 500 \t Training Loss: 35.91342129516602\n",
      "Epoch 70 \t Batch 520 \t Training Loss: 35.93311180334825\n",
      "Epoch 70 \t Batch 540 \t Training Loss: 35.97894961392438\n",
      "Epoch 70 \t Batch 560 \t Training Loss: 35.96324961866651\n",
      "Epoch 70 \t Batch 580 \t Training Loss: 35.997085324649156\n",
      "Epoch 70 \t Batch 600 \t Training Loss: 35.982412220637\n",
      "Epoch 70 \t Batch 620 \t Training Loss: 36.00419820662468\n",
      "Epoch 70 \t Batch 640 \t Training Loss: 36.01788787543774\n",
      "Epoch 70 \t Batch 660 \t Training Loss: 36.0219076127717\n",
      "Epoch 70 \t Batch 680 \t Training Loss: 36.028833594041714\n",
      "Epoch 70 \t Batch 700 \t Training Loss: 36.04828394208636\n",
      "Epoch 70 \t Batch 720 \t Training Loss: 36.074500767389935\n",
      "Epoch 70 \t Batch 740 \t Training Loss: 36.07756229091335\n",
      "Epoch 70 \t Batch 760 \t Training Loss: 36.09630351317556\n",
      "Epoch 70 \t Batch 780 \t Training Loss: 36.095889047475964\n",
      "Epoch 70 \t Batch 800 \t Training Loss: 36.10701908826828\n",
      "Epoch 70 \t Batch 820 \t Training Loss: 36.14446033152138\n",
      "Epoch 70 \t Batch 840 \t Training Loss: 36.16561562447321\n",
      "Epoch 70 \t Batch 860 \t Training Loss: 36.18836208387863\n",
      "Epoch 70 \t Batch 880 \t Training Loss: 36.18182056383653\n",
      "Epoch 70 \t Batch 900 \t Training Loss: 36.21702550888062\n",
      "Epoch 70 \t Batch 20 \t Validation Loss: 8.849082231521606\n",
      "Epoch 70 \t Batch 40 \t Validation Loss: 11.951104140281677\n",
      "Epoch 70 \t Batch 60 \t Validation Loss: 11.750265582402546\n",
      "Epoch 70 \t Batch 80 \t Validation Loss: 13.098463875055312\n",
      "Epoch 70 \t Batch 100 \t Validation Loss: 16.482289662361143\n",
      "Epoch 70 \t Batch 120 \t Validation Loss: 19.121587153275808\n",
      "Epoch 70 \t Batch 140 \t Validation Loss: 20.520497829573497\n",
      "Epoch 70 \t Batch 160 \t Validation Loss: 23.221197977662086\n",
      "Epoch 70 \t Batch 180 \t Validation Loss: 27.3746856768926\n",
      "Epoch 70 \t Batch 200 \t Validation Loss: 29.533512637615203\n",
      "Epoch 70 \t Batch 220 \t Validation Loss: 31.35797431902452\n",
      "Epoch 70 \t Batch 240 \t Validation Loss: 32.2126609702905\n",
      "Epoch 70 \t Batch 260 \t Validation Loss: 34.82091809602884\n",
      "Epoch 70 \t Batch 280 \t Validation Loss: 36.31165667091097\n",
      "Epoch 70 \t Batch 300 \t Validation Loss: 37.47963895003001\n",
      "Epoch 70 \t Batch 320 \t Validation Loss: 38.16112906783819\n",
      "Epoch 70 \t Batch 340 \t Validation Loss: 38.25641751429614\n",
      "Epoch 70 \t Batch 360 \t Validation Loss: 38.20514343447155\n",
      "Epoch 70 \t Batch 380 \t Validation Loss: 38.6739950769826\n",
      "Epoch 70 \t Batch 400 \t Validation Loss: 38.455484174489975\n",
      "Epoch 70 \t Batch 420 \t Validation Loss: 38.62413149674733\n",
      "Epoch 70 \t Batch 440 \t Validation Loss: 38.50282867930152\n",
      "Epoch 70 \t Batch 460 \t Validation Loss: 38.88795821459397\n",
      "Epoch 70 \t Batch 480 \t Validation Loss: 39.468594538172084\n",
      "Epoch 70 \t Batch 500 \t Validation Loss: 39.253168803215026\n",
      "Epoch 70 \t Batch 520 \t Validation Loss: 39.255985310444466\n",
      "Epoch 70 \t Batch 540 \t Validation Loss: 39.05364895485066\n",
      "Epoch 70 \t Batch 560 \t Validation Loss: 38.94784028785569\n",
      "Epoch 70 \t Batch 580 \t Validation Loss: 38.77193004591712\n",
      "Epoch 70 \t Batch 600 \t Validation Loss: 39.040091993808744\n",
      "Epoch 70 Training Loss: 36.222388536875485 Validation Loss: 39.78847574491005\n",
      "Epoch 70 completed\n",
      "Epoch 71 \t Batch 20 \t Training Loss: 36.83263034820557\n",
      "Epoch 71 \t Batch 40 \t Training Loss: 35.7053683757782\n",
      "Epoch 71 \t Batch 60 \t Training Loss: 35.32862405776977\n",
      "Epoch 71 \t Batch 80 \t Training Loss: 35.272867155075076\n",
      "Epoch 71 \t Batch 100 \t Training Loss: 35.255922145843506\n",
      "Epoch 71 \t Batch 120 \t Training Loss: 35.268761014938356\n",
      "Epoch 71 \t Batch 140 \t Training Loss: 35.32952834538051\n",
      "Epoch 71 \t Batch 160 \t Training Loss: 35.33381482362747\n",
      "Epoch 71 \t Batch 180 \t Training Loss: 35.33759640587701\n",
      "Epoch 71 \t Batch 200 \t Training Loss: 35.31984299659729\n",
      "Epoch 71 \t Batch 220 \t Training Loss: 35.33869483254173\n",
      "Epoch 71 \t Batch 240 \t Training Loss: 35.38747134208679\n",
      "Epoch 71 \t Batch 260 \t Training Loss: 35.429243615957404\n",
      "Epoch 71 \t Batch 280 \t Training Loss: 35.3908892086574\n",
      "Epoch 71 \t Batch 300 \t Training Loss: 35.4662592569987\n",
      "Epoch 71 \t Batch 320 \t Training Loss: 35.51475406289101\n",
      "Epoch 71 \t Batch 340 \t Training Loss: 35.56118703168981\n",
      "Epoch 71 \t Batch 360 \t Training Loss: 35.59416023360358\n",
      "Epoch 71 \t Batch 380 \t Training Loss: 35.54717442863866\n",
      "Epoch 71 \t Batch 400 \t Training Loss: 35.58490030765533\n",
      "Epoch 71 \t Batch 420 \t Training Loss: 35.58351322809855\n",
      "Epoch 71 \t Batch 440 \t Training Loss: 35.5943348494443\n",
      "Epoch 71 \t Batch 460 \t Training Loss: 35.605320631939435\n",
      "Epoch 71 \t Batch 480 \t Training Loss: 35.63057028055191\n",
      "Epoch 71 \t Batch 500 \t Training Loss: 35.59543143081665\n",
      "Epoch 71 \t Batch 520 \t Training Loss: 35.60789286540105\n",
      "Epoch 71 \t Batch 540 \t Training Loss: 35.64838491369177\n",
      "Epoch 71 \t Batch 560 \t Training Loss: 35.64580546447209\n",
      "Epoch 71 \t Batch 580 \t Training Loss: 35.66822769888516\n",
      "Epoch 71 \t Batch 600 \t Training Loss: 35.67377596219381\n",
      "Epoch 71 \t Batch 620 \t Training Loss: 35.71684091937158\n",
      "Epoch 71 \t Batch 640 \t Training Loss: 35.700322738289834\n",
      "Epoch 71 \t Batch 660 \t Training Loss: 35.702159852692574\n",
      "Epoch 71 \t Batch 680 \t Training Loss: 35.73114139332491\n",
      "Epoch 71 \t Batch 700 \t Training Loss: 35.76595131465367\n",
      "Epoch 71 \t Batch 720 \t Training Loss: 35.754140459166635\n",
      "Epoch 71 \t Batch 740 \t Training Loss: 35.7737152640884\n",
      "Epoch 71 \t Batch 760 \t Training Loss: 35.77793274176748\n",
      "Epoch 71 \t Batch 780 \t Training Loss: 35.80870615396744\n",
      "Epoch 71 \t Batch 800 \t Training Loss: 35.81086983919144\n",
      "Epoch 71 \t Batch 820 \t Training Loss: 35.839859855465775\n",
      "Epoch 71 \t Batch 840 \t Training Loss: 35.83688698042007\n",
      "Epoch 71 \t Batch 860 \t Training Loss: 35.856013783743215\n",
      "Epoch 71 \t Batch 880 \t Training Loss: 35.860058747638355\n",
      "Epoch 71 \t Batch 900 \t Training Loss: 35.877823164198134\n",
      "Epoch 71 \t Batch 20 \t Validation Loss: 16.545900344848633\n",
      "Epoch 71 \t Batch 40 \t Validation Loss: 18.963958287239073\n",
      "Epoch 71 \t Batch 60 \t Validation Loss: 18.35209264755249\n",
      "Epoch 71 \t Batch 80 \t Validation Loss: 19.18684787750244\n",
      "Epoch 71 \t Batch 100 \t Validation Loss: 21.68750394821167\n",
      "Epoch 71 \t Batch 120 \t Validation Loss: 23.665801286697388\n",
      "Epoch 71 \t Batch 140 \t Validation Loss: 24.648071234566824\n",
      "Epoch 71 \t Batch 160 \t Validation Loss: 26.860452198982237\n",
      "Epoch 71 \t Batch 180 \t Validation Loss: 30.122850312127007\n",
      "Epoch 71 \t Batch 200 \t Validation Loss: 31.776576709747314\n",
      "Epoch 71 \t Batch 220 \t Validation Loss: 33.133365735140714\n",
      "Epoch 71 \t Batch 240 \t Validation Loss: 33.65859862963359\n",
      "Epoch 71 \t Batch 260 \t Validation Loss: 35.946565752763014\n",
      "Epoch 71 \t Batch 280 \t Validation Loss: 37.204426298822675\n",
      "Epoch 71 \t Batch 300 \t Validation Loss: 38.081926380793256\n",
      "Epoch 71 \t Batch 320 \t Validation Loss: 38.613802754878996\n",
      "Epoch 71 \t Batch 340 \t Validation Loss: 38.66807500053854\n",
      "Epoch 71 \t Batch 360 \t Validation Loss: 38.57125077777439\n",
      "Epoch 71 \t Batch 380 \t Validation Loss: 39.041349134947126\n",
      "Epoch 71 \t Batch 400 \t Validation Loss: 38.92822768688202\n",
      "Epoch 71 \t Batch 420 \t Validation Loss: 39.093461804162885\n",
      "Epoch 71 \t Batch 440 \t Validation Loss: 39.11728330525485\n",
      "Epoch 71 \t Batch 460 \t Validation Loss: 39.646607962898585\n",
      "Epoch 71 \t Batch 480 \t Validation Loss: 40.207311781247455\n",
      "Epoch 71 \t Batch 500 \t Validation Loss: 39.98461364364624\n",
      "Epoch 71 \t Batch 520 \t Validation Loss: 40.164046991788425\n",
      "Epoch 71 \t Batch 540 \t Validation Loss: 40.03189984780771\n",
      "Epoch 71 \t Batch 560 \t Validation Loss: 39.97720501082284\n",
      "Epoch 71 \t Batch 580 \t Validation Loss: 39.87733166793297\n",
      "Epoch 71 \t Batch 600 \t Validation Loss: 40.15944662729899\n",
      "Epoch 71 Training Loss: 35.908847205641486 Validation Loss: 40.89469882729766\n",
      "Epoch 71 completed\n",
      "Epoch 72 \t Batch 20 \t Training Loss: 35.52916898727417\n",
      "Epoch 72 \t Batch 40 \t Training Loss: 35.2391815662384\n",
      "Epoch 72 \t Batch 60 \t Training Loss: 35.14126049677531\n",
      "Epoch 72 \t Batch 80 \t Training Loss: 35.076068258285524\n",
      "Epoch 72 \t Batch 100 \t Training Loss: 34.92935066223144\n",
      "Epoch 72 \t Batch 120 \t Training Loss: 35.13733774820964\n",
      "Epoch 72 \t Batch 140 \t Training Loss: 35.0662198475429\n",
      "Epoch 72 \t Batch 160 \t Training Loss: 35.00291423797607\n",
      "Epoch 72 \t Batch 180 \t Training Loss: 35.093490325080026\n",
      "Epoch 72 \t Batch 200 \t Training Loss: 35.10889720916748\n",
      "Epoch 72 \t Batch 220 \t Training Loss: 35.18249172730879\n",
      "Epoch 72 \t Batch 240 \t Training Loss: 35.187126231193545\n",
      "Epoch 72 \t Batch 260 \t Training Loss: 35.21859998703003\n",
      "Epoch 72 \t Batch 280 \t Training Loss: 35.17825677735465\n",
      "Epoch 72 \t Batch 300 \t Training Loss: 35.19126828511556\n",
      "Epoch 72 \t Batch 320 \t Training Loss: 35.25098996162414\n",
      "Epoch 72 \t Batch 340 \t Training Loss: 35.27664646821864\n",
      "Epoch 72 \t Batch 360 \t Training Loss: 35.23075140317281\n",
      "Epoch 72 \t Batch 380 \t Training Loss: 35.29655096656398\n",
      "Epoch 72 \t Batch 400 \t Training Loss: 35.338222894668576\n",
      "Epoch 72 \t Batch 420 \t Training Loss: 35.28768890925816\n",
      "Epoch 72 \t Batch 440 \t Training Loss: 35.33838185397062\n",
      "Epoch 72 \t Batch 460 \t Training Loss: 35.39992878126061\n",
      "Epoch 72 \t Batch 480 \t Training Loss: 35.39604104757309\n",
      "Epoch 72 \t Batch 500 \t Training Loss: 35.37763767623901\n",
      "Epoch 72 \t Batch 520 \t Training Loss: 35.422152669613176\n",
      "Epoch 72 \t Batch 540 \t Training Loss: 35.41665055663498\n",
      "Epoch 72 \t Batch 560 \t Training Loss: 35.42092548438481\n",
      "Epoch 72 \t Batch 580 \t Training Loss: 35.41359627493497\n",
      "Epoch 72 \t Batch 600 \t Training Loss: 35.44555512428283\n",
      "Epoch 72 \t Batch 620 \t Training Loss: 35.46675512559952\n",
      "Epoch 72 \t Batch 640 \t Training Loss: 35.464691534638405\n",
      "Epoch 72 \t Batch 660 \t Training Loss: 35.4828786127495\n",
      "Epoch 72 \t Batch 680 \t Training Loss: 35.48181873489828\n",
      "Epoch 72 \t Batch 700 \t Training Loss: 35.51538365227835\n",
      "Epoch 72 \t Batch 720 \t Training Loss: 35.545076518588594\n",
      "Epoch 72 \t Batch 740 \t Training Loss: 35.57138489388131\n",
      "Epoch 72 \t Batch 760 \t Training Loss: 35.605693169644006\n",
      "Epoch 72 \t Batch 780 \t Training Loss: 35.62017632508889\n",
      "Epoch 72 \t Batch 800 \t Training Loss: 35.626052508354185\n",
      "Epoch 72 \t Batch 820 \t Training Loss: 35.64517966014583\n",
      "Epoch 72 \t Batch 840 \t Training Loss: 35.68950450079782\n",
      "Epoch 72 \t Batch 860 \t Training Loss: 35.71189926945886\n",
      "Epoch 72 \t Batch 880 \t Training Loss: 35.70326931476593\n",
      "Epoch 72 \t Batch 900 \t Training Loss: 35.7104334280226\n",
      "Epoch 72 \t Batch 20 \t Validation Loss: 11.43880136013031\n",
      "Epoch 72 \t Batch 40 \t Validation Loss: 14.748749685287475\n",
      "Epoch 72 \t Batch 60 \t Validation Loss: 14.24545066356659\n",
      "Epoch 72 \t Batch 80 \t Validation Loss: 15.580889469385147\n",
      "Epoch 72 \t Batch 100 \t Validation Loss: 18.812352986335753\n",
      "Epoch 72 \t Batch 120 \t Validation Loss: 21.375605062643686\n",
      "Epoch 72 \t Batch 140 \t Validation Loss: 22.738733199664523\n",
      "Epoch 72 \t Batch 160 \t Validation Loss: 25.340275612473487\n",
      "Epoch 72 \t Batch 180 \t Validation Loss: 29.31049947473738\n",
      "Epoch 72 \t Batch 200 \t Validation Loss: 31.383708503246307\n",
      "Epoch 72 \t Batch 220 \t Validation Loss: 33.06364205750552\n",
      "Epoch 72 \t Batch 240 \t Validation Loss: 33.77847064137459\n",
      "Epoch 72 \t Batch 260 \t Validation Loss: 36.353731718430154\n",
      "Epoch 72 \t Batch 280 \t Validation Loss: 37.78518401724951\n",
      "Epoch 72 \t Batch 300 \t Validation Loss: 38.884583735466\n",
      "Epoch 72 \t Batch 320 \t Validation Loss: 39.4952643468976\n",
      "Epoch 72 \t Batch 340 \t Validation Loss: 39.53233601205489\n",
      "Epoch 72 \t Batch 360 \t Validation Loss: 39.42736890713374\n",
      "Epoch 72 \t Batch 380 \t Validation Loss: 39.92921096525694\n",
      "Epoch 72 \t Batch 400 \t Validation Loss: 39.71921665787697\n",
      "Epoch 72 \t Batch 420 \t Validation Loss: 39.82823688756852\n",
      "Epoch 72 \t Batch 440 \t Validation Loss: 39.73451452146877\n",
      "Epoch 72 \t Batch 460 \t Validation Loss: 40.22064120976821\n",
      "Epoch 72 \t Batch 480 \t Validation Loss: 40.77897912959258\n",
      "Epoch 72 \t Batch 500 \t Validation Loss: 40.58808996486664\n",
      "Epoch 72 \t Batch 520 \t Validation Loss: 40.69743171013319\n",
      "Epoch 72 \t Batch 540 \t Validation Loss: 40.44545823468103\n",
      "Epoch 72 \t Batch 560 \t Validation Loss: 40.299910983869005\n",
      "Epoch 72 \t Batch 580 \t Validation Loss: 40.07341305880711\n",
      "Epoch 72 \t Batch 600 \t Validation Loss: 40.314078069527945\n",
      "Epoch 72 Training Loss: 35.707699574977 Validation Loss: 41.01217742322327\n",
      "Epoch 72 completed\n",
      "Epoch 73 \t Batch 20 \t Training Loss: 34.993381309509275\n",
      "Epoch 73 \t Batch 40 \t Training Loss: 34.31671009063721\n",
      "Epoch 73 \t Batch 60 \t Training Loss: 34.48590459823608\n",
      "Epoch 73 \t Batch 80 \t Training Loss: 34.29520051479339\n",
      "Epoch 73 \t Batch 100 \t Training Loss: 34.39232559204102\n",
      "Epoch 73 \t Batch 120 \t Training Loss: 34.56863651275635\n",
      "Epoch 73 \t Batch 140 \t Training Loss: 34.529264967782154\n",
      "Epoch 73 \t Batch 160 \t Training Loss: 34.710122644901276\n",
      "Epoch 73 \t Batch 180 \t Training Loss: 34.8496072239346\n",
      "Epoch 73 \t Batch 200 \t Training Loss: 34.83653221130371\n",
      "Epoch 73 \t Batch 220 \t Training Loss: 34.925954749367456\n",
      "Epoch 73 \t Batch 240 \t Training Loss: 34.998544136683144\n",
      "Epoch 73 \t Batch 260 \t Training Loss: 35.043415883871226\n",
      "Epoch 73 \t Batch 280 \t Training Loss: 35.056014033726285\n",
      "Epoch 73 \t Batch 300 \t Training Loss: 35.069652252197265\n",
      "Epoch 73 \t Batch 320 \t Training Loss: 35.14446315765381\n",
      "Epoch 73 \t Batch 340 \t Training Loss: 35.13336984410005\n",
      "Epoch 73 \t Batch 360 \t Training Loss: 35.09143198331197\n",
      "Epoch 73 \t Batch 380 \t Training Loss: 34.9856265820955\n",
      "Epoch 73 \t Batch 400 \t Training Loss: 34.987725782394406\n",
      "Epoch 73 \t Batch 420 \t Training Loss: 35.00837316967192\n",
      "Epoch 73 \t Batch 440 \t Training Loss: 34.995623397827146\n",
      "Epoch 73 \t Batch 460 \t Training Loss: 35.053641252932344\n",
      "Epoch 73 \t Batch 480 \t Training Loss: 35.08604380289714\n",
      "Epoch 73 \t Batch 500 \t Training Loss: 35.09346439743042\n",
      "Epoch 73 \t Batch 520 \t Training Loss: 35.10865046061002\n",
      "Epoch 73 \t Batch 540 \t Training Loss: 35.11211399149012\n",
      "Epoch 73 \t Batch 560 \t Training Loss: 35.128415605000086\n",
      "Epoch 73 \t Batch 580 \t Training Loss: 35.15757119408969\n",
      "Epoch 73 \t Batch 600 \t Training Loss: 35.16072949727376\n",
      "Epoch 73 \t Batch 620 \t Training Loss: 35.18725214927427\n",
      "Epoch 73 \t Batch 640 \t Training Loss: 35.18713068664074\n",
      "Epoch 73 \t Batch 660 \t Training Loss: 35.20480856461958\n",
      "Epoch 73 \t Batch 680 \t Training Loss: 35.23799295705908\n",
      "Epoch 73 \t Batch 700 \t Training Loss: 35.23796634674072\n",
      "Epoch 73 \t Batch 720 \t Training Loss: 35.271300580766464\n",
      "Epoch 73 \t Batch 740 \t Training Loss: 35.25708873851879\n",
      "Epoch 73 \t Batch 760 \t Training Loss: 35.274214082015185\n",
      "Epoch 73 \t Batch 780 \t Training Loss: 35.310325468503514\n",
      "Epoch 73 \t Batch 800 \t Training Loss: 35.32246418237686\n",
      "Epoch 73 \t Batch 820 \t Training Loss: 35.32885349552806\n",
      "Epoch 73 \t Batch 840 \t Training Loss: 35.329896134421936\n",
      "Epoch 73 \t Batch 860 \t Training Loss: 35.38027475046557\n",
      "Epoch 73 \t Batch 880 \t Training Loss: 35.39108789834109\n",
      "Epoch 73 \t Batch 900 \t Training Loss: 35.404005201127795\n",
      "Epoch 73 \t Batch 20 \t Validation Loss: 10.590137267112732\n",
      "Epoch 73 \t Batch 40 \t Validation Loss: 14.191579055786132\n",
      "Epoch 73 \t Batch 60 \t Validation Loss: 13.53335116704305\n",
      "Epoch 73 \t Batch 80 \t Validation Loss: 14.950350785255432\n",
      "Epoch 73 \t Batch 100 \t Validation Loss: 18.228130836486816\n",
      "Epoch 73 \t Batch 120 \t Validation Loss: 20.483565815289815\n",
      "Epoch 73 \t Batch 140 \t Validation Loss: 21.75385959489005\n",
      "Epoch 73 \t Batch 160 \t Validation Loss: 24.37383688092232\n",
      "Epoch 73 \t Batch 180 \t Validation Loss: 28.01759629249573\n",
      "Epoch 73 \t Batch 200 \t Validation Loss: 29.962753281593322\n",
      "Epoch 73 \t Batch 220 \t Validation Loss: 31.514820744774557\n",
      "Epoch 73 \t Batch 240 \t Validation Loss: 32.19532269239426\n",
      "Epoch 73 \t Batch 260 \t Validation Loss: 34.66231735302852\n",
      "Epoch 73 \t Batch 280 \t Validation Loss: 36.01150341374534\n",
      "Epoch 73 \t Batch 300 \t Validation Loss: 37.027821162541706\n",
      "Epoch 73 \t Batch 320 \t Validation Loss: 37.62265895903111\n",
      "Epoch 73 \t Batch 340 \t Validation Loss: 37.73019991762498\n",
      "Epoch 73 \t Batch 360 \t Validation Loss: 37.677599724133806\n",
      "Epoch 73 \t Batch 380 \t Validation Loss: 38.233373860309\n",
      "Epoch 73 \t Batch 400 \t Validation Loss: 38.155711243152616\n",
      "Epoch 73 \t Batch 420 \t Validation Loss: 38.37694247336615\n",
      "Epoch 73 \t Batch 440 \t Validation Loss: 38.404773744669825\n",
      "Epoch 73 \t Batch 460 \t Validation Loss: 39.004315266401875\n",
      "Epoch 73 \t Batch 480 \t Validation Loss: 39.59490606983503\n",
      "Epoch 73 \t Batch 500 \t Validation Loss: 39.411381437301635\n",
      "Epoch 73 \t Batch 520 \t Validation Loss: 39.71445910013639\n",
      "Epoch 73 \t Batch 540 \t Validation Loss: 39.55125492237232\n",
      "Epoch 73 \t Batch 560 \t Validation Loss: 39.486878180503844\n",
      "Epoch 73 \t Batch 580 \t Validation Loss: 39.38821929076622\n",
      "Epoch 73 \t Batch 600 \t Validation Loss: 39.660009190241496\n",
      "Epoch 73 Training Loss: 35.40149237987389 Validation Loss: 40.40285972496132\n",
      "Epoch 73 completed\n",
      "Epoch 74 \t Batch 20 \t Training Loss: 34.703079319000246\n",
      "Epoch 74 \t Batch 40 \t Training Loss: 34.34482526779175\n",
      "Epoch 74 \t Batch 60 \t Training Loss: 34.467006301879884\n",
      "Epoch 74 \t Batch 80 \t Training Loss: 34.5101051568985\n",
      "Epoch 74 \t Batch 100 \t Training Loss: 34.42084243774414\n",
      "Epoch 74 \t Batch 120 \t Training Loss: 34.44233360290527\n",
      "Epoch 74 \t Batch 140 \t Training Loss: 34.44383313315255\n",
      "Epoch 74 \t Batch 160 \t Training Loss: 34.29850293397904\n",
      "Epoch 74 \t Batch 180 \t Training Loss: 34.11719383663601\n",
      "Epoch 74 \t Batch 200 \t Training Loss: 34.16885931968689\n",
      "Epoch 74 \t Batch 220 \t Training Loss: 34.20801673368974\n",
      "Epoch 74 \t Batch 240 \t Training Loss: 34.29801004727681\n",
      "Epoch 74 \t Batch 260 \t Training Loss: 34.35415388254019\n",
      "Epoch 74 \t Batch 280 \t Training Loss: 34.48569399969919\n",
      "Epoch 74 \t Batch 300 \t Training Loss: 34.48697598139445\n",
      "Epoch 74 \t Batch 320 \t Training Loss: 34.54117842316627\n",
      "Epoch 74 \t Batch 340 \t Training Loss: 34.61441731733434\n",
      "Epoch 74 \t Batch 360 \t Training Loss: 34.603264543745254\n",
      "Epoch 74 \t Batch 380 \t Training Loss: 34.5992145789297\n",
      "Epoch 74 \t Batch 400 \t Training Loss: 34.63661924362182\n",
      "Epoch 74 \t Batch 420 \t Training Loss: 34.70599954695928\n",
      "Epoch 74 \t Batch 440 \t Training Loss: 34.720620610497214\n",
      "Epoch 74 \t Batch 460 \t Training Loss: 34.716314722144084\n",
      "Epoch 74 \t Batch 480 \t Training Loss: 34.73496077855428\n",
      "Epoch 74 \t Batch 500 \t Training Loss: 34.790584354400636\n",
      "Epoch 74 \t Batch 520 \t Training Loss: 34.81057127805857\n",
      "Epoch 74 \t Batch 540 \t Training Loss: 34.80186595210323\n",
      "Epoch 74 \t Batch 560 \t Training Loss: 34.813488824026926\n",
      "Epoch 74 \t Batch 580 \t Training Loss: 34.867799403749665\n",
      "Epoch 74 \t Batch 600 \t Training Loss: 34.88018244107564\n",
      "Epoch 74 \t Batch 620 \t Training Loss: 34.90999968744094\n",
      "Epoch 74 \t Batch 640 \t Training Loss: 34.944664132595065\n",
      "Epoch 74 \t Batch 660 \t Training Loss: 34.97317712379224\n",
      "Epoch 74 \t Batch 680 \t Training Loss: 35.00167534772088\n",
      "Epoch 74 \t Batch 700 \t Training Loss: 35.02875195639474\n",
      "Epoch 74 \t Batch 720 \t Training Loss: 35.04317181110382\n",
      "Epoch 74 \t Batch 740 \t Training Loss: 35.058348671165675\n",
      "Epoch 74 \t Batch 760 \t Training Loss: 35.063351016295584\n",
      "Epoch 74 \t Batch 780 \t Training Loss: 35.07864768199432\n",
      "Epoch 74 \t Batch 800 \t Training Loss: 35.05292156457901\n",
      "Epoch 74 \t Batch 820 \t Training Loss: 35.10001367010722\n",
      "Epoch 74 \t Batch 840 \t Training Loss: 35.108620600473316\n",
      "Epoch 74 \t Batch 860 \t Training Loss: 35.085567330205166\n",
      "Epoch 74 \t Batch 880 \t Training Loss: 35.09956647482785\n",
      "Epoch 74 \t Batch 900 \t Training Loss: 35.09560439215766\n",
      "Epoch 74 \t Batch 20 \t Validation Loss: 13.919449138641358\n",
      "Epoch 74 \t Batch 40 \t Validation Loss: 17.31673320531845\n",
      "Epoch 74 \t Batch 60 \t Validation Loss: 16.528891714413962\n",
      "Epoch 74 \t Batch 80 \t Validation Loss: 17.47567759156227\n",
      "Epoch 74 \t Batch 100 \t Validation Loss: 20.45397570133209\n",
      "Epoch 74 \t Batch 120 \t Validation Loss: 22.712432245413464\n",
      "Epoch 74 \t Batch 140 \t Validation Loss: 23.732545256614685\n",
      "Epoch 74 \t Batch 160 \t Validation Loss: 25.811704030632974\n",
      "Epoch 74 \t Batch 180 \t Validation Loss: 29.25977400408851\n",
      "Epoch 74 \t Batch 200 \t Validation Loss: 31.045969321727753\n",
      "Epoch 74 \t Batch 220 \t Validation Loss: 32.458534108508715\n",
      "Epoch 74 \t Batch 240 \t Validation Loss: 32.9984558959802\n",
      "Epoch 74 \t Batch 260 \t Validation Loss: 35.377041261012735\n",
      "Epoch 74 \t Batch 280 \t Validation Loss: 36.68753455877304\n",
      "Epoch 74 \t Batch 300 \t Validation Loss: 37.563424728711446\n",
      "Epoch 74 \t Batch 320 \t Validation Loss: 38.02690234929323\n",
      "Epoch 74 \t Batch 340 \t Validation Loss: 38.0649240395602\n",
      "Epoch 74 \t Batch 360 \t Validation Loss: 37.904628574848175\n",
      "Epoch 74 \t Batch 380 \t Validation Loss: 38.313808306894806\n",
      "Epoch 74 \t Batch 400 \t Validation Loss: 38.110058003664015\n",
      "Epoch 74 \t Batch 420 \t Validation Loss: 38.24090063117799\n",
      "Epoch 74 \t Batch 440 \t Validation Loss: 38.14154617678035\n",
      "Epoch 74 \t Batch 460 \t Validation Loss: 38.55742773076762\n",
      "Epoch 74 \t Batch 480 \t Validation Loss: 39.09589101970196\n",
      "Epoch 74 \t Batch 500 \t Validation Loss: 38.84527793598175\n",
      "Epoch 74 \t Batch 520 \t Validation Loss: 38.873066884737746\n",
      "Epoch 74 \t Batch 540 \t Validation Loss: 38.70302762013895\n",
      "Epoch 74 \t Batch 560 \t Validation Loss: 38.61470490949495\n",
      "Epoch 74 \t Batch 580 \t Validation Loss: 38.47613374858067\n",
      "Epoch 74 \t Batch 600 \t Validation Loss: 38.74956964254379\n",
      "Epoch 74 Training Loss: 35.10912867954799 Validation Loss: 39.50260659394326\n",
      "Epoch 74 completed\n",
      "Epoch 75 \t Batch 20 \t Training Loss: 33.91885852813721\n",
      "Epoch 75 \t Batch 40 \t Training Loss: 33.912084627151486\n",
      "Epoch 75 \t Batch 60 \t Training Loss: 33.80175466537476\n",
      "Epoch 75 \t Batch 80 \t Training Loss: 34.494595646858215\n",
      "Epoch 75 \t Batch 100 \t Training Loss: 34.52151714324951\n",
      "Epoch 75 \t Batch 120 \t Training Loss: 34.41480941772461\n",
      "Epoch 75 \t Batch 140 \t Training Loss: 34.39240383420672\n",
      "Epoch 75 \t Batch 160 \t Training Loss: 34.332836949825285\n",
      "Epoch 75 \t Batch 180 \t Training Loss: 34.210586600833466\n",
      "Epoch 75 \t Batch 200 \t Training Loss: 34.30464244842529\n",
      "Epoch 75 \t Batch 220 \t Training Loss: 34.29239694421942\n",
      "Epoch 75 \t Batch 240 \t Training Loss: 34.328697085380554\n",
      "Epoch 75 \t Batch 260 \t Training Loss: 34.403258286989654\n",
      "Epoch 75 \t Batch 280 \t Training Loss: 34.409299639293124\n",
      "Epoch 75 \t Batch 300 \t Training Loss: 34.47672746658325\n",
      "Epoch 75 \t Batch 320 \t Training Loss: 34.57416763305664\n",
      "Epoch 75 \t Batch 340 \t Training Loss: 34.54978395349839\n",
      "Epoch 75 \t Batch 360 \t Training Loss: 34.557568618986345\n",
      "Epoch 75 \t Batch 380 \t Training Loss: 34.5313031296981\n",
      "Epoch 75 \t Batch 400 \t Training Loss: 34.58387580871582\n",
      "Epoch 75 \t Batch 420 \t Training Loss: 34.573329335167294\n",
      "Epoch 75 \t Batch 440 \t Training Loss: 34.59486274719238\n",
      "Epoch 75 \t Batch 460 \t Training Loss: 34.58715632065483\n",
      "Epoch 75 \t Batch 480 \t Training Loss: 34.61992317040761\n",
      "Epoch 75 \t Batch 500 \t Training Loss: 34.61687897872925\n",
      "Epoch 75 \t Batch 520 \t Training Loss: 34.648245888489946\n",
      "Epoch 75 \t Batch 540 \t Training Loss: 34.68338021878843\n",
      "Epoch 75 \t Batch 560 \t Training Loss: 34.694961711338586\n",
      "Epoch 75 \t Batch 580 \t Training Loss: 34.69477303274746\n",
      "Epoch 75 \t Batch 600 \t Training Loss: 34.70087217648824\n",
      "Epoch 75 \t Batch 620 \t Training Loss: 34.73786475273871\n",
      "Epoch 75 \t Batch 640 \t Training Loss: 34.74115285873413\n",
      "Epoch 75 \t Batch 660 \t Training Loss: 34.74572877305927\n",
      "Epoch 75 \t Batch 680 \t Training Loss: 34.79836203631233\n",
      "Epoch 75 \t Batch 700 \t Training Loss: 34.8289885193961\n",
      "Epoch 75 \t Batch 720 \t Training Loss: 34.829014049635994\n",
      "Epoch 75 \t Batch 740 \t Training Loss: 34.82572918711482\n",
      "Epoch 75 \t Batch 760 \t Training Loss: 34.834297614348564\n",
      "Epoch 75 \t Batch 780 \t Training Loss: 34.83287640351516\n",
      "Epoch 75 \t Batch 800 \t Training Loss: 34.85222937822342\n",
      "Epoch 75 \t Batch 820 \t Training Loss: 34.873232473978184\n",
      "Epoch 75 \t Batch 840 \t Training Loss: 34.88120589483352\n",
      "Epoch 75 \t Batch 860 \t Training Loss: 34.87453571363937\n",
      "Epoch 75 \t Batch 880 \t Training Loss: 34.88712172724984\n",
      "Epoch 75 \t Batch 900 \t Training Loss: 34.90371582455105\n",
      "Epoch 75 \t Batch 20 \t Validation Loss: 12.513144946098327\n",
      "Epoch 75 \t Batch 40 \t Validation Loss: 15.511100673675537\n",
      "Epoch 75 \t Batch 60 \t Validation Loss: 15.04402352174123\n",
      "Epoch 75 \t Batch 80 \t Validation Loss: 16.276749628782273\n",
      "Epoch 75 \t Batch 100 \t Validation Loss: 19.604106040000914\n",
      "Epoch 75 \t Batch 120 \t Validation Loss: 22.010127635796866\n",
      "Epoch 75 \t Batch 140 \t Validation Loss: 23.13050322873252\n",
      "Epoch 75 \t Batch 160 \t Validation Loss: 25.386641427874565\n",
      "Epoch 75 \t Batch 180 \t Validation Loss: 29.122356014781527\n",
      "Epoch 75 \t Batch 200 \t Validation Loss: 31.07606299638748\n",
      "Epoch 75 \t Batch 220 \t Validation Loss: 32.604826513203705\n",
      "Epoch 75 \t Batch 240 \t Validation Loss: 33.2116114238898\n",
      "Epoch 75 \t Batch 260 \t Validation Loss: 35.67790461503542\n",
      "Epoch 75 \t Batch 280 \t Validation Loss: 37.05970473119191\n",
      "Epoch 75 \t Batch 300 \t Validation Loss: 38.03811906019847\n",
      "Epoch 75 \t Batch 320 \t Validation Loss: 38.55833902209997\n",
      "Epoch 75 \t Batch 340 \t Validation Loss: 38.58696217677173\n",
      "Epoch 75 \t Batch 360 \t Validation Loss: 38.453370940685275\n",
      "Epoch 75 \t Batch 380 \t Validation Loss: 38.89377740433341\n",
      "Epoch 75 \t Batch 400 \t Validation Loss: 38.70962490439415\n",
      "Epoch 75 \t Batch 420 \t Validation Loss: 38.86669389974503\n",
      "Epoch 75 \t Batch 440 \t Validation Loss: 38.79754665439779\n",
      "Epoch 75 \t Batch 460 \t Validation Loss: 39.23604280948639\n",
      "Epoch 75 \t Batch 480 \t Validation Loss: 39.810999463995294\n",
      "Epoch 75 \t Batch 500 \t Validation Loss: 39.57701917552948\n",
      "Epoch 75 \t Batch 520 \t Validation Loss: 39.56546977024812\n",
      "Epoch 75 \t Batch 540 \t Validation Loss: 39.35921615936138\n",
      "Epoch 75 \t Batch 560 \t Validation Loss: 39.2413752411093\n",
      "Epoch 75 \t Batch 580 \t Validation Loss: 39.07920220391504\n",
      "Epoch 75 \t Batch 600 \t Validation Loss: 39.35986485083898\n",
      "Epoch 75 Training Loss: 34.91581804905749 Validation Loss: 40.08307076971252\n",
      "Epoch 75 completed\n",
      "Epoch 76 \t Batch 20 \t Training Loss: 33.09622488021851\n",
      "Epoch 76 \t Batch 40 \t Training Loss: 33.42404475212097\n",
      "Epoch 76 \t Batch 60 \t Training Loss: 33.77516692479451\n",
      "Epoch 76 \t Batch 80 \t Training Loss: 34.01823115348816\n",
      "Epoch 76 \t Batch 100 \t Training Loss: 34.11110422134399\n",
      "Epoch 76 \t Batch 120 \t Training Loss: 34.12973025639852\n",
      "Epoch 76 \t Batch 140 \t Training Loss: 34.02158591406686\n",
      "Epoch 76 \t Batch 160 \t Training Loss: 33.94821910858154\n",
      "Epoch 76 \t Batch 180 \t Training Loss: 33.990214898851185\n",
      "Epoch 76 \t Batch 200 \t Training Loss: 33.96655610084534\n",
      "Epoch 76 \t Batch 220 \t Training Loss: 33.9874906539917\n",
      "Epoch 76 \t Batch 240 \t Training Loss: 34.01501525243123\n",
      "Epoch 76 \t Batch 260 \t Training Loss: 34.067874688368576\n",
      "Epoch 76 \t Batch 280 \t Training Loss: 34.097471952438354\n",
      "Epoch 76 \t Batch 300 \t Training Loss: 34.13497489293416\n",
      "Epoch 76 \t Batch 320 \t Training Loss: 34.17567901611328\n",
      "Epoch 76 \t Batch 340 \t Training Loss: 34.195145881877224\n",
      "Epoch 76 \t Batch 360 \t Training Loss: 34.11939509179857\n",
      "Epoch 76 \t Batch 380 \t Training Loss: 34.14029737773694\n",
      "Epoch 76 \t Batch 400 \t Training Loss: 34.16089115619659\n",
      "Epoch 76 \t Batch 420 \t Training Loss: 34.18415149961199\n",
      "Epoch 76 \t Batch 440 \t Training Loss: 34.17986752770164\n",
      "Epoch 76 \t Batch 460 \t Training Loss: 34.19896768901659\n",
      "Epoch 76 \t Batch 480 \t Training Loss: 34.19647532701492\n",
      "Epoch 76 \t Batch 500 \t Training Loss: 34.188973346710206\n",
      "Epoch 76 \t Batch 520 \t Training Loss: 34.261733091794525\n",
      "Epoch 76 \t Batch 540 \t Training Loss: 34.28835530810886\n",
      "Epoch 76 \t Batch 560 \t Training Loss: 34.32655689035143\n",
      "Epoch 76 \t Batch 580 \t Training Loss: 34.34501799879403\n",
      "Epoch 76 \t Batch 600 \t Training Loss: 34.39164234161377\n",
      "Epoch 76 \t Batch 620 \t Training Loss: 34.404730778355756\n",
      "Epoch 76 \t Batch 640 \t Training Loss: 34.379137790203096\n",
      "Epoch 76 \t Batch 660 \t Training Loss: 34.39664261846831\n",
      "Epoch 76 \t Batch 680 \t Training Loss: 34.4277495804955\n",
      "Epoch 76 \t Batch 700 \t Training Loss: 34.46315818241664\n",
      "Epoch 76 \t Batch 720 \t Training Loss: 34.482910895347594\n",
      "Epoch 76 \t Batch 740 \t Training Loss: 34.50210760477427\n",
      "Epoch 76 \t Batch 760 \t Training Loss: 34.495378474185344\n",
      "Epoch 76 \t Batch 780 \t Training Loss: 34.52076700161665\n",
      "Epoch 76 \t Batch 800 \t Training Loss: 34.52934014320373\n",
      "Epoch 76 \t Batch 820 \t Training Loss: 34.55264973989347\n",
      "Epoch 76 \t Batch 840 \t Training Loss: 34.56957433110192\n",
      "Epoch 76 \t Batch 860 \t Training Loss: 34.567152222921685\n",
      "Epoch 76 \t Batch 880 \t Training Loss: 34.56599575172771\n",
      "Epoch 76 \t Batch 900 \t Training Loss: 34.584775307973224\n",
      "Epoch 76 \t Batch 20 \t Validation Loss: 17.74547381401062\n",
      "Epoch 76 \t Batch 40 \t Validation Loss: 21.215977001190186\n",
      "Epoch 76 \t Batch 60 \t Validation Loss: 19.971162684758504\n",
      "Epoch 76 \t Batch 80 \t Validation Loss: 20.513406360149382\n",
      "Epoch 76 \t Batch 100 \t Validation Loss: 23.183762865066527\n",
      "Epoch 76 \t Batch 120 \t Validation Loss: 25.299705211321513\n",
      "Epoch 76 \t Batch 140 \t Validation Loss: 26.243717227663314\n",
      "Epoch 76 \t Batch 160 \t Validation Loss: 28.527249962091446\n",
      "Epoch 76 \t Batch 180 \t Validation Loss: 31.7006831963857\n",
      "Epoch 76 \t Batch 200 \t Validation Loss: 33.45217086315155\n",
      "Epoch 76 \t Batch 220 \t Validation Loss: 34.95363142273643\n",
      "Epoch 76 \t Batch 240 \t Validation Loss: 35.366873586177825\n",
      "Epoch 76 \t Batch 260 \t Validation Loss: 37.683369163366464\n",
      "Epoch 76 \t Batch 280 \t Validation Loss: 38.835092881747656\n",
      "Epoch 76 \t Batch 300 \t Validation Loss: 39.739064048131304\n",
      "Epoch 76 \t Batch 320 \t Validation Loss: 40.15649979412556\n",
      "Epoch 76 \t Batch 340 \t Validation Loss: 40.201873978446514\n",
      "Epoch 76 \t Batch 360 \t Validation Loss: 40.15942419634925\n",
      "Epoch 76 \t Batch 380 \t Validation Loss: 40.62842435585825\n",
      "Epoch 76 \t Batch 400 \t Validation Loss: 40.494796483516694\n",
      "Epoch 76 \t Batch 420 \t Validation Loss: 40.6638419401078\n",
      "Epoch 76 \t Batch 440 \t Validation Loss: 40.693024000254546\n",
      "Epoch 76 \t Batch 460 \t Validation Loss: 41.2504035514334\n",
      "Epoch 76 \t Batch 480 \t Validation Loss: 41.7959452688694\n",
      "Epoch 76 \t Batch 500 \t Validation Loss: 41.551757341384885\n",
      "Epoch 76 \t Batch 520 \t Validation Loss: 41.699923713390646\n",
      "Epoch 76 \t Batch 540 \t Validation Loss: 41.57226357989841\n",
      "Epoch 76 \t Batch 560 \t Validation Loss: 41.50827452795846\n",
      "Epoch 76 \t Batch 580 \t Validation Loss: 41.407255830435915\n",
      "Epoch 76 \t Batch 600 \t Validation Loss: 41.68880061467489\n",
      "Epoch 76 Training Loss: 34.60016084627341 Validation Loss: 42.44704811913626\n",
      "Epoch 76 completed\n",
      "Epoch 77 \t Batch 20 \t Training Loss: 34.158134651184085\n",
      "Epoch 77 \t Batch 40 \t Training Loss: 33.85937161445618\n",
      "Epoch 77 \t Batch 60 \t Training Loss: 33.66976207097371\n",
      "Epoch 77 \t Batch 80 \t Training Loss: 33.85159356594086\n",
      "Epoch 77 \t Batch 100 \t Training Loss: 33.56798994064331\n",
      "Epoch 77 \t Batch 120 \t Training Loss: 33.4939306418101\n",
      "Epoch 77 \t Batch 140 \t Training Loss: 33.58956462315151\n",
      "Epoch 77 \t Batch 160 \t Training Loss: 33.65478028059006\n",
      "Epoch 77 \t Batch 180 \t Training Loss: 33.83149454328749\n",
      "Epoch 77 \t Batch 200 \t Training Loss: 33.82668956756592\n",
      "Epoch 77 \t Batch 220 \t Training Loss: 33.85968813462691\n",
      "Epoch 77 \t Batch 240 \t Training Loss: 33.81156653563182\n",
      "Epoch 77 \t Batch 260 \t Training Loss: 33.7950934116657\n",
      "Epoch 77 \t Batch 280 \t Training Loss: 33.868557936804635\n",
      "Epoch 77 \t Batch 300 \t Training Loss: 33.7787095006307\n",
      "Epoch 77 \t Batch 320 \t Training Loss: 33.766093897819516\n",
      "Epoch 77 \t Batch 340 \t Training Loss: 33.83194083606496\n",
      "Epoch 77 \t Batch 360 \t Training Loss: 33.90126276016235\n",
      "Epoch 77 \t Batch 380 \t Training Loss: 33.98289645847521\n",
      "Epoch 77 \t Batch 400 \t Training Loss: 34.00113994121551\n",
      "Epoch 77 \t Batch 420 \t Training Loss: 34.03951366061256\n",
      "Epoch 77 \t Batch 440 \t Training Loss: 34.08706655935808\n",
      "Epoch 77 \t Batch 460 \t Training Loss: 34.147776035640554\n",
      "Epoch 77 \t Batch 480 \t Training Loss: 34.1486312866211\n",
      "Epoch 77 \t Batch 500 \t Training Loss: 34.160819107055666\n",
      "Epoch 77 \t Batch 520 \t Training Loss: 34.17362289061913\n",
      "Epoch 77 \t Batch 540 \t Training Loss: 34.1773966188784\n",
      "Epoch 77 \t Batch 560 \t Training Loss: 34.22420976502555\n",
      "Epoch 77 \t Batch 580 \t Training Loss: 34.262847486035575\n",
      "Epoch 77 \t Batch 600 \t Training Loss: 34.281829080581666\n",
      "Epoch 77 \t Batch 620 \t Training Loss: 34.30988708003875\n",
      "Epoch 77 \t Batch 640 \t Training Loss: 34.310004287958144\n",
      "Epoch 77 \t Batch 660 \t Training Loss: 34.31759983120543\n",
      "Epoch 77 \t Batch 680 \t Training Loss: 34.31781433610355\n",
      "Epoch 77 \t Batch 700 \t Training Loss: 34.32547932488578\n",
      "Epoch 77 \t Batch 720 \t Training Loss: 34.34194045066833\n",
      "Epoch 77 \t Batch 740 \t Training Loss: 34.343858504939725\n",
      "Epoch 77 \t Batch 760 \t Training Loss: 34.34538078057138\n",
      "Epoch 77 \t Batch 780 \t Training Loss: 34.353047789060156\n",
      "Epoch 77 \t Batch 800 \t Training Loss: 34.35690863609314\n",
      "Epoch 77 \t Batch 820 \t Training Loss: 34.336409796738046\n",
      "Epoch 77 \t Batch 840 \t Training Loss: 34.33494012696402\n",
      "Epoch 77 \t Batch 860 \t Training Loss: 34.33985735427502\n",
      "Epoch 77 \t Batch 880 \t Training Loss: 34.36904633695429\n",
      "Epoch 77 \t Batch 900 \t Training Loss: 34.374586753845215\n",
      "Epoch 77 \t Batch 20 \t Validation Loss: 15.499619579315185\n",
      "Epoch 77 \t Batch 40 \t Validation Loss: 17.79058599472046\n",
      "Epoch 77 \t Batch 60 \t Validation Loss: 17.296092796325684\n",
      "Epoch 77 \t Batch 80 \t Validation Loss: 18.270483911037445\n",
      "Epoch 77 \t Batch 100 \t Validation Loss: 21.282129106521605\n",
      "Epoch 77 \t Batch 120 \t Validation Loss: 23.552220495541892\n",
      "Epoch 77 \t Batch 140 \t Validation Loss: 24.55590397289821\n",
      "Epoch 77 \t Batch 160 \t Validation Loss: 26.977465146780013\n",
      "Epoch 77 \t Batch 180 \t Validation Loss: 30.581916517681545\n",
      "Epoch 77 \t Batch 200 \t Validation Loss: 32.4276063489914\n",
      "Epoch 77 \t Batch 220 \t Validation Loss: 33.92484398321672\n",
      "Epoch 77 \t Batch 240 \t Validation Loss: 34.50797108809153\n",
      "Epoch 77 \t Batch 260 \t Validation Loss: 36.91934638023376\n",
      "Epoch 77 \t Batch 280 \t Validation Loss: 38.22250295025962\n",
      "Epoch 77 \t Batch 300 \t Validation Loss: 39.226772470474245\n",
      "Epoch 77 \t Batch 320 \t Validation Loss: 39.76976907551288\n",
      "Epoch 77 \t Batch 340 \t Validation Loss: 39.72962962599362\n",
      "Epoch 77 \t Batch 360 \t Validation Loss: 39.62341323163774\n",
      "Epoch 77 \t Batch 380 \t Validation Loss: 40.09813822947051\n",
      "Epoch 77 \t Batch 400 \t Validation Loss: 39.84551799535751\n",
      "Epoch 77 \t Batch 420 \t Validation Loss: 39.97714163689386\n",
      "Epoch 77 \t Batch 440 \t Validation Loss: 39.86018814823844\n",
      "Epoch 77 \t Batch 460 \t Validation Loss: 40.40934726673624\n",
      "Epoch 77 \t Batch 480 \t Validation Loss: 40.94542096654574\n",
      "Epoch 77 \t Batch 500 \t Validation Loss: 40.722544836044314\n",
      "Epoch 77 \t Batch 520 \t Validation Loss: 40.89816651894496\n",
      "Epoch 77 \t Batch 540 \t Validation Loss: 40.658746498602405\n",
      "Epoch 77 \t Batch 560 \t Validation Loss: 40.484675048078806\n",
      "Epoch 77 \t Batch 580 \t Validation Loss: 40.273818882580464\n",
      "Epoch 77 \t Batch 600 \t Validation Loss: 40.50248190085093\n",
      "Epoch 77 Training Loss: 34.39081194928584 Validation Loss: 41.19063475689331\n",
      "Epoch 77 completed\n",
      "Epoch 78 \t Batch 20 \t Training Loss: 33.905434131622314\n",
      "Epoch 78 \t Batch 40 \t Training Loss: 33.6756031036377\n",
      "Epoch 78 \t Batch 60 \t Training Loss: 33.304899501800534\n",
      "Epoch 78 \t Batch 80 \t Training Loss: 33.26157424449921\n",
      "Epoch 78 \t Batch 100 \t Training Loss: 33.54092819213867\n",
      "Epoch 78 \t Batch 120 \t Training Loss: 33.31875502268473\n",
      "Epoch 78 \t Batch 140 \t Training Loss: 33.32651691436767\n",
      "Epoch 78 \t Batch 160 \t Training Loss: 33.39599194526672\n",
      "Epoch 78 \t Batch 180 \t Training Loss: 33.475675943162706\n",
      "Epoch 78 \t Batch 200 \t Training Loss: 33.497002897262576\n",
      "Epoch 78 \t Batch 220 \t Training Loss: 33.467996926741165\n",
      "Epoch 78 \t Batch 240 \t Training Loss: 33.489561152458194\n",
      "Epoch 78 \t Batch 260 \t Training Loss: 33.555916808201715\n",
      "Epoch 78 \t Batch 280 \t Training Loss: 33.60333170890808\n",
      "Epoch 78 \t Batch 300 \t Training Loss: 33.62576769510905\n",
      "Epoch 78 \t Batch 320 \t Training Loss: 33.58303860425949\n",
      "Epoch 78 \t Batch 340 \t Training Loss: 33.69264127506929\n",
      "Epoch 78 \t Batch 360 \t Training Loss: 33.69493582513597\n",
      "Epoch 78 \t Batch 380 \t Training Loss: 33.67763003299111\n",
      "Epoch 78 \t Batch 400 \t Training Loss: 33.6782208442688\n",
      "Epoch 78 \t Batch 420 \t Training Loss: 33.68320886521112\n",
      "Epoch 78 \t Batch 440 \t Training Loss: 33.716653321006085\n",
      "Epoch 78 \t Batch 460 \t Training Loss: 33.717951538251796\n",
      "Epoch 78 \t Batch 480 \t Training Loss: 33.736133925120036\n",
      "Epoch 78 \t Batch 500 \t Training Loss: 33.784818420410154\n",
      "Epoch 78 \t Batch 520 \t Training Loss: 33.76556747509883\n",
      "Epoch 78 \t Batch 540 \t Training Loss: 33.73976544627437\n",
      "Epoch 78 \t Batch 560 \t Training Loss: 33.741182344300405\n",
      "Epoch 78 \t Batch 580 \t Training Loss: 33.7936821115428\n",
      "Epoch 78 \t Batch 600 \t Training Loss: 33.83859580039978\n",
      "Epoch 78 \t Batch 620 \t Training Loss: 33.83971425640968\n",
      "Epoch 78 \t Batch 640 \t Training Loss: 33.861610162258145\n",
      "Epoch 78 \t Batch 660 \t Training Loss: 33.89910093654286\n",
      "Epoch 78 \t Batch 680 \t Training Loss: 33.901467895507814\n",
      "Epoch 78 \t Batch 700 \t Training Loss: 33.91943652834211\n",
      "Epoch 78 \t Batch 720 \t Training Loss: 33.9630440738466\n",
      "Epoch 78 \t Batch 740 \t Training Loss: 33.94546068810128\n",
      "Epoch 78 \t Batch 760 \t Training Loss: 34.00127815196389\n",
      "Epoch 78 \t Batch 780 \t Training Loss: 33.96819063333365\n",
      "Epoch 78 \t Batch 800 \t Training Loss: 33.98927577495575\n",
      "Epoch 78 \t Batch 820 \t Training Loss: 34.01314444890836\n",
      "Epoch 78 \t Batch 840 \t Training Loss: 34.057037417093916\n",
      "Epoch 78 \t Batch 860 \t Training Loss: 34.046496914708335\n",
      "Epoch 78 \t Batch 880 \t Training Loss: 34.044367963617496\n",
      "Epoch 78 \t Batch 900 \t Training Loss: 34.047217235565185\n",
      "Epoch 78 \t Batch 20 \t Validation Loss: 10.576618647575378\n",
      "Epoch 78 \t Batch 40 \t Validation Loss: 13.444587349891663\n",
      "Epoch 78 \t Batch 60 \t Validation Loss: 13.087636963526409\n",
      "Epoch 78 \t Batch 80 \t Validation Loss: 14.472074925899506\n",
      "Epoch 78 \t Batch 100 \t Validation Loss: 17.618490056991575\n",
      "Epoch 78 \t Batch 120 \t Validation Loss: 20.041473857561748\n",
      "Epoch 78 \t Batch 140 \t Validation Loss: 21.319219309943062\n",
      "Epoch 78 \t Batch 160 \t Validation Loss: 23.91376695036888\n",
      "Epoch 78 \t Batch 180 \t Validation Loss: 27.35040454864502\n",
      "Epoch 78 \t Batch 200 \t Validation Loss: 29.44022430419922\n",
      "Epoch 78 \t Batch 220 \t Validation Loss: 30.9724834095348\n",
      "Epoch 78 \t Batch 240 \t Validation Loss: 31.595472729206087\n",
      "Epoch 78 \t Batch 260 \t Validation Loss: 34.09302845368018\n",
      "Epoch 78 \t Batch 280 \t Validation Loss: 35.51899324485234\n",
      "Epoch 78 \t Batch 300 \t Validation Loss: 36.41648678779602\n",
      "Epoch 78 \t Batch 320 \t Validation Loss: 36.89243297278881\n",
      "Epoch 78 \t Batch 340 \t Validation Loss: 37.03657295283149\n",
      "Epoch 78 \t Batch 360 \t Validation Loss: 36.98149159484439\n",
      "Epoch 78 \t Batch 380 \t Validation Loss: 37.5417262855329\n",
      "Epoch 78 \t Batch 400 \t Validation Loss: 37.4902277302742\n",
      "Epoch 78 \t Batch 420 \t Validation Loss: 37.74976687885466\n",
      "Epoch 78 \t Batch 440 \t Validation Loss: 37.8007828950882\n",
      "Epoch 78 \t Batch 460 \t Validation Loss: 38.39644227442534\n",
      "Epoch 78 \t Batch 480 \t Validation Loss: 39.03650271693866\n",
      "Epoch 78 \t Batch 500 \t Validation Loss: 38.88497347068787\n",
      "Epoch 78 \t Batch 520 \t Validation Loss: 39.09025234259092\n",
      "Epoch 78 \t Batch 540 \t Validation Loss: 38.94079120600665\n",
      "Epoch 78 \t Batch 560 \t Validation Loss: 38.8658740741866\n",
      "Epoch 78 \t Batch 580 \t Validation Loss: 38.77933113328342\n",
      "Epoch 78 \t Batch 600 \t Validation Loss: 39.082715474764505\n",
      "Epoch 78 Training Loss: 34.05869129517866 Validation Loss: 39.84042440916036\n",
      "Epoch 78 completed\n",
      "Epoch 79 \t Batch 20 \t Training Loss: 33.55427207946777\n",
      "Epoch 79 \t Batch 40 \t Training Loss: 33.34947261810303\n",
      "Epoch 79 \t Batch 60 \t Training Loss: 33.63560752868652\n",
      "Epoch 79 \t Batch 80 \t Training Loss: 33.39614341259003\n",
      "Epoch 79 \t Batch 100 \t Training Loss: 33.364092140197755\n",
      "Epoch 79 \t Batch 120 \t Training Loss: 33.37394062678019\n",
      "Epoch 79 \t Batch 140 \t Training Loss: 33.42786963326591\n",
      "Epoch 79 \t Batch 160 \t Training Loss: 33.538616037368776\n",
      "Epoch 79 \t Batch 180 \t Training Loss: 33.51690099504259\n",
      "Epoch 79 \t Batch 200 \t Training Loss: 33.47956055641174\n",
      "Epoch 79 \t Batch 220 \t Training Loss: 33.564197366887875\n",
      "Epoch 79 \t Batch 240 \t Training Loss: 33.60327538649241\n",
      "Epoch 79 \t Batch 260 \t Training Loss: 33.656426547123836\n",
      "Epoch 79 \t Batch 280 \t Training Loss: 33.575727687563216\n",
      "Epoch 79 \t Batch 300 \t Training Loss: 33.58704786300659\n",
      "Epoch 79 \t Batch 320 \t Training Loss: 33.55415449738503\n",
      "Epoch 79 \t Batch 340 \t Training Loss: 33.55599719776827\n",
      "Epoch 79 \t Batch 360 \t Training Loss: 33.55845283932156\n",
      "Epoch 79 \t Batch 380 \t Training Loss: 33.56093147679379\n",
      "Epoch 79 \t Batch 400 \t Training Loss: 33.58363813400268\n",
      "Epoch 79 \t Batch 420 \t Training Loss: 33.57527607509068\n",
      "Epoch 79 \t Batch 440 \t Training Loss: 33.567180780930954\n",
      "Epoch 79 \t Batch 460 \t Training Loss: 33.55797743175341\n",
      "Epoch 79 \t Batch 480 \t Training Loss: 33.61225281159083\n",
      "Epoch 79 \t Batch 500 \t Training Loss: 33.58894775390625\n",
      "Epoch 79 \t Batch 520 \t Training Loss: 33.619649153489334\n",
      "Epoch 79 \t Batch 540 \t Training Loss: 33.65937224847299\n",
      "Epoch 79 \t Batch 560 \t Training Loss: 33.69470683847155\n",
      "Epoch 79 \t Batch 580 \t Training Loss: 33.70305241880746\n",
      "Epoch 79 \t Batch 600 \t Training Loss: 33.69598860740662\n",
      "Epoch 79 \t Batch 620 \t Training Loss: 33.696771907806394\n",
      "Epoch 79 \t Batch 640 \t Training Loss: 33.72374212741852\n",
      "Epoch 79 \t Batch 660 \t Training Loss: 33.748070115754096\n",
      "Epoch 79 \t Batch 680 \t Training Loss: 33.75869636816137\n",
      "Epoch 79 \t Batch 700 \t Training Loss: 33.7689313016619\n",
      "Epoch 79 \t Batch 720 \t Training Loss: 33.79018886354235\n",
      "Epoch 79 \t Batch 740 \t Training Loss: 33.79692270691331\n",
      "Epoch 79 \t Batch 760 \t Training Loss: 33.80268955983614\n",
      "Epoch 79 \t Batch 780 \t Training Loss: 33.81130393101619\n",
      "Epoch 79 \t Batch 800 \t Training Loss: 33.82029255151749\n",
      "Epoch 79 \t Batch 820 \t Training Loss: 33.79754485851381\n",
      "Epoch 79 \t Batch 840 \t Training Loss: 33.81449622880845\n",
      "Epoch 79 \t Batch 860 \t Training Loss: 33.80450928488443\n",
      "Epoch 79 \t Batch 880 \t Training Loss: 33.79714623364535\n",
      "Epoch 79 \t Batch 900 \t Training Loss: 33.806412561204695\n",
      "Epoch 79 \t Batch 20 \t Validation Loss: 12.095949935913087\n",
      "Epoch 79 \t Batch 40 \t Validation Loss: 17.31894737482071\n",
      "Epoch 79 \t Batch 60 \t Validation Loss: 15.96152467727661\n",
      "Epoch 79 \t Batch 80 \t Validation Loss: 17.15159406661987\n",
      "Epoch 79 \t Batch 100 \t Validation Loss: 20.167474536895753\n",
      "Epoch 79 \t Batch 120 \t Validation Loss: 22.442242431640626\n",
      "Epoch 79 \t Batch 140 \t Validation Loss: 23.585965483529225\n",
      "Epoch 79 \t Batch 160 \t Validation Loss: 25.82327206134796\n",
      "Epoch 79 \t Batch 180 \t Validation Loss: 29.04617207845052\n",
      "Epoch 79 \t Batch 200 \t Validation Loss: 30.842541494369506\n",
      "Epoch 79 \t Batch 220 \t Validation Loss: 32.20769077647816\n",
      "Epoch 79 \t Batch 240 \t Validation Loss: 32.76923141479492\n",
      "Epoch 79 \t Batch 260 \t Validation Loss: 35.05525971559378\n",
      "Epoch 79 \t Batch 280 \t Validation Loss: 36.35769270147596\n",
      "Epoch 79 \t Batch 300 \t Validation Loss: 37.19096256573995\n",
      "Epoch 79 \t Batch 320 \t Validation Loss: 37.65787369906902\n",
      "Epoch 79 \t Batch 340 \t Validation Loss: 37.76103170899784\n",
      "Epoch 79 \t Batch 360 \t Validation Loss: 37.64425086710188\n",
      "Epoch 79 \t Batch 380 \t Validation Loss: 38.14861092567444\n",
      "Epoch 79 \t Batch 400 \t Validation Loss: 38.1153817152977\n",
      "Epoch 79 \t Batch 420 \t Validation Loss: 38.312115998495194\n",
      "Epoch 79 \t Batch 440 \t Validation Loss: 38.3683858459646\n",
      "Epoch 79 \t Batch 460 \t Validation Loss: 38.977796747373496\n",
      "Epoch 79 \t Batch 480 \t Validation Loss: 39.56503232916196\n",
      "Epoch 79 \t Batch 500 \t Validation Loss: 39.36598404121399\n",
      "Epoch 79 \t Batch 520 \t Validation Loss: 39.57001099586487\n",
      "Epoch 79 \t Batch 540 \t Validation Loss: 39.478619801556626\n",
      "Epoch 79 \t Batch 560 \t Validation Loss: 39.46407720702035\n",
      "Epoch 79 \t Batch 580 \t Validation Loss: 39.37472433879458\n",
      "Epoch 79 \t Batch 600 \t Validation Loss: 39.688328196207685\n",
      "Epoch 79 Training Loss: 33.80431335598978 Validation Loss: 40.525298010219224\n",
      "Epoch 79 completed\n",
      "Epoch 80 \t Batch 20 \t Training Loss: 32.92550401687622\n",
      "Epoch 80 \t Batch 40 \t Training Loss: 33.320423412322995\n",
      "Epoch 80 \t Batch 60 \t Training Loss: 33.40815273920695\n",
      "Epoch 80 \t Batch 80 \t Training Loss: 33.25085048675537\n",
      "Epoch 80 \t Batch 100 \t Training Loss: 33.2147114944458\n",
      "Epoch 80 \t Batch 120 \t Training Loss: 33.34353750546773\n",
      "Epoch 80 \t Batch 140 \t Training Loss: 33.330870328630716\n",
      "Epoch 80 \t Batch 160 \t Training Loss: 33.12091193199158\n",
      "Epoch 80 \t Batch 180 \t Training Loss: 33.18109584384494\n",
      "Epoch 80 \t Batch 200 \t Training Loss: 33.1549329662323\n",
      "Epoch 80 \t Batch 220 \t Training Loss: 33.084451207247646\n",
      "Epoch 80 \t Batch 240 \t Training Loss: 33.11854684352875\n",
      "Epoch 80 \t Batch 260 \t Training Loss: 33.074256896972656\n",
      "Epoch 80 \t Batch 280 \t Training Loss: 33.10281270572117\n",
      "Epoch 80 \t Batch 300 \t Training Loss: 33.19569756189982\n",
      "Epoch 80 \t Batch 320 \t Training Loss: 33.164841294288635\n",
      "Epoch 80 \t Batch 340 \t Training Loss: 33.223494047277114\n",
      "Epoch 80 \t Batch 360 \t Training Loss: 33.248549127578734\n",
      "Epoch 80 \t Batch 380 \t Training Loss: 33.25898520319085\n",
      "Epoch 80 \t Batch 400 \t Training Loss: 33.298364896774295\n",
      "Epoch 80 \t Batch 420 \t Training Loss: 33.30161828540621\n",
      "Epoch 80 \t Batch 440 \t Training Loss: 33.2949193390933\n",
      "Epoch 80 \t Batch 460 \t Training Loss: 33.28775102366572\n",
      "Epoch 80 \t Batch 480 \t Training Loss: 33.301058411598206\n",
      "Epoch 80 \t Batch 500 \t Training Loss: 33.33687850952148\n",
      "Epoch 80 \t Batch 520 \t Training Loss: 33.3323001568134\n",
      "Epoch 80 \t Batch 540 \t Training Loss: 33.36364546528569\n",
      "Epoch 80 \t Batch 560 \t Training Loss: 33.376103217261175\n",
      "Epoch 80 \t Batch 580 \t Training Loss: 33.38832446131213\n",
      "Epoch 80 \t Batch 600 \t Training Loss: 33.394704558054606\n",
      "Epoch 80 \t Batch 620 \t Training Loss: 33.377920255353374\n",
      "Epoch 80 \t Batch 640 \t Training Loss: 33.44917603135109\n",
      "Epoch 80 \t Batch 660 \t Training Loss: 33.45058029059208\n",
      "Epoch 80 \t Batch 680 \t Training Loss: 33.469758202047906\n",
      "Epoch 80 \t Batch 700 \t Training Loss: 33.464408653804234\n",
      "Epoch 80 \t Batch 720 \t Training Loss: 33.47719872792562\n",
      "Epoch 80 \t Batch 740 \t Training Loss: 33.47713503193211\n",
      "Epoch 80 \t Batch 760 \t Training Loss: 33.469714634042035\n",
      "Epoch 80 \t Batch 780 \t Training Loss: 33.48054512953147\n",
      "Epoch 80 \t Batch 800 \t Training Loss: 33.47595535039902\n",
      "Epoch 80 \t Batch 820 \t Training Loss: 33.507215999975436\n",
      "Epoch 80 \t Batch 840 \t Training Loss: 33.531762643087475\n",
      "Epoch 80 \t Batch 860 \t Training Loss: 33.56149898795194\n",
      "Epoch 80 \t Batch 880 \t Training Loss: 33.58615942651575\n",
      "Epoch 80 \t Batch 900 \t Training Loss: 33.56956485748291\n",
      "Epoch 80 \t Batch 20 \t Validation Loss: 13.085856866836547\n",
      "Epoch 80 \t Batch 40 \t Validation Loss: 17.310724902153016\n",
      "Epoch 80 \t Batch 60 \t Validation Loss: 16.073368525505067\n",
      "Epoch 80 \t Batch 80 \t Validation Loss: 17.290860730409623\n",
      "Epoch 80 \t Batch 100 \t Validation Loss: 20.11728922367096\n",
      "Epoch 80 \t Batch 120 \t Validation Loss: 22.256675056616466\n",
      "Epoch 80 \t Batch 140 \t Validation Loss: 23.247440123558043\n",
      "Epoch 80 \t Batch 160 \t Validation Loss: 25.351065734028815\n",
      "Epoch 80 \t Batch 180 \t Validation Loss: 28.286045410897998\n",
      "Epoch 80 \t Batch 200 \t Validation Loss: 30.049352910518646\n",
      "Epoch 80 \t Batch 220 \t Validation Loss: 31.272950421680104\n",
      "Epoch 80 \t Batch 240 \t Validation Loss: 31.633475929498672\n",
      "Epoch 80 \t Batch 260 \t Validation Loss: 33.86030900111565\n",
      "Epoch 80 \t Batch 280 \t Validation Loss: 35.08239713907242\n",
      "Epoch 80 \t Batch 300 \t Validation Loss: 35.83827115853627\n",
      "Epoch 80 \t Batch 320 \t Validation Loss: 36.218287114799026\n",
      "Epoch 80 \t Batch 340 \t Validation Loss: 36.31834953953238\n",
      "Epoch 80 \t Batch 360 \t Validation Loss: 36.20641416311264\n",
      "Epoch 80 \t Batch 380 \t Validation Loss: 36.69384141093806\n",
      "Epoch 80 \t Batch 400 \t Validation Loss: 36.63421414256096\n",
      "Epoch 80 \t Batch 420 \t Validation Loss: 36.882510833513166\n",
      "Epoch 80 \t Batch 440 \t Validation Loss: 36.95329713713039\n",
      "Epoch 80 \t Batch 460 \t Validation Loss: 37.52139751911163\n",
      "Epoch 80 \t Batch 480 \t Validation Loss: 38.158773548404376\n",
      "Epoch 80 \t Batch 500 \t Validation Loss: 37.94775670528412\n",
      "Epoch 80 \t Batch 520 \t Validation Loss: 38.13442778128844\n",
      "Epoch 80 \t Batch 540 \t Validation Loss: 38.061493113305836\n",
      "Epoch 80 \t Batch 560 \t Validation Loss: 38.07778361780303\n",
      "Epoch 80 \t Batch 580 \t Validation Loss: 38.0543376388221\n",
      "Epoch 80 \t Batch 600 \t Validation Loss: 38.41189505497614\n",
      "Epoch 80 Training Loss: 33.5747591827792 Validation Loss: 39.19400257801081\n",
      "Epoch 80 completed\n",
      "Epoch 81 \t Batch 20 \t Training Loss: 32.02819204330444\n",
      "Epoch 81 \t Batch 40 \t Training Loss: 31.906889247894288\n",
      "Epoch 81 \t Batch 60 \t Training Loss: 32.14862133661906\n",
      "Epoch 81 \t Batch 80 \t Training Loss: 32.143378829956056\n",
      "Epoch 81 \t Batch 100 \t Training Loss: 32.41965057373047\n",
      "Epoch 81 \t Batch 120 \t Training Loss: 32.560561402638754\n",
      "Epoch 81 \t Batch 140 \t Training Loss: 32.73684170586722\n",
      "Epoch 81 \t Batch 160 \t Training Loss: 32.77138593196869\n",
      "Epoch 81 \t Batch 180 \t Training Loss: 32.86134460237291\n",
      "Epoch 81 \t Batch 200 \t Training Loss: 32.99712914466858\n",
      "Epoch 81 \t Batch 220 \t Training Loss: 32.99797115325928\n",
      "Epoch 81 \t Batch 240 \t Training Loss: 33.101197163263954\n",
      "Epoch 81 \t Batch 260 \t Training Loss: 33.110755296853874\n",
      "Epoch 81 \t Batch 280 \t Training Loss: 33.03719641140529\n",
      "Epoch 81 \t Batch 300 \t Training Loss: 33.0934707069397\n",
      "Epoch 81 \t Batch 320 \t Training Loss: 33.089208126068115\n",
      "Epoch 81 \t Batch 340 \t Training Loss: 33.07120372547823\n",
      "Epoch 81 \t Batch 360 \t Training Loss: 33.150460709465875\n",
      "Epoch 81 \t Batch 380 \t Training Loss: 33.16686404880724\n",
      "Epoch 81 \t Batch 400 \t Training Loss: 33.17037488460541\n",
      "Epoch 81 \t Batch 420 \t Training Loss: 33.18102299826486\n",
      "Epoch 81 \t Batch 440 \t Training Loss: 33.16386306069114\n",
      "Epoch 81 \t Batch 460 \t Training Loss: 33.19783631200376\n",
      "Epoch 81 \t Batch 480 \t Training Loss: 33.25349128643672\n",
      "Epoch 81 \t Batch 500 \t Training Loss: 33.274996757507324\n",
      "Epoch 81 \t Batch 520 \t Training Loss: 33.297472517306986\n",
      "Epoch 81 \t Batch 540 \t Training Loss: 33.30388871652109\n",
      "Epoch 81 \t Batch 560 \t Training Loss: 33.33543943337032\n",
      "Epoch 81 \t Batch 580 \t Training Loss: 33.31830050369789\n",
      "Epoch 81 \t Batch 600 \t Training Loss: 33.324260314305626\n",
      "Epoch 81 \t Batch 620 \t Training Loss: 33.316852698787564\n",
      "Epoch 81 \t Batch 640 \t Training Loss: 33.316392454504964\n",
      "Epoch 81 \t Batch 660 \t Training Loss: 33.31777440562393\n",
      "Epoch 81 \t Batch 680 \t Training Loss: 33.328034813263834\n",
      "Epoch 81 \t Batch 700 \t Training Loss: 33.332765227726526\n",
      "Epoch 81 \t Batch 720 \t Training Loss: 33.35305973423852\n",
      "Epoch 81 \t Batch 740 \t Training Loss: 33.36885710020323\n",
      "Epoch 81 \t Batch 760 \t Training Loss: 33.36040778912996\n",
      "Epoch 81 \t Batch 780 \t Training Loss: 33.37206921210656\n",
      "Epoch 81 \t Batch 800 \t Training Loss: 33.36490287303925\n",
      "Epoch 81 \t Batch 820 \t Training Loss: 33.36927352533108\n",
      "Epoch 81 \t Batch 840 \t Training Loss: 33.37087197303772\n",
      "Epoch 81 \t Batch 860 \t Training Loss: 33.37829165126002\n",
      "Epoch 81 \t Batch 880 \t Training Loss: 33.38427686908028\n",
      "Epoch 81 \t Batch 900 \t Training Loss: 33.36923122829861\n",
      "Epoch 81 \t Batch 20 \t Validation Loss: 10.13410382270813\n",
      "Epoch 81 \t Batch 40 \t Validation Loss: 15.159393382072448\n",
      "Epoch 81 \t Batch 60 \t Validation Loss: 14.114433741569519\n",
      "Epoch 81 \t Batch 80 \t Validation Loss: 15.394427222013473\n",
      "Epoch 81 \t Batch 100 \t Validation Loss: 18.744649596214295\n",
      "Epoch 81 \t Batch 120 \t Validation Loss: 21.103520707289377\n",
      "Epoch 81 \t Batch 140 \t Validation Loss: 22.334230923652648\n",
      "Epoch 81 \t Batch 160 \t Validation Loss: 24.877437493205072\n",
      "Epoch 81 \t Batch 180 \t Validation Loss: 28.482302917374504\n",
      "Epoch 81 \t Batch 200 \t Validation Loss: 30.39141473531723\n",
      "Epoch 81 \t Batch 220 \t Validation Loss: 31.895856430313803\n",
      "Epoch 81 \t Batch 240 \t Validation Loss: 32.56824627518654\n",
      "Epoch 81 \t Batch 260 \t Validation Loss: 34.97685784376585\n",
      "Epoch 81 \t Batch 280 \t Validation Loss: 36.32384492499488\n",
      "Epoch 81 \t Batch 300 \t Validation Loss: 37.339886457125345\n",
      "Epoch 81 \t Batch 320 \t Validation Loss: 37.933484475314614\n",
      "Epoch 81 \t Batch 340 \t Validation Loss: 38.01795716425952\n",
      "Epoch 81 \t Batch 360 \t Validation Loss: 37.97850647370021\n",
      "Epoch 81 \t Batch 380 \t Validation Loss: 38.511128810832375\n",
      "Epoch 81 \t Batch 400 \t Validation Loss: 38.37984520792961\n",
      "Epoch 81 \t Batch 420 \t Validation Loss: 38.604758379572914\n",
      "Epoch 81 \t Batch 440 \t Validation Loss: 38.59022009914572\n",
      "Epoch 81 \t Batch 460 \t Validation Loss: 39.098688924830896\n",
      "Epoch 81 \t Batch 480 \t Validation Loss: 39.74171072145303\n",
      "Epoch 81 \t Batch 500 \t Validation Loss: 39.56411466884613\n",
      "Epoch 81 \t Batch 520 \t Validation Loss: 39.77979797124863\n",
      "Epoch 81 \t Batch 540 \t Validation Loss: 39.634401278142576\n",
      "Epoch 81 \t Batch 560 \t Validation Loss: 39.57611673133714\n",
      "Epoch 81 \t Batch 580 \t Validation Loss: 39.51170633496909\n",
      "Epoch 81 \t Batch 600 \t Validation Loss: 39.82121714830399\n",
      "Epoch 81 Training Loss: 33.369926569116025 Validation Loss: 40.55946294286034\n",
      "Epoch 81 completed\n",
      "Epoch 82 \t Batch 20 \t Training Loss: 31.98418025970459\n",
      "Epoch 82 \t Batch 40 \t Training Loss: 32.12707490921021\n",
      "Epoch 82 \t Batch 60 \t Training Loss: 32.218331050872806\n",
      "Epoch 82 \t Batch 80 \t Training Loss: 32.136777019500734\n",
      "Epoch 82 \t Batch 100 \t Training Loss: 32.28035186767578\n",
      "Epoch 82 \t Batch 120 \t Training Loss: 32.35243055025737\n",
      "Epoch 82 \t Batch 140 \t Training Loss: 32.584721142905096\n",
      "Epoch 82 \t Batch 160 \t Training Loss: 32.55440406799316\n",
      "Epoch 82 \t Batch 180 \t Training Loss: 32.537369865841335\n",
      "Epoch 82 \t Batch 200 \t Training Loss: 32.55700876235962\n",
      "Epoch 82 \t Batch 220 \t Training Loss: 32.56123205531727\n",
      "Epoch 82 \t Batch 240 \t Training Loss: 32.519004782040916\n",
      "Epoch 82 \t Batch 260 \t Training Loss: 32.53900887415959\n",
      "Epoch 82 \t Batch 280 \t Training Loss: 32.56071047782898\n",
      "Epoch 82 \t Batch 300 \t Training Loss: 32.614903602600094\n",
      "Epoch 82 \t Batch 320 \t Training Loss: 32.593879878520966\n",
      "Epoch 82 \t Batch 340 \t Training Loss: 32.69928128859576\n",
      "Epoch 82 \t Batch 360 \t Training Loss: 32.741669268078276\n",
      "Epoch 82 \t Batch 380 \t Training Loss: 32.757887614400765\n",
      "Epoch 82 \t Batch 400 \t Training Loss: 32.79962090015411\n",
      "Epoch 82 \t Batch 420 \t Training Loss: 32.83900865827288\n",
      "Epoch 82 \t Batch 440 \t Training Loss: 32.84495652372187\n",
      "Epoch 82 \t Batch 460 \t Training Loss: 32.838299444447394\n",
      "Epoch 82 \t Batch 480 \t Training Loss: 32.84104922612508\n",
      "Epoch 82 \t Batch 500 \t Training Loss: 32.88007935333252\n",
      "Epoch 82 \t Batch 520 \t Training Loss: 32.90211785023029\n",
      "Epoch 82 \t Batch 540 \t Training Loss: 32.90179051293267\n",
      "Epoch 82 \t Batch 560 \t Training Loss: 32.89549611295973\n",
      "Epoch 82 \t Batch 580 \t Training Loss: 32.894737365328034\n",
      "Epoch 82 \t Batch 600 \t Training Loss: 32.92771195411682\n",
      "Epoch 82 \t Batch 620 \t Training Loss: 32.943466623367804\n",
      "Epoch 82 \t Batch 640 \t Training Loss: 32.93700877130031\n",
      "Epoch 82 \t Batch 660 \t Training Loss: 32.95711367925008\n",
      "Epoch 82 \t Batch 680 \t Training Loss: 32.94306826591492\n",
      "Epoch 82 \t Batch 700 \t Training Loss: 32.93747854505266\n",
      "Epoch 82 \t Batch 720 \t Training Loss: 32.962916588783266\n",
      "Epoch 82 \t Batch 740 \t Training Loss: 32.96863921655191\n",
      "Epoch 82 \t Batch 760 \t Training Loss: 32.96925404448258\n",
      "Epoch 82 \t Batch 780 \t Training Loss: 32.983063607338146\n",
      "Epoch 82 \t Batch 800 \t Training Loss: 32.988139760494235\n",
      "Epoch 82 \t Batch 820 \t Training Loss: 33.01277758202902\n",
      "Epoch 82 \t Batch 840 \t Training Loss: 33.031546093168714\n",
      "Epoch 82 \t Batch 860 \t Training Loss: 33.02962939683781\n",
      "Epoch 82 \t Batch 880 \t Training Loss: 33.07119962952354\n",
      "Epoch 82 \t Batch 900 \t Training Loss: 33.07693731307983\n",
      "Epoch 82 \t Batch 20 \t Validation Loss: 10.81173017024994\n",
      "Epoch 82 \t Batch 40 \t Validation Loss: 14.023043048381805\n",
      "Epoch 82 \t Batch 60 \t Validation Loss: 13.467707228660583\n",
      "Epoch 82 \t Batch 80 \t Validation Loss: 14.72750751376152\n",
      "Epoch 82 \t Batch 100 \t Validation Loss: 18.13038827419281\n",
      "Epoch 82 \t Batch 120 \t Validation Loss: 20.80928506453832\n",
      "Epoch 82 \t Batch 140 \t Validation Loss: 22.135716230528697\n",
      "Epoch 82 \t Batch 160 \t Validation Loss: 24.577018281817438\n",
      "Epoch 82 \t Batch 180 \t Validation Loss: 28.200994356473288\n",
      "Epoch 82 \t Batch 200 \t Validation Loss: 30.141637527942656\n",
      "Epoch 82 \t Batch 220 \t Validation Loss: 31.68972496769645\n",
      "Epoch 82 \t Batch 240 \t Validation Loss: 32.3404495815436\n",
      "Epoch 82 \t Batch 260 \t Validation Loss: 34.74442661175361\n",
      "Epoch 82 \t Batch 280 \t Validation Loss: 36.09480405705316\n",
      "Epoch 82 \t Batch 300 \t Validation Loss: 37.08126220226288\n",
      "Epoch 82 \t Batch 320 \t Validation Loss: 37.657106862962245\n",
      "Epoch 82 \t Batch 340 \t Validation Loss: 37.77443491851582\n",
      "Epoch 82 \t Batch 360 \t Validation Loss: 37.68279061184989\n",
      "Epoch 82 \t Batch 380 \t Validation Loss: 38.13763583208385\n",
      "Epoch 82 \t Batch 400 \t Validation Loss: 37.98972816824913\n",
      "Epoch 82 \t Batch 420 \t Validation Loss: 38.21210227580298\n",
      "Epoch 82 \t Batch 440 \t Validation Loss: 38.16546271822669\n",
      "Epoch 82 \t Batch 460 \t Validation Loss: 38.6194352015205\n",
      "Epoch 82 \t Batch 480 \t Validation Loss: 39.25530731578668\n",
      "Epoch 82 \t Batch 500 \t Validation Loss: 39.05764206218719\n",
      "Epoch 82 \t Batch 520 \t Validation Loss: 39.075472489687115\n",
      "Epoch 82 \t Batch 540 \t Validation Loss: 38.91115671705317\n",
      "Epoch 82 \t Batch 560 \t Validation Loss: 38.82366745386805\n",
      "Epoch 82 \t Batch 580 \t Validation Loss: 38.67216709646685\n",
      "Epoch 82 \t Batch 600 \t Validation Loss: 38.981975888411206\n",
      "Epoch 82 Training Loss: 33.085358369051434 Validation Loss: 39.73668796288503\n",
      "Epoch 82 completed\n",
      "Epoch 83 \t Batch 20 \t Training Loss: 32.9998703956604\n",
      "Epoch 83 \t Batch 40 \t Training Loss: 32.385350704193115\n",
      "Epoch 83 \t Batch 60 \t Training Loss: 32.02346986134847\n",
      "Epoch 83 \t Batch 80 \t Training Loss: 32.15728147029877\n",
      "Epoch 83 \t Batch 100 \t Training Loss: 32.25908744812012\n",
      "Epoch 83 \t Batch 120 \t Training Loss: 32.26251271565755\n",
      "Epoch 83 \t Batch 140 \t Training Loss: 32.11711499350412\n",
      "Epoch 83 \t Batch 160 \t Training Loss: 32.18634748458862\n",
      "Epoch 83 \t Batch 180 \t Training Loss: 32.27450526555379\n",
      "Epoch 83 \t Batch 200 \t Training Loss: 32.23560382843018\n",
      "Epoch 83 \t Batch 220 \t Training Loss: 32.29517452066595\n",
      "Epoch 83 \t Batch 240 \t Training Loss: 32.33488040765126\n",
      "Epoch 83 \t Batch 260 \t Training Loss: 32.424719157585734\n",
      "Epoch 83 \t Batch 280 \t Training Loss: 32.39820521899632\n",
      "Epoch 83 \t Batch 300 \t Training Loss: 32.43713359196981\n",
      "Epoch 83 \t Batch 320 \t Training Loss: 32.43717184066772\n",
      "Epoch 83 \t Batch 340 \t Training Loss: 32.38600296132705\n",
      "Epoch 83 \t Batch 360 \t Training Loss: 32.4045344458686\n",
      "Epoch 83 \t Batch 380 \t Training Loss: 32.41000616173995\n",
      "Epoch 83 \t Batch 400 \t Training Loss: 32.407032041549684\n",
      "Epoch 83 \t Batch 420 \t Training Loss: 32.505715501876104\n",
      "Epoch 83 \t Batch 440 \t Training Loss: 32.47257817441767\n",
      "Epoch 83 \t Batch 460 \t Training Loss: 32.52005085737809\n",
      "Epoch 83 \t Batch 480 \t Training Loss: 32.55404550631841\n",
      "Epoch 83 \t Batch 500 \t Training Loss: 32.5667314491272\n",
      "Epoch 83 \t Batch 520 \t Training Loss: 32.6055183667403\n",
      "Epoch 83 \t Batch 540 \t Training Loss: 32.62385207282172\n",
      "Epoch 83 \t Batch 560 \t Training Loss: 32.6134455374309\n",
      "Epoch 83 \t Batch 580 \t Training Loss: 32.613710268612564\n",
      "Epoch 83 \t Batch 600 \t Training Loss: 32.63209363937378\n",
      "Epoch 83 \t Batch 620 \t Training Loss: 32.639913734312984\n",
      "Epoch 83 \t Batch 640 \t Training Loss: 32.65217230319977\n",
      "Epoch 83 \t Batch 660 \t Training Loss: 32.65725900187637\n",
      "Epoch 83 \t Batch 680 \t Training Loss: 32.667791257185094\n",
      "Epoch 83 \t Batch 700 \t Training Loss: 32.680177424294605\n",
      "Epoch 83 \t Batch 720 \t Training Loss: 32.705848007731966\n",
      "Epoch 83 \t Batch 740 \t Training Loss: 32.70970329851718\n",
      "Epoch 83 \t Batch 760 \t Training Loss: 32.72409621037935\n",
      "Epoch 83 \t Batch 780 \t Training Loss: 32.726737137329884\n",
      "Epoch 83 \t Batch 800 \t Training Loss: 32.7475726723671\n",
      "Epoch 83 \t Batch 820 \t Training Loss: 32.775417555832284\n",
      "Epoch 83 \t Batch 840 \t Training Loss: 32.83358395667303\n",
      "Epoch 83 \t Batch 860 \t Training Loss: 32.859016997315166\n",
      "Epoch 83 \t Batch 880 \t Training Loss: 32.87481994412162\n",
      "Epoch 83 \t Batch 900 \t Training Loss: 32.86154393725925\n",
      "Epoch 83 \t Batch 20 \t Validation Loss: 9.41277780532837\n",
      "Epoch 83 \t Batch 40 \t Validation Loss: 13.863520646095276\n",
      "Epoch 83 \t Batch 60 \t Validation Loss: 13.536519813537598\n",
      "Epoch 83 \t Batch 80 \t Validation Loss: 15.089890366792678\n",
      "Epoch 83 \t Batch 100 \t Validation Loss: 18.562827153205873\n",
      "Epoch 83 \t Batch 120 \t Validation Loss: 21.12047404050827\n",
      "Epoch 83 \t Batch 140 \t Validation Loss: 22.37768292086465\n",
      "Epoch 83 \t Batch 160 \t Validation Loss: 25.065712490677832\n",
      "Epoch 83 \t Batch 180 \t Validation Loss: 29.0778600136439\n",
      "Epoch 83 \t Batch 200 \t Validation Loss: 31.21247402906418\n",
      "Epoch 83 \t Batch 220 \t Validation Loss: 32.926251027800824\n",
      "Epoch 83 \t Batch 240 \t Validation Loss: 33.64569698770841\n",
      "Epoch 83 \t Batch 260 \t Validation Loss: 36.26864788899055\n",
      "Epoch 83 \t Batch 280 \t Validation Loss: 37.73225621325629\n",
      "Epoch 83 \t Batch 300 \t Validation Loss: 38.83951902866364\n",
      "Epoch 83 \t Batch 320 \t Validation Loss: 39.445449851453304\n",
      "Epoch 83 \t Batch 340 \t Validation Loss: 39.47059873552883\n",
      "Epoch 83 \t Batch 360 \t Validation Loss: 39.388557202286194\n",
      "Epoch 83 \t Batch 380 \t Validation Loss: 39.923010325431825\n",
      "Epoch 83 \t Batch 400 \t Validation Loss: 39.72366531491279\n",
      "Epoch 83 \t Batch 420 \t Validation Loss: 39.8681633188611\n",
      "Epoch 83 \t Batch 440 \t Validation Loss: 39.814562035690656\n",
      "Epoch 83 \t Batch 460 \t Validation Loss: 40.356698385528894\n",
      "Epoch 83 \t Batch 480 \t Validation Loss: 40.937350647648174\n",
      "Epoch 83 \t Batch 500 \t Validation Loss: 40.71684136295318\n",
      "Epoch 83 \t Batch 520 \t Validation Loss: 40.87884695438238\n",
      "Epoch 83 \t Batch 540 \t Validation Loss: 40.63794174635852\n",
      "Epoch 83 \t Batch 560 \t Validation Loss: 40.50942461575781\n",
      "Epoch 83 \t Batch 580 \t Validation Loss: 40.337210077252884\n",
      "Epoch 83 \t Batch 600 \t Validation Loss: 40.56914094050725\n",
      "Epoch 83 Training Loss: 32.87804602952664 Validation Loss: 41.30672557400418\n",
      "Epoch 83 completed\n",
      "Epoch 84 \t Batch 20 \t Training Loss: 32.22844667434693\n",
      "Epoch 84 \t Batch 40 \t Training Loss: 31.918480825424194\n",
      "Epoch 84 \t Batch 60 \t Training Loss: 32.25717840194702\n",
      "Epoch 84 \t Batch 80 \t Training Loss: 32.27175574302673\n",
      "Epoch 84 \t Batch 100 \t Training Loss: 32.16659566879272\n",
      "Epoch 84 \t Batch 120 \t Training Loss: 32.17627216974894\n",
      "Epoch 84 \t Batch 140 \t Training Loss: 32.16183178765433\n",
      "Epoch 84 \t Batch 160 \t Training Loss: 32.18103451728821\n",
      "Epoch 84 \t Batch 180 \t Training Loss: 32.1338849597507\n",
      "Epoch 84 \t Batch 200 \t Training Loss: 32.229259185791015\n",
      "Epoch 84 \t Batch 220 \t Training Loss: 32.181968602267176\n",
      "Epoch 84 \t Batch 240 \t Training Loss: 32.2404407342275\n",
      "Epoch 84 \t Batch 260 \t Training Loss: 32.25282570765569\n",
      "Epoch 84 \t Batch 280 \t Training Loss: 32.286595364979334\n",
      "Epoch 84 \t Batch 300 \t Training Loss: 32.34303409576416\n",
      "Epoch 84 \t Batch 320 \t Training Loss: 32.3341383934021\n",
      "Epoch 84 \t Batch 340 \t Training Loss: 32.33190051247092\n",
      "Epoch 84 \t Batch 360 \t Training Loss: 32.30901621712579\n",
      "Epoch 84 \t Batch 380 \t Training Loss: 32.30970399254247\n",
      "Epoch 84 \t Batch 400 \t Training Loss: 32.30049960136414\n",
      "Epoch 84 \t Batch 420 \t Training Loss: 32.257217979431154\n",
      "Epoch 84 \t Batch 440 \t Training Loss: 32.26251287893815\n",
      "Epoch 84 \t Batch 460 \t Training Loss: 32.28969769685165\n",
      "Epoch 84 \t Batch 480 \t Training Loss: 32.2917156457901\n",
      "Epoch 84 \t Batch 500 \t Training Loss: 32.32801461410522\n",
      "Epoch 84 \t Batch 520 \t Training Loss: 32.369213529733514\n",
      "Epoch 84 \t Batch 540 \t Training Loss: 32.37144144905938\n",
      "Epoch 84 \t Batch 560 \t Training Loss: 32.411801729883464\n",
      "Epoch 84 \t Batch 580 \t Training Loss: 32.46127571566352\n",
      "Epoch 84 \t Batch 600 \t Training Loss: 32.47887576103211\n",
      "Epoch 84 \t Batch 620 \t Training Loss: 32.490045190626574\n",
      "Epoch 84 \t Batch 640 \t Training Loss: 32.46847348511219\n",
      "Epoch 84 \t Batch 660 \t Training Loss: 32.47795431541674\n",
      "Epoch 84 \t Batch 680 \t Training Loss: 32.51766171174891\n",
      "Epoch 84 \t Batch 700 \t Training Loss: 32.52725657599313\n",
      "Epoch 84 \t Batch 720 \t Training Loss: 32.52576918072171\n",
      "Epoch 84 \t Batch 740 \t Training Loss: 32.527676541096454\n",
      "Epoch 84 \t Batch 760 \t Training Loss: 32.53819569286547\n",
      "Epoch 84 \t Batch 780 \t Training Loss: 32.56192845809154\n",
      "Epoch 84 \t Batch 800 \t Training Loss: 32.561178708076476\n",
      "Epoch 84 \t Batch 820 \t Training Loss: 32.57250434828968\n",
      "Epoch 84 \t Batch 840 \t Training Loss: 32.565375298545476\n",
      "Epoch 84 \t Batch 860 \t Training Loss: 32.56354595672253\n",
      "Epoch 84 \t Batch 880 \t Training Loss: 32.579559003223075\n",
      "Epoch 84 \t Batch 900 \t Training Loss: 32.58607057359484\n",
      "Epoch 84 \t Batch 20 \t Validation Loss: 8.275163698196412\n",
      "Epoch 84 \t Batch 40 \t Validation Loss: 11.82315222620964\n",
      "Epoch 84 \t Batch 60 \t Validation Loss: 11.609233152866363\n",
      "Epoch 84 \t Batch 80 \t Validation Loss: 13.02064914405346\n",
      "Epoch 84 \t Batch 100 \t Validation Loss: 16.6813716673851\n",
      "Epoch 84 \t Batch 120 \t Validation Loss: 19.274217365185418\n",
      "Epoch 84 \t Batch 140 \t Validation Loss: 20.80519577945982\n",
      "Epoch 84 \t Batch 160 \t Validation Loss: 23.49307134002447\n",
      "Epoch 84 \t Batch 180 \t Validation Loss: 26.380543898211585\n",
      "Epoch 84 \t Batch 200 \t Validation Loss: 28.16588168978691\n",
      "Epoch 84 \t Batch 220 \t Validation Loss: 29.41976552334699\n",
      "Epoch 84 \t Batch 240 \t Validation Loss: 30.01507201095422\n",
      "Epoch 84 \t Batch 260 \t Validation Loss: 32.11026271214852\n",
      "Epoch 84 \t Batch 280 \t Validation Loss: 33.380247411557605\n",
      "Epoch 84 \t Batch 300 \t Validation Loss: 34.2216641831398\n",
      "Epoch 84 \t Batch 320 \t Validation Loss: 34.74592159017921\n",
      "Epoch 84 \t Batch 340 \t Validation Loss: 35.019222755291885\n",
      "Epoch 84 \t Batch 360 \t Validation Loss: 35.144519633054735\n",
      "Epoch 84 \t Batch 380 \t Validation Loss: 35.83387421746003\n",
      "Epoch 84 \t Batch 400 \t Validation Loss: 36.11289675414562\n",
      "Epoch 84 \t Batch 420 \t Validation Loss: 36.59270912068231\n",
      "Epoch 84 \t Batch 440 \t Validation Loss: 37.00634501793168\n",
      "Epoch 84 \t Batch 460 \t Validation Loss: 37.83641380797262\n",
      "Epoch 84 \t Batch 480 \t Validation Loss: 38.55475596735875\n",
      "Epoch 84 \t Batch 500 \t Validation Loss: 38.46387635946274\n",
      "Epoch 84 \t Batch 520 \t Validation Loss: 38.924897290651614\n",
      "Epoch 84 \t Batch 540 \t Validation Loss: 38.8790042148696\n",
      "Epoch 84 \t Batch 560 \t Validation Loss: 38.905211875694135\n",
      "Epoch 84 \t Batch 580 \t Validation Loss: 38.85962871477522\n",
      "Epoch 84 \t Batch 600 \t Validation Loss: 39.23656782587369\n",
      "Epoch 84 Training Loss: 32.59229892160796 Validation Loss: 40.010282180138994\n",
      "Epoch 84 completed\n",
      "Epoch 85 \t Batch 20 \t Training Loss: 32.336305713653566\n",
      "Epoch 85 \t Batch 40 \t Training Loss: 31.95481448173523\n",
      "Epoch 85 \t Batch 60 \t Training Loss: 31.633266162872314\n",
      "Epoch 85 \t Batch 80 \t Training Loss: 31.696652722358703\n",
      "Epoch 85 \t Batch 100 \t Training Loss: 31.70491781234741\n",
      "Epoch 85 \t Batch 120 \t Training Loss: 31.526801824569702\n",
      "Epoch 85 \t Batch 140 \t Training Loss: 31.450406537737166\n",
      "Epoch 85 \t Batch 160 \t Training Loss: 31.49912942647934\n",
      "Epoch 85 \t Batch 180 \t Training Loss: 31.615787400139702\n",
      "Epoch 85 \t Batch 200 \t Training Loss: 31.63092946052551\n",
      "Epoch 85 \t Batch 220 \t Training Loss: 31.60733429301869\n",
      "Epoch 85 \t Batch 240 \t Training Loss: 31.680908370018006\n",
      "Epoch 85 \t Batch 260 \t Training Loss: 31.717202927516055\n",
      "Epoch 85 \t Batch 280 \t Training Loss: 31.75400004386902\n",
      "Epoch 85 \t Batch 300 \t Training Loss: 31.803286005655924\n",
      "Epoch 85 \t Batch 320 \t Training Loss: 31.819827204942705\n",
      "Epoch 85 \t Batch 340 \t Training Loss: 31.869997636009664\n",
      "Epoch 85 \t Batch 360 \t Training Loss: 31.823642683029174\n",
      "Epoch 85 \t Batch 380 \t Training Loss: 31.811890205584074\n",
      "Epoch 85 \t Batch 400 \t Training Loss: 31.79514627933502\n",
      "Epoch 85 \t Batch 420 \t Training Loss: 31.813530227116175\n",
      "Epoch 85 \t Batch 440 \t Training Loss: 31.8055852023038\n",
      "Epoch 85 \t Batch 460 \t Training Loss: 31.877150004843006\n",
      "Epoch 85 \t Batch 480 \t Training Loss: 31.9117893576622\n",
      "Epoch 85 \t Batch 500 \t Training Loss: 31.946749492645264\n",
      "Epoch 85 \t Batch 520 \t Training Loss: 32.00217186487638\n",
      "Epoch 85 \t Batch 540 \t Training Loss: 32.02061568012944\n",
      "Epoch 85 \t Batch 560 \t Training Loss: 32.01919920785087\n",
      "Epoch 85 \t Batch 580 \t Training Loss: 32.04126844406128\n",
      "Epoch 85 \t Batch 600 \t Training Loss: 32.06362387975057\n",
      "Epoch 85 \t Batch 620 \t Training Loss: 32.06087651714202\n",
      "Epoch 85 \t Batch 640 \t Training Loss: 32.04615622162819\n",
      "Epoch 85 \t Batch 660 \t Training Loss: 32.055169481219664\n",
      "Epoch 85 \t Batch 680 \t Training Loss: 32.049020820505476\n",
      "Epoch 85 \t Batch 700 \t Training Loss: 32.050805168151854\n",
      "Epoch 85 \t Batch 720 \t Training Loss: 32.06780546771155\n",
      "Epoch 85 \t Batch 740 \t Training Loss: 32.08741453789376\n",
      "Epoch 85 \t Batch 760 \t Training Loss: 32.11207276896427\n",
      "Epoch 85 \t Batch 780 \t Training Loss: 32.143650084275464\n",
      "Epoch 85 \t Batch 800 \t Training Loss: 32.154147984981535\n",
      "Epoch 85 \t Batch 820 \t Training Loss: 32.16049911219899\n",
      "Epoch 85 \t Batch 840 \t Training Loss: 32.167616233371554\n",
      "Epoch 85 \t Batch 860 \t Training Loss: 32.183929161692774\n",
      "Epoch 85 \t Batch 880 \t Training Loss: 32.238762482729825\n",
      "Epoch 85 \t Batch 900 \t Training Loss: 32.25423835754395\n",
      "Epoch 85 \t Batch 20 \t Validation Loss: 8.989356422424317\n",
      "Epoch 85 \t Batch 40 \t Validation Loss: 13.225258803367614\n",
      "Epoch 85 \t Batch 60 \t Validation Loss: 12.4839035431544\n",
      "Epoch 85 \t Batch 80 \t Validation Loss: 13.864703625440598\n",
      "Epoch 85 \t Batch 100 \t Validation Loss: 17.300969309806824\n",
      "Epoch 85 \t Batch 120 \t Validation Loss: 19.96785018046697\n",
      "Epoch 85 \t Batch 140 \t Validation Loss: 21.42152361529214\n",
      "Epoch 85 \t Batch 160 \t Validation Loss: 24.084846690297127\n",
      "Epoch 85 \t Batch 180 \t Validation Loss: 28.18568553659651\n",
      "Epoch 85 \t Batch 200 \t Validation Loss: 30.31447101354599\n",
      "Epoch 85 \t Batch 220 \t Validation Loss: 32.11559037078511\n",
      "Epoch 85 \t Batch 240 \t Validation Loss: 32.93006470799446\n",
      "Epoch 85 \t Batch 260 \t Validation Loss: 35.528424859046936\n",
      "Epoch 85 \t Batch 280 \t Validation Loss: 36.98703202690397\n",
      "Epoch 85 \t Batch 300 \t Validation Loss: 38.158726007143656\n",
      "Epoch 85 \t Batch 320 \t Validation Loss: 38.79022255092859\n",
      "Epoch 85 \t Batch 340 \t Validation Loss: 38.86542957670548\n",
      "Epoch 85 \t Batch 360 \t Validation Loss: 38.795796644687655\n",
      "Epoch 85 \t Batch 380 \t Validation Loss: 39.28315558308049\n",
      "Epoch 85 \t Batch 400 \t Validation Loss: 39.11977938771248\n",
      "Epoch 85 \t Batch 420 \t Validation Loss: 39.27019093490782\n",
      "Epoch 85 \t Batch 440 \t Validation Loss: 39.18440553166649\n",
      "Epoch 85 \t Batch 460 \t Validation Loss: 39.608801502766816\n",
      "Epoch 85 \t Batch 480 \t Validation Loss: 40.21118311583996\n",
      "Epoch 85 \t Batch 500 \t Validation Loss: 39.978102704048155\n",
      "Epoch 85 \t Batch 520 \t Validation Loss: 39.98671546440858\n",
      "Epoch 85 \t Batch 540 \t Validation Loss: 39.79274500829202\n",
      "Epoch 85 \t Batch 560 \t Validation Loss: 39.69048287272453\n",
      "Epoch 85 \t Batch 580 \t Validation Loss: 39.5429442512578\n",
      "Epoch 85 \t Batch 600 \t Validation Loss: 39.791144467194876\n",
      "Epoch 85 Training Loss: 32.273132590457166 Validation Loss: 40.53034896122945\n",
      "Epoch 85 completed\n",
      "Epoch 86 \t Batch 20 \t Training Loss: 31.933504009246825\n",
      "Epoch 86 \t Batch 40 \t Training Loss: 31.432748079299927\n",
      "Epoch 86 \t Batch 60 \t Training Loss: 31.310346412658692\n",
      "Epoch 86 \t Batch 80 \t Training Loss: 31.32522256374359\n",
      "Epoch 86 \t Batch 100 \t Training Loss: 31.21951328277588\n",
      "Epoch 86 \t Batch 120 \t Training Loss: 31.291093762715658\n",
      "Epoch 86 \t Batch 140 \t Training Loss: 31.276269572121755\n",
      "Epoch 86 \t Batch 160 \t Training Loss: 31.335705745220185\n",
      "Epoch 86 \t Batch 180 \t Training Loss: 31.432587496439616\n",
      "Epoch 86 \t Batch 200 \t Training Loss: 31.469084339141844\n",
      "Epoch 86 \t Batch 220 \t Training Loss: 31.461974542791193\n",
      "Epoch 86 \t Batch 240 \t Training Loss: 31.48997449874878\n",
      "Epoch 86 \t Batch 260 \t Training Loss: 31.502447685828574\n",
      "Epoch 86 \t Batch 280 \t Training Loss: 31.528188596452985\n",
      "Epoch 86 \t Batch 300 \t Training Loss: 31.523651472727458\n",
      "Epoch 86 \t Batch 320 \t Training Loss: 31.543505454063414\n",
      "Epoch 86 \t Batch 340 \t Training Loss: 31.614061737060545\n",
      "Epoch 86 \t Batch 360 \t Training Loss: 31.694254525502522\n",
      "Epoch 86 \t Batch 380 \t Training Loss: 31.708441643965873\n",
      "Epoch 86 \t Batch 400 \t Training Loss: 31.704144115448\n",
      "Epoch 86 \t Batch 420 \t Training Loss: 31.741768682570683\n",
      "Epoch 86 \t Batch 440 \t Training Loss: 31.765267684242943\n",
      "Epoch 86 \t Batch 460 \t Training Loss: 31.790995000756304\n",
      "Epoch 86 \t Batch 480 \t Training Loss: 31.78548666636149\n",
      "Epoch 86 \t Batch 500 \t Training Loss: 31.78050611114502\n",
      "Epoch 86 \t Batch 520 \t Training Loss: 31.79306223942683\n",
      "Epoch 86 \t Batch 540 \t Training Loss: 31.808229647742376\n",
      "Epoch 86 \t Batch 560 \t Training Loss: 31.819592159135002\n",
      "Epoch 86 \t Batch 580 \t Training Loss: 31.82386940265524\n",
      "Epoch 86 \t Batch 600 \t Training Loss: 31.808799715042113\n",
      "Epoch 86 \t Batch 620 \t Training Loss: 31.861984483657345\n",
      "Epoch 86 \t Batch 640 \t Training Loss: 31.924201861023903\n",
      "Epoch 86 \t Batch 660 \t Training Loss: 31.954961655356666\n",
      "Epoch 86 \t Batch 680 \t Training Loss: 31.97605549587923\n",
      "Epoch 86 \t Batch 700 \t Training Loss: 31.989992016383578\n",
      "Epoch 86 \t Batch 720 \t Training Loss: 31.997095166312324\n",
      "Epoch 86 \t Batch 740 \t Training Loss: 31.983423583571977\n",
      "Epoch 86 \t Batch 760 \t Training Loss: 31.982753964474327\n",
      "Epoch 86 \t Batch 780 \t Training Loss: 31.978185741717997\n",
      "Epoch 86 \t Batch 800 \t Training Loss: 31.997652068138123\n",
      "Epoch 86 \t Batch 820 \t Training Loss: 32.04133460114642\n",
      "Epoch 86 \t Batch 840 \t Training Loss: 32.052593394688195\n",
      "Epoch 86 \t Batch 860 \t Training Loss: 32.098216518135956\n",
      "Epoch 86 \t Batch 880 \t Training Loss: 32.11527845209295\n",
      "Epoch 86 \t Batch 900 \t Training Loss: 32.12059758080377\n",
      "Epoch 86 \t Batch 20 \t Validation Loss: 10.405794286727906\n",
      "Epoch 86 \t Batch 40 \t Validation Loss: 14.839302611351012\n",
      "Epoch 86 \t Batch 60 \t Validation Loss: 13.733264406522116\n",
      "Epoch 86 \t Batch 80 \t Validation Loss: 14.952966916561127\n",
      "Epoch 86 \t Batch 100 \t Validation Loss: 18.39115469932556\n",
      "Epoch 86 \t Batch 120 \t Validation Loss: 20.797585606575012\n",
      "Epoch 86 \t Batch 140 \t Validation Loss: 22.165429162979127\n",
      "Epoch 86 \t Batch 160 \t Validation Loss: 24.890632849931716\n",
      "Epoch 86 \t Batch 180 \t Validation Loss: 28.35858221054077\n",
      "Epoch 86 \t Batch 200 \t Validation Loss: 30.495720148086548\n",
      "Epoch 86 \t Batch 220 \t Validation Loss: 31.988164147463714\n",
      "Epoch 86 \t Batch 240 \t Validation Loss: 32.6206774075826\n",
      "Epoch 86 \t Batch 260 \t Validation Loss: 35.078790657336896\n",
      "Epoch 86 \t Batch 280 \t Validation Loss: 36.46531647273472\n",
      "Epoch 86 \t Batch 300 \t Validation Loss: 37.44442638397217\n",
      "Epoch 86 \t Batch 320 \t Validation Loss: 37.99072803556919\n",
      "Epoch 86 \t Batch 340 \t Validation Loss: 38.18228171011981\n",
      "Epoch 86 \t Batch 360 \t Validation Loss: 38.214741338623895\n",
      "Epoch 86 \t Batch 380 \t Validation Loss: 38.87224377832915\n",
      "Epoch 86 \t Batch 400 \t Validation Loss: 38.938517515659335\n",
      "Epoch 86 \t Batch 420 \t Validation Loss: 39.250174978801184\n",
      "Epoch 86 \t Batch 440 \t Validation Loss: 39.47108835957267\n",
      "Epoch 86 \t Batch 460 \t Validation Loss: 40.26907669357632\n",
      "Epoch 86 \t Batch 480 \t Validation Loss: 40.921516511837645\n",
      "Epoch 86 \t Batch 500 \t Validation Loss: 40.76792543983459\n",
      "Epoch 86 \t Batch 520 \t Validation Loss: 41.23129642743331\n",
      "Epoch 86 \t Batch 540 \t Validation Loss: 41.127493870699844\n",
      "Epoch 86 \t Batch 560 \t Validation Loss: 41.09866622686386\n",
      "Epoch 86 \t Batch 580 \t Validation Loss: 40.973733745772265\n",
      "Epoch 86 \t Batch 600 \t Validation Loss: 41.304309242566426\n",
      "Epoch 86 Training Loss: 32.10776016002393 Validation Loss: 42.05232745796055\n",
      "Epoch 86 completed\n",
      "Epoch 87 \t Batch 20 \t Training Loss: 30.67524471282959\n",
      "Epoch 87 \t Batch 40 \t Training Loss: 31.40856957435608\n",
      "Epoch 87 \t Batch 60 \t Training Loss: 31.32538563410441\n",
      "Epoch 87 \t Batch 80 \t Training Loss: 31.21954083442688\n",
      "Epoch 87 \t Batch 100 \t Training Loss: 31.37052242279053\n",
      "Epoch 87 \t Batch 120 \t Training Loss: 31.36459280649821\n",
      "Epoch 87 \t Batch 140 \t Training Loss: 31.359995460510255\n",
      "Epoch 87 \t Batch 160 \t Training Loss: 31.29392340183258\n",
      "Epoch 87 \t Batch 180 \t Training Loss: 31.23102816475762\n",
      "Epoch 87 \t Batch 200 \t Training Loss: 31.335155057907105\n",
      "Epoch 87 \t Batch 220 \t Training Loss: 31.34185724258423\n",
      "Epoch 87 \t Batch 240 \t Training Loss: 31.274742499987283\n",
      "Epoch 87 \t Batch 260 \t Training Loss: 31.254461530538705\n",
      "Epoch 87 \t Batch 280 \t Training Loss: 31.241603517532347\n",
      "Epoch 87 \t Batch 300 \t Training Loss: 31.24097319920858\n",
      "Epoch 87 \t Batch 320 \t Training Loss: 31.266129636764525\n",
      "Epoch 87 \t Batch 340 \t Training Loss: 31.2922152968014\n",
      "Epoch 87 \t Batch 360 \t Training Loss: 31.34209164513482\n",
      "Epoch 87 \t Batch 380 \t Training Loss: 31.354831073158667\n",
      "Epoch 87 \t Batch 400 \t Training Loss: 31.413213710784913\n",
      "Epoch 87 \t Batch 420 \t Training Loss: 31.452990572793144\n",
      "Epoch 87 \t Batch 440 \t Training Loss: 31.479613572900945\n",
      "Epoch 87 \t Batch 460 \t Training Loss: 31.499050969662875\n",
      "Epoch 87 \t Batch 480 \t Training Loss: 31.593297378222147\n",
      "Epoch 87 \t Batch 500 \t Training Loss: 31.593431560516358\n",
      "Epoch 87 \t Batch 520 \t Training Loss: 31.602608585357665\n",
      "Epoch 87 \t Batch 540 \t Training Loss: 31.626911813241463\n",
      "Epoch 87 \t Batch 560 \t Training Loss: 31.637023643084934\n",
      "Epoch 87 \t Batch 580 \t Training Loss: 31.663889565961114\n",
      "Epoch 87 \t Batch 600 \t Training Loss: 31.70235940615336\n",
      "Epoch 87 \t Batch 620 \t Training Loss: 31.704928595019926\n",
      "Epoch 87 \t Batch 640 \t Training Loss: 31.73806856572628\n",
      "Epoch 87 \t Batch 660 \t Training Loss: 31.737048747322778\n",
      "Epoch 87 \t Batch 680 \t Training Loss: 31.75431893012103\n",
      "Epoch 87 \t Batch 700 \t Training Loss: 31.761874452318462\n",
      "Epoch 87 \t Batch 720 \t Training Loss: 31.790597910351224\n",
      "Epoch 87 \t Batch 740 \t Training Loss: 31.79712950216757\n",
      "Epoch 87 \t Batch 760 \t Training Loss: 31.80612355784366\n",
      "Epoch 87 \t Batch 780 \t Training Loss: 31.808406213613658\n",
      "Epoch 87 \t Batch 800 \t Training Loss: 31.827994024753572\n",
      "Epoch 87 \t Batch 820 \t Training Loss: 31.85161413332311\n",
      "Epoch 87 \t Batch 840 \t Training Loss: 31.865513740267073\n",
      "Epoch 87 \t Batch 860 \t Training Loss: 31.870799738861795\n",
      "Epoch 87 \t Batch 880 \t Training Loss: 31.886249121752652\n",
      "Epoch 87 \t Batch 900 \t Training Loss: 31.905213873121472\n",
      "Epoch 87 \t Batch 20 \t Validation Loss: 7.5203649520874025\n",
      "Epoch 87 \t Batch 40 \t Validation Loss: 10.588188707828522\n",
      "Epoch 87 \t Batch 60 \t Validation Loss: 10.481431770324708\n",
      "Epoch 87 \t Batch 80 \t Validation Loss: 11.987711215019226\n",
      "Epoch 87 \t Batch 100 \t Validation Loss: 15.550364112854004\n",
      "Epoch 87 \t Batch 120 \t Validation Loss: 18.113145351409912\n",
      "Epoch 87 \t Batch 140 \t Validation Loss: 19.662822042192733\n",
      "Epoch 87 \t Batch 160 \t Validation Loss: 22.672584092617036\n",
      "Epoch 87 \t Batch 180 \t Validation Loss: 26.77086936102973\n",
      "Epoch 87 \t Batch 200 \t Validation Loss: 28.993218870162963\n",
      "Epoch 87 \t Batch 220 \t Validation Loss: 30.818530039353803\n",
      "Epoch 87 \t Batch 240 \t Validation Loss: 31.727400954564413\n",
      "Epoch 87 \t Batch 260 \t Validation Loss: 34.343266362410326\n",
      "Epoch 87 \t Batch 280 \t Validation Loss: 35.865804300989424\n",
      "Epoch 87 \t Batch 300 \t Validation Loss: 37.00987623532613\n",
      "Epoch 87 \t Batch 320 \t Validation Loss: 37.67312944829464\n",
      "Epoch 87 \t Batch 340 \t Validation Loss: 37.84445288321551\n",
      "Epoch 87 \t Batch 360 \t Validation Loss: 37.83623866240183\n",
      "Epoch 87 \t Batch 380 \t Validation Loss: 38.41510393995988\n",
      "Epoch 87 \t Batch 400 \t Validation Loss: 38.35045095682144\n",
      "Epoch 87 \t Batch 420 \t Validation Loss: 38.61049677303859\n",
      "Epoch 87 \t Batch 440 \t Validation Loss: 38.68525913195177\n",
      "Epoch 87 \t Batch 460 \t Validation Loss: 39.28535418925078\n",
      "Epoch 87 \t Batch 480 \t Validation Loss: 39.90892002383868\n",
      "Epoch 87 \t Batch 500 \t Validation Loss: 39.75188648414612\n",
      "Epoch 87 \t Batch 520 \t Validation Loss: 39.96565521130195\n",
      "Epoch 87 \t Batch 540 \t Validation Loss: 39.80683869079307\n",
      "Epoch 87 \t Batch 560 \t Validation Loss: 39.747180378437044\n",
      "Epoch 87 \t Batch 580 \t Validation Loss: 39.598272196999915\n",
      "Epoch 87 \t Batch 600 \t Validation Loss: 39.887050639788306\n",
      "Epoch 87 Training Loss: 31.91821369766019 Validation Loss: 40.665356953422744\n",
      "Epoch 87 completed\n",
      "Epoch 88 \t Batch 20 \t Training Loss: 31.700442504882812\n",
      "Epoch 88 \t Batch 40 \t Training Loss: 31.39500331878662\n",
      "Epoch 88 \t Batch 60 \t Training Loss: 31.522381337483726\n",
      "Epoch 88 \t Batch 80 \t Training Loss: 31.516493153572082\n",
      "Epoch 88 \t Batch 100 \t Training Loss: 31.377222805023194\n",
      "Epoch 88 \t Batch 120 \t Training Loss: 31.394896094004313\n",
      "Epoch 88 \t Batch 140 \t Training Loss: 31.358090809413365\n",
      "Epoch 88 \t Batch 160 \t Training Loss: 31.41488460302353\n",
      "Epoch 88 \t Batch 180 \t Training Loss: 31.420479520161948\n",
      "Epoch 88 \t Batch 200 \t Training Loss: 31.38698781967163\n",
      "Epoch 88 \t Batch 220 \t Training Loss: 31.37692025791515\n",
      "Epoch 88 \t Batch 240 \t Training Loss: 31.363165171941123\n",
      "Epoch 88 \t Batch 260 \t Training Loss: 31.366550687643198\n",
      "Epoch 88 \t Batch 280 \t Training Loss: 31.33654867580959\n",
      "Epoch 88 \t Batch 300 \t Training Loss: 31.33052748998006\n",
      "Epoch 88 \t Batch 320 \t Training Loss: 31.380939370393754\n",
      "Epoch 88 \t Batch 340 \t Training Loss: 31.398983972212847\n",
      "Epoch 88 \t Batch 360 \t Training Loss: 31.4356634510888\n",
      "Epoch 88 \t Batch 380 \t Training Loss: 31.45322202381335\n",
      "Epoch 88 \t Batch 400 \t Training Loss: 31.417400002479553\n",
      "Epoch 88 \t Batch 420 \t Training Loss: 31.417701167152043\n",
      "Epoch 88 \t Batch 440 \t Training Loss: 31.45129321271723\n",
      "Epoch 88 \t Batch 460 \t Training Loss: 31.428660989844282\n",
      "Epoch 88 \t Batch 480 \t Training Loss: 31.45033065080643\n",
      "Epoch 88 \t Batch 500 \t Training Loss: 31.468930152893066\n",
      "Epoch 88 \t Batch 520 \t Training Loss: 31.43912763595581\n",
      "Epoch 88 \t Batch 540 \t Training Loss: 31.449531170173927\n",
      "Epoch 88 \t Batch 560 \t Training Loss: 31.455185052326748\n",
      "Epoch 88 \t Batch 580 \t Training Loss: 31.481046091277022\n",
      "Epoch 88 \t Batch 600 \t Training Loss: 31.533516359329223\n",
      "Epoch 88 \t Batch 620 \t Training Loss: 31.589366814397998\n",
      "Epoch 88 \t Batch 640 \t Training Loss: 31.59392329752445\n",
      "Epoch 88 \t Batch 660 \t Training Loss: 31.60185693105062\n",
      "Epoch 88 \t Batch 680 \t Training Loss: 31.635018668455235\n",
      "Epoch 88 \t Batch 700 \t Training Loss: 31.646101561955042\n",
      "Epoch 88 \t Batch 720 \t Training Loss: 31.68362216419644\n",
      "Epoch 88 \t Batch 740 \t Training Loss: 31.68890881667266\n",
      "Epoch 88 \t Batch 760 \t Training Loss: 31.682540557259006\n",
      "Epoch 88 \t Batch 780 \t Training Loss: 31.679003720405774\n",
      "Epoch 88 \t Batch 800 \t Training Loss: 31.701933057308196\n",
      "Epoch 88 \t Batch 820 \t Training Loss: 31.734487977842004\n",
      "Epoch 88 \t Batch 840 \t Training Loss: 31.75883146467663\n",
      "Epoch 88 \t Batch 860 \t Training Loss: 31.7483130543731\n",
      "Epoch 88 \t Batch 880 \t Training Loss: 31.75423928607594\n",
      "Epoch 88 \t Batch 900 \t Training Loss: 31.760537170834013\n",
      "Epoch 88 \t Batch 20 \t Validation Loss: 9.986643266677856\n",
      "Epoch 88 \t Batch 40 \t Validation Loss: 13.728646326065064\n",
      "Epoch 88 \t Batch 60 \t Validation Loss: 13.217242042223612\n",
      "Epoch 88 \t Batch 80 \t Validation Loss: 14.454356694221497\n",
      "Epoch 88 \t Batch 100 \t Validation Loss: 18.063180751800537\n",
      "Epoch 88 \t Batch 120 \t Validation Loss: 20.715423393249512\n",
      "Epoch 88 \t Batch 140 \t Validation Loss: 22.09633116040911\n",
      "Epoch 88 \t Batch 160 \t Validation Loss: 24.60452780723572\n",
      "Epoch 88 \t Batch 180 \t Validation Loss: 28.10765464040968\n",
      "Epoch 88 \t Batch 200 \t Validation Loss: 30.10468062877655\n",
      "Epoch 88 \t Batch 220 \t Validation Loss: 31.64689612388611\n",
      "Epoch 88 \t Batch 240 \t Validation Loss: 32.27703436215719\n",
      "Epoch 88 \t Batch 260 \t Validation Loss: 34.71869605504549\n",
      "Epoch 88 \t Batch 280 \t Validation Loss: 36.0650496006012\n",
      "Epoch 88 \t Batch 300 \t Validation Loss: 37.02645678202311\n",
      "Epoch 88 \t Batch 320 \t Validation Loss: 37.5984421312809\n",
      "Epoch 88 \t Batch 340 \t Validation Loss: 37.72030811870799\n",
      "Epoch 88 \t Batch 360 \t Validation Loss: 37.64174801508586\n",
      "Epoch 88 \t Batch 380 \t Validation Loss: 38.15648347955001\n",
      "Epoch 88 \t Batch 400 \t Validation Loss: 38.042286124229435\n",
      "Epoch 88 \t Batch 420 \t Validation Loss: 38.25366211845761\n",
      "Epoch 88 \t Batch 440 \t Validation Loss: 38.24957876638933\n",
      "Epoch 88 \t Batch 460 \t Validation Loss: 38.832636207083\n",
      "Epoch 88 \t Batch 480 \t Validation Loss: 39.43785228331884\n",
      "Epoch 88 \t Batch 500 \t Validation Loss: 39.25412390136719\n",
      "Epoch 88 \t Batch 520 \t Validation Loss: 39.42236896661612\n",
      "Epoch 88 \t Batch 540 \t Validation Loss: 39.28679478256791\n",
      "Epoch 88 \t Batch 560 \t Validation Loss: 39.21977938924517\n",
      "Epoch 88 \t Batch 580 \t Validation Loss: 39.09906081824467\n",
      "Epoch 88 \t Batch 600 \t Validation Loss: 39.408406016031904\n",
      "Epoch 88 Training Loss: 31.75408848811392 Validation Loss: 40.203230387204655\n",
      "Epoch 88 completed\n",
      "Epoch 89 \t Batch 20 \t Training Loss: 30.837298583984374\n",
      "Epoch 89 \t Batch 40 \t Training Loss: 30.724176692962647\n",
      "Epoch 89 \t Batch 60 \t Training Loss: 30.553534412384032\n",
      "Epoch 89 \t Batch 80 \t Training Loss: 30.983157896995543\n",
      "Epoch 89 \t Batch 100 \t Training Loss: 31.257002716064452\n",
      "Epoch 89 \t Batch 120 \t Training Loss: 31.152111355463664\n",
      "Epoch 89 \t Batch 140 \t Training Loss: 31.08393350328718\n",
      "Epoch 89 \t Batch 160 \t Training Loss: 31.31447606086731\n",
      "Epoch 89 \t Batch 180 \t Training Loss: 31.24436961279975\n",
      "Epoch 89 \t Batch 200 \t Training Loss: 31.213418731689455\n",
      "Epoch 89 \t Batch 220 \t Training Loss: 31.134593998302112\n",
      "Epoch 89 \t Batch 240 \t Training Loss: 31.109006547927855\n",
      "Epoch 89 \t Batch 260 \t Training Loss: 31.11977594082172\n",
      "Epoch 89 \t Batch 280 \t Training Loss: 31.16650928088597\n",
      "Epoch 89 \t Batch 300 \t Training Loss: 31.200314725240073\n",
      "Epoch 89 \t Batch 320 \t Training Loss: 31.215248733758926\n",
      "Epoch 89 \t Batch 340 \t Training Loss: 31.195905376883115\n",
      "Epoch 89 \t Batch 360 \t Training Loss: 31.21977529525757\n",
      "Epoch 89 \t Batch 380 \t Training Loss: 31.21874260651438\n",
      "Epoch 89 \t Batch 400 \t Training Loss: 31.20465020656586\n",
      "Epoch 89 \t Batch 420 \t Training Loss: 31.18086412066505\n",
      "Epoch 89 \t Batch 440 \t Training Loss: 31.20097624171864\n",
      "Epoch 89 \t Batch 460 \t Training Loss: 31.228148012575897\n",
      "Epoch 89 \t Batch 480 \t Training Loss: 31.25843616724014\n",
      "Epoch 89 \t Batch 500 \t Training Loss: 31.246287284851075\n",
      "Epoch 89 \t Batch 520 \t Training Loss: 31.263433980941773\n",
      "Epoch 89 \t Batch 540 \t Training Loss: 31.28060026168823\n",
      "Epoch 89 \t Batch 560 \t Training Loss: 31.337073251179287\n",
      "Epoch 89 \t Batch 580 \t Training Loss: 31.322221167334195\n",
      "Epoch 89 \t Batch 600 \t Training Loss: 31.360737806955974\n",
      "Epoch 89 \t Batch 620 \t Training Loss: 31.37307171052502\n",
      "Epoch 89 \t Batch 640 \t Training Loss: 31.369406923651695\n",
      "Epoch 89 \t Batch 660 \t Training Loss: 31.34176862601078\n",
      "Epoch 89 \t Batch 680 \t Training Loss: 31.366574971816117\n",
      "Epoch 89 \t Batch 700 \t Training Loss: 31.382798502785818\n",
      "Epoch 89 \t Batch 720 \t Training Loss: 31.400110949410333\n",
      "Epoch 89 \t Batch 740 \t Training Loss: 31.40183641072866\n",
      "Epoch 89 \t Batch 760 \t Training Loss: 31.39018757217809\n",
      "Epoch 89 \t Batch 780 \t Training Loss: 31.402474799522988\n",
      "Epoch 89 \t Batch 800 \t Training Loss: 31.44008950471878\n",
      "Epoch 89 \t Batch 820 \t Training Loss: 31.450135231018066\n",
      "Epoch 89 \t Batch 840 \t Training Loss: 31.449837907155356\n",
      "Epoch 89 \t Batch 860 \t Training Loss: 31.453029610389887\n",
      "Epoch 89 \t Batch 880 \t Training Loss: 31.44636737433347\n",
      "Epoch 89 \t Batch 900 \t Training Loss: 31.45980967627631\n",
      "Epoch 89 \t Batch 20 \t Validation Loss: 12.350798535346986\n",
      "Epoch 89 \t Batch 40 \t Validation Loss: 16.75401473045349\n",
      "Epoch 89 \t Batch 60 \t Validation Loss: 15.814269590377808\n",
      "Epoch 89 \t Batch 80 \t Validation Loss: 16.957329654693602\n",
      "Epoch 89 \t Batch 100 \t Validation Loss: 19.82511650085449\n",
      "Epoch 89 \t Batch 120 \t Validation Loss: 22.103175115585326\n",
      "Epoch 89 \t Batch 140 \t Validation Loss: 23.17782882962908\n",
      "Epoch 89 \t Batch 160 \t Validation Loss: 25.77444063425064\n",
      "Epoch 89 \t Batch 180 \t Validation Loss: 29.908953385882906\n",
      "Epoch 89 \t Batch 200 \t Validation Loss: 32.05777822971344\n",
      "Epoch 89 \t Batch 220 \t Validation Loss: 33.83667078451677\n",
      "Epoch 89 \t Batch 240 \t Validation Loss: 34.59519296487172\n",
      "Epoch 89 \t Batch 260 \t Validation Loss: 37.200496842310976\n",
      "Epoch 89 \t Batch 280 \t Validation Loss: 38.64021347931453\n",
      "Epoch 89 \t Batch 300 \t Validation Loss: 39.78717825889587\n",
      "Epoch 89 \t Batch 320 \t Validation Loss: 40.38545625507832\n",
      "Epoch 89 \t Batch 340 \t Validation Loss: 40.41232061666601\n",
      "Epoch 89 \t Batch 360 \t Validation Loss: 40.30515328778161\n",
      "Epoch 89 \t Batch 380 \t Validation Loss: 40.76203061404981\n",
      "Epoch 89 \t Batch 400 \t Validation Loss: 40.497545402050015\n",
      "Epoch 89 \t Batch 420 \t Validation Loss: 40.61858006886074\n",
      "Epoch 89 \t Batch 440 \t Validation Loss: 40.49277357188138\n",
      "Epoch 89 \t Batch 460 \t Validation Loss: 40.92466402053833\n",
      "Epoch 89 \t Batch 480 \t Validation Loss: 41.47960897286733\n",
      "Epoch 89 \t Batch 500 \t Validation Loss: 41.248937519073486\n",
      "Epoch 89 \t Batch 520 \t Validation Loss: 41.33683837377108\n",
      "Epoch 89 \t Batch 540 \t Validation Loss: 41.11112718935366\n",
      "Epoch 89 \t Batch 560 \t Validation Loss: 41.00354518890381\n",
      "Epoch 89 \t Batch 580 \t Validation Loss: 40.849387586527854\n",
      "Epoch 89 \t Batch 600 \t Validation Loss: 41.08953228314718\n",
      "Epoch 89 Training Loss: 31.455739134660593 Validation Loss: 41.807783724425676\n",
      "Epoch 89 completed\n",
      "Epoch 90 \t Batch 20 \t Training Loss: 30.29685745239258\n",
      "Epoch 90 \t Batch 40 \t Training Loss: 30.75420699119568\n",
      "Epoch 90 \t Batch 60 \t Training Loss: 30.859290885925294\n",
      "Epoch 90 \t Batch 80 \t Training Loss: 30.477419304847718\n",
      "Epoch 90 \t Batch 100 \t Training Loss: 30.31926025390625\n",
      "Epoch 90 \t Batch 120 \t Training Loss: 30.288885100682577\n",
      "Epoch 90 \t Batch 140 \t Training Loss: 30.352936962672644\n",
      "Epoch 90 \t Batch 160 \t Training Loss: 30.541831982135772\n",
      "Epoch 90 \t Batch 180 \t Training Loss: 30.580274730258516\n",
      "Epoch 90 \t Batch 200 \t Training Loss: 30.60122688293457\n",
      "Epoch 90 \t Batch 220 \t Training Loss: 30.637600421905518\n",
      "Epoch 90 \t Batch 240 \t Training Loss: 30.668402767181398\n",
      "Epoch 90 \t Batch 260 \t Training Loss: 30.68089386133047\n",
      "Epoch 90 \t Batch 280 \t Training Loss: 30.72034798349653\n",
      "Epoch 90 \t Batch 300 \t Training Loss: 30.660734723409018\n",
      "Epoch 90 \t Batch 320 \t Training Loss: 30.711867076158523\n",
      "Epoch 90 \t Batch 340 \t Training Loss: 30.706303949917064\n",
      "Epoch 90 \t Batch 360 \t Training Loss: 30.757163974973892\n",
      "Epoch 90 \t Batch 380 \t Training Loss: 30.7347758443732\n",
      "Epoch 90 \t Batch 400 \t Training Loss: 30.747203359603883\n",
      "Epoch 90 \t Batch 420 \t Training Loss: 30.730371670495895\n",
      "Epoch 90 \t Batch 440 \t Training Loss: 30.73642432039434\n",
      "Epoch 90 \t Batch 460 \t Training Loss: 30.767489578412928\n",
      "Epoch 90 \t Batch 480 \t Training Loss: 30.80323230822881\n",
      "Epoch 90 \t Batch 500 \t Training Loss: 30.820854747772216\n",
      "Epoch 90 \t Batch 520 \t Training Loss: 30.82338936145489\n",
      "Epoch 90 \t Batch 540 \t Training Loss: 30.867324189786558\n",
      "Epoch 90 \t Batch 560 \t Training Loss: 30.87683113643101\n",
      "Epoch 90 \t Batch 580 \t Training Loss: 30.91574637643222\n",
      "Epoch 90 \t Batch 600 \t Training Loss: 30.970825045903524\n",
      "Epoch 90 \t Batch 620 \t Training Loss: 30.970840226450274\n",
      "Epoch 90 \t Batch 640 \t Training Loss: 30.982457062602045\n",
      "Epoch 90 \t Batch 660 \t Training Loss: 30.983398616675174\n",
      "Epoch 90 \t Batch 680 \t Training Loss: 31.00508133102866\n",
      "Epoch 90 \t Batch 700 \t Training Loss: 31.03408463886806\n",
      "Epoch 90 \t Batch 720 \t Training Loss: 31.04147427611881\n",
      "Epoch 90 \t Batch 740 \t Training Loss: 31.046432343044795\n",
      "Epoch 90 \t Batch 760 \t Training Loss: 31.05318801277562\n",
      "Epoch 90 \t Batch 780 \t Training Loss: 31.077398821023795\n",
      "Epoch 90 \t Batch 800 \t Training Loss: 31.080827629566194\n",
      "Epoch 90 \t Batch 820 \t Training Loss: 31.107683072439055\n",
      "Epoch 90 \t Batch 840 \t Training Loss: 31.09949995449611\n",
      "Epoch 90 \t Batch 860 \t Training Loss: 31.135785870219387\n",
      "Epoch 90 \t Batch 880 \t Training Loss: 31.164383671500467\n",
      "Epoch 90 \t Batch 900 \t Training Loss: 31.166011123657228\n",
      "Epoch 90 \t Batch 20 \t Validation Loss: 12.382093262672424\n",
      "Epoch 90 \t Batch 40 \t Validation Loss: 16.19224774837494\n",
      "Epoch 90 \t Batch 60 \t Validation Loss: 15.456720805168152\n",
      "Epoch 90 \t Batch 80 \t Validation Loss: 16.654516714811326\n",
      "Epoch 90 \t Batch 100 \t Validation Loss: 19.75640733242035\n",
      "Epoch 90 \t Batch 120 \t Validation Loss: 22.06022451321284\n",
      "Epoch 90 \t Batch 140 \t Validation Loss: 23.24477060522352\n",
      "Epoch 90 \t Batch 160 \t Validation Loss: 25.906824895739554\n",
      "Epoch 90 \t Batch 180 \t Validation Loss: 29.902863515747917\n",
      "Epoch 90 \t Batch 200 \t Validation Loss: 32.10266660928726\n",
      "Epoch 90 \t Batch 220 \t Validation Loss: 33.812880552898754\n",
      "Epoch 90 \t Batch 240 \t Validation Loss: 34.561433086792626\n",
      "Epoch 90 \t Batch 260 \t Validation Loss: 37.14843058402722\n",
      "Epoch 90 \t Batch 280 \t Validation Loss: 38.57058883224215\n",
      "Epoch 90 \t Batch 300 \t Validation Loss: 39.65956942081451\n",
      "Epoch 90 \t Batch 320 \t Validation Loss: 40.243015609681606\n",
      "Epoch 90 \t Batch 340 \t Validation Loss: 40.31567006812376\n",
      "Epoch 90 \t Batch 360 \t Validation Loss: 40.280951631069186\n",
      "Epoch 90 \t Batch 380 \t Validation Loss: 40.79038759909178\n",
      "Epoch 90 \t Batch 400 \t Validation Loss: 40.67075353980064\n",
      "Epoch 90 \t Batch 420 \t Validation Loss: 40.89150513580867\n",
      "Epoch 90 \t Batch 440 \t Validation Loss: 40.88578501506285\n",
      "Epoch 90 \t Batch 460 \t Validation Loss: 41.39790820142497\n",
      "Epoch 90 \t Batch 480 \t Validation Loss: 41.9713324179252\n",
      "Epoch 90 \t Batch 500 \t Validation Loss: 41.754815970420836\n",
      "Epoch 90 \t Batch 520 \t Validation Loss: 41.941822271163645\n",
      "Epoch 90 \t Batch 540 \t Validation Loss: 41.796706880463496\n",
      "Epoch 90 \t Batch 560 \t Validation Loss: 41.7633295084749\n",
      "Epoch 90 \t Batch 580 \t Validation Loss: 41.658548587766184\n",
      "Epoch 90 \t Batch 600 \t Validation Loss: 41.93533902724584\n",
      "Epoch 90 Training Loss: 31.18746731715478 Validation Loss: 42.68574474926119\n",
      "Epoch 90 completed\n",
      "Epoch 91 \t Batch 20 \t Training Loss: 30.223623943328857\n",
      "Epoch 91 \t Batch 40 \t Training Loss: 30.02633981704712\n",
      "Epoch 91 \t Batch 60 \t Training Loss: 30.072110652923584\n",
      "Epoch 91 \t Batch 80 \t Training Loss: 30.139817452430727\n",
      "Epoch 91 \t Batch 100 \t Training Loss: 30.262690925598143\n",
      "Epoch 91 \t Batch 120 \t Training Loss: 30.314610115687053\n",
      "Epoch 91 \t Batch 140 \t Training Loss: 30.459415803636823\n",
      "Epoch 91 \t Batch 160 \t Training Loss: 30.484784257411956\n",
      "Epoch 91 \t Batch 180 \t Training Loss: 30.502064609527586\n",
      "Epoch 91 \t Batch 200 \t Training Loss: 30.519929580688476\n",
      "Epoch 91 \t Batch 220 \t Training Loss: 30.544874173944645\n",
      "Epoch 91 \t Batch 240 \t Training Loss: 30.518857288360596\n",
      "Epoch 91 \t Batch 260 \t Training Loss: 30.560032448401817\n",
      "Epoch 91 \t Batch 280 \t Training Loss: 30.61687444959368\n",
      "Epoch 91 \t Batch 300 \t Training Loss: 30.618159205118815\n",
      "Epoch 91 \t Batch 320 \t Training Loss: 30.659429037570952\n",
      "Epoch 91 \t Batch 340 \t Training Loss: 30.667413279589486\n",
      "Epoch 91 \t Batch 360 \t Training Loss: 30.67718941900465\n",
      "Epoch 91 \t Batch 380 \t Training Loss: 30.695171311027124\n",
      "Epoch 91 \t Batch 400 \t Training Loss: 30.697012972831725\n",
      "Epoch 91 \t Batch 420 \t Training Loss: 30.68444407780965\n",
      "Epoch 91 \t Batch 440 \t Training Loss: 30.657465362548827\n",
      "Epoch 91 \t Batch 460 \t Training Loss: 30.672493076324464\n",
      "Epoch 91 \t Batch 480 \t Training Loss: 30.6901087085406\n",
      "Epoch 91 \t Batch 500 \t Training Loss: 30.691427112579344\n",
      "Epoch 91 \t Batch 520 \t Training Loss: 30.71585121888381\n",
      "Epoch 91 \t Batch 540 \t Training Loss: 30.766536454801205\n",
      "Epoch 91 \t Batch 560 \t Training Loss: 30.78022470474243\n",
      "Epoch 91 \t Batch 580 \t Training Loss: 30.7795392036438\n",
      "Epoch 91 \t Batch 600 \t Training Loss: 30.803838583628337\n",
      "Epoch 91 \t Batch 620 \t Training Loss: 30.787416405831614\n",
      "Epoch 91 \t Batch 640 \t Training Loss: 30.79378513395786\n",
      "Epoch 91 \t Batch 660 \t Training Loss: 30.788169944647585\n",
      "Epoch 91 \t Batch 680 \t Training Loss: 30.800837598127476\n",
      "Epoch 91 \t Batch 700 \t Training Loss: 30.808841163090296\n",
      "Epoch 91 \t Batch 720 \t Training Loss: 30.81603179242876\n",
      "Epoch 91 \t Batch 740 \t Training Loss: 30.813463943069046\n",
      "Epoch 91 \t Batch 760 \t Training Loss: 30.84824488037511\n",
      "Epoch 91 \t Batch 780 \t Training Loss: 30.868686592884554\n",
      "Epoch 91 \t Batch 800 \t Training Loss: 30.889445593357085\n",
      "Epoch 91 \t Batch 820 \t Training Loss: 30.894156311779486\n",
      "Epoch 91 \t Batch 840 \t Training Loss: 30.885307695752097\n",
      "Epoch 91 \t Batch 860 \t Training Loss: 30.905710994365602\n",
      "Epoch 91 \t Batch 880 \t Training Loss: 30.928063095699656\n",
      "Epoch 91 \t Batch 900 \t Training Loss: 30.959705011579725\n",
      "Epoch 91 \t Batch 20 \t Validation Loss: 10.614165902137756\n",
      "Epoch 91 \t Batch 40 \t Validation Loss: 15.216883850097656\n",
      "Epoch 91 \t Batch 60 \t Validation Loss: 14.197364151477814\n",
      "Epoch 91 \t Batch 80 \t Validation Loss: 15.208100751042366\n",
      "Epoch 91 \t Batch 100 \t Validation Loss: 18.534204218387604\n",
      "Epoch 91 \t Batch 120 \t Validation Loss: 20.93676837484042\n",
      "Epoch 91 \t Batch 140 \t Validation Loss: 22.247166921411242\n",
      "Epoch 91 \t Batch 160 \t Validation Loss: 24.7476241722703\n",
      "Epoch 91 \t Batch 180 \t Validation Loss: 28.17951637506485\n",
      "Epoch 91 \t Batch 200 \t Validation Loss: 30.119449192285536\n",
      "Epoch 91 \t Batch 220 \t Validation Loss: 31.588273842768235\n",
      "Epoch 91 \t Batch 240 \t Validation Loss: 32.184352863828344\n",
      "Epoch 91 \t Batch 260 \t Validation Loss: 34.60714723972174\n",
      "Epoch 91 \t Batch 280 \t Validation Loss: 35.94406086461885\n",
      "Epoch 91 \t Batch 300 \t Validation Loss: 36.926839988231656\n",
      "Epoch 91 \t Batch 320 \t Validation Loss: 37.48642214909196\n",
      "Epoch 91 \t Batch 340 \t Validation Loss: 37.61739055198782\n",
      "Epoch 91 \t Batch 360 \t Validation Loss: 37.58570517235332\n",
      "Epoch 91 \t Batch 380 \t Validation Loss: 38.113771351387626\n",
      "Epoch 91 \t Batch 400 \t Validation Loss: 38.04232523739338\n",
      "Epoch 91 \t Batch 420 \t Validation Loss: 38.28658207314355\n",
      "Epoch 91 \t Batch 440 \t Validation Loss: 38.3085575087504\n",
      "Epoch 91 \t Batch 460 \t Validation Loss: 38.91107860492623\n",
      "Epoch 91 \t Batch 480 \t Validation Loss: 39.55181439568599\n",
      "Epoch 91 \t Batch 500 \t Validation Loss: 39.38821386194229\n",
      "Epoch 91 \t Batch 520 \t Validation Loss: 39.597169368542154\n",
      "Epoch 91 \t Batch 540 \t Validation Loss: 39.49360744909004\n",
      "Epoch 91 \t Batch 560 \t Validation Loss: 39.47302773211683\n",
      "Epoch 91 \t Batch 580 \t Validation Loss: 39.38991165695519\n",
      "Epoch 91 \t Batch 600 \t Validation Loss: 39.724177063703536\n",
      "Epoch 91 Training Loss: 30.979142125586257 Validation Loss: 40.501441951308934\n",
      "Epoch 91 completed\n",
      "Epoch 92 \t Batch 20 \t Training Loss: 29.878514194488524\n",
      "Epoch 92 \t Batch 40 \t Training Loss: 29.983072185516356\n",
      "Epoch 92 \t Batch 60 \t Training Loss: 29.893054262797037\n",
      "Epoch 92 \t Batch 80 \t Training Loss: 29.871176075935363\n",
      "Epoch 92 \t Batch 100 \t Training Loss: 29.960543785095215\n",
      "Epoch 92 \t Batch 120 \t Training Loss: 29.909388383229572\n",
      "Epoch 92 \t Batch 140 \t Training Loss: 29.943167999812534\n",
      "Epoch 92 \t Batch 160 \t Training Loss: 30.01315015554428\n",
      "Epoch 92 \t Batch 180 \t Training Loss: 30.035019810994466\n",
      "Epoch 92 \t Batch 200 \t Training Loss: 30.17245213508606\n",
      "Epoch 92 \t Batch 220 \t Training Loss: 30.189704608917236\n",
      "Epoch 92 \t Batch 240 \t Training Loss: 30.18453719615936\n",
      "Epoch 92 \t Batch 260 \t Training Loss: 30.183384880652795\n",
      "Epoch 92 \t Batch 280 \t Training Loss: 30.250821270261493\n",
      "Epoch 92 \t Batch 300 \t Training Loss: 30.224521993001304\n",
      "Epoch 92 \t Batch 320 \t Training Loss: 30.303912657499314\n",
      "Epoch 92 \t Batch 340 \t Training Loss: 30.33785329145544\n",
      "Epoch 92 \t Batch 360 \t Training Loss: 30.344610489739313\n",
      "Epoch 92 \t Batch 380 \t Training Loss: 30.332733184412906\n",
      "Epoch 92 \t Batch 400 \t Training Loss: 30.34502559661865\n",
      "Epoch 92 \t Batch 420 \t Training Loss: 30.3750838188898\n",
      "Epoch 92 \t Batch 440 \t Training Loss: 30.399914108623157\n",
      "Epoch 92 \t Batch 460 \t Training Loss: 30.44607179061226\n",
      "Epoch 92 \t Batch 480 \t Training Loss: 30.470766007900238\n",
      "Epoch 92 \t Batch 500 \t Training Loss: 30.49850411224365\n",
      "Epoch 92 \t Batch 520 \t Training Loss: 30.520651450523964\n",
      "Epoch 92 \t Batch 540 \t Training Loss: 30.56402017098886\n",
      "Epoch 92 \t Batch 560 \t Training Loss: 30.58874125140054\n",
      "Epoch 92 \t Batch 580 \t Training Loss: 30.59564560528459\n",
      "Epoch 92 \t Batch 600 \t Training Loss: 30.61524024963379\n",
      "Epoch 92 \t Batch 620 \t Training Loss: 30.627008216611802\n",
      "Epoch 92 \t Batch 640 \t Training Loss: 30.629503312706948\n",
      "Epoch 92 \t Batch 660 \t Training Loss: 30.638376412247165\n",
      "Epoch 92 \t Batch 680 \t Training Loss: 30.65099496560938\n",
      "Epoch 92 \t Batch 700 \t Training Loss: 30.67302019391741\n",
      "Epoch 92 \t Batch 720 \t Training Loss: 30.693831472926668\n",
      "Epoch 92 \t Batch 740 \t Training Loss: 30.676511223251755\n",
      "Epoch 92 \t Batch 760 \t Training Loss: 30.682368255916394\n",
      "Epoch 92 \t Batch 780 \t Training Loss: 30.691313017331638\n",
      "Epoch 92 \t Batch 800 \t Training Loss: 30.699905931949615\n",
      "Epoch 92 \t Batch 820 \t Training Loss: 30.713830426844154\n",
      "Epoch 92 \t Batch 840 \t Training Loss: 30.732348939350672\n",
      "Epoch 92 \t Batch 860 \t Training Loss: 30.748473899309026\n",
      "Epoch 92 \t Batch 880 \t Training Loss: 30.744897831570018\n",
      "Epoch 92 \t Batch 900 \t Training Loss: 30.743559121025932\n",
      "Epoch 92 \t Batch 20 \t Validation Loss: 10.84961440563202\n",
      "Epoch 92 \t Batch 40 \t Validation Loss: 14.665683627128601\n",
      "Epoch 92 \t Batch 60 \t Validation Loss: 14.112072889010111\n",
      "Epoch 92 \t Batch 80 \t Validation Loss: 15.357386803627014\n",
      "Epoch 92 \t Batch 100 \t Validation Loss: 18.458244686126708\n",
      "Epoch 92 \t Batch 120 \t Validation Loss: 20.92676266034444\n",
      "Epoch 92 \t Batch 140 \t Validation Loss: 22.172461605072023\n",
      "Epoch 92 \t Batch 160 \t Validation Loss: 24.53098965883255\n",
      "Epoch 92 \t Batch 180 \t Validation Loss: 27.636822350819905\n",
      "Epoch 92 \t Batch 200 \t Validation Loss: 29.501249628067015\n",
      "Epoch 92 \t Batch 220 \t Validation Loss: 30.791567091508345\n",
      "Epoch 92 \t Batch 240 \t Validation Loss: 31.316758529345194\n",
      "Epoch 92 \t Batch 260 \t Validation Loss: 33.54641722899217\n",
      "Epoch 92 \t Batch 280 \t Validation Loss: 34.823361839566914\n",
      "Epoch 92 \t Batch 300 \t Validation Loss: 35.65766850789388\n",
      "Epoch 92 \t Batch 320 \t Validation Loss: 36.10688839554787\n",
      "Epoch 92 \t Batch 340 \t Validation Loss: 36.28752038619098\n",
      "Epoch 92 \t Batch 360 \t Validation Loss: 36.30736444791158\n",
      "Epoch 92 \t Batch 380 \t Validation Loss: 36.80924987792969\n",
      "Epoch 92 \t Batch 400 \t Validation Loss: 36.83218586444855\n",
      "Epoch 92 \t Batch 420 \t Validation Loss: 37.14759122303554\n",
      "Epoch 92 \t Batch 440 \t Validation Loss: 37.21698527769609\n",
      "Epoch 92 \t Batch 460 \t Validation Loss: 37.81835666739423\n",
      "Epoch 92 \t Batch 480 \t Validation Loss: 38.454295047124226\n",
      "Epoch 92 \t Batch 500 \t Validation Loss: 38.29439214324951\n",
      "Epoch 92 \t Batch 520 \t Validation Loss: 38.51433887481689\n",
      "Epoch 92 \t Batch 540 \t Validation Loss: 38.475577700579606\n",
      "Epoch 92 \t Batch 560 \t Validation Loss: 38.53390571049282\n",
      "Epoch 92 \t Batch 580 \t Validation Loss: 38.49966623700898\n",
      "Epoch 92 \t Batch 600 \t Validation Loss: 38.87751916249593\n",
      "Epoch 92 Training Loss: 30.76602840943581 Validation Loss: 39.71319096429007\n",
      "Epoch 92 completed\n",
      "Epoch 93 \t Batch 20 \t Training Loss: 29.78470869064331\n",
      "Epoch 93 \t Batch 40 \t Training Loss: 29.704609870910645\n",
      "Epoch 93 \t Batch 60 \t Training Loss: 29.598973528544107\n",
      "Epoch 93 \t Batch 80 \t Training Loss: 29.691038727760315\n",
      "Epoch 93 \t Batch 100 \t Training Loss: 29.676121196746827\n",
      "Epoch 93 \t Batch 120 \t Training Loss: 29.776452016830444\n",
      "Epoch 93 \t Batch 140 \t Training Loss: 29.84603407723563\n",
      "Epoch 93 \t Batch 160 \t Training Loss: 29.85321980714798\n",
      "Epoch 93 \t Batch 180 \t Training Loss: 29.85346964730157\n",
      "Epoch 93 \t Batch 200 \t Training Loss: 29.866208448410035\n",
      "Epoch 93 \t Batch 220 \t Training Loss: 29.93081915595315\n",
      "Epoch 93 \t Batch 240 \t Training Loss: 29.97768562634786\n",
      "Epoch 93 \t Batch 260 \t Training Loss: 29.99814439186683\n",
      "Epoch 93 \t Batch 280 \t Training Loss: 30.074736949375698\n",
      "Epoch 93 \t Batch 300 \t Training Loss: 30.11533264160156\n",
      "Epoch 93 \t Batch 320 \t Training Loss: 30.121740114688873\n",
      "Epoch 93 \t Batch 340 \t Training Loss: 30.09208481171552\n",
      "Epoch 93 \t Batch 360 \t Training Loss: 30.065922212600707\n",
      "Epoch 93 \t Batch 380 \t Training Loss: 30.073845491911236\n",
      "Epoch 93 \t Batch 400 \t Training Loss: 30.08771303653717\n",
      "Epoch 93 \t Batch 420 \t Training Loss: 30.131658899216426\n",
      "Epoch 93 \t Batch 440 \t Training Loss: 30.136664260517467\n",
      "Epoch 93 \t Batch 460 \t Training Loss: 30.135596043130626\n",
      "Epoch 93 \t Batch 480 \t Training Loss: 30.154069578647615\n",
      "Epoch 93 \t Batch 500 \t Training Loss: 30.18050444030762\n",
      "Epoch 93 \t Batch 520 \t Training Loss: 30.227151801035955\n",
      "Epoch 93 \t Batch 540 \t Training Loss: 30.27512987278126\n",
      "Epoch 93 \t Batch 560 \t Training Loss: 30.313841969626292\n",
      "Epoch 93 \t Batch 580 \t Training Loss: 30.33799134287341\n",
      "Epoch 93 \t Batch 600 \t Training Loss: 30.346851479212443\n",
      "Epoch 93 \t Batch 620 \t Training Loss: 30.361894653689475\n",
      "Epoch 93 \t Batch 640 \t Training Loss: 30.362568762898444\n",
      "Epoch 93 \t Batch 660 \t Training Loss: 30.359614461841005\n",
      "Epoch 93 \t Batch 680 \t Training Loss: 30.376607092689067\n",
      "Epoch 93 \t Batch 700 \t Training Loss: 30.392507405962263\n",
      "Epoch 93 \t Batch 720 \t Training Loss: 30.396149020724827\n",
      "Epoch 93 \t Batch 740 \t Training Loss: 30.385903141949626\n",
      "Epoch 93 \t Batch 760 \t Training Loss: 30.400987394232498\n",
      "Epoch 93 \t Batch 780 \t Training Loss: 30.421165422292855\n",
      "Epoch 93 \t Batch 800 \t Training Loss: 30.426239109039308\n",
      "Epoch 93 \t Batch 820 \t Training Loss: 30.43527917164128\n",
      "Epoch 93 \t Batch 840 \t Training Loss: 30.458908362615677\n",
      "Epoch 93 \t Batch 860 \t Training Loss: 30.472879083766493\n",
      "Epoch 93 \t Batch 880 \t Training Loss: 30.504041134227407\n",
      "Epoch 93 \t Batch 900 \t Training Loss: 30.50006944656372\n",
      "Epoch 93 \t Batch 20 \t Validation Loss: 11.016330099105835\n",
      "Epoch 93 \t Batch 40 \t Validation Loss: 15.640631413459777\n",
      "Epoch 93 \t Batch 60 \t Validation Loss: 14.880132977167765\n",
      "Epoch 93 \t Batch 80 \t Validation Loss: 16.137933552265167\n",
      "Epoch 93 \t Batch 100 \t Validation Loss: 19.108464002609253\n",
      "Epoch 93 \t Batch 120 \t Validation Loss: 21.387413112322488\n",
      "Epoch 93 \t Batch 140 \t Validation Loss: 22.54607070514134\n",
      "Epoch 93 \t Batch 160 \t Validation Loss: 25.09398698210716\n",
      "Epoch 93 \t Batch 180 \t Validation Loss: 28.722086482577854\n",
      "Epoch 93 \t Batch 200 \t Validation Loss: 30.6951344871521\n",
      "Epoch 93 \t Batch 220 \t Validation Loss: 32.179398796775125\n",
      "Epoch 93 \t Batch 240 \t Validation Loss: 32.81530547142029\n",
      "Epoch 93 \t Batch 260 \t Validation Loss: 35.269746428269606\n",
      "Epoch 93 \t Batch 280 \t Validation Loss: 36.63265333856855\n",
      "Epoch 93 \t Batch 300 \t Validation Loss: 37.633699830373125\n",
      "Epoch 93 \t Batch 320 \t Validation Loss: 38.18688168227673\n",
      "Epoch 93 \t Batch 340 \t Validation Loss: 38.29296761400559\n",
      "Epoch 93 \t Batch 360 \t Validation Loss: 38.225810668203565\n",
      "Epoch 93 \t Batch 380 \t Validation Loss: 38.76539296350981\n",
      "Epoch 93 \t Batch 400 \t Validation Loss: 38.68653398275375\n",
      "Epoch 93 \t Batch 420 \t Validation Loss: 38.92147090775626\n",
      "Epoch 93 \t Batch 440 \t Validation Loss: 38.98530509038405\n",
      "Epoch 93 \t Batch 460 \t Validation Loss: 39.593701261022815\n",
      "Epoch 93 \t Batch 480 \t Validation Loss: 40.21700858871142\n",
      "Epoch 93 \t Batch 500 \t Validation Loss: 40.04769447135925\n",
      "Epoch 93 \t Batch 520 \t Validation Loss: 40.283486599188585\n",
      "Epoch 93 \t Batch 540 \t Validation Loss: 40.12784057723151\n",
      "Epoch 93 \t Batch 560 \t Validation Loss: 40.09470977953502\n",
      "Epoch 93 \t Batch 580 \t Validation Loss: 39.99165449964589\n",
      "Epoch 93 \t Batch 600 \t Validation Loss: 40.29903123378754\n",
      "Epoch 93 Training Loss: 30.5262601180581 Validation Loss: 41.085748500638196\n",
      "Epoch 93 completed\n",
      "Epoch 94 \t Batch 20 \t Training Loss: 29.340009212493896\n",
      "Epoch 94 \t Batch 40 \t Training Loss: 29.647548389434814\n",
      "Epoch 94 \t Batch 60 \t Training Loss: 29.88628609975179\n",
      "Epoch 94 \t Batch 80 \t Training Loss: 29.798956632614136\n",
      "Epoch 94 \t Batch 100 \t Training Loss: 29.70871768951416\n",
      "Epoch 94 \t Batch 120 \t Training Loss: 29.619300635655723\n",
      "Epoch 94 \t Batch 140 \t Training Loss: 29.555441992623464\n",
      "Epoch 94 \t Batch 160 \t Training Loss: 29.59976222515106\n",
      "Epoch 94 \t Batch 180 \t Training Loss: 29.71315272649129\n",
      "Epoch 94 \t Batch 200 \t Training Loss: 29.77224983215332\n",
      "Epoch 94 \t Batch 220 \t Training Loss: 29.831319124048406\n",
      "Epoch 94 \t Batch 240 \t Training Loss: 29.85714762210846\n",
      "Epoch 94 \t Batch 260 \t Training Loss: 29.884757687495306\n",
      "Epoch 94 \t Batch 280 \t Training Loss: 29.874835314069475\n",
      "Epoch 94 \t Batch 300 \t Training Loss: 29.880357093811035\n",
      "Epoch 94 \t Batch 320 \t Training Loss: 30.00350347161293\n",
      "Epoch 94 \t Batch 340 \t Training Loss: 30.053793879116284\n",
      "Epoch 94 \t Batch 360 \t Training Loss: 30.03762828509013\n",
      "Epoch 94 \t Batch 380 \t Training Loss: 29.997827710603413\n",
      "Epoch 94 \t Batch 400 \t Training Loss: 30.001527271270753\n",
      "Epoch 94 \t Batch 420 \t Training Loss: 29.989106568836032\n",
      "Epoch 94 \t Batch 440 \t Training Loss: 29.987358860536055\n",
      "Epoch 94 \t Batch 460 \t Training Loss: 30.013277530670166\n",
      "Epoch 94 \t Batch 480 \t Training Loss: 29.983953936894736\n",
      "Epoch 94 \t Batch 500 \t Training Loss: 30.023068767547606\n",
      "Epoch 94 \t Batch 520 \t Training Loss: 30.05420209444486\n",
      "Epoch 94 \t Batch 540 \t Training Loss: 30.06926349710535\n",
      "Epoch 94 \t Batch 560 \t Training Loss: 30.05702577659062\n",
      "Epoch 94 \t Batch 580 \t Training Loss: 30.05184615562702\n",
      "Epoch 94 \t Batch 600 \t Training Loss: 30.08430473645528\n",
      "Epoch 94 \t Batch 620 \t Training Loss: 30.099786573840724\n",
      "Epoch 94 \t Batch 640 \t Training Loss: 30.091296303272248\n",
      "Epoch 94 \t Batch 660 \t Training Loss: 30.08780763510502\n",
      "Epoch 94 \t Batch 680 \t Training Loss: 30.119374850217035\n",
      "Epoch 94 \t Batch 700 \t Training Loss: 30.17820581981114\n",
      "Epoch 94 \t Batch 720 \t Training Loss: 30.19464685122172\n",
      "Epoch 94 \t Batch 740 \t Training Loss: 30.218401653702195\n",
      "Epoch 94 \t Batch 760 \t Training Loss: 30.22346898129112\n",
      "Epoch 94 \t Batch 780 \t Training Loss: 30.237217411628137\n",
      "Epoch 94 \t Batch 800 \t Training Loss: 30.26079074382782\n",
      "Epoch 94 \t Batch 820 \t Training Loss: 30.287509404159174\n",
      "Epoch 94 \t Batch 840 \t Training Loss: 30.295484756288076\n",
      "Epoch 94 \t Batch 860 \t Training Loss: 30.29841128726338\n",
      "Epoch 94 \t Batch 880 \t Training Loss: 30.327555099400605\n",
      "Epoch 94 \t Batch 900 \t Training Loss: 30.360069397820368\n",
      "Epoch 94 \t Batch 20 \t Validation Loss: 12.412825131416321\n",
      "Epoch 94 \t Batch 40 \t Validation Loss: 16.131821715831755\n",
      "Epoch 94 \t Batch 60 \t Validation Loss: 15.257260584831238\n",
      "Epoch 94 \t Batch 80 \t Validation Loss: 16.366661924123765\n",
      "Epoch 94 \t Batch 100 \t Validation Loss: 19.354735808372496\n",
      "Epoch 94 \t Batch 120 \t Validation Loss: 21.717190515995025\n",
      "Epoch 94 \t Batch 140 \t Validation Loss: 22.89890387058258\n",
      "Epoch 94 \t Batch 160 \t Validation Loss: 25.32003869712353\n",
      "Epoch 94 \t Batch 180 \t Validation Loss: 29.019566411442227\n",
      "Epoch 94 \t Batch 200 \t Validation Loss: 31.037007791996004\n",
      "Epoch 94 \t Batch 220 \t Validation Loss: 32.57804218855771\n",
      "Epoch 94 \t Batch 240 \t Validation Loss: 33.19862214922905\n",
      "Epoch 94 \t Batch 260 \t Validation Loss: 35.7101693208401\n",
      "Epoch 94 \t Batch 280 \t Validation Loss: 37.09055325474058\n",
      "Epoch 94 \t Batch 300 \t Validation Loss: 38.060341763496396\n",
      "Epoch 94 \t Batch 320 \t Validation Loss: 38.56867068260908\n",
      "Epoch 94 \t Batch 340 \t Validation Loss: 38.62063996791839\n",
      "Epoch 94 \t Batch 360 \t Validation Loss: 38.535807677110036\n",
      "Epoch 94 \t Batch 380 \t Validation Loss: 39.00808606775183\n",
      "Epoch 94 \t Batch 400 \t Validation Loss: 38.821093143224715\n",
      "Epoch 94 \t Batch 420 \t Validation Loss: 39.00418878169287\n",
      "Epoch 94 \t Batch 440 \t Validation Loss: 38.91796462210742\n",
      "Epoch 94 \t Batch 460 \t Validation Loss: 39.42599634813226\n",
      "Epoch 94 \t Batch 480 \t Validation Loss: 40.01945057213307\n",
      "Epoch 94 \t Batch 500 \t Validation Loss: 39.81826906871795\n",
      "Epoch 94 \t Batch 520 \t Validation Loss: 39.94355165683306\n",
      "Epoch 94 \t Batch 540 \t Validation Loss: 39.777338994873894\n",
      "Epoch 94 \t Batch 560 \t Validation Loss: 39.710495757205145\n",
      "Epoch 94 \t Batch 580 \t Validation Loss: 39.58219156840752\n",
      "Epoch 94 \t Batch 600 \t Validation Loss: 39.87873963276545\n",
      "Epoch 94 Training Loss: 30.37300200342742 Validation Loss: 40.63915899125013\n",
      "Epoch 94 completed\n",
      "Epoch 95 \t Batch 20 \t Training Loss: 29.27089262008667\n",
      "Epoch 95 \t Batch 40 \t Training Loss: 29.879147338867188\n",
      "Epoch 95 \t Batch 60 \t Training Loss: 29.819370714823403\n",
      "Epoch 95 \t Batch 80 \t Training Loss: 29.9081298828125\n",
      "Epoch 95 \t Batch 100 \t Training Loss: 29.845403594970705\n",
      "Epoch 95 \t Batch 120 \t Training Loss: 29.780341863632202\n",
      "Epoch 95 \t Batch 140 \t Training Loss: 29.717228562491282\n",
      "Epoch 95 \t Batch 160 \t Training Loss: 29.757586705684663\n",
      "Epoch 95 \t Batch 180 \t Training Loss: 29.772726652357314\n",
      "Epoch 95 \t Batch 200 \t Training Loss: 29.75250810623169\n",
      "Epoch 95 \t Batch 220 \t Training Loss: 29.777672160755504\n",
      "Epoch 95 \t Batch 240 \t Training Loss: 29.73620003859202\n",
      "Epoch 95 \t Batch 260 \t Training Loss: 29.709755956209623\n",
      "Epoch 95 \t Batch 280 \t Training Loss: 29.72669622557504\n",
      "Epoch 95 \t Batch 300 \t Training Loss: 29.701232293446857\n",
      "Epoch 95 \t Batch 320 \t Training Loss: 29.765236365795136\n",
      "Epoch 95 \t Batch 340 \t Training Loss: 29.776267865124872\n",
      "Epoch 95 \t Batch 360 \t Training Loss: 29.810705545213487\n",
      "Epoch 95 \t Batch 380 \t Training Loss: 29.84247753745631\n",
      "Epoch 95 \t Batch 400 \t Training Loss: 29.864559245109557\n",
      "Epoch 95 \t Batch 420 \t Training Loss: 29.858248383658275\n",
      "Epoch 95 \t Batch 440 \t Training Loss: 29.90369642864574\n",
      "Epoch 95 \t Batch 460 \t Training Loss: 29.926451032058054\n",
      "Epoch 95 \t Batch 480 \t Training Loss: 29.969101770718893\n",
      "Epoch 95 \t Batch 500 \t Training Loss: 29.96550926208496\n",
      "Epoch 95 \t Batch 520 \t Training Loss: 29.967069713885966\n",
      "Epoch 95 \t Batch 540 \t Training Loss: 29.981959791536685\n",
      "Epoch 95 \t Batch 560 \t Training Loss: 30.004812475613186\n",
      "Epoch 95 \t Batch 580 \t Training Loss: 30.001486669737716\n",
      "Epoch 95 \t Batch 600 \t Training Loss: 30.033004423777264\n",
      "Epoch 95 \t Batch 620 \t Training Loss: 30.05081056471794\n",
      "Epoch 95 \t Batch 640 \t Training Loss: 30.063216829299925\n",
      "Epoch 95 \t Batch 660 \t Training Loss: 30.04468928539392\n",
      "Epoch 95 \t Batch 680 \t Training Loss: 30.033846465279073\n",
      "Epoch 95 \t Batch 700 \t Training Loss: 30.05191269193377\n",
      "Epoch 95 \t Batch 720 \t Training Loss: 30.053968856069776\n",
      "Epoch 95 \t Batch 740 \t Training Loss: 30.05912419138728\n",
      "Epoch 95 \t Batch 760 \t Training Loss: 30.045930636556523\n",
      "Epoch 95 \t Batch 780 \t Training Loss: 30.053591586381962\n",
      "Epoch 95 \t Batch 800 \t Training Loss: 30.062080204486847\n",
      "Epoch 95 \t Batch 820 \t Training Loss: 30.073291078427943\n",
      "Epoch 95 \t Batch 840 \t Training Loss: 30.077689275287447\n",
      "Epoch 95 \t Batch 860 \t Training Loss: 30.064176404198935\n",
      "Epoch 95 \t Batch 880 \t Training Loss: 30.07404910217632\n",
      "Epoch 95 \t Batch 900 \t Training Loss: 30.09077065149943\n",
      "Epoch 95 \t Batch 20 \t Validation Loss: 9.290133953094482\n",
      "Epoch 95 \t Batch 40 \t Validation Loss: 13.941082775592804\n",
      "Epoch 95 \t Batch 60 \t Validation Loss: 13.343738834063211\n",
      "Epoch 95 \t Batch 80 \t Validation Loss: 14.846053665876388\n",
      "Epoch 95 \t Batch 100 \t Validation Loss: 18.51106017589569\n",
      "Epoch 95 \t Batch 120 \t Validation Loss: 21.064968168735504\n",
      "Epoch 95 \t Batch 140 \t Validation Loss: 22.553615198816573\n",
      "Epoch 95 \t Batch 160 \t Validation Loss: 25.258800503611564\n",
      "Epoch 95 \t Batch 180 \t Validation Loss: 29.30939454767439\n",
      "Epoch 95 \t Batch 200 \t Validation Loss: 31.507497975826265\n",
      "Epoch 95 \t Batch 220 \t Validation Loss: 33.19686653614044\n",
      "Epoch 95 \t Batch 240 \t Validation Loss: 33.959310458103815\n",
      "Epoch 95 \t Batch 260 \t Validation Loss: 36.533886012664205\n",
      "Epoch 95 \t Batch 280 \t Validation Loss: 37.97240402528218\n",
      "Epoch 95 \t Batch 300 \t Validation Loss: 39.09586226304372\n",
      "Epoch 95 \t Batch 320 \t Validation Loss: 39.70547512322664\n",
      "Epoch 95 \t Batch 340 \t Validation Loss: 39.77211398096646\n",
      "Epoch 95 \t Batch 360 \t Validation Loss: 39.6905705359247\n",
      "Epoch 95 \t Batch 380 \t Validation Loss: 40.249423404743794\n",
      "Epoch 95 \t Batch 400 \t Validation Loss: 40.14379629254341\n",
      "Epoch 95 \t Batch 420 \t Validation Loss: 40.35341801075708\n",
      "Epoch 95 \t Batch 440 \t Validation Loss: 40.41339744762941\n",
      "Epoch 95 \t Batch 460 \t Validation Loss: 41.03219084636025\n",
      "Epoch 95 \t Batch 480 \t Validation Loss: 41.629341066877046\n",
      "Epoch 95 \t Batch 500 \t Validation Loss: 41.45260574054718\n",
      "Epoch 95 \t Batch 520 \t Validation Loss: 41.743013419554785\n",
      "Epoch 95 \t Batch 540 \t Validation Loss: 41.554084482016385\n",
      "Epoch 95 \t Batch 560 \t Validation Loss: 41.46531144636018\n",
      "Epoch 95 \t Batch 580 \t Validation Loss: 41.32427958208939\n",
      "Epoch 95 \t Batch 600 \t Validation Loss: 41.59075936396917\n",
      "Epoch 95 Training Loss: 30.100513813929833 Validation Loss: 42.32286799805505\n",
      "Epoch 95 completed\n",
      "Epoch 96 \t Batch 20 \t Training Loss: 28.265846920013427\n",
      "Epoch 96 \t Batch 40 \t Training Loss: 29.23393044471741\n",
      "Epoch 96 \t Batch 60 \t Training Loss: 29.339071305592856\n",
      "Epoch 96 \t Batch 80 \t Training Loss: 29.723743677139282\n",
      "Epoch 96 \t Batch 100 \t Training Loss: 29.668043785095215\n",
      "Epoch 96 \t Batch 120 \t Training Loss: 29.584832636515298\n",
      "Epoch 96 \t Batch 140 \t Training Loss: 29.592661557878767\n",
      "Epoch 96 \t Batch 160 \t Training Loss: 29.558743405342103\n",
      "Epoch 96 \t Batch 180 \t Training Loss: 29.606231350368923\n",
      "Epoch 96 \t Batch 200 \t Training Loss: 29.589295816421508\n",
      "Epoch 96 \t Batch 220 \t Training Loss: 29.653031782670453\n",
      "Epoch 96 \t Batch 240 \t Training Loss: 29.696757435798645\n",
      "Epoch 96 \t Batch 260 \t Training Loss: 29.681486254472\n",
      "Epoch 96 \t Batch 280 \t Training Loss: 29.6396639415196\n",
      "Epoch 96 \t Batch 300 \t Training Loss: 29.625595633188883\n",
      "Epoch 96 \t Batch 320 \t Training Loss: 29.666136652231216\n",
      "Epoch 96 \t Batch 340 \t Training Loss: 29.648814162086037\n",
      "Epoch 96 \t Batch 360 \t Training Loss: 29.623860353893704\n",
      "Epoch 96 \t Batch 380 \t Training Loss: 29.610880545565955\n",
      "Epoch 96 \t Batch 400 \t Training Loss: 29.653694581985473\n",
      "Epoch 96 \t Batch 420 \t Training Loss: 29.647155988784064\n",
      "Epoch 96 \t Batch 440 \t Training Loss: 29.640394488247956\n",
      "Epoch 96 \t Batch 460 \t Training Loss: 29.696990494106128\n",
      "Epoch 96 \t Batch 480 \t Training Loss: 29.706819081306456\n",
      "Epoch 96 \t Batch 500 \t Training Loss: 29.711886192321778\n",
      "Epoch 96 \t Batch 520 \t Training Loss: 29.727059268951415\n",
      "Epoch 96 \t Batch 540 \t Training Loss: 29.73677516160188\n",
      "Epoch 96 \t Batch 560 \t Training Loss: 29.74357168333871\n",
      "Epoch 96 \t Batch 580 \t Training Loss: 29.786997009145804\n",
      "Epoch 96 \t Batch 600 \t Training Loss: 29.849019784927368\n",
      "Epoch 96 \t Batch 620 \t Training Loss: 29.83021665388538\n",
      "Epoch 96 \t Batch 640 \t Training Loss: 29.834919875860216\n",
      "Epoch 96 \t Batch 660 \t Training Loss: 29.83550201184822\n",
      "Epoch 96 \t Batch 680 \t Training Loss: 29.853762360180127\n",
      "Epoch 96 \t Batch 700 \t Training Loss: 29.84594287599836\n",
      "Epoch 96 \t Batch 720 \t Training Loss: 29.893142827351888\n",
      "Epoch 96 \t Batch 740 \t Training Loss: 29.89846209706487\n",
      "Epoch 96 \t Batch 760 \t Training Loss: 29.902622072320234\n",
      "Epoch 96 \t Batch 780 \t Training Loss: 29.89242933224409\n",
      "Epoch 96 \t Batch 800 \t Training Loss: 29.91244188308716\n",
      "Epoch 96 \t Batch 820 \t Training Loss: 29.89063077089263\n",
      "Epoch 96 \t Batch 840 \t Training Loss: 29.88848057474409\n",
      "Epoch 96 \t Batch 860 \t Training Loss: 29.89753520211508\n",
      "Epoch 96 \t Batch 880 \t Training Loss: 29.90116574980996\n",
      "Epoch 96 \t Batch 900 \t Training Loss: 29.91544599533081\n",
      "Epoch 96 \t Batch 20 \t Validation Loss: 8.5001762509346\n",
      "Epoch 96 \t Batch 40 \t Validation Loss: 12.977257788181305\n",
      "Epoch 96 \t Batch 60 \t Validation Loss: 12.440901907285054\n",
      "Epoch 96 \t Batch 80 \t Validation Loss: 13.852757900953293\n",
      "Epoch 96 \t Batch 100 \t Validation Loss: 17.350191378593443\n",
      "Epoch 96 \t Batch 120 \t Validation Loss: 19.957418811321258\n",
      "Epoch 96 \t Batch 140 \t Validation Loss: 21.359899319921222\n",
      "Epoch 96 \t Batch 160 \t Validation Loss: 23.942226925492285\n",
      "Epoch 96 \t Batch 180 \t Validation Loss: 27.42368128299713\n",
      "Epoch 96 \t Batch 200 \t Validation Loss: 29.363662321567535\n",
      "Epoch 96 \t Batch 220 \t Validation Loss: 30.848817281289534\n",
      "Epoch 96 \t Batch 240 \t Validation Loss: 31.51098988254865\n",
      "Epoch 96 \t Batch 260 \t Validation Loss: 33.898782178071826\n",
      "Epoch 96 \t Batch 280 \t Validation Loss: 35.26184556313923\n",
      "Epoch 96 \t Batch 300 \t Validation Loss: 36.23536639054616\n",
      "Epoch 96 \t Batch 320 \t Validation Loss: 36.81324905306101\n",
      "Epoch 96 \t Batch 340 \t Validation Loss: 36.96319911199458\n",
      "Epoch 96 \t Batch 360 \t Validation Loss: 36.90936871767044\n",
      "Epoch 96 \t Batch 380 \t Validation Loss: 37.47537534236908\n",
      "Epoch 96 \t Batch 400 \t Validation Loss: 37.426516290903095\n",
      "Epoch 96 \t Batch 420 \t Validation Loss: 37.73115085533687\n",
      "Epoch 96 \t Batch 440 \t Validation Loss: 37.80540673624385\n",
      "Epoch 96 \t Batch 460 \t Validation Loss: 38.5034205612929\n",
      "Epoch 96 \t Batch 480 \t Validation Loss: 39.15143686234951\n",
      "Epoch 96 \t Batch 500 \t Validation Loss: 39.00268486118317\n",
      "Epoch 96 \t Batch 520 \t Validation Loss: 39.26380981206894\n",
      "Epoch 96 \t Batch 540 \t Validation Loss: 39.13223850727081\n",
      "Epoch 96 \t Batch 560 \t Validation Loss: 39.117266442946026\n",
      "Epoch 96 \t Batch 580 \t Validation Loss: 39.009168709557635\n",
      "Epoch 96 \t Batch 600 \t Validation Loss: 39.32477448383967\n",
      "Epoch 96 Training Loss: 29.924387435746688 Validation Loss: 40.109711454286206\n",
      "Epoch 96 completed\n",
      "Epoch 97 \t Batch 20 \t Training Loss: 29.83779067993164\n",
      "Epoch 97 \t Batch 40 \t Training Loss: 29.28761796951294\n",
      "Epoch 97 \t Batch 60 \t Training Loss: 29.62184845606486\n",
      "Epoch 97 \t Batch 80 \t Training Loss: 29.476946711540222\n",
      "Epoch 97 \t Batch 100 \t Training Loss: 29.402029056549072\n",
      "Epoch 97 \t Batch 120 \t Training Loss: 29.5266419728597\n",
      "Epoch 97 \t Batch 140 \t Training Loss: 29.444338226318358\n",
      "Epoch 97 \t Batch 160 \t Training Loss: 29.386657297611237\n",
      "Epoch 97 \t Batch 180 \t Training Loss: 29.358755196465385\n",
      "Epoch 97 \t Batch 200 \t Training Loss: 29.321962690353395\n",
      "Epoch 97 \t Batch 220 \t Training Loss: 29.289535201679577\n",
      "Epoch 97 \t Batch 240 \t Training Loss: 29.295499777793886\n",
      "Epoch 97 \t Batch 260 \t Training Loss: 29.383030553964467\n",
      "Epoch 97 \t Batch 280 \t Training Loss: 29.373421594074795\n",
      "Epoch 97 \t Batch 300 \t Training Loss: 29.337435010274252\n",
      "Epoch 97 \t Batch 320 \t Training Loss: 29.29114618897438\n",
      "Epoch 97 \t Batch 340 \t Training Loss: 29.315167690725886\n",
      "Epoch 97 \t Batch 360 \t Training Loss: 29.34431619114346\n",
      "Epoch 97 \t Batch 380 \t Training Loss: 29.363280331461052\n",
      "Epoch 97 \t Batch 400 \t Training Loss: 29.424845752716063\n",
      "Epoch 97 \t Batch 420 \t Training Loss: 29.47974997020903\n",
      "Epoch 97 \t Batch 440 \t Training Loss: 29.480975831638684\n",
      "Epoch 97 \t Batch 460 \t Training Loss: 29.47048697678939\n",
      "Epoch 97 \t Batch 480 \t Training Loss: 29.491675420602164\n",
      "Epoch 97 \t Batch 500 \t Training Loss: 29.457068283081053\n",
      "Epoch 97 \t Batch 520 \t Training Loss: 29.480965229181145\n",
      "Epoch 97 \t Batch 540 \t Training Loss: 29.49209978668778\n",
      "Epoch 97 \t Batch 560 \t Training Loss: 29.503540958677018\n",
      "Epoch 97 \t Batch 580 \t Training Loss: 29.50866942241274\n",
      "Epoch 97 \t Batch 600 \t Training Loss: 29.520037914911907\n",
      "Epoch 97 \t Batch 620 \t Training Loss: 29.54845490609446\n",
      "Epoch 97 \t Batch 640 \t Training Loss: 29.573670491576195\n",
      "Epoch 97 \t Batch 660 \t Training Loss: 29.594535347909638\n",
      "Epoch 97 \t Batch 680 \t Training Loss: 29.61865123860976\n",
      "Epoch 97 \t Batch 700 \t Training Loss: 29.60376442500523\n",
      "Epoch 97 \t Batch 720 \t Training Loss: 29.60662007331848\n",
      "Epoch 97 \t Batch 740 \t Training Loss: 29.641660216047956\n",
      "Epoch 97 \t Batch 760 \t Training Loss: 29.639493806738603\n",
      "Epoch 97 \t Batch 780 \t Training Loss: 29.654130678910477\n",
      "Epoch 97 \t Batch 800 \t Training Loss: 29.669359838962556\n",
      "Epoch 97 \t Batch 820 \t Training Loss: 29.683104880263166\n",
      "Epoch 97 \t Batch 840 \t Training Loss: 29.688410350254603\n",
      "Epoch 97 \t Batch 860 \t Training Loss: 29.696885559170745\n",
      "Epoch 97 \t Batch 880 \t Training Loss: 29.718679135495965\n",
      "Epoch 97 \t Batch 900 \t Training Loss: 29.738738530476887\n",
      "Epoch 97 \t Batch 20 \t Validation Loss: 9.92389509677887\n",
      "Epoch 97 \t Batch 40 \t Validation Loss: 13.092118167877198\n",
      "Epoch 97 \t Batch 60 \t Validation Loss: 12.699262428283692\n",
      "Epoch 97 \t Batch 80 \t Validation Loss: 14.031023633480071\n",
      "Epoch 97 \t Batch 100 \t Validation Loss: 17.465654687881468\n",
      "Epoch 97 \t Batch 120 \t Validation Loss: 20.08190772533417\n",
      "Epoch 97 \t Batch 140 \t Validation Loss: 21.500777850832257\n",
      "Epoch 97 \t Batch 160 \t Validation Loss: 24.16403097510338\n",
      "Epoch 97 \t Batch 180 \t Validation Loss: 28.12391168276469\n",
      "Epoch 97 \t Batch 200 \t Validation Loss: 30.175662932395934\n",
      "Epoch 97 \t Batch 220 \t Validation Loss: 31.790004673871127\n",
      "Epoch 97 \t Batch 240 \t Validation Loss: 32.53443332115809\n",
      "Epoch 97 \t Batch 260 \t Validation Loss: 35.035326132407555\n",
      "Epoch 97 \t Batch 280 \t Validation Loss: 36.45600582190922\n",
      "Epoch 97 \t Batch 300 \t Validation Loss: 37.59170831998189\n",
      "Epoch 97 \t Batch 320 \t Validation Loss: 38.20092931091786\n",
      "Epoch 97 \t Batch 340 \t Validation Loss: 38.259778648264266\n",
      "Epoch 97 \t Batch 360 \t Validation Loss: 38.1749912765291\n",
      "Epoch 97 \t Batch 380 \t Validation Loss: 38.68664079214397\n",
      "Epoch 97 \t Batch 400 \t Validation Loss: 38.522908008098604\n",
      "Epoch 97 \t Batch 420 \t Validation Loss: 38.6966186500731\n",
      "Epoch 97 \t Batch 440 \t Validation Loss: 38.628414034843445\n",
      "Epoch 97 \t Batch 460 \t Validation Loss: 39.260280063877936\n",
      "Epoch 97 \t Batch 480 \t Validation Loss: 39.87424761255582\n",
      "Epoch 97 \t Batch 500 \t Validation Loss: 39.69760872077942\n",
      "Epoch 97 \t Batch 520 \t Validation Loss: 39.90689459580641\n",
      "Epoch 97 \t Batch 540 \t Validation Loss: 39.72557016302038\n",
      "Epoch 97 \t Batch 560 \t Validation Loss: 39.668599697521756\n",
      "Epoch 97 \t Batch 580 \t Validation Loss: 39.544054241838126\n",
      "Epoch 97 \t Batch 600 \t Validation Loss: 39.82744388262431\n",
      "Epoch 97 Training Loss: 29.732553020298415 Validation Loss: 40.56680712452182\n",
      "Epoch 97 completed\n",
      "Epoch 98 \t Batch 20 \t Training Loss: 29.164379787445068\n",
      "Epoch 98 \t Batch 40 \t Training Loss: 29.237903690338136\n",
      "Epoch 98 \t Batch 60 \t Training Loss: 28.970651213328043\n",
      "Epoch 98 \t Batch 80 \t Training Loss: 28.963192963600157\n",
      "Epoch 98 \t Batch 100 \t Training Loss: 28.88101833343506\n",
      "Epoch 98 \t Batch 120 \t Training Loss: 28.94889440536499\n",
      "Epoch 98 \t Batch 140 \t Training Loss: 28.944030148642405\n",
      "Epoch 98 \t Batch 160 \t Training Loss: 29.034763753414154\n",
      "Epoch 98 \t Batch 180 \t Training Loss: 29.093055566151936\n",
      "Epoch 98 \t Batch 200 \t Training Loss: 29.153573598861694\n",
      "Epoch 98 \t Batch 220 \t Training Loss: 29.181984077800404\n",
      "Epoch 98 \t Batch 240 \t Training Loss: 29.205178864796956\n",
      "Epoch 98 \t Batch 260 \t Training Loss: 29.181473423884466\n",
      "Epoch 98 \t Batch 280 \t Training Loss: 29.171219519206456\n",
      "Epoch 98 \t Batch 300 \t Training Loss: 29.140985196431476\n",
      "Epoch 98 \t Batch 320 \t Training Loss: 29.142945593595506\n",
      "Epoch 98 \t Batch 340 \t Training Loss: 29.175925742878633\n",
      "Epoch 98 \t Batch 360 \t Training Loss: 29.17370940844218\n",
      "Epoch 98 \t Batch 380 \t Training Loss: 29.202520009091025\n",
      "Epoch 98 \t Batch 400 \t Training Loss: 29.247127327919006\n",
      "Epoch 98 \t Batch 420 \t Training Loss: 29.262520962669736\n",
      "Epoch 98 \t Batch 440 \t Training Loss: 29.294631563533436\n",
      "Epoch 98 \t Batch 460 \t Training Loss: 29.271570168370786\n",
      "Epoch 98 \t Batch 480 \t Training Loss: 29.307282582918802\n",
      "Epoch 98 \t Batch 500 \t Training Loss: 29.309567363739014\n",
      "Epoch 98 \t Batch 520 \t Training Loss: 29.35536342033973\n",
      "Epoch 98 \t Batch 540 \t Training Loss: 29.342226526472302\n",
      "Epoch 98 \t Batch 560 \t Training Loss: 29.369756122997828\n",
      "Epoch 98 \t Batch 580 \t Training Loss: 29.392725441373628\n",
      "Epoch 98 \t Batch 600 \t Training Loss: 29.39401075363159\n",
      "Epoch 98 \t Batch 620 \t Training Loss: 29.370797566444644\n",
      "Epoch 98 \t Batch 640 \t Training Loss: 29.380581098794938\n",
      "Epoch 98 \t Batch 660 \t Training Loss: 29.378168848789098\n",
      "Epoch 98 \t Batch 680 \t Training Loss: 29.39676090408774\n",
      "Epoch 98 \t Batch 700 \t Training Loss: 29.382533291407995\n",
      "Epoch 98 \t Batch 720 \t Training Loss: 29.37902647919125\n",
      "Epoch 98 \t Batch 740 \t Training Loss: 29.3875256693041\n",
      "Epoch 98 \t Batch 760 \t Training Loss: 29.39535100836503\n",
      "Epoch 98 \t Batch 780 \t Training Loss: 29.41472735771766\n",
      "Epoch 98 \t Batch 800 \t Training Loss: 29.43055268526077\n",
      "Epoch 98 \t Batch 820 \t Training Loss: 29.43106513139678\n",
      "Epoch 98 \t Batch 840 \t Training Loss: 29.438574977148146\n",
      "Epoch 98 \t Batch 860 \t Training Loss: 29.44846037487651\n",
      "Epoch 98 \t Batch 880 \t Training Loss: 29.45166671926325\n",
      "Epoch 98 \t Batch 900 \t Training Loss: 29.453116402096217\n",
      "Epoch 98 \t Batch 20 \t Validation Loss: 10.19634850025177\n",
      "Epoch 98 \t Batch 40 \t Validation Loss: 14.091544604301452\n",
      "Epoch 98 \t Batch 60 \t Validation Loss: 13.347440369923909\n",
      "Epoch 98 \t Batch 80 \t Validation Loss: 14.55838103890419\n",
      "Epoch 98 \t Batch 100 \t Validation Loss: 18.041044793128968\n",
      "Epoch 98 \t Batch 120 \t Validation Loss: 20.553368572394053\n",
      "Epoch 98 \t Batch 140 \t Validation Loss: 21.922318271228246\n",
      "Epoch 98 \t Batch 160 \t Validation Loss: 24.54243895113468\n",
      "Epoch 98 \t Batch 180 \t Validation Loss: 27.80250213676029\n",
      "Epoch 98 \t Batch 200 \t Validation Loss: 29.75111934900284\n",
      "Epoch 98 \t Batch 220 \t Validation Loss: 31.089011159810152\n",
      "Epoch 98 \t Batch 240 \t Validation Loss: 31.63555364012718\n",
      "Epoch 98 \t Batch 260 \t Validation Loss: 33.944145886714644\n",
      "Epoch 98 \t Batch 280 \t Validation Loss: 35.26875218153\n",
      "Epoch 98 \t Batch 300 \t Validation Loss: 36.18156259059906\n",
      "Epoch 98 \t Batch 320 \t Validation Loss: 36.71720492988825\n",
      "Epoch 98 \t Batch 340 \t Validation Loss: 36.840878583403196\n",
      "Epoch 98 \t Batch 360 \t Validation Loss: 36.83819527758492\n",
      "Epoch 98 \t Batch 380 \t Validation Loss: 37.47106708350935\n",
      "Epoch 98 \t Batch 400 \t Validation Loss: 37.50112805962563\n",
      "Epoch 98 \t Batch 420 \t Validation Loss: 37.83402086098989\n",
      "Epoch 98 \t Batch 440 \t Validation Loss: 37.983946632255204\n",
      "Epoch 98 \t Batch 460 \t Validation Loss: 38.73751751132633\n",
      "Epoch 98 \t Batch 480 \t Validation Loss: 39.37363723218441\n",
      "Epoch 98 \t Batch 500 \t Validation Loss: 39.230751488685605\n",
      "Epoch 98 \t Batch 520 \t Validation Loss: 39.64660759247266\n",
      "Epoch 98 \t Batch 540 \t Validation Loss: 39.53191543773369\n",
      "Epoch 98 \t Batch 560 \t Validation Loss: 39.52440602353641\n",
      "Epoch 98 \t Batch 580 \t Validation Loss: 39.46399167077295\n",
      "Epoch 98 \t Batch 600 \t Validation Loss: 39.782300474643705\n",
      "Epoch 98 Training Loss: 29.487195652469815 Validation Loss: 40.5656478675929\n",
      "Epoch 98 completed\n",
      "Epoch 99 \t Batch 20 \t Training Loss: 27.86668815612793\n",
      "Epoch 99 \t Batch 40 \t Training Loss: 28.234571599960326\n",
      "Epoch 99 \t Batch 60 \t Training Loss: 27.996642367045084\n",
      "Epoch 99 \t Batch 80 \t Training Loss: 28.15748074054718\n",
      "Epoch 99 \t Batch 100 \t Training Loss: 28.295496578216554\n",
      "Epoch 99 \t Batch 120 \t Training Loss: 28.545883258183796\n",
      "Epoch 99 \t Batch 140 \t Training Loss: 28.63295751299177\n",
      "Epoch 99 \t Batch 160 \t Training Loss: 28.70040844678879\n",
      "Epoch 99 \t Batch 180 \t Training Loss: 28.747840648227267\n",
      "Epoch 99 \t Batch 200 \t Training Loss: 28.770093231201173\n",
      "Epoch 99 \t Batch 220 \t Training Loss: 28.85234151320024\n",
      "Epoch 99 \t Batch 240 \t Training Loss: 28.86303632259369\n",
      "Epoch 99 \t Batch 260 \t Training Loss: 28.91407261628371\n",
      "Epoch 99 \t Batch 280 \t Training Loss: 28.95694331441607\n",
      "Epoch 99 \t Batch 300 \t Training Loss: 28.961456508636473\n",
      "Epoch 99 \t Batch 320 \t Training Loss: 29.042134195566177\n",
      "Epoch 99 \t Batch 340 \t Training Loss: 29.04182268030503\n",
      "Epoch 99 \t Batch 360 \t Training Loss: 29.037567403581406\n",
      "Epoch 99 \t Batch 380 \t Training Loss: 29.028609571958842\n",
      "Epoch 99 \t Batch 400 \t Training Loss: 29.049124965667726\n",
      "Epoch 99 \t Batch 420 \t Training Loss: 29.084258388337634\n",
      "Epoch 99 \t Batch 440 \t Training Loss: 29.108535341783004\n",
      "Epoch 99 \t Batch 460 \t Training Loss: 29.133510092030402\n",
      "Epoch 99 \t Batch 480 \t Training Loss: 29.122431766986846\n",
      "Epoch 99 \t Batch 500 \t Training Loss: 29.132394412994383\n",
      "Epoch 99 \t Batch 520 \t Training Loss: 29.12937959891099\n",
      "Epoch 99 \t Batch 540 \t Training Loss: 29.08447781668769\n",
      "Epoch 99 \t Batch 560 \t Training Loss: 29.122170536858693\n",
      "Epoch 99 \t Batch 580 \t Training Loss: 29.134448587483373\n",
      "Epoch 99 \t Batch 600 \t Training Loss: 29.13892780939738\n",
      "Epoch 99 \t Batch 620 \t Training Loss: 29.17745907691217\n",
      "Epoch 99 \t Batch 640 \t Training Loss: 29.165679082274437\n",
      "Epoch 99 \t Batch 660 \t Training Loss: 29.196923559362236\n",
      "Epoch 99 \t Batch 680 \t Training Loss: 29.194933335921345\n",
      "Epoch 99 \t Batch 700 \t Training Loss: 29.224715322766986\n",
      "Epoch 99 \t Batch 720 \t Training Loss: 29.25072271823883\n",
      "Epoch 99 \t Batch 740 \t Training Loss: 29.286489267606992\n",
      "Epoch 99 \t Batch 760 \t Training Loss: 29.28225884688528\n",
      "Epoch 99 \t Batch 780 \t Training Loss: 29.279324900798308\n",
      "Epoch 99 \t Batch 800 \t Training Loss: 29.289700615406037\n",
      "Epoch 99 \t Batch 820 \t Training Loss: 29.282660786698504\n",
      "Epoch 99 \t Batch 840 \t Training Loss: 29.290194931484404\n",
      "Epoch 99 \t Batch 860 \t Training Loss: 29.299050681535586\n",
      "Epoch 99 \t Batch 880 \t Training Loss: 29.300665575807745\n",
      "Epoch 99 \t Batch 900 \t Training Loss: 29.337261475457087\n",
      "Epoch 99 \t Batch 20 \t Validation Loss: 6.398960590362549\n",
      "Epoch 99 \t Batch 40 \t Validation Loss: 10.21234496831894\n",
      "Epoch 99 \t Batch 60 \t Validation Loss: 10.05313260157903\n",
      "Epoch 99 \t Batch 80 \t Validation Loss: 11.712785884737968\n",
      "Epoch 99 \t Batch 100 \t Validation Loss: 15.351897809505463\n",
      "Epoch 99 \t Batch 120 \t Validation Loss: 18.14609575867653\n",
      "Epoch 99 \t Batch 140 \t Validation Loss: 19.775804688249316\n",
      "Epoch 99 \t Batch 160 \t Validation Loss: 22.73086997717619\n",
      "Epoch 99 \t Batch 180 \t Validation Loss: 26.671168771055008\n",
      "Epoch 99 \t Batch 200 \t Validation Loss: 28.818771246671677\n",
      "Epoch 99 \t Batch 220 \t Validation Loss: 30.45763594779101\n",
      "Epoch 99 \t Batch 240 \t Validation Loss: 31.241425362229347\n",
      "Epoch 99 \t Batch 260 \t Validation Loss: 33.766851446261775\n",
      "Epoch 99 \t Batch 280 \t Validation Loss: 35.25348108070237\n",
      "Epoch 99 \t Batch 300 \t Validation Loss: 36.38507197618485\n",
      "Epoch 99 \t Batch 320 \t Validation Loss: 37.0575470559299\n",
      "Epoch 99 \t Batch 340 \t Validation Loss: 37.234745327865376\n",
      "Epoch 99 \t Batch 360 \t Validation Loss: 37.27026303542985\n",
      "Epoch 99 \t Batch 380 \t Validation Loss: 37.872534414968996\n",
      "Epoch 99 \t Batch 400 \t Validation Loss: 37.871789879202844\n",
      "Epoch 99 \t Batch 420 \t Validation Loss: 38.15278074116934\n",
      "Epoch 99 \t Batch 440 \t Validation Loss: 38.25052312016487\n",
      "Epoch 99 \t Batch 460 \t Validation Loss: 38.88736746777659\n",
      "Epoch 99 \t Batch 480 \t Validation Loss: 39.558410648008184\n",
      "Epoch 99 \t Batch 500 \t Validation Loss: 39.41233844327927\n",
      "Epoch 99 \t Batch 520 \t Validation Loss: 39.68999658318666\n",
      "Epoch 99 \t Batch 540 \t Validation Loss: 39.53559801975886\n",
      "Epoch 99 \t Batch 560 \t Validation Loss: 39.46005969686168\n",
      "Epoch 99 \t Batch 580 \t Validation Loss: 39.32715065027105\n",
      "Epoch 99 \t Batch 600 \t Validation Loss: 39.62668298681577\n",
      "Epoch 99 Training Loss: 29.35718139931316 Validation Loss: 40.3838774194191\n",
      "Epoch 99 completed\n",
      "Epoch 100 \t Batch 20 \t Training Loss: 29.400416755676268\n",
      "Epoch 100 \t Batch 40 \t Training Loss: 28.97004222869873\n",
      "Epoch 100 \t Batch 60 \t Training Loss: 28.81952781677246\n",
      "Epoch 100 \t Batch 80 \t Training Loss: 28.62973337173462\n",
      "Epoch 100 \t Batch 100 \t Training Loss: 28.538805179595947\n",
      "Epoch 100 \t Batch 120 \t Training Loss: 28.539743026097614\n",
      "Epoch 100 \t Batch 140 \t Training Loss: 28.679201330457413\n",
      "Epoch 100 \t Batch 160 \t Training Loss: 28.729166424274446\n",
      "Epoch 100 \t Batch 180 \t Training Loss: 28.72648884455363\n",
      "Epoch 100 \t Batch 200 \t Training Loss: 28.7218421459198\n",
      "Epoch 100 \t Batch 220 \t Training Loss: 28.712260567058216\n",
      "Epoch 100 \t Batch 240 \t Training Loss: 28.77984891732534\n",
      "Epoch 100 \t Batch 260 \t Training Loss: 28.84426602583665\n",
      "Epoch 100 \t Batch 280 \t Training Loss: 28.855299704415458\n",
      "Epoch 100 \t Batch 300 \t Training Loss: 28.844216995239258\n",
      "Epoch 100 \t Batch 320 \t Training Loss: 28.885644286870956\n",
      "Epoch 100 \t Batch 340 \t Training Loss: 28.94068493562586\n",
      "Epoch 100 \t Batch 360 \t Training Loss: 28.938268862830267\n",
      "Epoch 100 \t Batch 380 \t Training Loss: 28.959424355155544\n",
      "Epoch 100 \t Batch 400 \t Training Loss: 29.02887001514435\n",
      "Epoch 100 \t Batch 420 \t Training Loss: 29.022855672382175\n",
      "Epoch 100 \t Batch 440 \t Training Loss: 29.023650433800437\n",
      "Epoch 100 \t Batch 460 \t Training Loss: 29.0814387653185\n",
      "Epoch 100 \t Batch 480 \t Training Loss: 29.08791084686915\n",
      "Epoch 100 \t Batch 500 \t Training Loss: 29.103755908966065\n",
      "Epoch 100 \t Batch 520 \t Training Loss: 29.08316719348614\n",
      "Epoch 100 \t Batch 540 \t Training Loss: 29.0870687979239\n",
      "Epoch 100 \t Batch 560 \t Training Loss: 29.087425357954842\n",
      "Epoch 100 \t Batch 580 \t Training Loss: 29.064312632330534\n",
      "Epoch 100 \t Batch 600 \t Training Loss: 29.044143390655517\n",
      "Epoch 100 \t Batch 620 \t Training Loss: 29.05915381216234\n",
      "Epoch 100 \t Batch 640 \t Training Loss: 29.04286550283432\n",
      "Epoch 100 \t Batch 660 \t Training Loss: 29.053470756068375\n",
      "Epoch 100 \t Batch 680 \t Training Loss: 29.069240609337303\n",
      "Epoch 100 \t Batch 700 \t Training Loss: 29.07920038495745\n",
      "Epoch 100 \t Batch 720 \t Training Loss: 29.09284295241038\n",
      "Epoch 100 \t Batch 740 \t Training Loss: 29.136705829001762\n",
      "Epoch 100 \t Batch 760 \t Training Loss: 29.156013772362158\n",
      "Epoch 100 \t Batch 780 \t Training Loss: 29.14443842814519\n",
      "Epoch 100 \t Batch 800 \t Training Loss: 29.15170924901962\n",
      "Epoch 100 \t Batch 820 \t Training Loss: 29.163059316030363\n",
      "Epoch 100 \t Batch 840 \t Training Loss: 29.179980741228377\n",
      "Epoch 100 \t Batch 860 \t Training Loss: 29.18704992116884\n",
      "Epoch 100 \t Batch 880 \t Training Loss: 29.162279445474798\n",
      "Epoch 100 \t Batch 900 \t Training Loss: 29.17931539323595\n",
      "Epoch 100 \t Batch 20 \t Validation Loss: 9.237409710884094\n",
      "Epoch 100 \t Batch 40 \t Validation Loss: 14.003119623661041\n",
      "Epoch 100 \t Batch 60 \t Validation Loss: 13.42039134502411\n",
      "Epoch 100 \t Batch 80 \t Validation Loss: 14.69784420132637\n",
      "Epoch 100 \t Batch 100 \t Validation Loss: 18.2148961687088\n",
      "Epoch 100 \t Batch 120 \t Validation Loss: 20.87577359278997\n",
      "Epoch 100 \t Batch 140 \t Validation Loss: 22.29446795667921\n",
      "Epoch 100 \t Batch 160 \t Validation Loss: 24.69009599983692\n",
      "Epoch 100 \t Batch 180 \t Validation Loss: 27.62332202328576\n",
      "Epoch 100 \t Batch 200 \t Validation Loss: 29.32465508699417\n",
      "Epoch 100 \t Batch 220 \t Validation Loss: 30.532722197879444\n",
      "Epoch 100 \t Batch 240 \t Validation Loss: 30.991160116593043\n",
      "Epoch 100 \t Batch 260 \t Validation Loss: 33.173524344884434\n",
      "Epoch 100 \t Batch 280 \t Validation Loss: 34.41850747551237\n",
      "Epoch 100 \t Batch 300 \t Validation Loss: 35.22041756470998\n",
      "Epoch 100 \t Batch 320 \t Validation Loss: 35.689892353117465\n",
      "Epoch 100 \t Batch 340 \t Validation Loss: 35.8520589057137\n",
      "Epoch 100 \t Batch 360 \t Validation Loss: 35.83329298363792\n",
      "Epoch 100 \t Batch 380 \t Validation Loss: 36.40469525111349\n",
      "Epoch 100 \t Batch 400 \t Validation Loss: 36.42026661992073\n",
      "Epoch 100 \t Batch 420 \t Validation Loss: 36.71547624156589\n",
      "Epoch 100 \t Batch 440 \t Validation Loss: 36.78777116537094\n",
      "Epoch 100 \t Batch 460 \t Validation Loss: 37.47620592635611\n",
      "Epoch 100 \t Batch 480 \t Validation Loss: 38.152343520522116\n",
      "Epoch 100 \t Batch 500 \t Validation Loss: 38.014888382911685\n",
      "Epoch 100 \t Batch 520 \t Validation Loss: 38.295252984303694\n",
      "Epoch 100 \t Batch 540 \t Validation Loss: 38.23746720861506\n",
      "Epoch 100 \t Batch 560 \t Validation Loss: 38.253638190031054\n",
      "Epoch 100 \t Batch 580 \t Validation Loss: 38.21069412642512\n",
      "Epoch 100 \t Batch 600 \t Validation Loss: 38.59433772802353\n",
      "Epoch 100 Training Loss: 29.17902000285807 Validation Loss: 39.4061815963163\n",
      "Epoch 100 completed\n"
     ]
    }
   ],
   "source": [
    "model = BiggerFCN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data/data_no_outliers', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data/data_no_outliers', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'bigger_model1.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "del model\n",
    "del optimizer\n",
    "del criterion\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining bigger FCN pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiggerFCN_pyramid(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (32, 64, 128, 256, 512, 1024, 512, 256, 128, 64),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(BiggerFCN_pyramid, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.conv_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Batch 50 \t Training Loss: 93.40515243530274\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 74.78540946960449\n",
      "Epoch 1 \t Batch 150 \t Training Loss: 68.18138196309407\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 64.18197254180909\n",
      "Epoch 1 \t Batch 250 \t Training Loss: 61.46015383911133\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 59.868708305358886\n",
      "Epoch 1 \t Batch 350 \t Training Loss: 58.70942961556571\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 58.04661428451538\n",
      "Epoch 1 \t Batch 450 \t Training Loss: 57.37459713406033\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 56.78074750518799\n",
      "Epoch 1 \t Batch 550 \t Training Loss: 56.322076644897464\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 55.92796649297078\n",
      "Epoch 1 \t Batch 650 \t Training Loss: 55.68372137803298\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 55.35711478642055\n",
      "Epoch 1 \t Batch 750 \t Training Loss: 55.001108912150066\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 54.70549154281616\n",
      "Epoch 1 \t Batch 850 \t Training Loss: 54.48975201775046\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 54.254909400939944\n",
      "Epoch 1 \t Batch 950 \t Training Loss: 54.05523837440892\n",
      "Epoch 1 \t Batch 1000 \t Training Loss: 53.887276634216306\n",
      "Epoch 1 \t Batch 1050 \t Training Loss: 53.6528976549421\n",
      "Epoch 1 \t Batch 1100 \t Training Loss: 53.522891408746894\n",
      "Epoch 1 \t Batch 1150 \t Training Loss: 53.28752286164657\n",
      "Epoch 1 \t Batch 1200 \t Training Loss: 53.122172635396325\n",
      "Epoch 1 \t Batch 1250 \t Training Loss: 52.98377886657715\n",
      "Epoch 1 \t Batch 1300 \t Training Loss: 52.921556352468635\n",
      "Epoch 1 \t Batch 1350 \t Training Loss: 52.80302176581489\n",
      "Epoch 1 \t Batch 1400 \t Training Loss: 52.737274510519846\n",
      "Epoch 1 \t Batch 1450 \t Training Loss: 52.63266988293878\n",
      "Epoch 1 \t Batch 1500 \t Training Loss: 52.61463165283203\n",
      "Epoch 1 \t Batch 1550 \t Training Loss: 52.571716217533236\n",
      "Epoch 1 \t Batch 1600 \t Training Loss: 52.541396479606625\n",
      "Epoch 1 \t Batch 1650 \t Training Loss: 52.50752118659742\n",
      "Epoch 1 \t Batch 1700 \t Training Loss: 52.41098004060633\n",
      "Epoch 1 \t Batch 1750 \t Training Loss: 52.34342631966727\n",
      "Epoch 1 \t Batch 1800 \t Training Loss: 52.2646198548211\n",
      "Epoch 1 \t Batch 1850 \t Training Loss: 52.24720888498667\n",
      "Epoch 1 \t Batch 1900 \t Training Loss: 52.18866980703253\n",
      "Epoch 1 \t Batch 1950 \t Training Loss: 52.1556092599722\n",
      "Epoch 1 \t Batch 2000 \t Training Loss: 52.039817178726196\n",
      "Epoch 1 \t Batch 2050 \t Training Loss: 51.957247467041014\n",
      "Epoch 1 \t Batch 2100 \t Training Loss: 51.91311411721366\n",
      "Epoch 1 \t Batch 2150 \t Training Loss: 51.85195350646973\n",
      "Epoch 1 \t Batch 2200 \t Training Loss: 51.80614816145464\n",
      "Epoch 1 \t Batch 2250 \t Training Loss: 51.75272367180718\n",
      "Epoch 1 \t Batch 2300 \t Training Loss: 51.70305899412735\n",
      "Epoch 1 \t Batch 2350 \t Training Loss: 51.7001516382745\n",
      "Epoch 1 \t Batch 2400 \t Training Loss: 51.670165132681525\n",
      "Epoch 1 \t Batch 2450 \t Training Loss: 51.6276855710088\n",
      "Epoch 1 \t Batch 2500 \t Training Loss: 51.56460018844604\n",
      "Epoch 1 \t Batch 2550 \t Training Loss: 51.4970555675731\n",
      "Epoch 1 \t Batch 2600 \t Training Loss: 51.43796624623812\n",
      "Epoch 1 \t Batch 2650 \t Training Loss: 51.418284971848976\n",
      "Epoch 1 \t Batch 2700 \t Training Loss: 51.369671488867866\n",
      "Epoch 1 \t Batch 2750 \t Training Loss: 51.32012532251532\n",
      "Epoch 1 \t Batch 2800 \t Training Loss: 51.28711129256657\n",
      "Epoch 1 \t Batch 2850 \t Training Loss: 51.25036985899273\n",
      "Epoch 1 \t Batch 2900 \t Training Loss: 51.17513389521632\n",
      "Epoch 1 \t Batch 2950 \t Training Loss: 51.1299672472679\n",
      "Epoch 1 \t Batch 3000 \t Training Loss: 51.09663390541077\n",
      "Epoch 1 \t Batch 3050 \t Training Loss: 51.05698748416588\n",
      "Epoch 1 \t Batch 3100 \t Training Loss: 51.00631645264164\n",
      "Epoch 1 \t Batch 3150 \t Training Loss: 50.98362997569735\n",
      "Epoch 1 \t Batch 3200 \t Training Loss: 50.9470761436224\n",
      "Epoch 1 \t Batch 3250 \t Training Loss: 50.91224263763428\n",
      "Epoch 1 \t Batch 3300 \t Training Loss: 50.86750641736117\n",
      "Epoch 1 \t Batch 3350 \t Training Loss: 50.81276229118233\n",
      "Epoch 1 \t Batch 3400 \t Training Loss: 50.80008166986353\n",
      "Epoch 1 \t Batch 3450 \t Training Loss: 50.78905675086422\n",
      "Epoch 1 \t Batch 3500 \t Training Loss: 50.76283225631714\n",
      "Epoch 1 \t Batch 3550 \t Training Loss: 50.72789667639934\n",
      "Epoch 1 \t Batch 3600 \t Training Loss: 50.6798674164878\n",
      "Epoch 1 \t Batch 3650 \t Training Loss: 50.62488343173511\n",
      "Epoch 1 \t Batch 50 \t Validation Loss: 17.604797344207764\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 20.121028203964233\n",
      "Epoch 1 \t Batch 150 \t Validation Loss: 18.974962673187257\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 18.350405433177947\n",
      "Epoch 1 \t Batch 250 \t Validation Loss: 19.786328863143922\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 19.46418256441752\n",
      "Epoch 1 \t Batch 350 \t Validation Loss: 20.929634189605714\n",
      "Epoch 1 \t Batch 400 \t Validation Loss: 21.456647012233734\n",
      "Epoch 1 \t Batch 450 \t Validation Loss: 22.76209277258979\n",
      "Epoch 1 \t Batch 500 \t Validation Loss: 23.14925138092041\n",
      "Epoch 1 \t Batch 550 \t Validation Loss: 23.359154127294367\n",
      "Epoch 1 \t Batch 600 \t Validation Loss: 24.565498070716856\n",
      "Epoch 1 \t Batch 650 \t Validation Loss: 26.76888487155621\n",
      "Epoch 1 \t Batch 700 \t Validation Loss: 29.448066047940934\n",
      "Epoch 1 \t Batch 750 \t Validation Loss: 31.160469182332356\n",
      "Epoch 1 \t Batch 800 \t Validation Loss: 32.79489908337593\n",
      "Epoch 1 \t Batch 850 \t Validation Loss: 34.31251230688656\n",
      "Epoch 1 \t Batch 900 \t Validation Loss: 35.044188788731894\n",
      "Epoch 1 \t Batch 950 \t Validation Loss: 35.47770157563059\n",
      "Epoch 1 \t Batch 1000 \t Validation Loss: 37.45481083774567\n",
      "Epoch 1 \t Batch 1050 \t Validation Loss: 38.85820062682742\n",
      "Epoch 1 \t Batch 1100 \t Validation Loss: 39.87537093726071\n",
      "Epoch 1 \t Batch 1150 \t Validation Loss: 40.05481827279796\n",
      "Epoch 1 \t Batch 1200 \t Validation Loss: 41.047385934193926\n",
      "Epoch 1 \t Batch 1250 \t Validation Loss: 41.5599774105072\n",
      "Epoch 1 \t Batch 1300 \t Validation Loss: 41.80084105675037\n",
      "Epoch 1 \t Batch 1350 \t Validation Loss: 41.578817478815715\n",
      "Epoch 1 \t Batch 1400 \t Validation Loss: 41.508711614268165\n",
      "Epoch 1 \t Batch 1450 \t Validation Loss: 41.41292663409792\n",
      "Epoch 1 \t Batch 1500 \t Validation Loss: 41.77004805660248\n",
      "Epoch 1 \t Batch 1550 \t Validation Loss: 41.39316735821386\n",
      "Epoch 1 \t Batch 1600 \t Validation Loss: 41.10198357433081\n",
      "Epoch 1 \t Batch 1650 \t Validation Loss: 41.068744535446164\n",
      "Epoch 1 \t Batch 1700 \t Validation Loss: 40.8388499285193\n",
      "Epoch 1 \t Batch 1750 \t Validation Loss: 40.57670497322083\n",
      "Epoch 1 \t Batch 1800 \t Validation Loss: 40.35343743350771\n",
      "Epoch 1 \t Batch 1850 \t Validation Loss: 40.81716297484733\n",
      "Epoch 1 \t Batch 1900 \t Validation Loss: 41.041682623060126\n",
      "Epoch 1 \t Batch 1950 \t Validation Loss: 40.89152121446072\n",
      "Epoch 1 \t Batch 2000 \t Validation Loss: 40.731321826219556\n",
      "Epoch 1 \t Batch 2050 \t Validation Loss: 40.56289370397242\n",
      "Epoch 1 \t Batch 2100 \t Validation Loss: 40.401283180827186\n",
      "Epoch 1 \t Batch 2150 \t Validation Loss: 40.31737867776738\n",
      "Epoch 1 \t Batch 2200 \t Validation Loss: 40.19985921014439\n",
      "Epoch 1 \t Batch 2250 \t Validation Loss: 40.12269194391039\n",
      "Epoch 1 \t Batch 2300 \t Validation Loss: 40.00606111609417\n",
      "Epoch 1 \t Batch 2350 \t Validation Loss: 40.085268344067515\n",
      "Epoch 1 \t Batch 2400 \t Validation Loss: 40.189870778322216\n",
      "Epoch 1 \t Batch 2450 \t Validation Loss: 40.5825455009694\n",
      "Epoch 1 Training Loss: 50.6065969649162 Validation Loss: 40.7162654742405\n",
      "Validation Loss Decreased(inf--->100324.8781285286) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 50 \t Training Loss: 48.8255419921875\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 47.95237976074219\n",
      "Epoch 2 \t Batch 150 \t Training Loss: 48.41108571370443\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 48.35641073226929\n",
      "Epoch 2 \t Batch 250 \t Training Loss: 48.2243395614624\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 48.75330011367798\n",
      "Epoch 2 \t Batch 350 \t Training Loss: 48.893672414507186\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 48.900860781669614\n",
      "Epoch 2 \t Batch 450 \t Training Loss: 48.75393461015489\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 48.62566527175903\n",
      "Epoch 2 \t Batch 550 \t Training Loss: 48.66230949055065\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 48.523928543726605\n",
      "Epoch 2 \t Batch 650 \t Training Loss: 48.59379697066087\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 48.60860780170986\n",
      "Epoch 2 \t Batch 750 \t Training Loss: 48.59679813893636\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 48.487339823246\n",
      "Epoch 2 \t Batch 850 \t Training Loss: 48.49778203852036\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 48.48101882722643\n",
      "Epoch 2 \t Batch 950 \t Training Loss: 48.387599107842696\n",
      "Epoch 2 \t Batch 1000 \t Training Loss: 48.39380829811096\n",
      "Epoch 2 \t Batch 1050 \t Training Loss: 48.44850488026937\n",
      "Epoch 2 \t Batch 1100 \t Training Loss: 48.46226578278975\n",
      "Epoch 2 \t Batch 1150 \t Training Loss: 48.42336805758269\n",
      "Epoch 2 \t Batch 1200 \t Training Loss: 48.40763060092926\n",
      "Epoch 2 \t Batch 1250 \t Training Loss: 48.44434080657959\n",
      "Epoch 2 \t Batch 1300 \t Training Loss: 48.44700611554659\n",
      "Epoch 2 \t Batch 1350 \t Training Loss: 48.35619976891412\n",
      "Epoch 2 \t Batch 1400 \t Training Loss: 48.39517077173505\n",
      "Epoch 2 \t Batch 1450 \t Training Loss: 48.41689803945607\n",
      "Epoch 2 \t Batch 1500 \t Training Loss: 48.44228535588582\n",
      "Epoch 2 \t Batch 1550 \t Training Loss: 48.44600738648445\n",
      "Epoch 2 \t Batch 1600 \t Training Loss: 48.44969823718071\n",
      "Epoch 2 \t Batch 1650 \t Training Loss: 48.44350761066784\n",
      "Epoch 2 \t Batch 1700 \t Training Loss: 48.472258856156294\n",
      "Epoch 2 \t Batch 1750 \t Training Loss: 48.498979497637066\n",
      "Epoch 2 \t Batch 1800 \t Training Loss: 48.552305714289346\n",
      "Epoch 2 \t Batch 1850 \t Training Loss: 48.49224238936966\n",
      "Epoch 2 \t Batch 1900 \t Training Loss: 48.478637746509754\n",
      "Epoch 2 \t Batch 1950 \t Training Loss: 48.4432355372111\n",
      "Epoch 2 \t Batch 2000 \t Training Loss: 48.449509629249576\n",
      "Epoch 2 \t Batch 2050 \t Training Loss: 48.4263875114627\n",
      "Epoch 2 \t Batch 2100 \t Training Loss: 48.43009909493583\n",
      "Epoch 2 \t Batch 2150 \t Training Loss: 48.40936630071596\n",
      "Epoch 2 \t Batch 2200 \t Training Loss: 48.42507551019842\n",
      "Epoch 2 \t Batch 2250 \t Training Loss: 48.373584465874565\n",
      "Epoch 2 \t Batch 2300 \t Training Loss: 48.36463211391283\n",
      "Epoch 2 \t Batch 2350 \t Training Loss: 48.33412891793758\n",
      "Epoch 2 \t Batch 2400 \t Training Loss: 48.326358688672386\n",
      "Epoch 2 \t Batch 2450 \t Training Loss: 48.33025781592544\n",
      "Epoch 2 \t Batch 2500 \t Training Loss: 48.294295824432375\n",
      "Epoch 2 \t Batch 2550 \t Training Loss: 48.24385741963106\n",
      "Epoch 2 \t Batch 2600 \t Training Loss: 48.20239312685453\n",
      "Epoch 2 \t Batch 2650 \t Training Loss: 48.185645916057084\n",
      "Epoch 2 \t Batch 2700 \t Training Loss: 48.18135611216227\n",
      "Epoch 2 \t Batch 2750 \t Training Loss: 48.167085205771706\n",
      "Epoch 2 \t Batch 2800 \t Training Loss: 48.19620006765638\n",
      "Epoch 2 \t Batch 2850 \t Training Loss: 48.159984964069565\n",
      "Epoch 2 \t Batch 2900 \t Training Loss: 48.16391142549186\n",
      "Epoch 2 \t Batch 2950 \t Training Loss: 48.14532943790242\n",
      "Epoch 2 \t Batch 3000 \t Training Loss: 48.14615876197815\n",
      "Epoch 2 \t Batch 3050 \t Training Loss: 48.14571591299088\n",
      "Epoch 2 \t Batch 3100 \t Training Loss: 48.12482146416941\n",
      "Epoch 2 \t Batch 3150 \t Training Loss: 48.10532820141505\n",
      "Epoch 2 \t Batch 3200 \t Training Loss: 48.0925116878748\n",
      "Epoch 2 \t Batch 3250 \t Training Loss: 48.080013509310206\n",
      "Epoch 2 \t Batch 3300 \t Training Loss: 48.07043316638831\n",
      "Epoch 2 \t Batch 3350 \t Training Loss: 48.09691349826642\n",
      "Epoch 2 \t Batch 3400 \t Training Loss: 48.0564710594626\n",
      "Epoch 2 \t Batch 3450 \t Training Loss: 48.05076485785885\n",
      "Epoch 2 \t Batch 3500 \t Training Loss: 48.02563176073347\n",
      "Epoch 2 \t Batch 3550 \t Training Loss: 48.04199484166965\n",
      "Epoch 2 \t Batch 3600 \t Training Loss: 48.023163465393914\n",
      "Epoch 2 \t Batch 3650 \t Training Loss: 48.01198945816249\n",
      "Epoch 2 \t Batch 50 \t Validation Loss: 16.960568418502806\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 20.413950877189635\n",
      "Epoch 2 \t Batch 150 \t Validation Loss: 18.885547234217327\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 18.347154440879823\n",
      "Epoch 2 \t Batch 250 \t Validation Loss: 19.506028060913085\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 18.80744815826416\n",
      "Epoch 2 \t Batch 350 \t Validation Loss: 20.531198261805944\n",
      "Epoch 2 \t Batch 400 \t Validation Loss: 21.17618036031723\n",
      "Epoch 2 \t Batch 450 \t Validation Loss: 22.54347077475654\n",
      "Epoch 2 \t Batch 500 \t Validation Loss: 22.89671830177307\n",
      "Epoch 2 \t Batch 550 \t Validation Loss: 23.090159322565253\n",
      "Epoch 2 \t Batch 600 \t Validation Loss: 23.99002339839935\n",
      "Epoch 2 \t Batch 650 \t Validation Loss: 25.761926461733303\n",
      "Epoch 2 \t Batch 700 \t Validation Loss: 27.891415815353394\n",
      "Epoch 2 \t Batch 750 \t Validation Loss: 29.325473369598388\n",
      "Epoch 2 \t Batch 800 \t Validation Loss: 30.792334996461868\n",
      "Epoch 2 \t Batch 850 \t Validation Loss: 31.79964833876666\n",
      "Epoch 2 \t Batch 900 \t Validation Loss: 32.11736465242174\n",
      "Epoch 2 \t Batch 950 \t Validation Loss: 32.35798899399607\n",
      "Epoch 2 \t Batch 1000 \t Validation Loss: 34.04679172134399\n",
      "Epoch 2 \t Batch 1050 \t Validation Loss: 35.208530080204916\n",
      "Epoch 2 \t Batch 1100 \t Validation Loss: 36.05477093631571\n",
      "Epoch 2 \t Batch 1150 \t Validation Loss: 36.1903728864504\n",
      "Epoch 2 \t Batch 1200 \t Validation Loss: 37.031541367967925\n",
      "Epoch 2 \t Batch 1250 \t Validation Loss: 37.37307454395294\n",
      "Epoch 2 \t Batch 1300 \t Validation Loss: 37.63644493414805\n",
      "Epoch 2 \t Batch 1350 \t Validation Loss: 37.500451057045545\n",
      "Epoch 2 \t Batch 1400 \t Validation Loss: 37.412017917803354\n",
      "Epoch 2 \t Batch 1450 \t Validation Loss: 37.326855336057726\n",
      "Epoch 2 \t Batch 1500 \t Validation Loss: 37.556569264729816\n",
      "Epoch 2 \t Batch 1550 \t Validation Loss: 37.340119580914894\n",
      "Epoch 2 \t Batch 1600 \t Validation Loss: 37.178556819558146\n",
      "Epoch 2 \t Batch 1650 \t Validation Loss: 37.24339995008526\n",
      "Epoch 2 \t Batch 1700 \t Validation Loss: 37.184852901346545\n",
      "Epoch 2 \t Batch 1750 \t Validation Loss: 37.05921240343366\n",
      "Epoch 2 \t Batch 1800 \t Validation Loss: 37.01388536771139\n",
      "Epoch 2 \t Batch 1850 \t Validation Loss: 37.55964556513606\n",
      "Epoch 2 \t Batch 1900 \t Validation Loss: 37.821717143811675\n",
      "Epoch 2 \t Batch 1950 \t Validation Loss: 37.70739625148284\n",
      "Epoch 2 \t Batch 2000 \t Validation Loss: 37.611000324726106\n",
      "Epoch 2 \t Batch 2050 \t Validation Loss: 37.581400700080685\n",
      "Epoch 2 \t Batch 2100 \t Validation Loss: 37.46552020890372\n",
      "Epoch 2 \t Batch 2150 \t Validation Loss: 37.308066268743474\n",
      "Epoch 2 \t Batch 2200 \t Validation Loss: 37.14661871129816\n",
      "Epoch 2 \t Batch 2250 \t Validation Loss: 37.028465376536055\n",
      "Epoch 2 \t Batch 2300 \t Validation Loss: 36.851229634906936\n",
      "Epoch 2 \t Batch 2350 \t Validation Loss: 36.84863687068858\n",
      "Epoch 2 \t Batch 2400 \t Validation Loss: 36.94710788091024\n",
      "Epoch 2 \t Batch 2450 \t Validation Loss: 37.2979328392963\n",
      "Epoch 2 Training Loss: 48.01341719975778 Validation Loss: 37.4423808157444\n",
      "Validation Loss Decreased(100324.8781285286--->92258.0263299942) Saving The Model\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 50 \t Training Loss: 46.82738121032715\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 46.20737724304199\n",
      "Epoch 3 \t Batch 150 \t Training Loss: 46.74354601542155\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 46.97314065933227\n",
      "Epoch 3 \t Batch 250 \t Training Loss: 47.43228440856934\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 47.47651186625163\n",
      "Epoch 3 \t Batch 350 \t Training Loss: 47.526460680280415\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 47.364394950866696\n",
      "Epoch 3 \t Batch 450 \t Training Loss: 47.319086146884494\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 47.3650348777771\n",
      "Epoch 3 \t Batch 550 \t Training Loss: 47.34622762159868\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 47.26449198087057\n",
      "Epoch 3 \t Batch 650 \t Training Loss: 47.314476749713606\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 47.14888746806553\n",
      "Epoch 3 \t Batch 750 \t Training Loss: 47.10598350270589\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 47.117059223651886\n",
      "Epoch 3 \t Batch 850 \t Training Loss: 47.192276198443246\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 47.23583667755127\n",
      "Epoch 3 \t Batch 950 \t Training Loss: 47.141529677541634\n",
      "Epoch 3 \t Batch 1000 \t Training Loss: 47.07440504455566\n",
      "Epoch 3 \t Batch 1050 \t Training Loss: 47.09619455246698\n",
      "Epoch 3 \t Batch 1100 \t Training Loss: 47.13286127957431\n",
      "Epoch 3 \t Batch 1150 \t Training Loss: 47.12936596580174\n",
      "Epoch 3 \t Batch 1200 \t Training Loss: 47.211665193239845\n",
      "Epoch 3 \t Batch 1250 \t Training Loss: 47.2359696182251\n",
      "Epoch 3 \t Batch 1300 \t Training Loss: 47.314030255537766\n",
      "Epoch 3 \t Batch 1350 \t Training Loss: 47.3967481217561\n",
      "Epoch 3 \t Batch 1400 \t Training Loss: 47.39967509542193\n",
      "Epoch 3 \t Batch 1450 \t Training Loss: 47.439157624080266\n",
      "Epoch 3 \t Batch 1500 \t Training Loss: 47.36867653401693\n",
      "Epoch 3 \t Batch 1550 \t Training Loss: 47.38862392056373\n",
      "Epoch 3 \t Batch 1600 \t Training Loss: 47.416085662841795\n",
      "Epoch 3 \t Batch 1650 \t Training Loss: 47.47035180294152\n",
      "Epoch 3 \t Batch 1700 \t Training Loss: 47.4473959238389\n",
      "Epoch 3 \t Batch 1750 \t Training Loss: 47.36626336451939\n",
      "Epoch 3 \t Batch 1800 \t Training Loss: 47.392775734795464\n",
      "Epoch 3 \t Batch 1850 \t Training Loss: 47.39304959271405\n",
      "Epoch 3 \t Batch 1900 \t Training Loss: 47.378968592191995\n",
      "Epoch 3 \t Batch 1950 \t Training Loss: 47.3733461243067\n",
      "Epoch 3 \t Batch 2000 \t Training Loss: 47.38517109584809\n",
      "Epoch 3 \t Batch 2050 \t Training Loss: 47.36812744047584\n",
      "Epoch 3 \t Batch 2100 \t Training Loss: 47.39390698296683\n",
      "Epoch 3 \t Batch 2150 \t Training Loss: 47.38441307156585\n",
      "Epoch 3 \t Batch 2200 \t Training Loss: 47.342526494806464\n",
      "Epoch 3 \t Batch 2250 \t Training Loss: 47.31330515797933\n",
      "Epoch 3 \t Batch 2300 \t Training Loss: 47.28003679441369\n",
      "Epoch 3 \t Batch 2350 \t Training Loss: 47.26312264949718\n",
      "Epoch 3 \t Batch 2400 \t Training Loss: 47.27110698541006\n",
      "Epoch 3 \t Batch 2450 \t Training Loss: 47.304790574677135\n",
      "Epoch 3 \t Batch 2500 \t Training Loss: 47.28235644989014\n",
      "Epoch 3 \t Batch 2550 \t Training Loss: 47.26787068236108\n",
      "Epoch 3 \t Batch 2600 \t Training Loss: 47.295062568371115\n",
      "Epoch 3 \t Batch 2650 \t Training Loss: 47.27342083841\n",
      "Epoch 3 \t Batch 2700 \t Training Loss: 47.274506419146505\n",
      "Epoch 3 \t Batch 2750 \t Training Loss: 47.27528052520752\n",
      "Epoch 3 \t Batch 2800 \t Training Loss: 47.315921583856856\n",
      "Epoch 3 \t Batch 2850 \t Training Loss: 47.30433680417245\n",
      "Epoch 3 \t Batch 2900 \t Training Loss: 47.29530963305769\n",
      "Epoch 3 \t Batch 2950 \t Training Loss: 47.282459574553926\n",
      "Epoch 3 \t Batch 3000 \t Training Loss: 47.26024733734131\n",
      "Epoch 3 \t Batch 3050 \t Training Loss: 47.263239510958314\n",
      "Epoch 3 \t Batch 3100 \t Training Loss: 47.28530729232296\n",
      "Epoch 3 \t Batch 3150 \t Training Loss: 47.305591784280445\n",
      "Epoch 3 \t Batch 3200 \t Training Loss: 47.28552502512932\n",
      "Epoch 3 \t Batch 3250 \t Training Loss: 47.2607096287654\n",
      "Epoch 3 \t Batch 3300 \t Training Loss: 47.25638270060222\n",
      "Epoch 3 \t Batch 3350 \t Training Loss: 47.248288257655815\n",
      "Epoch 3 \t Batch 3400 \t Training Loss: 47.26198659223669\n",
      "Epoch 3 \t Batch 3450 \t Training Loss: 47.24006342238274\n",
      "Epoch 3 \t Batch 3500 \t Training Loss: 47.210370567321775\n",
      "Epoch 3 \t Batch 3550 \t Training Loss: 47.23020976751623\n",
      "Epoch 3 \t Batch 3600 \t Training Loss: 47.22250435617235\n",
      "Epoch 3 \t Batch 3650 \t Training Loss: 47.217590390035554\n",
      "Epoch 3 \t Batch 50 \t Validation Loss: 12.381550970077514\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 14.826970658302308\n",
      "Epoch 3 \t Batch 150 \t Validation Loss: 13.840067841211955\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 13.546929430961608\n",
      "Epoch 3 \t Batch 250 \t Validation Loss: 14.590641668319702\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 14.264457103411356\n",
      "Epoch 3 \t Batch 350 \t Validation Loss: 16.373063329969135\n",
      "Epoch 3 \t Batch 400 \t Validation Loss: 17.376015336513518\n",
      "Epoch 3 \t Batch 450 \t Validation Loss: 18.99023117489285\n",
      "Epoch 3 \t Batch 500 \t Validation Loss: 19.648725503921508\n",
      "Epoch 3 \t Batch 550 \t Validation Loss: 20.05830079685558\n",
      "Epoch 3 \t Batch 600 \t Validation Loss: 21.13159362634023\n",
      "Epoch 3 \t Batch 650 \t Validation Loss: 22.96719573534452\n",
      "Epoch 3 \t Batch 700 \t Validation Loss: 25.227386128561836\n",
      "Epoch 3 \t Batch 750 \t Validation Loss: 26.662738803863526\n",
      "Epoch 3 \t Batch 800 \t Validation Loss: 27.97118004322052\n",
      "Epoch 3 \t Batch 850 \t Validation Loss: 29.118084623673383\n",
      "Epoch 3 \t Batch 900 \t Validation Loss: 29.5944415251414\n",
      "Epoch 3 \t Batch 950 \t Validation Loss: 29.97140624498066\n",
      "Epoch 3 \t Batch 1000 \t Validation Loss: 31.585184655189515\n",
      "Epoch 3 \t Batch 1050 \t Validation Loss: 32.7278403036935\n",
      "Epoch 3 \t Batch 1100 \t Validation Loss: 33.54171550924128\n",
      "Epoch 3 \t Batch 1150 \t Validation Loss: 33.77205748558045\n",
      "Epoch 3 \t Batch 1200 \t Validation Loss: 34.63817568500837\n",
      "Epoch 3 \t Batch 1250 \t Validation Loss: 35.07290860748291\n",
      "Epoch 3 \t Batch 1300 \t Validation Loss: 35.40655538265522\n",
      "Epoch 3 \t Batch 1350 \t Validation Loss: 35.317486962212456\n",
      "Epoch 3 \t Batch 1400 \t Validation Loss: 35.29511442048209\n",
      "Epoch 3 \t Batch 1450 \t Validation Loss: 35.20612009180003\n",
      "Epoch 3 \t Batch 1500 \t Validation Loss: 35.518419589678444\n",
      "Epoch 3 \t Batch 1550 \t Validation Loss: 35.340149234341034\n",
      "Epoch 3 \t Batch 1600 \t Validation Loss: 35.218719577491285\n",
      "Epoch 3 \t Batch 1650 \t Validation Loss: 35.310923923434636\n",
      "Epoch 3 \t Batch 1700 \t Validation Loss: 35.30292034738204\n",
      "Epoch 3 \t Batch 1750 \t Validation Loss: 35.17387401063102\n",
      "Epoch 3 \t Batch 1800 \t Validation Loss: 35.02945474015342\n",
      "Epoch 3 \t Batch 1850 \t Validation Loss: 35.62334375510345\n",
      "Epoch 3 \t Batch 1900 \t Validation Loss: 35.93952113277034\n",
      "Epoch 3 \t Batch 1950 \t Validation Loss: 35.86374315237388\n",
      "Epoch 3 \t Batch 2000 \t Validation Loss: 35.763902158498766\n",
      "Epoch 3 \t Batch 2050 \t Validation Loss: 35.63173202491388\n",
      "Epoch 3 \t Batch 2100 \t Validation Loss: 35.46181564671652\n",
      "Epoch 3 \t Batch 2150 \t Validation Loss: 35.37704397622929\n",
      "Epoch 3 \t Batch 2200 \t Validation Loss: 35.307702401117844\n",
      "Epoch 3 \t Batch 2250 \t Validation Loss: 35.21272353808085\n",
      "Epoch 3 \t Batch 2300 \t Validation Loss: 35.0366463248626\n",
      "Epoch 3 \t Batch 2350 \t Validation Loss: 35.06195968161238\n",
      "Epoch 3 \t Batch 2400 \t Validation Loss: 35.24404811640581\n",
      "Epoch 3 \t Batch 2450 \t Validation Loss: 35.58754091437982\n",
      "Epoch 3 Training Loss: 47.222983910967244 Validation Loss: 35.728260269993314\n",
      "Validation Loss Decreased(92258.0263299942--->88034.43330526352) Saving The Model\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 50 \t Training Loss: 48.910714340209964\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 47.80675296783447\n",
      "Epoch 4 \t Batch 150 \t Training Loss: 47.53447065989177\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 47.316736459732056\n",
      "Epoch 4 \t Batch 250 \t Training Loss: 46.95562368011475\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 46.73836971282959\n",
      "Epoch 4 \t Batch 350 \t Training Loss: 46.840614384242464\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 46.54478603363037\n",
      "Epoch 4 \t Batch 450 \t Training Loss: 46.44283804151747\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 46.4410193939209\n",
      "Epoch 4 \t Batch 550 \t Training Loss: 46.588764488913796\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 46.70622486114502\n",
      "Epoch 4 \t Batch 650 \t Training Loss: 46.58520776015062\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 46.51345116206578\n",
      "Epoch 4 \t Batch 750 \t Training Loss: 46.58016111246745\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 46.61962978363037\n",
      "Epoch 4 \t Batch 850 \t Training Loss: 46.649321333941295\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 46.69430383046468\n",
      "Epoch 4 \t Batch 950 \t Training Loss: 46.611909460770455\n",
      "Epoch 4 \t Batch 1000 \t Training Loss: 46.519035655975344\n",
      "Epoch 4 \t Batch 1050 \t Training Loss: 46.59170567830404\n",
      "Epoch 4 \t Batch 1100 \t Training Loss: 46.59706981312145\n",
      "Epoch 4 \t Batch 1150 \t Training Loss: 46.53937701349673\n",
      "Epoch 4 \t Batch 1200 \t Training Loss: 46.50441412607829\n",
      "Epoch 4 \t Batch 1250 \t Training Loss: 46.480846714782714\n",
      "Epoch 4 \t Batch 1300 \t Training Loss: 46.52846534729004\n",
      "Epoch 4 \t Batch 1350 \t Training Loss: 46.55482548466435\n",
      "Epoch 4 \t Batch 1400 \t Training Loss: 46.571475034441264\n",
      "Epoch 4 \t Batch 1450 \t Training Loss: 46.60092874198124\n",
      "Epoch 4 \t Batch 1500 \t Training Loss: 46.671360593159996\n",
      "Epoch 4 \t Batch 1550 \t Training Loss: 46.745095867033925\n",
      "Epoch 4 \t Batch 1600 \t Training Loss: 46.781074950695036\n",
      "Epoch 4 \t Batch 1650 \t Training Loss: 46.762717333706945\n",
      "Epoch 4 \t Batch 1700 \t Training Loss: 46.7029531770594\n",
      "Epoch 4 \t Batch 1750 \t Training Loss: 46.684770562308174\n",
      "Epoch 4 \t Batch 1800 \t Training Loss: 46.75166444566515\n",
      "Epoch 4 \t Batch 1850 \t Training Loss: 46.715731003220014\n",
      "Epoch 4 \t Batch 1900 \t Training Loss: 46.710892416301526\n",
      "Epoch 4 \t Batch 1950 \t Training Loss: 46.717621351388786\n",
      "Epoch 4 \t Batch 2000 \t Training Loss: 46.72852429485321\n",
      "Epoch 4 \t Batch 2050 \t Training Loss: 46.70760106435636\n",
      "Epoch 4 \t Batch 2100 \t Training Loss: 46.72820710954212\n",
      "Epoch 4 \t Batch 2150 \t Training Loss: 46.753996861480005\n",
      "Epoch 4 \t Batch 2200 \t Training Loss: 46.78844723528082\n",
      "Epoch 4 \t Batch 2250 \t Training Loss: 46.79260944196913\n",
      "Epoch 4 \t Batch 2300 \t Training Loss: 46.79953321788622\n",
      "Epoch 4 \t Batch 2350 \t Training Loss: 46.776253335019376\n",
      "Epoch 4 \t Batch 2400 \t Training Loss: 46.77150209585825\n",
      "Epoch 4 \t Batch 2450 \t Training Loss: 46.7669031836062\n",
      "Epoch 4 \t Batch 2500 \t Training Loss: 46.74587556304932\n",
      "Epoch 4 \t Batch 2550 \t Training Loss: 46.73865484349868\n",
      "Epoch 4 \t Batch 2600 \t Training Loss: 46.72975883630606\n",
      "Epoch 4 \t Batch 2650 \t Training Loss: 46.71823275584095\n",
      "Epoch 4 \t Batch 2700 \t Training Loss: 46.659099334434224\n",
      "Epoch 4 \t Batch 2750 \t Training Loss: 46.64106429498846\n",
      "Epoch 4 \t Batch 2800 \t Training Loss: 46.648560160228186\n",
      "Epoch 4 \t Batch 2850 \t Training Loss: 46.679145695201136\n",
      "Epoch 4 \t Batch 2900 \t Training Loss: 46.679779415788325\n",
      "Epoch 4 \t Batch 2950 \t Training Loss: 46.69785895654711\n",
      "Epoch 4 \t Batch 3000 \t Training Loss: 46.68348030980428\n",
      "Epoch 4 \t Batch 3050 \t Training Loss: 46.66537802649326\n",
      "Epoch 4 \t Batch 3100 \t Training Loss: 46.681673059771136\n",
      "Epoch 4 \t Batch 3150 \t Training Loss: 46.715038619268505\n",
      "Epoch 4 \t Batch 3200 \t Training Loss: 46.74445991516113\n",
      "Epoch 4 \t Batch 3250 \t Training Loss: 46.716727399385896\n",
      "Epoch 4 \t Batch 3300 \t Training Loss: 46.70757093429565\n",
      "Epoch 4 \t Batch 3350 \t Training Loss: 46.70063033431324\n",
      "Epoch 4 \t Batch 3400 \t Training Loss: 46.7005094685274\n",
      "Epoch 4 \t Batch 3450 \t Training Loss: 46.72701099119325\n",
      "Epoch 4 \t Batch 3500 \t Training Loss: 46.71621352277483\n",
      "Epoch 4 \t Batch 3550 \t Training Loss: 46.71181054021271\n",
      "Epoch 4 \t Batch 3600 \t Training Loss: 46.692161984443665\n",
      "Epoch 4 \t Batch 3650 \t Training Loss: 46.70024562156364\n",
      "Epoch 4 \t Batch 50 \t Validation Loss: 23.343321437835694\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 28.87709496498108\n",
      "Epoch 4 \t Batch 150 \t Validation Loss: 25.678470497131347\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 25.3380318069458\n",
      "Epoch 4 \t Batch 250 \t Validation Loss: 27.636813541412355\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 26.424235814412434\n",
      "Epoch 4 \t Batch 350 \t Validation Loss: 26.905296876089913\n",
      "Epoch 4 \t Batch 400 \t Validation Loss: 26.63624562263489\n",
      "Epoch 4 \t Batch 450 \t Validation Loss: 27.5724247317844\n",
      "Epoch 4 \t Batch 500 \t Validation Loss: 27.45522220802307\n",
      "Epoch 4 \t Batch 550 \t Validation Loss: 27.20672966696999\n",
      "Epoch 4 \t Batch 600 \t Validation Loss: 27.90230664730072\n",
      "Epoch 4 \t Batch 650 \t Validation Loss: 29.592263366992658\n",
      "Epoch 4 \t Batch 700 \t Validation Loss: 31.529785176685877\n",
      "Epoch 4 \t Batch 750 \t Validation Loss: 32.89838540013631\n",
      "Epoch 4 \t Batch 800 \t Validation Loss: 34.26468775868416\n",
      "Epoch 4 \t Batch 850 \t Validation Loss: 35.24731341642492\n",
      "Epoch 4 \t Batch 900 \t Validation Loss: 35.503745209376014\n",
      "Epoch 4 \t Batch 950 \t Validation Loss: 35.644021709843685\n",
      "Epoch 4 \t Batch 1000 \t Validation Loss: 37.33159472370148\n",
      "Epoch 4 \t Batch 1050 \t Validation Loss: 38.46838791075207\n",
      "Epoch 4 \t Batch 1100 \t Validation Loss: 39.309151311354206\n",
      "Epoch 4 \t Batch 1150 \t Validation Loss: 39.379853975876514\n",
      "Epoch 4 \t Batch 1200 \t Validation Loss: 40.118631432056425\n",
      "Epoch 4 \t Batch 1250 \t Validation Loss: 40.396960665130614\n",
      "Epoch 4 \t Batch 1300 \t Validation Loss: 40.657769647744985\n",
      "Epoch 4 \t Batch 1350 \t Validation Loss: 40.46095722198486\n",
      "Epoch 4 \t Batch 1400 \t Validation Loss: 40.329870944023135\n",
      "Epoch 4 \t Batch 1450 \t Validation Loss: 40.2965773431186\n",
      "Epoch 4 \t Batch 1500 \t Validation Loss: 40.52871953296661\n",
      "Epoch 4 \t Batch 1550 \t Validation Loss: 40.31407020538084\n",
      "Epoch 4 \t Batch 1600 \t Validation Loss: 40.1453675994277\n",
      "Epoch 4 \t Batch 1650 \t Validation Loss: 40.190873995405255\n",
      "Epoch 4 \t Batch 1700 \t Validation Loss: 40.17928244282218\n",
      "Epoch 4 \t Batch 1750 \t Validation Loss: 40.051627995354785\n",
      "Epoch 4 \t Batch 1800 \t Validation Loss: 39.990207223097485\n",
      "Epoch 4 \t Batch 1850 \t Validation Loss: 40.547966644957256\n",
      "Epoch 4 \t Batch 1900 \t Validation Loss: 40.79526459166878\n",
      "Epoch 4 \t Batch 1950 \t Validation Loss: 40.64907901201493\n",
      "Epoch 4 \t Batch 2000 \t Validation Loss: 40.54194065785408\n",
      "Epoch 4 \t Batch 2050 \t Validation Loss: 40.5734099553271\n",
      "Epoch 4 \t Batch 2100 \t Validation Loss: 40.41856419949305\n",
      "Epoch 4 \t Batch 2150 \t Validation Loss: 40.17855771153472\n",
      "Epoch 4 \t Batch 2200 \t Validation Loss: 39.94700359756296\n",
      "Epoch 4 \t Batch 2250 \t Validation Loss: 39.713663708368934\n",
      "Epoch 4 \t Batch 2300 \t Validation Loss: 39.368721820582515\n",
      "Epoch 4 \t Batch 2350 \t Validation Loss: 39.248584416977906\n",
      "Epoch 4 \t Batch 2400 \t Validation Loss: 39.31982903560003\n",
      "Epoch 4 \t Batch 2450 \t Validation Loss: 39.56853183259769\n",
      "Epoch 4 Training Loss: 46.714993291748776 Validation Loss: 39.691350801231025\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 50 \t Training Loss: 45.89309066772461\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 45.66738620758056\n",
      "Epoch 5 \t Batch 150 \t Training Loss: 45.813055750528974\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 45.82154897689819\n",
      "Epoch 5 \t Batch 250 \t Training Loss: 45.86363203430176\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 45.885780042012534\n",
      "Epoch 5 \t Batch 350 \t Training Loss: 46.097541656494144\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 46.30074746131897\n",
      "Epoch 5 \t Batch 450 \t Training Loss: 46.422003792656795\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 46.483174182891844\n",
      "Epoch 5 \t Batch 550 \t Training Loss: 46.45562209042636\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 46.44072806040446\n",
      "Epoch 5 \t Batch 650 \t Training Loss: 46.49024313119742\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 46.37285912377494\n",
      "Epoch 5 \t Batch 750 \t Training Loss: 46.25903966522217\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 46.25695994615555\n",
      "Epoch 5 \t Batch 850 \t Training Loss: 46.31084054610309\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 46.24491301006741\n",
      "Epoch 5 \t Batch 950 \t Training Loss: 46.25850410662199\n",
      "Epoch 5 \t Batch 1000 \t Training Loss: 46.27334576225281\n",
      "Epoch 5 \t Batch 1050 \t Training Loss: 46.288064417157855\n",
      "Epoch 5 \t Batch 1100 \t Training Loss: 46.33634547840465\n",
      "Epoch 5 \t Batch 1150 \t Training Loss: 46.33045266856318\n",
      "Epoch 5 \t Batch 1200 \t Training Loss: 46.40311920801798\n",
      "Epoch 5 \t Batch 1250 \t Training Loss: 46.41641301574707\n",
      "Epoch 5 \t Batch 1300 \t Training Loss: 46.39806136498085\n",
      "Epoch 5 \t Batch 1350 \t Training Loss: 46.43288859049479\n",
      "Epoch 5 \t Batch 1400 \t Training Loss: 46.360282299859186\n",
      "Epoch 5 \t Batch 1450 \t Training Loss: 46.36934649368812\n",
      "Epoch 5 \t Batch 1500 \t Training Loss: 46.43425198237101\n",
      "Epoch 5 \t Batch 1550 \t Training Loss: 46.44748959695139\n",
      "Epoch 5 \t Batch 1600 \t Training Loss: 46.43112248659134\n",
      "Epoch 5 \t Batch 1650 \t Training Loss: 46.42933362903017\n",
      "Epoch 5 \t Batch 1700 \t Training Loss: 46.445864014345055\n",
      "Epoch 5 \t Batch 1750 \t Training Loss: 46.481591248648506\n",
      "Epoch 5 \t Batch 1800 \t Training Loss: 46.487231401867334\n",
      "Epoch 5 \t Batch 1850 \t Training Loss: 46.429612912358465\n",
      "Epoch 5 \t Batch 1900 \t Training Loss: 46.413215919293854\n",
      "Epoch 5 \t Batch 1950 \t Training Loss: 46.35283042712089\n",
      "Epoch 5 \t Batch 2000 \t Training Loss: 46.37935664367676\n",
      "Epoch 5 \t Batch 2050 \t Training Loss: 46.40759654533572\n",
      "Epoch 5 \t Batch 2100 \t Training Loss: 46.411361941383\n",
      "Epoch 5 \t Batch 2150 \t Training Loss: 46.41585829978766\n",
      "Epoch 5 \t Batch 2200 \t Training Loss: 46.41242779124867\n",
      "Epoch 5 \t Batch 2250 \t Training Loss: 46.41394929843479\n",
      "Epoch 5 \t Batch 2300 \t Training Loss: 46.387193241119384\n",
      "Epoch 5 \t Batch 2350 \t Training Loss: 46.361469152734635\n",
      "Epoch 5 \t Batch 2400 \t Training Loss: 46.39172560771306\n",
      "Epoch 5 \t Batch 2450 \t Training Loss: 46.41101377603959\n",
      "Epoch 5 \t Batch 2500 \t Training Loss: 46.3421807182312\n",
      "Epoch 5 \t Batch 2550 \t Training Loss: 46.367661672105974\n",
      "Epoch 5 \t Batch 2600 \t Training Loss: 46.38017184991103\n",
      "Epoch 5 \t Batch 2650 \t Training Loss: 46.35654168326899\n",
      "Epoch 5 \t Batch 2700 \t Training Loss: 46.36470766985858\n",
      "Epoch 5 \t Batch 2750 \t Training Loss: 46.355523463162505\n",
      "Epoch 5 \t Batch 2800 \t Training Loss: 46.36267808709826\n",
      "Epoch 5 \t Batch 2850 \t Training Loss: 46.36353438728734\n",
      "Epoch 5 \t Batch 2900 \t Training Loss: 46.34518392036701\n",
      "Epoch 5 \t Batch 2950 \t Training Loss: 46.32785037865073\n",
      "Epoch 5 \t Batch 3000 \t Training Loss: 46.34248149172465\n",
      "Epoch 5 \t Batch 3050 \t Training Loss: 46.35681678146612\n",
      "Epoch 5 \t Batch 3100 \t Training Loss: 46.34441567328668\n",
      "Epoch 5 \t Batch 3150 \t Training Loss: 46.34281221177843\n",
      "Epoch 5 \t Batch 3200 \t Training Loss: 46.343523947000506\n",
      "Epoch 5 \t Batch 3250 \t Training Loss: 46.351629807692305\n",
      "Epoch 5 \t Batch 3300 \t Training Loss: 46.336496912638346\n",
      "Epoch 5 \t Batch 3350 \t Training Loss: 46.32584001455734\n",
      "Epoch 5 \t Batch 3400 \t Training Loss: 46.29992526110481\n",
      "Epoch 5 \t Batch 3450 \t Training Loss: 46.28011316050654\n",
      "Epoch 5 \t Batch 3500 \t Training Loss: 46.26890395900181\n",
      "Epoch 5 \t Batch 3550 \t Training Loss: 46.257730990127776\n",
      "Epoch 5 \t Batch 3600 \t Training Loss: 46.271515651279024\n",
      "Epoch 5 \t Batch 3650 \t Training Loss: 46.271645634272325\n",
      "Epoch 5 \t Batch 50 \t Validation Loss: 14.436369981765747\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 18.24052812099457\n",
      "Epoch 5 \t Batch 150 \t Validation Loss: 17.170410378774008\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 16.82074653148651\n",
      "Epoch 5 \t Batch 250 \t Validation Loss: 17.830552408218384\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 17.38168864885966\n",
      "Epoch 5 \t Batch 350 \t Validation Loss: 19.012725469044277\n",
      "Epoch 5 \t Batch 400 \t Validation Loss: 19.590551186800003\n",
      "Epoch 5 \t Batch 450 \t Validation Loss: 21.060557678010728\n",
      "Epoch 5 \t Batch 500 \t Validation Loss: 21.38178125476837\n",
      "Epoch 5 \t Batch 550 \t Validation Loss: 21.623044717095116\n",
      "Epoch 5 \t Batch 600 \t Validation Loss: 22.564424030780792\n",
      "Epoch 5 \t Batch 650 \t Validation Loss: 24.40258941283593\n",
      "Epoch 5 \t Batch 700 \t Validation Loss: 26.30726209436144\n",
      "Epoch 5 \t Batch 750 \t Validation Loss: 27.619591264724733\n",
      "Epoch 5 \t Batch 800 \t Validation Loss: 28.76147046148777\n",
      "Epoch 5 \t Batch 850 \t Validation Loss: 29.80201995569117\n",
      "Epoch 5 \t Batch 900 \t Validation Loss: 30.253535022205778\n",
      "Epoch 5 \t Batch 950 \t Validation Loss: 30.501664737400255\n",
      "Epoch 5 \t Batch 1000 \t Validation Loss: 32.188665078639985\n",
      "Epoch 5 \t Batch 1050 \t Validation Loss: 33.272252645946686\n",
      "Epoch 5 \t Batch 1100 \t Validation Loss: 34.08384170272134\n",
      "Epoch 5 \t Batch 1150 \t Validation Loss: 34.232957681572955\n",
      "Epoch 5 \t Batch 1200 \t Validation Loss: 34.947280135949455\n",
      "Epoch 5 \t Batch 1250 \t Validation Loss: 35.36682286529541\n",
      "Epoch 5 \t Batch 1300 \t Validation Loss: 35.66433438007648\n",
      "Epoch 5 \t Batch 1350 \t Validation Loss: 35.57663338908443\n",
      "Epoch 5 \t Batch 1400 \t Validation Loss: 35.51929233278547\n",
      "Epoch 5 \t Batch 1450 \t Validation Loss: 35.46782690048218\n",
      "Epoch 5 \t Batch 1500 \t Validation Loss: 35.72108529106776\n",
      "Epoch 5 \t Batch 1550 \t Validation Loss: 35.52971529453031\n",
      "Epoch 5 \t Batch 1600 \t Validation Loss: 35.37500332131982\n",
      "Epoch 5 \t Batch 1650 \t Validation Loss: 35.48124898924972\n",
      "Epoch 5 \t Batch 1700 \t Validation Loss: 35.40824160926482\n",
      "Epoch 5 \t Batch 1750 \t Validation Loss: 35.262165897505625\n",
      "Epoch 5 \t Batch 1800 \t Validation Loss: 35.271388118664426\n",
      "Epoch 5 \t Batch 1850 \t Validation Loss: 35.846505841564486\n",
      "Epoch 5 \t Batch 1900 \t Validation Loss: 36.138918990210485\n",
      "Epoch 5 \t Batch 1950 \t Validation Loss: 36.06601351823562\n",
      "Epoch 5 \t Batch 2000 \t Validation Loss: 36.053619238972665\n",
      "Epoch 5 \t Batch 2050 \t Validation Loss: 36.06840945976536\n",
      "Epoch 5 \t Batch 2100 \t Validation Loss: 35.94896570262455\n",
      "Epoch 5 \t Batch 2150 \t Validation Loss: 35.82454868726952\n",
      "Epoch 5 \t Batch 2200 \t Validation Loss: 35.71904504830187\n",
      "Epoch 5 \t Batch 2250 \t Validation Loss: 35.58890322547489\n",
      "Epoch 5 \t Batch 2300 \t Validation Loss: 35.358232420527415\n",
      "Epoch 5 \t Batch 2350 \t Validation Loss: 35.35766327340552\n",
      "Epoch 5 \t Batch 2400 \t Validation Loss: 35.524892746905486\n",
      "Epoch 5 \t Batch 2450 \t Validation Loss: 35.88969530991145\n",
      "Epoch 5 Training Loss: 46.300528036469075 Validation Loss: 36.102427402003244\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 50 \t Training Loss: 47.36103004455566\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 46.428396987915036\n",
      "Epoch 6 \t Batch 150 \t Training Loss: 46.25241486867269\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 45.91112225532532\n",
      "Epoch 6 \t Batch 250 \t Training Loss: 46.09746898651123\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 46.01116912206014\n",
      "Epoch 6 \t Batch 350 \t Training Loss: 46.02424819946289\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 45.93353311061859\n",
      "Epoch 6 \t Batch 450 \t Training Loss: 46.048012741936574\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 45.86525462341309\n",
      "Epoch 6 \t Batch 550 \t Training Loss: 45.9912747816606\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 45.954644406636554\n",
      "Epoch 6 \t Batch 650 \t Training Loss: 46.11735849820651\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 46.15152743203299\n",
      "Epoch 6 \t Batch 750 \t Training Loss: 46.15391053263346\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 46.279026679992675\n",
      "Epoch 6 \t Batch 850 \t Training Loss: 46.26256102842443\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 46.216215777926976\n",
      "Epoch 6 \t Batch 950 \t Training Loss: 46.28585843136436\n",
      "Epoch 6 \t Batch 1000 \t Training Loss: 46.23256616783142\n",
      "Epoch 6 \t Batch 1050 \t Training Loss: 46.17347847348168\n",
      "Epoch 6 \t Batch 1100 \t Training Loss: 46.24440768848766\n",
      "Epoch 6 \t Batch 1150 \t Training Loss: 46.22109509011973\n",
      "Epoch 6 \t Batch 1200 \t Training Loss: 46.18551999251048\n",
      "Epoch 6 \t Batch 1250 \t Training Loss: 46.223391746520996\n",
      "Epoch 6 \t Batch 1300 \t Training Loss: 46.28861605277428\n",
      "Epoch 6 \t Batch 1350 \t Training Loss: 46.25501763944273\n",
      "Epoch 6 \t Batch 1400 \t Training Loss: 46.25251319067819\n",
      "Epoch 6 \t Batch 1450 \t Training Loss: 46.15221861082932\n",
      "Epoch 6 \t Batch 1500 \t Training Loss: 46.1126070543925\n",
      "Epoch 6 \t Batch 1550 \t Training Loss: 46.07804083793394\n",
      "Epoch 6 \t Batch 1600 \t Training Loss: 46.04633285045624\n",
      "Epoch 6 \t Batch 1650 \t Training Loss: 46.023203356193775\n",
      "Epoch 6 \t Batch 1700 \t Training Loss: 46.000124286202826\n",
      "Epoch 6 \t Batch 1750 \t Training Loss: 46.007050931658064\n",
      "Epoch 6 \t Batch 1800 \t Training Loss: 46.065662127600774\n",
      "Epoch 6 \t Batch 1850 \t Training Loss: 46.11377842052563\n",
      "Epoch 6 \t Batch 1900 \t Training Loss: 46.11386600594771\n",
      "Epoch 6 \t Batch 1950 \t Training Loss: 46.07048667125213\n",
      "Epoch 6 \t Batch 2000 \t Training Loss: 46.10526746273041\n",
      "Epoch 6 \t Batch 2050 \t Training Loss: 46.108843044187964\n",
      "Epoch 6 \t Batch 2100 \t Training Loss: 46.11925415220715\n",
      "Epoch 6 \t Batch 2150 \t Training Loss: 46.063067431782564\n",
      "Epoch 6 \t Batch 2200 \t Training Loss: 46.080705195340244\n",
      "Epoch 6 \t Batch 2250 \t Training Loss: 46.048789057413735\n",
      "Epoch 6 \t Batch 2300 \t Training Loss: 46.057341665185014\n",
      "Epoch 6 \t Batch 2350 \t Training Loss: 46.05742202434134\n",
      "Epoch 6 \t Batch 2400 \t Training Loss: 46.03187061627706\n",
      "Epoch 6 \t Batch 2450 \t Training Loss: 45.99990515728386\n",
      "Epoch 6 \t Batch 2500 \t Training Loss: 45.986916483306885\n",
      "Epoch 6 \t Batch 2550 \t Training Loss: 45.990219857458975\n",
      "Epoch 6 \t Batch 2600 \t Training Loss: 46.01097954089825\n",
      "Epoch 6 \t Batch 2650 \t Training Loss: 46.041020235745414\n",
      "Epoch 6 \t Batch 2700 \t Training Loss: 46.055348760816784\n",
      "Epoch 6 \t Batch 2750 \t Training Loss: 46.074967984286225\n",
      "Epoch 6 \t Batch 2800 \t Training Loss: 46.06975287437439\n",
      "Epoch 6 \t Batch 2850 \t Training Loss: 46.08426851707593\n",
      "Epoch 6 \t Batch 2900 \t Training Loss: 46.085190511900805\n",
      "Epoch 6 \t Batch 2950 \t Training Loss: 46.09955046637584\n",
      "Epoch 6 \t Batch 3000 \t Training Loss: 46.06757048352559\n",
      "Epoch 6 \t Batch 3050 \t Training Loss: 46.024827483755644\n",
      "Epoch 6 \t Batch 3100 \t Training Loss: 46.01614425966817\n",
      "Epoch 6 \t Batch 3150 \t Training Loss: 45.992765349130785\n",
      "Epoch 6 \t Batch 3200 \t Training Loss: 45.98772489547729\n",
      "Epoch 6 \t Batch 3250 \t Training Loss: 45.99382330439641\n",
      "Epoch 6 \t Batch 3300 \t Training Loss: 46.0107622978904\n",
      "Epoch 6 \t Batch 3350 \t Training Loss: 46.008800424604274\n",
      "Epoch 6 \t Batch 3400 \t Training Loss: 46.008429954753204\n",
      "Epoch 6 \t Batch 3450 \t Training Loss: 46.00366899849712\n",
      "Epoch 6 \t Batch 3500 \t Training Loss: 45.99663095746722\n",
      "Epoch 6 \t Batch 3550 \t Training Loss: 45.99273650908135\n",
      "Epoch 6 \t Batch 3600 \t Training Loss: 45.98862526575724\n",
      "Epoch 6 \t Batch 3650 \t Training Loss: 45.98297919129672\n",
      "Epoch 6 \t Batch 50 \t Validation Loss: 21.838182640075683\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 26.61379207611084\n",
      "Epoch 6 \t Batch 150 \t Validation Loss: 23.203028818766278\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 22.812124876976014\n",
      "Epoch 6 \t Batch 250 \t Validation Loss: 25.322985580444335\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 24.198987747828166\n",
      "Epoch 6 \t Batch 350 \t Validation Loss: 25.0346089049748\n",
      "Epoch 6 \t Batch 400 \t Validation Loss: 25.034489303827286\n",
      "Epoch 6 \t Batch 450 \t Validation Loss: 25.81061885939704\n",
      "Epoch 6 \t Batch 500 \t Validation Loss: 25.757655640602113\n",
      "Epoch 6 \t Batch 550 \t Validation Loss: 25.670474521463568\n",
      "Epoch 6 \t Batch 600 \t Validation Loss: 26.462118707497915\n",
      "Epoch 6 \t Batch 650 \t Validation Loss: 28.232070346978993\n",
      "Epoch 6 \t Batch 700 \t Validation Loss: 30.150548579352243\n",
      "Epoch 6 \t Batch 750 \t Validation Loss: 31.390289150238036\n",
      "Epoch 6 \t Batch 800 \t Validation Loss: 32.56710834383964\n",
      "Epoch 6 \t Batch 850 \t Validation Loss: 33.40925408868229\n",
      "Epoch 6 \t Batch 900 \t Validation Loss: 33.71116329828898\n",
      "Epoch 6 \t Batch 950 \t Validation Loss: 33.876587706114115\n",
      "Epoch 6 \t Batch 1000 \t Validation Loss: 35.46152036190033\n",
      "Epoch 6 \t Batch 1050 \t Validation Loss: 36.49950700214931\n",
      "Epoch 6 \t Batch 1100 \t Validation Loss: 37.254054159684614\n",
      "Epoch 6 \t Batch 1150 \t Validation Loss: 37.376353702959804\n",
      "Epoch 6 \t Batch 1200 \t Validation Loss: 38.17509715358416\n",
      "Epoch 6 \t Batch 1250 \t Validation Loss: 38.52776745071411\n",
      "Epoch 6 \t Batch 1300 \t Validation Loss: 38.80336247224074\n",
      "Epoch 6 \t Batch 1350 \t Validation Loss: 38.639230753580726\n",
      "Epoch 6 \t Batch 1400 \t Validation Loss: 38.55495703016009\n",
      "Epoch 6 \t Batch 1450 \t Validation Loss: 38.583282034643766\n",
      "Epoch 6 \t Batch 1500 \t Validation Loss: 38.76502141046524\n",
      "Epoch 6 \t Batch 1550 \t Validation Loss: 38.53084157005433\n",
      "Epoch 6 \t Batch 1600 \t Validation Loss: 38.292542034834625\n",
      "Epoch 6 \t Batch 1650 \t Validation Loss: 38.37138795260227\n",
      "Epoch 6 \t Batch 1700 \t Validation Loss: 38.30021186085308\n",
      "Epoch 6 \t Batch 1750 \t Validation Loss: 38.13607821696145\n",
      "Epoch 6 \t Batch 1800 \t Validation Loss: 38.17805202841759\n",
      "Epoch 6 \t Batch 1850 \t Validation Loss: 38.72268654423791\n",
      "Epoch 6 \t Batch 1900 \t Validation Loss: 38.9530650343393\n",
      "Epoch 6 \t Batch 1950 \t Validation Loss: 38.83281964118664\n",
      "Epoch 6 \t Batch 2000 \t Validation Loss: 38.800539673209194\n",
      "Epoch 6 \t Batch 2050 \t Validation Loss: 38.93057652996808\n",
      "Epoch 6 \t Batch 2100 \t Validation Loss: 38.79034502063479\n",
      "Epoch 6 \t Batch 2150 \t Validation Loss: 38.54939367505007\n",
      "Epoch 6 \t Batch 2200 \t Validation Loss: 38.32552120544694\n",
      "Epoch 6 \t Batch 2250 \t Validation Loss: 38.1035505806605\n",
      "Epoch 6 \t Batch 2300 \t Validation Loss: 37.78665038326512\n",
      "Epoch 6 \t Batch 2350 \t Validation Loss: 37.68147732602789\n",
      "Epoch 6 \t Batch 2400 \t Validation Loss: 37.750208072761694\n",
      "Epoch 6 \t Batch 2450 \t Validation Loss: 38.03137367491819\n",
      "Epoch 6 Training Loss: 45.97165623303978 Validation Loss: 38.19188302145763\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 50 \t Training Loss: 46.04557884216309\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 45.35086339950561\n",
      "Epoch 7 \t Batch 150 \t Training Loss: 45.7432887395223\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 45.934986124038694\n",
      "Epoch 7 \t Batch 250 \t Training Loss: 46.01826739501953\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 45.793138313293454\n",
      "Epoch 7 \t Batch 350 \t Training Loss: 45.84791184561593\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 45.70497413635254\n",
      "Epoch 7 \t Batch 450 \t Training Loss: 45.591053564283584\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 45.70054936599731\n",
      "Epoch 7 \t Batch 550 \t Training Loss: 45.550667242570356\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 45.64788578351339\n",
      "Epoch 7 \t Batch 650 \t Training Loss: 45.68261172661415\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 45.89481816700527\n",
      "Epoch 7 \t Batch 750 \t Training Loss: 45.92740160369873\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 45.95860934257507\n",
      "Epoch 7 \t Batch 850 \t Training Loss: 45.885380145802216\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 45.71905272801717\n",
      "Epoch 7 \t Batch 950 \t Training Loss: 45.7490526038722\n",
      "Epoch 7 \t Batch 1000 \t Training Loss: 45.790533697128296\n",
      "Epoch 7 \t Batch 1050 \t Training Loss: 45.7784954561506\n",
      "Epoch 7 \t Batch 1100 \t Training Loss: 45.77657625545155\n",
      "Epoch 7 \t Batch 1150 \t Training Loss: 45.76648036127505\n",
      "Epoch 7 \t Batch 1200 \t Training Loss: 45.72269606908162\n",
      "Epoch 7 \t Batch 1250 \t Training Loss: 45.69099910583496\n",
      "Epoch 7 \t Batch 1300 \t Training Loss: 45.65775808774508\n",
      "Epoch 7 \t Batch 1350 \t Training Loss: 45.611952217949764\n",
      "Epoch 7 \t Batch 1400 \t Training Loss: 45.56428448813302\n",
      "Epoch 7 \t Batch 1450 \t Training Loss: 45.55654082857329\n",
      "Epoch 7 \t Batch 1500 \t Training Loss: 45.525288117726646\n",
      "Epoch 7 \t Batch 1550 \t Training Loss: 45.48499827354185\n",
      "Epoch 7 \t Batch 1600 \t Training Loss: 45.52511255502701\n",
      "Epoch 7 \t Batch 1650 \t Training Loss: 45.52764188130697\n",
      "Epoch 7 \t Batch 1700 \t Training Loss: 45.52166857663323\n",
      "Epoch 7 \t Batch 1750 \t Training Loss: 45.57763716670445\n",
      "Epoch 7 \t Batch 1800 \t Training Loss: 45.56128169271681\n",
      "Epoch 7 \t Batch 1850 \t Training Loss: 45.53110787778287\n",
      "Epoch 7 \t Batch 1900 \t Training Loss: 45.57414594047948\n",
      "Epoch 7 \t Batch 1950 \t Training Loss: 45.56503414154053\n",
      "Epoch 7 \t Batch 2000 \t Training Loss: 45.5595888633728\n",
      "Epoch 7 \t Batch 2050 \t Training Loss: 45.558160837685186\n",
      "Epoch 7 \t Batch 2100 \t Training Loss: 45.63733377365839\n",
      "Epoch 7 \t Batch 2150 \t Training Loss: 45.66783327147018\n",
      "Epoch 7 \t Batch 2200 \t Training Loss: 45.660012073516846\n",
      "Epoch 7 \t Batch 2250 \t Training Loss: 45.68809253438314\n",
      "Epoch 7 \t Batch 2300 \t Training Loss: 45.66876904114433\n",
      "Epoch 7 \t Batch 2350 \t Training Loss: 45.63444135706475\n",
      "Epoch 7 \t Batch 2400 \t Training Loss: 45.61026218811671\n",
      "Epoch 7 \t Batch 2450 \t Training Loss: 45.61107754454321\n",
      "Epoch 7 \t Batch 2500 \t Training Loss: 45.593174301147464\n",
      "Epoch 7 \t Batch 2550 \t Training Loss: 45.60952288160137\n",
      "Epoch 7 \t Batch 2600 \t Training Loss: 45.59409698339609\n",
      "Epoch 7 \t Batch 2650 \t Training Loss: 45.5952942873397\n",
      "Epoch 7 \t Batch 2700 \t Training Loss: 45.60741509755452\n",
      "Epoch 7 \t Batch 2750 \t Training Loss: 45.63798978909579\n",
      "Epoch 7 \t Batch 2800 \t Training Loss: 45.620450541632515\n",
      "Epoch 7 \t Batch 2850 \t Training Loss: 45.61554509346945\n",
      "Epoch 7 \t Batch 2900 \t Training Loss: 45.61986636983937\n",
      "Epoch 7 \t Batch 2950 \t Training Loss: 45.6212307532359\n",
      "Epoch 7 \t Batch 3000 \t Training Loss: 45.60829159545899\n",
      "Epoch 7 \t Batch 3050 \t Training Loss: 45.590011916864114\n",
      "Epoch 7 \t Batch 3100 \t Training Loss: 45.59296102585331\n",
      "Epoch 7 \t Batch 3150 \t Training Loss: 45.59796851990715\n",
      "Epoch 7 \t Batch 3200 \t Training Loss: 45.60448567271233\n",
      "Epoch 7 \t Batch 3250 \t Training Loss: 45.61376046518179\n",
      "Epoch 7 \t Batch 3300 \t Training Loss: 45.61172766136401\n",
      "Epoch 7 \t Batch 3350 \t Training Loss: 45.62390904099194\n",
      "Epoch 7 \t Batch 3400 \t Training Loss: 45.63861628364114\n",
      "Epoch 7 \t Batch 3450 \t Training Loss: 45.6648921645897\n",
      "Epoch 7 \t Batch 3500 \t Training Loss: 45.64968198721749\n",
      "Epoch 7 \t Batch 3550 \t Training Loss: 45.63244571793247\n",
      "Epoch 7 \t Batch 3600 \t Training Loss: 45.63382377253638\n",
      "Epoch 7 \t Batch 3650 \t Training Loss: 45.6148898770058\n",
      "Epoch 7 \t Batch 50 \t Validation Loss: 17.369662628173828\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 22.263417453765868\n",
      "Epoch 7 \t Batch 150 \t Validation Loss: 19.940590473810833\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 19.47070610523224\n",
      "Epoch 7 \t Batch 250 \t Validation Loss: 20.84857162475586\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 19.824369883537294\n",
      "Epoch 7 \t Batch 350 \t Validation Loss: 21.388528378350394\n",
      "Epoch 7 \t Batch 400 \t Validation Loss: 21.87836452126503\n",
      "Epoch 7 \t Batch 450 \t Validation Loss: 23.154038065804375\n",
      "Epoch 7 \t Batch 500 \t Validation Loss: 23.420218501091004\n",
      "Epoch 7 \t Batch 550 \t Validation Loss: 23.68559796853499\n",
      "Epoch 7 \t Batch 600 \t Validation Loss: 24.694574182033538\n",
      "Epoch 7 \t Batch 650 \t Validation Loss: 26.617112606488742\n",
      "Epoch 7 \t Batch 700 \t Validation Loss: 28.62175691672734\n",
      "Epoch 7 \t Batch 750 \t Validation Loss: 29.966573563257853\n",
      "Epoch 7 \t Batch 800 \t Validation Loss: 31.38572839438915\n",
      "Epoch 7 \t Batch 850 \t Validation Loss: 32.26233032282661\n",
      "Epoch 7 \t Batch 900 \t Validation Loss: 32.6466707584593\n",
      "Epoch 7 \t Batch 950 \t Validation Loss: 32.943867986076754\n",
      "Epoch 7 \t Batch 1000 \t Validation Loss: 34.65018983030319\n",
      "Epoch 7 \t Batch 1050 \t Validation Loss: 35.74387687410627\n",
      "Epoch 7 \t Batch 1100 \t Validation Loss: 36.534576350992374\n",
      "Epoch 7 \t Batch 1150 \t Validation Loss: 36.62621670930282\n",
      "Epoch 7 \t Batch 1200 \t Validation Loss: 37.54332071463267\n",
      "Epoch 7 \t Batch 1250 \t Validation Loss: 37.943269640350344\n",
      "Epoch 7 \t Batch 1300 \t Validation Loss: 38.266521980579085\n",
      "Epoch 7 \t Batch 1350 \t Validation Loss: 38.13720170480234\n",
      "Epoch 7 \t Batch 1400 \t Validation Loss: 38.139285774230956\n",
      "Epoch 7 \t Batch 1450 \t Validation Loss: 38.22565076762232\n",
      "Epoch 7 \t Batch 1500 \t Validation Loss: 38.44459981123607\n",
      "Epoch 7 \t Batch 1550 \t Validation Loss: 38.19624204543329\n",
      "Epoch 7 \t Batch 1600 \t Validation Loss: 38.055542412698266\n",
      "Epoch 7 \t Batch 1650 \t Validation Loss: 38.16038081400322\n",
      "Epoch 7 \t Batch 1700 \t Validation Loss: 38.07153039343217\n",
      "Epoch 7 \t Batch 1750 \t Validation Loss: 37.93500062479291\n",
      "Epoch 7 \t Batch 1800 \t Validation Loss: 38.00518120527268\n",
      "Epoch 7 \t Batch 1850 \t Validation Loss: 38.52255104451566\n",
      "Epoch 7 \t Batch 1900 \t Validation Loss: 38.774676518440245\n",
      "Epoch 7 \t Batch 1950 \t Validation Loss: 38.64252353007977\n",
      "Epoch 7 \t Batch 2000 \t Validation Loss: 38.58644953513146\n",
      "Epoch 7 \t Batch 2050 \t Validation Loss: 38.7609078432874\n",
      "Epoch 7 \t Batch 2100 \t Validation Loss: 38.71652627604348\n",
      "Epoch 7 \t Batch 2150 \t Validation Loss: 38.61151204064835\n",
      "Epoch 7 \t Batch 2200 \t Validation Loss: 38.51142050417987\n",
      "Epoch 7 \t Batch 2250 \t Validation Loss: 38.43430693456862\n",
      "Epoch 7 \t Batch 2300 \t Validation Loss: 38.28626083104507\n",
      "Epoch 7 \t Batch 2350 \t Validation Loss: 38.304340078272716\n",
      "Epoch 7 \t Batch 2400 \t Validation Loss: 38.43580922504266\n",
      "Epoch 7 \t Batch 2450 \t Validation Loss: 38.7970547339381\n",
      "Epoch 7 Training Loss: 45.60764729417657 Validation Loss: 38.93611436017922\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 50 \t Training Loss: 44.903062934875486\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 45.445007762908936\n",
      "Epoch 8 \t Batch 150 \t Training Loss: 45.23446547190348\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 45.11758021354675\n",
      "Epoch 8 \t Batch 250 \t Training Loss: 45.19196058654785\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 45.13509558995565\n",
      "Epoch 8 \t Batch 350 \t Training Loss: 45.17142543792725\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 45.233245277404784\n",
      "Epoch 8 \t Batch 450 \t Training Loss: 45.41457836574978\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 45.385721603393556\n",
      "Epoch 8 \t Batch 550 \t Training Loss: 45.511905701377174\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 45.481189530690514\n",
      "Epoch 8 \t Batch 650 \t Training Loss: 45.4646262887808\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 45.47986975533622\n",
      "Epoch 8 \t Batch 750 \t Training Loss: 45.650711512247724\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 45.54463092803955\n",
      "Epoch 8 \t Batch 850 \t Training Loss: 45.492965092378505\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 45.44318812052409\n",
      "Epoch 8 \t Batch 950 \t Training Loss: 45.370370226408305\n",
      "Epoch 8 \t Batch 1000 \t Training Loss: 45.35922771644592\n",
      "Epoch 8 \t Batch 1050 \t Training Loss: 45.38390877314976\n",
      "Epoch 8 \t Batch 1100 \t Training Loss: 45.41936406048861\n",
      "Epoch 8 \t Batch 1150 \t Training Loss: 45.40454588185186\n",
      "Epoch 8 \t Batch 1200 \t Training Loss: 45.42610571702321\n",
      "Epoch 8 \t Batch 1250 \t Training Loss: 45.37657014312744\n",
      "Epoch 8 \t Batch 1300 \t Training Loss: 45.35072457973774\n",
      "Epoch 8 \t Batch 1350 \t Training Loss: 45.37164598111753\n",
      "Epoch 8 \t Batch 1400 \t Training Loss: 45.33938236372811\n",
      "Epoch 8 \t Batch 1450 \t Training Loss: 45.35022511317812\n",
      "Epoch 8 \t Batch 1500 \t Training Loss: 45.28846985117595\n",
      "Epoch 8 \t Batch 1550 \t Training Loss: 45.34187355779832\n",
      "Epoch 8 \t Batch 1600 \t Training Loss: 45.37932879209519\n",
      "Epoch 8 \t Batch 1650 \t Training Loss: 45.38738889636416\n",
      "Epoch 8 \t Batch 1700 \t Training Loss: 45.38261416379143\n",
      "Epoch 8 \t Batch 1750 \t Training Loss: 45.36042739759173\n",
      "Epoch 8 \t Batch 1800 \t Training Loss: 45.35217990557353\n",
      "Epoch 8 \t Batch 1850 \t Training Loss: 45.35363182480271\n",
      "Epoch 8 \t Batch 1900 \t Training Loss: 45.357085772062604\n",
      "Epoch 8 \t Batch 1950 \t Training Loss: 45.328439646011745\n",
      "Epoch 8 \t Batch 2000 \t Training Loss: 45.3229293088913\n",
      "Epoch 8 \t Batch 2050 \t Training Loss: 45.31173203817228\n",
      "Epoch 8 \t Batch 2100 \t Training Loss: 45.32081167311895\n",
      "Epoch 8 \t Batch 2150 \t Training Loss: 45.3061101363426\n",
      "Epoch 8 \t Batch 2200 \t Training Loss: 45.2835936346921\n",
      "Epoch 8 \t Batch 2250 \t Training Loss: 45.28512074279785\n",
      "Epoch 8 \t Batch 2300 \t Training Loss: 45.29644927646803\n",
      "Epoch 8 \t Batch 2350 \t Training Loss: 45.306502851932606\n",
      "Epoch 8 \t Batch 2400 \t Training Loss: 45.30134048541387\n",
      "Epoch 8 \t Batch 2450 \t Training Loss: 45.31843636571145\n",
      "Epoch 8 \t Batch 2500 \t Training Loss: 45.317790514373776\n",
      "Epoch 8 \t Batch 2550 \t Training Loss: 45.33071228999717\n",
      "Epoch 8 \t Batch 2600 \t Training Loss: 45.33062728661757\n",
      "Epoch 8 \t Batch 2650 \t Training Loss: 45.319245119634665\n",
      "Epoch 8 \t Batch 2700 \t Training Loss: 45.296522323467116\n",
      "Epoch 8 \t Batch 2750 \t Training Loss: 45.31131648185036\n",
      "Epoch 8 \t Batch 2800 \t Training Loss: 45.316481967653544\n",
      "Epoch 8 \t Batch 2850 \t Training Loss: 45.326754782158034\n",
      "Epoch 8 \t Batch 2900 \t Training Loss: 45.342835121154785\n",
      "Epoch 8 \t Batch 2950 \t Training Loss: 45.34547220262431\n",
      "Epoch 8 \t Batch 3000 \t Training Loss: 45.37096233177185\n",
      "Epoch 8 \t Batch 3050 \t Training Loss: 45.37041262141994\n",
      "Epoch 8 \t Batch 3100 \t Training Loss: 45.347139202240974\n",
      "Epoch 8 \t Batch 3150 \t Training Loss: 45.35332253955659\n",
      "Epoch 8 \t Batch 3200 \t Training Loss: 45.34248782277107\n",
      "Epoch 8 \t Batch 3250 \t Training Loss: 45.323973118121806\n",
      "Epoch 8 \t Batch 3300 \t Training Loss: 45.31305624065977\n",
      "Epoch 8 \t Batch 3350 \t Training Loss: 45.317245991265594\n",
      "Epoch 8 \t Batch 3400 \t Training Loss: 45.337324978323544\n",
      "Epoch 8 \t Batch 3450 \t Training Loss: 45.33063527867414\n",
      "Epoch 8 \t Batch 3500 \t Training Loss: 45.33728127833775\n",
      "Epoch 8 \t Batch 3550 \t Training Loss: 45.3523599619261\n",
      "Epoch 8 \t Batch 3600 \t Training Loss: 45.327905084821914\n",
      "Epoch 8 \t Batch 3650 \t Training Loss: 45.32119815565135\n",
      "Epoch 8 \t Batch 50 \t Validation Loss: 10.677752380371095\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 13.775510292053223\n",
      "Epoch 8 \t Batch 150 \t Validation Loss: 12.144245238304139\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 12.556842104196548\n",
      "Epoch 8 \t Batch 250 \t Validation Loss: 13.841504332542419\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 13.231841609477996\n",
      "Epoch 8 \t Batch 350 \t Validation Loss: 15.445109368051801\n",
      "Epoch 8 \t Batch 400 \t Validation Loss: 16.689826977849005\n",
      "Epoch 8 \t Batch 450 \t Validation Loss: 18.479430701467727\n",
      "Epoch 8 \t Batch 500 \t Validation Loss: 19.232832380771637\n",
      "Epoch 8 \t Batch 550 \t Validation Loss: 19.934665484428407\n",
      "Epoch 8 \t Batch 600 \t Validation Loss: 21.648387364149094\n",
      "Epoch 8 \t Batch 650 \t Validation Loss: 24.025574615551875\n",
      "Epoch 8 \t Batch 700 \t Validation Loss: 26.651703020163946\n",
      "Epoch 8 \t Batch 750 \t Validation Loss: 28.685660997072855\n",
      "Epoch 8 \t Batch 800 \t Validation Loss: 30.533307723104954\n",
      "Epoch 8 \t Batch 850 \t Validation Loss: 31.830373322262485\n",
      "Epoch 8 \t Batch 900 \t Validation Loss: 32.483007083733874\n",
      "Epoch 8 \t Batch 950 \t Validation Loss: 33.03120596659811\n",
      "Epoch 8 \t Batch 1000 \t Validation Loss: 34.98297160172462\n",
      "Epoch 8 \t Batch 1050 \t Validation Loss: 36.34110994180043\n",
      "Epoch 8 \t Batch 1100 \t Validation Loss: 37.387637143785305\n",
      "Epoch 8 \t Batch 1150 \t Validation Loss: 37.71552919325621\n",
      "Epoch 8 \t Batch 1200 \t Validation Loss: 38.74553175310294\n",
      "Epoch 8 \t Batch 1250 \t Validation Loss: 39.28761215305328\n",
      "Epoch 8 \t Batch 1300 \t Validation Loss: 39.682878297842464\n",
      "Epoch 8 \t Batch 1350 \t Validation Loss: 39.72744598547617\n",
      "Epoch 8 \t Batch 1400 \t Validation Loss: 39.76333411506244\n",
      "Epoch 8 \t Batch 1450 \t Validation Loss: 39.87414990803291\n",
      "Epoch 8 \t Batch 1500 \t Validation Loss: 40.15707776021957\n",
      "Epoch 8 \t Batch 1550 \t Validation Loss: 40.16906495509609\n",
      "Epoch 8 \t Batch 1600 \t Validation Loss: 40.127209522873166\n",
      "Epoch 8 \t Batch 1650 \t Validation Loss: 40.28140677437638\n",
      "Epoch 8 \t Batch 1700 \t Validation Loss: 40.416678970140566\n",
      "Epoch 8 \t Batch 1750 \t Validation Loss: 40.363858757155285\n",
      "Epoch 8 \t Batch 1800 \t Validation Loss: 40.40175553864903\n",
      "Epoch 8 \t Batch 1850 \t Validation Loss: 41.10612827932513\n",
      "Epoch 8 \t Batch 1900 \t Validation Loss: 41.46683775462602\n",
      "Epoch 8 \t Batch 1950 \t Validation Loss: 41.441990534586786\n",
      "Epoch 8 \t Batch 2000 \t Validation Loss: 41.43412984287739\n",
      "Epoch 8 \t Batch 2050 \t Validation Loss: 41.504088569385246\n",
      "Epoch 8 \t Batch 2100 \t Validation Loss: 41.41092877535593\n",
      "Epoch 8 \t Batch 2150 \t Validation Loss: 41.1555637405085\n",
      "Epoch 8 \t Batch 2200 \t Validation Loss: 40.909635046287015\n",
      "Epoch 8 \t Batch 2250 \t Validation Loss: 40.67736304526859\n",
      "Epoch 8 \t Batch 2300 \t Validation Loss: 40.34125459111255\n",
      "Epoch 8 \t Batch 2350 \t Validation Loss: 40.20395976553572\n",
      "Epoch 8 \t Batch 2400 \t Validation Loss: 40.25862023174763\n",
      "Epoch 8 \t Batch 2450 \t Validation Loss: 40.50364110304385\n",
      "Epoch 8 Training Loss: 45.31371942029264 Validation Loss: 40.63279050143508\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 50 \t Training Loss: 46.94631233215332\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 45.08006141662597\n",
      "Epoch 9 \t Batch 150 \t Training Loss: 45.02513946533203\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 44.77830465316772\n",
      "Epoch 9 \t Batch 250 \t Training Loss: 44.97586380004883\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 44.77973272323609\n",
      "Epoch 9 \t Batch 350 \t Training Loss: 44.62310769762312\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 44.648172330856326\n",
      "Epoch 9 \t Batch 450 \t Training Loss: 44.60482958899604\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 44.760559520721436\n",
      "Epoch 9 \t Batch 550 \t Training Loss: 44.64773537722501\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 44.61574204126994\n",
      "Epoch 9 \t Batch 650 \t Training Loss: 44.78432055546687\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 44.65421976634434\n",
      "Epoch 9 \t Batch 750 \t Training Loss: 44.71893058013916\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 44.67107066631317\n",
      "Epoch 9 \t Batch 850 \t Training Loss: 44.7414613387164\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 44.7002734459771\n",
      "Epoch 9 \t Batch 950 \t Training Loss: 44.65014945180793\n",
      "Epoch 9 \t Batch 1000 \t Training Loss: 44.74271632003784\n",
      "Epoch 9 \t Batch 1050 \t Training Loss: 44.78459344046456\n",
      "Epoch 9 \t Batch 1100 \t Training Loss: 44.81633216337724\n",
      "Epoch 9 \t Batch 1150 \t Training Loss: 44.819044761657715\n",
      "Epoch 9 \t Batch 1200 \t Training Loss: 44.84129567305247\n",
      "Epoch 9 \t Batch 1250 \t Training Loss: 44.80439090423584\n",
      "Epoch 9 \t Batch 1300 \t Training Loss: 44.79911841759315\n",
      "Epoch 9 \t Batch 1350 \t Training Loss: 44.802076704237194\n",
      "Epoch 9 \t Batch 1400 \t Training Loss: 44.9026124055045\n",
      "Epoch 9 \t Batch 1450 \t Training Loss: 44.883650419301\n",
      "Epoch 9 \t Batch 1500 \t Training Loss: 44.90816853459676\n",
      "Epoch 9 \t Batch 1550 \t Training Loss: 44.920940317953786\n",
      "Epoch 9 \t Batch 1600 \t Training Loss: 44.96927119731903\n",
      "Epoch 9 \t Batch 1650 \t Training Loss: 44.95343870451956\n",
      "Epoch 9 \t Batch 1700 \t Training Loss: 45.03762092365938\n",
      "Epoch 9 \t Batch 1750 \t Training Loss: 45.075067039489745\n",
      "Epoch 9 \t Batch 1800 \t Training Loss: 45.04966820293003\n",
      "Epoch 9 \t Batch 1850 \t Training Loss: 45.02554582750475\n",
      "Epoch 9 \t Batch 1900 \t Training Loss: 45.02649433437146\n",
      "Epoch 9 \t Batch 1950 \t Training Loss: 44.981019460237945\n",
      "Epoch 9 \t Batch 2000 \t Training Loss: 44.990372250556945\n",
      "Epoch 9 \t Batch 2050 \t Training Loss: 44.99641050385266\n",
      "Epoch 9 \t Batch 2100 \t Training Loss: 44.99000774111067\n",
      "Epoch 9 \t Batch 2150 \t Training Loss: 44.97198413139166\n",
      "Epoch 9 \t Batch 2200 \t Training Loss: 44.97679187948054\n",
      "Epoch 9 \t Batch 2250 \t Training Loss: 44.95602896457248\n",
      "Epoch 9 \t Batch 2300 \t Training Loss: 44.951015302409296\n",
      "Epoch 9 \t Batch 2350 \t Training Loss: 44.95155262805046\n",
      "Epoch 9 \t Batch 2400 \t Training Loss: 44.94848559776942\n",
      "Epoch 9 \t Batch 2450 \t Training Loss: 44.925868692203444\n",
      "Epoch 9 \t Batch 2500 \t Training Loss: 44.93615102767944\n",
      "Epoch 9 \t Batch 2550 \t Training Loss: 44.96540192921957\n",
      "Epoch 9 \t Batch 2600 \t Training Loss: 44.97361583489638\n",
      "Epoch 9 \t Batch 2650 \t Training Loss: 44.971011411558905\n",
      "Epoch 9 \t Batch 2700 \t Training Loss: 44.97123227437337\n",
      "Epoch 9 \t Batch 2750 \t Training Loss: 44.954243456753815\n",
      "Epoch 9 \t Batch 2800 \t Training Loss: 44.94612907545907\n",
      "Epoch 9 \t Batch 2850 \t Training Loss: 44.95982926218133\n",
      "Epoch 9 \t Batch 2900 \t Training Loss: 44.96961994171143\n",
      "Epoch 9 \t Batch 2950 \t Training Loss: 44.9778188207594\n",
      "Epoch 9 \t Batch 3000 \t Training Loss: 45.003989436467485\n",
      "Epoch 9 \t Batch 3050 \t Training Loss: 45.02646559480761\n",
      "Epoch 9 \t Batch 3100 \t Training Loss: 45.032516392738586\n",
      "Epoch 9 \t Batch 3150 \t Training Loss: 45.014635468134806\n",
      "Epoch 9 \t Batch 3200 \t Training Loss: 44.996002874970436\n",
      "Epoch 9 \t Batch 3250 \t Training Loss: 44.97991942831186\n",
      "Epoch 9 \t Batch 3300 \t Training Loss: 44.96035876534202\n",
      "Epoch 9 \t Batch 3350 \t Training Loss: 44.954988397626735\n",
      "Epoch 9 \t Batch 3400 \t Training Loss: 44.97715299830717\n",
      "Epoch 9 \t Batch 3450 \t Training Loss: 44.95599745874819\n",
      "Epoch 9 \t Batch 3500 \t Training Loss: 44.956182867867604\n",
      "Epoch 9 \t Batch 3550 \t Training Loss: 44.99606295411016\n",
      "Epoch 9 \t Batch 3600 \t Training Loss: 44.99045225567288\n",
      "Epoch 9 \t Batch 3650 \t Training Loss: 45.01214148220951\n",
      "Epoch 9 \t Batch 50 \t Validation Loss: 22.173672714233398\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 26.255732078552246\n",
      "Epoch 9 \t Batch 150 \t Validation Loss: 23.48519191424052\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 23.21740489244461\n",
      "Epoch 9 \t Batch 250 \t Validation Loss: 25.64834850502014\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 24.888026049931845\n",
      "Epoch 9 \t Batch 350 \t Validation Loss: 25.888522735323225\n",
      "Epoch 9 \t Batch 400 \t Validation Loss: 25.8344665825367\n",
      "Epoch 9 \t Batch 450 \t Validation Loss: 26.62476989428202\n",
      "Epoch 9 \t Batch 500 \t Validation Loss: 26.538589131355284\n",
      "Epoch 9 \t Batch 550 \t Validation Loss: 26.463152610605412\n",
      "Epoch 9 \t Batch 600 \t Validation Loss: 27.088098069032032\n",
      "Epoch 9 \t Batch 650 \t Validation Loss: 28.531548961492685\n",
      "Epoch 9 \t Batch 700 \t Validation Loss: 30.18315855366843\n",
      "Epoch 9 \t Batch 750 \t Validation Loss: 31.26823327445984\n",
      "Epoch 9 \t Batch 800 \t Validation Loss: 32.461954237818716\n",
      "Epoch 9 \t Batch 850 \t Validation Loss: 33.23769896058475\n",
      "Epoch 9 \t Batch 900 \t Validation Loss: 33.5348476934433\n",
      "Epoch 9 \t Batch 950 \t Validation Loss: 33.769808818917525\n",
      "Epoch 9 \t Batch 1000 \t Validation Loss: 35.39281035375595\n",
      "Epoch 9 \t Batch 1050 \t Validation Loss: 36.51015698115031\n",
      "Epoch 9 \t Batch 1100 \t Validation Loss: 37.27034706549211\n",
      "Epoch 9 \t Batch 1150 \t Validation Loss: 37.22161429944246\n",
      "Epoch 9 \t Batch 1200 \t Validation Loss: 37.99996060927709\n",
      "Epoch 9 \t Batch 1250 \t Validation Loss: 38.3571900592804\n",
      "Epoch 9 \t Batch 1300 \t Validation Loss: 38.55908433657426\n",
      "Epoch 9 \t Batch 1350 \t Validation Loss: 38.376663522013914\n",
      "Epoch 9 \t Batch 1400 \t Validation Loss: 38.25598482302257\n",
      "Epoch 9 \t Batch 1450 \t Validation Loss: 38.23457932932624\n",
      "Epoch 9 \t Batch 1500 \t Validation Loss: 38.47532827313741\n",
      "Epoch 9 \t Batch 1550 \t Validation Loss: 38.199421597142376\n",
      "Epoch 9 \t Batch 1600 \t Validation Loss: 38.07034343957901\n",
      "Epoch 9 \t Batch 1650 \t Validation Loss: 38.133283841104216\n",
      "Epoch 9 \t Batch 1700 \t Validation Loss: 38.00146389624652\n",
      "Epoch 9 \t Batch 1750 \t Validation Loss: 37.84311711665562\n",
      "Epoch 9 \t Batch 1800 \t Validation Loss: 37.87825236585405\n",
      "Epoch 9 \t Batch 1850 \t Validation Loss: 38.38616592355677\n",
      "Epoch 9 \t Batch 1900 \t Validation Loss: 38.676908497559396\n",
      "Epoch 9 \t Batch 1950 \t Validation Loss: 38.51983787487715\n",
      "Epoch 9 \t Batch 2000 \t Validation Loss: 38.43446466779709\n",
      "Epoch 9 \t Batch 2050 \t Validation Loss: 38.55309625067362\n",
      "Epoch 9 \t Batch 2100 \t Validation Loss: 38.48461555253892\n",
      "Epoch 9 \t Batch 2150 \t Validation Loss: 38.32610472523889\n",
      "Epoch 9 \t Batch 2200 \t Validation Loss: 38.16385671225461\n",
      "Epoch 9 \t Batch 2250 \t Validation Loss: 38.00724837705824\n",
      "Epoch 9 \t Batch 2300 \t Validation Loss: 37.772866505332615\n",
      "Epoch 9 \t Batch 2350 \t Validation Loss: 37.740822690598506\n",
      "Epoch 9 \t Batch 2400 \t Validation Loss: 37.837646063963575\n",
      "Epoch 9 \t Batch 2450 \t Validation Loss: 38.14313982885711\n",
      "Epoch 9 Training Loss: 45.024297064237274 Validation Loss: 38.27917763145713\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 50 \t Training Loss: 44.35974861145019\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 45.090077781677245\n",
      "Epoch 10 \t Batch 150 \t Training Loss: 44.741207186381025\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 45.01953886032104\n",
      "Epoch 10 \t Batch 250 \t Training Loss: 45.052282722473144\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 45.117897548675536\n",
      "Epoch 10 \t Batch 350 \t Training Loss: 44.95673872266497\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 44.7079306936264\n",
      "Epoch 10 \t Batch 450 \t Training Loss: 44.59158433278402\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 44.70108865737915\n",
      "Epoch 10 \t Batch 550 \t Training Loss: 44.598206856467506\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 44.46719267527262\n",
      "Epoch 10 \t Batch 650 \t Training Loss: 44.5167041015625\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 44.46196684701102\n",
      "Epoch 10 \t Batch 750 \t Training Loss: 44.36746977996826\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 44.42188803195953\n",
      "Epoch 10 \t Batch 850 \t Training Loss: 44.55102256774902\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 44.53386169009738\n",
      "Epoch 10 \t Batch 950 \t Training Loss: 44.542900808233966\n",
      "Epoch 10 \t Batch 1000 \t Training Loss: 44.487201070785524\n",
      "Epoch 10 \t Batch 1050 \t Training Loss: 44.50731583913167\n",
      "Epoch 10 \t Batch 1100 \t Training Loss: 44.48014547174627\n",
      "Epoch 10 \t Batch 1150 \t Training Loss: 44.45243545200514\n",
      "Epoch 10 \t Batch 1200 \t Training Loss: 44.476459217071536\n",
      "Epoch 10 \t Batch 1250 \t Training Loss: 44.499556411743164\n",
      "Epoch 10 \t Batch 1300 \t Training Loss: 44.46958731137789\n",
      "Epoch 10 \t Batch 1350 \t Training Loss: 44.48290790275291\n",
      "Epoch 10 \t Batch 1400 \t Training Loss: 44.52417380605425\n",
      "Epoch 10 \t Batch 1450 \t Training Loss: 44.56350030570194\n",
      "Epoch 10 \t Batch 1500 \t Training Loss: 44.59875967788696\n",
      "Epoch 10 \t Batch 1550 \t Training Loss: 44.61252278358705\n",
      "Epoch 10 \t Batch 1600 \t Training Loss: 44.55859766125679\n",
      "Epoch 10 \t Batch 1650 \t Training Loss: 44.50880716266054\n",
      "Epoch 10 \t Batch 1700 \t Training Loss: 44.486407680511476\n",
      "Epoch 10 \t Batch 1750 \t Training Loss: 44.52453522273473\n",
      "Epoch 10 \t Batch 1800 \t Training Loss: 44.56553367614746\n",
      "Epoch 10 \t Batch 1850 \t Training Loss: 44.55176005801639\n",
      "Epoch 10 \t Batch 1900 \t Training Loss: 44.51619049072266\n",
      "Epoch 10 \t Batch 1950 \t Training Loss: 44.52997301346216\n",
      "Epoch 10 \t Batch 2000 \t Training Loss: 44.59491524505615\n",
      "Epoch 10 \t Batch 2050 \t Training Loss: 44.584537909903176\n",
      "Epoch 10 \t Batch 2100 \t Training Loss: 44.60755872998919\n",
      "Epoch 10 \t Batch 2150 \t Training Loss: 44.646021235266396\n",
      "Epoch 10 \t Batch 2200 \t Training Loss: 44.61484252496199\n",
      "Epoch 10 \t Batch 2250 \t Training Loss: 44.60224593522813\n",
      "Epoch 10 \t Batch 2300 \t Training Loss: 44.623576534934664\n",
      "Epoch 10 \t Batch 2350 \t Training Loss: 44.625181932652254\n",
      "Epoch 10 \t Batch 2400 \t Training Loss: 44.62539804935455\n",
      "Epoch 10 \t Batch 2450 \t Training Loss: 44.64017084705586\n",
      "Epoch 10 \t Batch 2500 \t Training Loss: 44.6745071975708\n",
      "Epoch 10 \t Batch 2550 \t Training Loss: 44.670496871050666\n",
      "Epoch 10 \t Batch 2600 \t Training Loss: 44.67756351324228\n",
      "Epoch 10 \t Batch 2650 \t Training Loss: 44.67658953396779\n",
      "Epoch 10 \t Batch 2700 \t Training Loss: 44.65137065463596\n",
      "Epoch 10 \t Batch 2750 \t Training Loss: 44.66178890436346\n",
      "Epoch 10 \t Batch 2800 \t Training Loss: 44.64184031350272\n",
      "Epoch 10 \t Batch 2850 \t Training Loss: 44.651967217294796\n",
      "Epoch 10 \t Batch 2900 \t Training Loss: 44.64448232256133\n",
      "Epoch 10 \t Batch 2950 \t Training Loss: 44.6508528247122\n",
      "Epoch 10 \t Batch 3000 \t Training Loss: 44.62885158220927\n",
      "Epoch 10 \t Batch 3050 \t Training Loss: 44.65506086443291\n",
      "Epoch 10 \t Batch 3100 \t Training Loss: 44.639289108399424\n",
      "Epoch 10 \t Batch 3150 \t Training Loss: 44.645654568747865\n",
      "Epoch 10 \t Batch 3200 \t Training Loss: 44.635474480986595\n",
      "Epoch 10 \t Batch 3250 \t Training Loss: 44.62290546475924\n",
      "Epoch 10 \t Batch 3300 \t Training Loss: 44.60630573503899\n",
      "Epoch 10 \t Batch 3350 \t Training Loss: 44.61128853584403\n",
      "Epoch 10 \t Batch 3400 \t Training Loss: 44.600095302357396\n",
      "Epoch 10 \t Batch 3450 \t Training Loss: 44.63489345550537\n",
      "Epoch 10 \t Batch 3500 \t Training Loss: 44.63639546258109\n",
      "Epoch 10 \t Batch 3550 \t Training Loss: 44.66375952223657\n",
      "Epoch 10 \t Batch 3600 \t Training Loss: 44.67732603496975\n",
      "Epoch 10 \t Batch 3650 \t Training Loss: 44.6779999991639\n",
      "Epoch 10 \t Batch 50 \t Validation Loss: 16.492961769104003\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 20.157016553878783\n",
      "Epoch 10 \t Batch 150 \t Validation Loss: 19.52676100095113\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 19.201684987545015\n",
      "Epoch 10 \t Batch 250 \t Validation Loss: 20.32432190132141\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 20.007190243403116\n",
      "Epoch 10 \t Batch 350 \t Validation Loss: 21.756625322614397\n",
      "Epoch 10 \t Batch 400 \t Validation Loss: 22.151616201400756\n",
      "Epoch 10 \t Batch 450 \t Validation Loss: 23.43719025717841\n",
      "Epoch 10 \t Batch 500 \t Validation Loss: 23.666153059959413\n",
      "Epoch 10 \t Batch 550 \t Validation Loss: 23.81183136766607\n",
      "Epoch 10 \t Batch 600 \t Validation Loss: 24.987609253724415\n",
      "Epoch 10 \t Batch 650 \t Validation Loss: 27.283439102906446\n",
      "Epoch 10 \t Batch 700 \t Validation Loss: 29.525523886680602\n",
      "Epoch 10 \t Batch 750 \t Validation Loss: 30.956372860590616\n",
      "Epoch 10 \t Batch 800 \t Validation Loss: 32.52894319474697\n",
      "Epoch 10 \t Batch 850 \t Validation Loss: 33.533876792122335\n",
      "Epoch 10 \t Batch 900 \t Validation Loss: 33.9782652786043\n",
      "Epoch 10 \t Batch 950 \t Validation Loss: 34.226225831383154\n",
      "Epoch 10 \t Batch 1000 \t Validation Loss: 36.03692783975601\n",
      "Epoch 10 \t Batch 1050 \t Validation Loss: 37.21974096343631\n",
      "Epoch 10 \t Batch 1100 \t Validation Loss: 38.10562082940882\n",
      "Epoch 10 \t Batch 1150 \t Validation Loss: 38.23370857777803\n",
      "Epoch 10 \t Batch 1200 \t Validation Loss: 39.22117069284121\n",
      "Epoch 10 \t Batch 1250 \t Validation Loss: 39.64306862220764\n",
      "Epoch 10 \t Batch 1300 \t Validation Loss: 39.977484530668995\n",
      "Epoch 10 \t Batch 1350 \t Validation Loss: 39.813001226495814\n",
      "Epoch 10 \t Batch 1400 \t Validation Loss: 39.81925383159093\n",
      "Epoch 10 \t Batch 1450 \t Validation Loss: 40.033724199492355\n",
      "Epoch 10 \t Batch 1500 \t Validation Loss: 40.267160611629485\n",
      "Epoch 10 \t Batch 1550 \t Validation Loss: 40.01101748881801\n",
      "Epoch 10 \t Batch 1600 \t Validation Loss: 39.90525723531842\n",
      "Epoch 10 \t Batch 1650 \t Validation Loss: 40.06352922743017\n",
      "Epoch 10 \t Batch 1700 \t Validation Loss: 40.002630836402666\n",
      "Epoch 10 \t Batch 1750 \t Validation Loss: 39.931700229781015\n",
      "Epoch 10 \t Batch 1800 \t Validation Loss: 40.169299629131956\n",
      "Epoch 10 \t Batch 1850 \t Validation Loss: 40.730730706421106\n",
      "Epoch 10 \t Batch 1900 \t Validation Loss: 40.99461195506547\n",
      "Epoch 10 \t Batch 1950 \t Validation Loss: 40.83107986609141\n",
      "Epoch 10 \t Batch 2000 \t Validation Loss: 40.80355417025089\n",
      "Epoch 10 \t Batch 2050 \t Validation Loss: 41.120082826963284\n",
      "Epoch 10 \t Batch 2100 \t Validation Loss: 41.08363857326054\n",
      "Epoch 10 \t Batch 2150 \t Validation Loss: 40.894567184115566\n",
      "Epoch 10 \t Batch 2200 \t Validation Loss: 40.66484810883349\n",
      "Epoch 10 \t Batch 2250 \t Validation Loss: 40.45595588864221\n",
      "Epoch 10 \t Batch 2300 \t Validation Loss: 40.187759963740476\n",
      "Epoch 10 \t Batch 2350 \t Validation Loss: 40.113415597956234\n",
      "Epoch 10 \t Batch 2400 \t Validation Loss: 40.1750813293457\n",
      "Epoch 10 \t Batch 2450 \t Validation Loss: 40.4457944918652\n",
      "Epoch 10 Training Loss: 44.67244740010868 Validation Loss: 40.56962352391187\n",
      "Epoch 10 completed\n",
      "Epoch 11 \t Batch 50 \t Training Loss: 42.703837890625\n",
      "Epoch 11 \t Batch 100 \t Training Loss: 43.055020809173584\n",
      "Epoch 11 \t Batch 150 \t Training Loss: 43.12676797231038\n",
      "Epoch 11 \t Batch 200 \t Training Loss: 43.25356680870056\n",
      "Epoch 11 \t Batch 250 \t Training Loss: 43.730025634765624\n",
      "Epoch 11 \t Batch 300 \t Training Loss: 43.73833413441976\n",
      "Epoch 11 \t Batch 350 \t Training Loss: 43.69150814601353\n",
      "Epoch 11 \t Batch 400 \t Training Loss: 43.40913311958313\n",
      "Epoch 11 \t Batch 450 \t Training Loss: 43.5893666627672\n",
      "Epoch 11 \t Batch 500 \t Training Loss: 43.40555666732788\n",
      "Epoch 11 \t Batch 550 \t Training Loss: 43.5048186007413\n",
      "Epoch 11 \t Batch 600 \t Training Loss: 43.577091979980466\n",
      "Epoch 11 \t Batch 650 \t Training Loss: 43.71290800534762\n",
      "Epoch 11 \t Batch 700 \t Training Loss: 43.756995192936486\n",
      "Epoch 11 \t Batch 750 \t Training Loss: 43.86839311472575\n",
      "Epoch 11 \t Batch 800 \t Training Loss: 43.87752294063568\n",
      "Epoch 11 \t Batch 850 \t Training Loss: 43.94174950319178\n",
      "Epoch 11 \t Batch 900 \t Training Loss: 43.967698171403676\n",
      "Epoch 11 \t Batch 950 \t Training Loss: 43.97654063375373\n",
      "Epoch 11 \t Batch 1000 \t Training Loss: 43.953921432495115\n",
      "Epoch 11 \t Batch 1050 \t Training Loss: 44.02558001200358\n",
      "Epoch 11 \t Batch 1100 \t Training Loss: 44.05105783289129\n",
      "Epoch 11 \t Batch 1150 \t Training Loss: 44.108805749312694\n",
      "Epoch 11 \t Batch 1200 \t Training Loss: 44.10769136269887\n",
      "Epoch 11 \t Batch 1250 \t Training Loss: 44.09222279510498\n",
      "Epoch 11 \t Batch 1300 \t Training Loss: 44.09770279224102\n",
      "Epoch 11 \t Batch 1350 \t Training Loss: 44.16601506833677\n",
      "Epoch 11 \t Batch 1400 \t Training Loss: 44.230228527614045\n",
      "Epoch 11 \t Batch 1450 \t Training Loss: 44.192996215820315\n",
      "Epoch 11 \t Batch 1500 \t Training Loss: 44.128678068796795\n",
      "Epoch 11 \t Batch 1550 \t Training Loss: 44.17381329813311\n",
      "Epoch 11 \t Batch 1600 \t Training Loss: 44.16019995927811\n",
      "Epoch 11 \t Batch 1650 \t Training Loss: 44.18023377852006\n",
      "Epoch 11 \t Batch 1700 \t Training Loss: 44.12759347803453\n",
      "Epoch 11 \t Batch 1750 \t Training Loss: 44.170634253365655\n",
      "Epoch 11 \t Batch 1800 \t Training Loss: 44.191828371683755\n",
      "Epoch 11 \t Batch 1850 \t Training Loss: 44.20970463314572\n",
      "Epoch 11 \t Batch 1900 \t Training Loss: 44.18879526841013\n",
      "Epoch 11 \t Batch 1950 \t Training Loss: 44.14982291979668\n",
      "Epoch 11 \t Batch 2000 \t Training Loss: 44.16280669403076\n",
      "Epoch 11 \t Batch 2050 \t Training Loss: 44.15189148879633\n",
      "Epoch 11 \t Batch 2100 \t Training Loss: 44.16221808478946\n",
      "Epoch 11 \t Batch 2150 \t Training Loss: 44.19380146115325\n",
      "Epoch 11 \t Batch 2200 \t Training Loss: 44.17900353345004\n",
      "Epoch 11 \t Batch 2250 \t Training Loss: 44.163694116380476\n",
      "Epoch 11 \t Batch 2300 \t Training Loss: 44.17554804843405\n",
      "Epoch 11 \t Batch 2350 \t Training Loss: 44.188620520246786\n",
      "Epoch 11 \t Batch 2400 \t Training Loss: 44.19467699607213\n",
      "Epoch 11 \t Batch 2450 \t Training Loss: 44.17823423891652\n",
      "Epoch 11 \t Batch 2500 \t Training Loss: 44.17528362731934\n",
      "Epoch 11 \t Batch 2550 \t Training Loss: 44.18390463959937\n",
      "Epoch 11 \t Batch 2600 \t Training Loss: 44.218866256567146\n",
      "Epoch 11 \t Batch 2650 \t Training Loss: 44.22927132876414\n",
      "Epoch 11 \t Batch 2700 \t Training Loss: 44.246269904242624\n",
      "Epoch 11 \t Batch 2750 \t Training Loss: 44.21286110409823\n",
      "Epoch 11 \t Batch 2800 \t Training Loss: 44.24509376730238\n",
      "Epoch 11 \t Batch 2850 \t Training Loss: 44.263759068940814\n",
      "Epoch 11 \t Batch 2900 \t Training Loss: 44.25679082804713\n",
      "Epoch 11 \t Batch 2950 \t Training Loss: 44.26996454012596\n",
      "Epoch 11 \t Batch 3000 \t Training Loss: 44.26616451390584\n",
      "Epoch 11 \t Batch 3050 \t Training Loss: 44.2760980355935\n",
      "Epoch 11 \t Batch 3100 \t Training Loss: 44.29255027278777\n",
      "Epoch 11 \t Batch 3150 \t Training Loss: 44.288280065627326\n",
      "Epoch 11 \t Batch 3200 \t Training Loss: 44.27630014002323\n",
      "Epoch 11 \t Batch 3250 \t Training Loss: 44.32356891045204\n",
      "Epoch 11 \t Batch 3300 \t Training Loss: 44.32800753506747\n",
      "Epoch 11 \t Batch 3350 \t Training Loss: 44.35262862817565\n",
      "Epoch 11 \t Batch 3400 \t Training Loss: 44.38326198016896\n",
      "Epoch 11 \t Batch 3450 \t Training Loss: 44.40293281112892\n",
      "Epoch 11 \t Batch 3500 \t Training Loss: 44.4026831942967\n",
      "Epoch 11 \t Batch 3550 \t Training Loss: 44.383764449643415\n",
      "Epoch 11 \t Batch 3600 \t Training Loss: 44.372147724363536\n",
      "Epoch 11 \t Batch 3650 \t Training Loss: 44.385120162441304\n",
      "Epoch 11 \t Batch 50 \t Validation Loss: 23.368311958312987\n",
      "Epoch 11 \t Batch 100 \t Validation Loss: 29.486760358810425\n",
      "Epoch 11 \t Batch 150 \t Validation Loss: 25.851078322728476\n",
      "Epoch 11 \t Batch 200 \t Validation Loss: 25.471810121536254\n",
      "Epoch 11 \t Batch 250 \t Validation Loss: 27.71878956604004\n",
      "Epoch 11 \t Batch 300 \t Validation Loss: 26.324840819040933\n",
      "Epoch 11 \t Batch 350 \t Validation Loss: 27.488968376432148\n",
      "Epoch 11 \t Batch 400 \t Validation Loss: 27.27261080622673\n",
      "Epoch 11 \t Batch 450 \t Validation Loss: 27.895290920469495\n",
      "Epoch 11 \t Batch 500 \t Validation Loss: 27.683085574150084\n",
      "Epoch 11 \t Batch 550 \t Validation Loss: 27.45101107597351\n",
      "Epoch 11 \t Batch 600 \t Validation Loss: 27.944112560749055\n",
      "Epoch 11 \t Batch 650 \t Validation Loss: 29.36077340566195\n",
      "Epoch 11 \t Batch 700 \t Validation Loss: 30.977549836976188\n",
      "Epoch 11 \t Batch 750 \t Validation Loss: 32.030381549199426\n",
      "Epoch 11 \t Batch 800 \t Validation Loss: 33.061708917021754\n",
      "Epoch 11 \t Batch 850 \t Validation Loss: 33.73094454092138\n",
      "Epoch 11 \t Batch 900 \t Validation Loss: 33.92884992281596\n",
      "Epoch 11 \t Batch 950 \t Validation Loss: 33.99669442728946\n",
      "Epoch 11 \t Batch 1000 \t Validation Loss: 35.47067791843414\n",
      "Epoch 11 \t Batch 1050 \t Validation Loss: 36.38464470000494\n",
      "Epoch 11 \t Batch 1100 \t Validation Loss: 37.044913191361864\n",
      "Epoch 11 \t Batch 1150 \t Validation Loss: 37.06737323014632\n",
      "Epoch 11 \t Batch 1200 \t Validation Loss: 37.822132370471955\n",
      "Epoch 11 \t Batch 1250 \t Validation Loss: 38.12444291191101\n",
      "Epoch 11 \t Batch 1300 \t Validation Loss: 38.369302894518924\n",
      "Epoch 11 \t Batch 1350 \t Validation Loss: 38.18759998003642\n",
      "Epoch 11 \t Batch 1400 \t Validation Loss: 38.07303821052824\n",
      "Epoch 11 \t Batch 1450 \t Validation Loss: 38.0183281789977\n",
      "Epoch 11 \t Batch 1500 \t Validation Loss: 38.148767451763156\n",
      "Epoch 11 \t Batch 1550 \t Validation Loss: 37.89964752581812\n",
      "Epoch 11 \t Batch 1600 \t Validation Loss: 37.70128805175423\n",
      "Epoch 11 \t Batch 1650 \t Validation Loss: 37.76086124376817\n",
      "Epoch 11 \t Batch 1700 \t Validation Loss: 37.6808962671897\n",
      "Epoch 11 \t Batch 1750 \t Validation Loss: 37.51581057861873\n",
      "Epoch 11 \t Batch 1800 \t Validation Loss: 37.49202502051989\n",
      "Epoch 11 \t Batch 1850 \t Validation Loss: 38.03071055450955\n",
      "Epoch 11 \t Batch 1900 \t Validation Loss: 38.27875790558363\n",
      "Epoch 11 \t Batch 1950 \t Validation Loss: 38.15066419320229\n",
      "Epoch 11 \t Batch 2000 \t Validation Loss: 38.07655216372013\n",
      "Epoch 11 \t Batch 2050 \t Validation Loss: 38.14587852768782\n",
      "Epoch 11 \t Batch 2100 \t Validation Loss: 38.02272720756985\n",
      "Epoch 11 \t Batch 2150 \t Validation Loss: 37.84242489005244\n",
      "Epoch 11 \t Batch 2200 \t Validation Loss: 37.66983756726438\n",
      "Epoch 11 \t Batch 2250 \t Validation Loss: 37.50956543869442\n",
      "Epoch 11 \t Batch 2300 \t Validation Loss: 37.25735367868258\n",
      "Epoch 11 \t Batch 2350 \t Validation Loss: 37.217835070528885\n",
      "Epoch 11 \t Batch 2400 \t Validation Loss: 37.32486307670673\n",
      "Epoch 11 \t Batch 2450 \t Validation Loss: 37.63626765455518\n",
      "Epoch 11 Training Loss: 44.40686905059066 Validation Loss: 37.77893399524611\n",
      "Epoch 11 completed\n",
      "Epoch 12 \t Batch 50 \t Training Loss: 44.57724723815918\n",
      "Epoch 12 \t Batch 100 \t Training Loss: 44.45345317840576\n",
      "Epoch 12 \t Batch 150 \t Training Loss: 44.58758697509766\n",
      "Epoch 12 \t Batch 200 \t Training Loss: 44.370618200302125\n",
      "Epoch 12 \t Batch 250 \t Training Loss: 44.80767652893066\n",
      "Epoch 12 \t Batch 300 \t Training Loss: 44.818288790384926\n",
      "Epoch 12 \t Batch 350 \t Training Loss: 44.75534803662981\n",
      "Epoch 12 \t Batch 400 \t Training Loss: 44.584242129325865\n",
      "Epoch 12 \t Batch 450 \t Training Loss: 44.756173066033256\n",
      "Epoch 12 \t Batch 500 \t Training Loss: 44.78061919403076\n",
      "Epoch 12 \t Batch 550 \t Training Loss: 44.66625036066229\n",
      "Epoch 12 \t Batch 600 \t Training Loss: 44.54145627657572\n",
      "Epoch 12 \t Batch 650 \t Training Loss: 44.39569341806265\n",
      "Epoch 12 \t Batch 700 \t Training Loss: 44.390816922869\n",
      "Epoch 12 \t Batch 750 \t Training Loss: 44.32849359130859\n",
      "Epoch 12 \t Batch 800 \t Training Loss: 44.22934404611588\n",
      "Epoch 12 \t Batch 850 \t Training Loss: 44.21480661504409\n",
      "Epoch 12 \t Batch 900 \t Training Loss: 44.30397005928887\n",
      "Epoch 12 \t Batch 950 \t Training Loss: 44.322759983665065\n",
      "Epoch 12 \t Batch 1000 \t Training Loss: 44.18959597969055\n",
      "Epoch 12 \t Batch 1050 \t Training Loss: 44.136632448832195\n",
      "Epoch 12 \t Batch 1100 \t Training Loss: 44.08994861602783\n",
      "Epoch 12 \t Batch 1150 \t Training Loss: 44.083163279657775\n",
      "Epoch 12 \t Batch 1200 \t Training Loss: 44.030527294476826\n",
      "Epoch 12 \t Batch 1250 \t Training Loss: 44.08077143859863\n",
      "Epoch 12 \t Batch 1300 \t Training Loss: 44.10501236842229\n",
      "Epoch 12 \t Batch 1350 \t Training Loss: 44.15413470515498\n",
      "Epoch 12 \t Batch 1400 \t Training Loss: 44.14640361649649\n",
      "Epoch 12 \t Batch 1450 \t Training Loss: 44.17074099178972\n",
      "Epoch 12 \t Batch 1500 \t Training Loss: 44.170665181477865\n",
      "Epoch 12 \t Batch 1550 \t Training Loss: 44.17574056686894\n",
      "Epoch 12 \t Batch 1600 \t Training Loss: 44.14887004375458\n",
      "Epoch 12 \t Batch 1650 \t Training Loss: 44.15480398235899\n",
      "Epoch 12 \t Batch 1700 \t Training Loss: 44.12038027482874\n",
      "Epoch 12 \t Batch 1750 \t Training Loss: 44.14054570879255\n",
      "Epoch 12 \t Batch 1800 \t Training Loss: 44.10859064102173\n",
      "Epoch 12 \t Batch 1850 \t Training Loss: 44.10348006583549\n",
      "Epoch 12 \t Batch 1900 \t Training Loss: 44.15606204384252\n",
      "Epoch 12 \t Batch 1950 \t Training Loss: 44.19204443026812\n",
      "Epoch 12 \t Batch 2000 \t Training Loss: 44.17948763561249\n",
      "Epoch 12 \t Batch 2050 \t Training Loss: 44.17964928882878\n",
      "Epoch 12 \t Batch 2100 \t Training Loss: 44.20049806140718\n",
      "Epoch 12 \t Batch 2150 \t Training Loss: 44.19967512441236\n",
      "Epoch 12 \t Batch 2200 \t Training Loss: 44.23624845331366\n",
      "Epoch 12 \t Batch 2250 \t Training Loss: 44.227796463012695\n",
      "Epoch 12 \t Batch 2300 \t Training Loss: 44.27647764537645\n",
      "Epoch 12 \t Batch 2350 \t Training Loss: 44.313032357236175\n",
      "Epoch 12 \t Batch 2400 \t Training Loss: 44.2982272084554\n",
      "Epoch 12 \t Batch 2450 \t Training Loss: 44.28111765180315\n",
      "Epoch 12 \t Batch 2500 \t Training Loss: 44.30783701019287\n",
      "Epoch 12 \t Batch 2550 \t Training Loss: 44.28107964908376\n",
      "Epoch 12 \t Batch 2600 \t Training Loss: 44.275847099744354\n",
      "Epoch 12 \t Batch 2650 \t Training Loss: 44.25904974235679\n",
      "Epoch 12 \t Batch 2700 \t Training Loss: 44.244820319281686\n",
      "Epoch 12 \t Batch 2750 \t Training Loss: 44.233165310252794\n",
      "Epoch 12 \t Batch 2800 \t Training Loss: 44.24867604868753\n",
      "Epoch 12 \t Batch 2850 \t Training Loss: 44.23862730528179\n",
      "Epoch 12 \t Batch 2900 \t Training Loss: 44.23275487505156\n",
      "Epoch 12 \t Batch 2950 \t Training Loss: 44.22150795047566\n",
      "Epoch 12 \t Batch 3000 \t Training Loss: 44.214786591211954\n",
      "Epoch 12 \t Batch 3050 \t Training Loss: 44.18349533643879\n",
      "Epoch 12 \t Batch 3100 \t Training Loss: 44.206065098547164\n",
      "Epoch 12 \t Batch 3150 \t Training Loss: 44.21343114580427\n",
      "Epoch 12 \t Batch 3200 \t Training Loss: 44.192530653476716\n",
      "Epoch 12 \t Batch 3250 \t Training Loss: 44.16697439809946\n",
      "Epoch 12 \t Batch 3300 \t Training Loss: 44.15033309242942\n",
      "Epoch 12 \t Batch 3350 \t Training Loss: 44.121633687375194\n",
      "Epoch 12 \t Batch 3400 \t Training Loss: 44.09939737263848\n",
      "Epoch 12 \t Batch 3450 \t Training Loss: 44.12072234222855\n",
      "Epoch 12 \t Batch 3500 \t Training Loss: 44.10213726043701\n",
      "Epoch 12 \t Batch 3550 \t Training Loss: 44.07325274668949\n",
      "Epoch 12 \t Batch 3600 \t Training Loss: 44.103691903750104\n",
      "Epoch 12 \t Batch 3650 \t Training Loss: 44.105464376684736\n",
      "Epoch 12 \t Batch 50 \t Validation Loss: 31.404325618743897\n",
      "Epoch 12 \t Batch 100 \t Validation Loss: 38.00676113128662\n",
      "Epoch 12 \t Batch 150 \t Validation Loss: 31.774925734202068\n",
      "Epoch 12 \t Batch 200 \t Validation Loss: 31.5526824259758\n",
      "Epoch 12 \t Batch 250 \t Validation Loss: 35.692260328292846\n",
      "Epoch 12 \t Batch 300 \t Validation Loss: 33.47870570341746\n",
      "Epoch 12 \t Batch 350 \t Validation Loss: 33.3653931753976\n",
      "Epoch 12 \t Batch 400 \t Validation Loss: 32.28305422544479\n",
      "Epoch 12 \t Batch 450 \t Validation Loss: 32.44139213350084\n",
      "Epoch 12 \t Batch 500 \t Validation Loss: 31.726801248550416\n",
      "Epoch 12 \t Batch 550 \t Validation Loss: 31.09095392920754\n",
      "Epoch 12 \t Batch 600 \t Validation Loss: 31.453439540863037\n",
      "Epoch 12 \t Batch 650 \t Validation Loss: 32.96114915407621\n",
      "Epoch 12 \t Batch 700 \t Validation Loss: 34.40611558777945\n",
      "Epoch 12 \t Batch 750 \t Validation Loss: 35.341398022969564\n",
      "Epoch 12 \t Batch 800 \t Validation Loss: 36.340767085552216\n",
      "Epoch 12 \t Batch 850 \t Validation Loss: 36.922830592884736\n",
      "Epoch 12 \t Batch 900 \t Validation Loss: 37.03758768187629\n",
      "Epoch 12 \t Batch 950 \t Validation Loss: 37.02139254118267\n",
      "Epoch 12 \t Batch 1000 \t Validation Loss: 38.47724654579162\n",
      "Epoch 12 \t Batch 1050 \t Validation Loss: 39.39612860225496\n",
      "Epoch 12 \t Batch 1100 \t Validation Loss: 40.0255879909342\n",
      "Epoch 12 \t Batch 1150 \t Validation Loss: 39.97914831534676\n",
      "Epoch 12 \t Batch 1200 \t Validation Loss: 40.75590563098589\n",
      "Epoch 12 \t Batch 1250 \t Validation Loss: 41.04086553955078\n",
      "Epoch 12 \t Batch 1300 \t Validation Loss: 41.35507736352774\n",
      "Epoch 12 \t Batch 1350 \t Validation Loss: 41.12401137034098\n",
      "Epoch 12 \t Batch 1400 \t Validation Loss: 41.04609088761466\n",
      "Epoch 12 \t Batch 1450 \t Validation Loss: 41.03095937465799\n",
      "Epoch 12 \t Batch 1500 \t Validation Loss: 41.12711079311371\n",
      "Epoch 12 \t Batch 1550 \t Validation Loss: 40.88982071138197\n",
      "Epoch 12 \t Batch 1600 \t Validation Loss: 40.753027783334254\n",
      "Epoch 12 \t Batch 1650 \t Validation Loss: 40.851108300469136\n",
      "Epoch 12 \t Batch 1700 \t Validation Loss: 40.83625776992125\n",
      "Epoch 12 \t Batch 1750 \t Validation Loss: 40.73253715324402\n",
      "Epoch 12 \t Batch 1800 \t Validation Loss: 40.75596586253908\n",
      "Epoch 12 \t Batch 1850 \t Validation Loss: 41.28709294422253\n",
      "Epoch 12 \t Batch 1900 \t Validation Loss: 41.534003287114594\n",
      "Epoch 12 \t Batch 1950 \t Validation Loss: 41.40329917296385\n",
      "Epoch 12 \t Batch 2000 \t Validation Loss: 41.30168022179603\n",
      "Epoch 12 \t Batch 2050 \t Validation Loss: 41.435645901982376\n",
      "Epoch 12 \t Batch 2100 \t Validation Loss: 41.34671327431997\n",
      "Epoch 12 \t Batch 2150 \t Validation Loss: 41.10014256876568\n",
      "Epoch 12 \t Batch 2200 \t Validation Loss: 40.85275871125135\n",
      "Epoch 12 \t Batch 2250 \t Validation Loss: 40.66291047011482\n",
      "Epoch 12 \t Batch 2300 \t Validation Loss: 40.38509881662286\n",
      "Epoch 12 \t Batch 2350 \t Validation Loss: 40.29100773770759\n",
      "Epoch 12 \t Batch 2400 \t Validation Loss: 40.33141619225343\n",
      "Epoch 12 \t Batch 2450 \t Validation Loss: 40.63589548402903\n",
      "Epoch 12 Training Loss: 44.104561999683945 Validation Loss: 40.79126824361163\n",
      "Epoch 12 completed\n",
      "Epoch 13 \t Batch 50 \t Training Loss: 44.00549488067627\n",
      "Epoch 13 \t Batch 100 \t Training Loss: 43.65460287094116\n",
      "Epoch 13 \t Batch 150 \t Training Loss: 43.404582824707035\n",
      "Epoch 13 \t Batch 200 \t Training Loss: 43.30532824516296\n",
      "Epoch 13 \t Batch 250 \t Training Loss: 43.22554032897949\n",
      "Epoch 13 \t Batch 300 \t Training Loss: 43.0874450747172\n",
      "Epoch 13 \t Batch 350 \t Training Loss: 43.26758052280971\n",
      "Epoch 13 \t Batch 400 \t Training Loss: 43.19214220046997\n",
      "Epoch 13 \t Batch 450 \t Training Loss: 43.30452646891276\n",
      "Epoch 13 \t Batch 500 \t Training Loss: 43.34053801727295\n",
      "Epoch 13 \t Batch 550 \t Training Loss: 43.53049361142245\n",
      "Epoch 13 \t Batch 600 \t Training Loss: 43.462195552190146\n",
      "Epoch 13 \t Batch 650 \t Training Loss: 43.47552130185641\n",
      "Epoch 13 \t Batch 700 \t Training Loss: 43.57881899969918\n",
      "Epoch 13 \t Batch 750 \t Training Loss: 43.51837079620361\n",
      "Epoch 13 \t Batch 800 \t Training Loss: 43.55817833185196\n",
      "Epoch 13 \t Batch 850 \t Training Loss: 43.59419784770292\n",
      "Epoch 13 \t Batch 900 \t Training Loss: 43.54806632147895\n",
      "Epoch 13 \t Batch 950 \t Training Loss: 43.65749363949424\n",
      "Epoch 13 \t Batch 1000 \t Training Loss: 43.71217099761963\n",
      "Epoch 13 \t Batch 1050 \t Training Loss: 43.672038737705776\n",
      "Epoch 13 \t Batch 1100 \t Training Loss: 43.67811011227695\n",
      "Epoch 13 \t Batch 1150 \t Training Loss: 43.67160111302915\n",
      "Epoch 13 \t Batch 1200 \t Training Loss: 43.681048494974775\n",
      "Epoch 13 \t Batch 1250 \t Training Loss: 43.712967352294925\n",
      "Epoch 13 \t Batch 1300 \t Training Loss: 43.71074623694787\n",
      "Epoch 13 \t Batch 1350 \t Training Loss: 43.70759993659125\n",
      "Epoch 13 \t Batch 1400 \t Training Loss: 43.6499669442858\n",
      "Epoch 13 \t Batch 1450 \t Training Loss: 43.63687092090475\n",
      "Epoch 13 \t Batch 1500 \t Training Loss: 43.572646945953366\n",
      "Epoch 13 \t Batch 1550 \t Training Loss: 43.533055015071746\n",
      "Epoch 13 \t Batch 1600 \t Training Loss: 43.51366690993309\n",
      "Epoch 13 \t Batch 1650 \t Training Loss: 43.47585586432255\n",
      "Epoch 13 \t Batch 1700 \t Training Loss: 43.454381416545196\n",
      "Epoch 13 \t Batch 1750 \t Training Loss: 43.426470279148646\n",
      "Epoch 13 \t Batch 1800 \t Training Loss: 43.43278690020243\n",
      "Epoch 13 \t Batch 1850 \t Training Loss: 43.511562020585345\n",
      "Epoch 13 \t Batch 1900 \t Training Loss: 43.53690684368736\n",
      "Epoch 13 \t Batch 1950 \t Training Loss: 43.57207520020314\n",
      "Epoch 13 \t Batch 2000 \t Training Loss: 43.58106673336029\n",
      "Epoch 13 \t Batch 2050 \t Training Loss: 43.61365845843059\n",
      "Epoch 13 \t Batch 2100 \t Training Loss: 43.62806126367478\n",
      "Epoch 13 \t Batch 2150 \t Training Loss: 43.66772432903911\n",
      "Epoch 13 \t Batch 2200 \t Training Loss: 43.6654625043002\n",
      "Epoch 13 \t Batch 2250 \t Training Loss: 43.66459216562907\n",
      "Epoch 13 \t Batch 2300 \t Training Loss: 43.674197274083674\n",
      "Epoch 13 \t Batch 2350 \t Training Loss: 43.65579980566147\n",
      "Epoch 13 \t Batch 2400 \t Training Loss: 43.658201327323916\n",
      "Epoch 13 \t Batch 2450 \t Training Loss: 43.638505301183585\n",
      "Epoch 13 \t Batch 2500 \t Training Loss: 43.6256725227356\n",
      "Epoch 13 \t Batch 2550 \t Training Loss: 43.618995578242284\n",
      "Epoch 13 \t Batch 2600 \t Training Loss: 43.63581701131967\n",
      "Epoch 13 \t Batch 2650 \t Training Loss: 43.63881634550275\n",
      "Epoch 13 \t Batch 2700 \t Training Loss: 43.655149333388714\n",
      "Epoch 13 \t Batch 2750 \t Training Loss: 43.64697893801603\n",
      "Epoch 13 \t Batch 2800 \t Training Loss: 43.65253378323146\n",
      "Epoch 13 \t Batch 2850 \t Training Loss: 43.669866290176124\n",
      "Epoch 13 \t Batch 2900 \t Training Loss: 43.684861806672195\n",
      "Epoch 13 \t Batch 2950 \t Training Loss: 43.706972082105736\n",
      "Epoch 13 \t Batch 3000 \t Training Loss: 43.72777978833516\n",
      "Epoch 13 \t Batch 3050 \t Training Loss: 43.727931081427904\n",
      "Epoch 13 \t Batch 3100 \t Training Loss: 43.71182583101334\n",
      "Epoch 13 \t Batch 3150 \t Training Loss: 43.68561366550506\n",
      "Epoch 13 \t Batch 3200 \t Training Loss: 43.706464744210244\n",
      "Epoch 13 \t Batch 3250 \t Training Loss: 43.70473337144118\n",
      "Epoch 13 \t Batch 3300 \t Training Loss: 43.7273111632376\n",
      "Epoch 13 \t Batch 3350 \t Training Loss: 43.74969910237326\n",
      "Epoch 13 \t Batch 3400 \t Training Loss: 43.740099985459274\n",
      "Epoch 13 \t Batch 3450 \t Training Loss: 43.739956041418985\n",
      "Epoch 13 \t Batch 3500 \t Training Loss: 43.74375701577323\n",
      "Epoch 13 \t Batch 3550 \t Training Loss: 43.745830792709135\n",
      "Epoch 13 \t Batch 3600 \t Training Loss: 43.77127567715115\n",
      "Epoch 13 \t Batch 3650 \t Training Loss: 43.774721730898506\n",
      "Epoch 13 \t Batch 50 \t Validation Loss: 27.457467765808104\n",
      "Epoch 13 \t Batch 100 \t Validation Loss: 33.7331548500061\n",
      "Epoch 13 \t Batch 150 \t Validation Loss: 30.55917398452759\n",
      "Epoch 13 \t Batch 200 \t Validation Loss: 29.958480367660524\n",
      "Epoch 13 \t Batch 250 \t Validation Loss: 32.923796371459964\n",
      "Epoch 13 \t Batch 300 \t Validation Loss: 31.721916785240172\n",
      "Epoch 13 \t Batch 350 \t Validation Loss: 31.806033857890537\n",
      "Epoch 13 \t Batch 400 \t Validation Loss: 30.947828599214553\n",
      "Epoch 13 \t Batch 450 \t Validation Loss: 31.160508048799304\n",
      "Epoch 13 \t Batch 500 \t Validation Loss: 30.55864809989929\n",
      "Epoch 13 \t Batch 550 \t Validation Loss: 30.054905718890105\n",
      "Epoch 13 \t Batch 600 \t Validation Loss: 30.575379774570465\n",
      "Epoch 13 \t Batch 650 \t Validation Loss: 32.282573191569405\n",
      "Epoch 13 \t Batch 700 \t Validation Loss: 33.90971177646092\n",
      "Epoch 13 \t Batch 750 \t Validation Loss: 34.892090401967366\n",
      "Epoch 13 \t Batch 800 \t Validation Loss: 36.09602766990662\n",
      "Epoch 13 \t Batch 850 \t Validation Loss: 36.725793135025924\n",
      "Epoch 13 \t Batch 900 \t Validation Loss: 36.879612344106036\n",
      "Epoch 13 \t Batch 950 \t Validation Loss: 36.92300249601665\n",
      "Epoch 13 \t Batch 1000 \t Validation Loss: 38.513604734420774\n",
      "Epoch 13 \t Batch 1050 \t Validation Loss: 39.55031373932248\n",
      "Epoch 13 \t Batch 1100 \t Validation Loss: 40.26318503249775\n",
      "Epoch 13 \t Batch 1150 \t Validation Loss: 40.183132625662765\n",
      "Epoch 13 \t Batch 1200 \t Validation Loss: 41.050187795559566\n",
      "Epoch 13 \t Batch 1250 \t Validation Loss: 41.34280520401001\n",
      "Epoch 13 \t Batch 1300 \t Validation Loss: 41.59647721584027\n",
      "Epoch 13 \t Batch 1350 \t Validation Loss: 41.31660297181871\n",
      "Epoch 13 \t Batch 1400 \t Validation Loss: 41.2214999798366\n",
      "Epoch 13 \t Batch 1450 \t Validation Loss: 41.33327737742457\n",
      "Epoch 13 \t Batch 1500 \t Validation Loss: 41.45481226253509\n",
      "Epoch 13 \t Batch 1550 \t Validation Loss: 41.110236534918506\n",
      "Epoch 13 \t Batch 1600 \t Validation Loss: 40.849866437017916\n",
      "Epoch 13 \t Batch 1650 \t Validation Loss: 40.88897075335185\n",
      "Epoch 13 \t Batch 1700 \t Validation Loss: 40.716858158392064\n",
      "Epoch 13 \t Batch 1750 \t Validation Loss: 40.511450260434835\n",
      "Epoch 13 \t Batch 1800 \t Validation Loss: 40.6751292347908\n",
      "Epoch 13 \t Batch 1850 \t Validation Loss: 41.158893225644086\n",
      "Epoch 13 \t Batch 1900 \t Validation Loss: 41.38002318156393\n",
      "Epoch 13 \t Batch 1950 \t Validation Loss: 41.19126685338143\n",
      "Epoch 13 \t Batch 2000 \t Validation Loss: 41.160018385648726\n",
      "Epoch 13 \t Batch 2050 \t Validation Loss: 41.49865535247616\n",
      "Epoch 13 \t Batch 2100 \t Validation Loss: 41.385338231268385\n",
      "Epoch 13 \t Batch 2150 \t Validation Loss: 41.13366581695024\n",
      "Epoch 13 \t Batch 2200 \t Validation Loss: 40.86916678363627\n",
      "Epoch 13 \t Batch 2250 \t Validation Loss: 40.61783243751526\n",
      "Epoch 13 \t Batch 2300 \t Validation Loss: 40.281304848816085\n",
      "Epoch 13 \t Batch 2350 \t Validation Loss: 40.162481280083355\n",
      "Epoch 13 \t Batch 2400 \t Validation Loss: 40.201017890870574\n",
      "Epoch 13 \t Batch 2450 \t Validation Loss: 40.45034240771313\n",
      "Epoch 13 Training Loss: 43.76398835967385 Validation Loss: 40.59029639899344\n",
      "Epoch 13 completed\n",
      "Epoch 14 \t Batch 50 \t Training Loss: 43.995808143615726\n",
      "Epoch 14 \t Batch 100 \t Training Loss: 43.133537445068356\n",
      "Epoch 14 \t Batch 150 \t Training Loss: 43.04944868723551\n",
      "Epoch 14 \t Batch 200 \t Training Loss: 42.88609938621521\n",
      "Epoch 14 \t Batch 250 \t Training Loss: 42.9310277633667\n",
      "Epoch 14 \t Batch 300 \t Training Loss: 43.2199826558431\n",
      "Epoch 14 \t Batch 350 \t Training Loss: 43.09638032095773\n",
      "Epoch 14 \t Batch 400 \t Training Loss: 42.92744663715362\n",
      "Epoch 14 \t Batch 450 \t Training Loss: 42.90233677758111\n",
      "Epoch 14 \t Batch 500 \t Training Loss: 42.88030898666382\n",
      "Epoch 14 \t Batch 550 \t Training Loss: 42.79811707583341\n",
      "Epoch 14 \t Batch 600 \t Training Loss: 42.84960464795431\n",
      "Epoch 14 \t Batch 650 \t Training Loss: 42.8766981946505\n",
      "Epoch 14 \t Batch 700 \t Training Loss: 42.84977822167533\n",
      "Epoch 14 \t Batch 750 \t Training Loss: 42.87337022654216\n",
      "Epoch 14 \t Batch 800 \t Training Loss: 43.04516182184219\n",
      "Epoch 14 \t Batch 850 \t Training Loss: 43.063952248517204\n",
      "Epoch 14 \t Batch 900 \t Training Loss: 43.02659308751424\n",
      "Epoch 14 \t Batch 950 \t Training Loss: 42.966651980751436\n",
      "Epoch 14 \t Batch 1000 \t Training Loss: 43.021704006195066\n",
      "Epoch 14 \t Batch 1050 \t Training Loss: 42.980631039937336\n",
      "Epoch 14 \t Batch 1100 \t Training Loss: 42.95862582293424\n",
      "Epoch 14 \t Batch 1150 \t Training Loss: 43.050968593099846\n",
      "Epoch 14 \t Batch 1200 \t Training Loss: 43.026080204645794\n",
      "Epoch 14 \t Batch 1250 \t Training Loss: 43.076216040039064\n",
      "Epoch 14 \t Batch 1300 \t Training Loss: 43.15949589362511\n",
      "Epoch 14 \t Batch 1350 \t Training Loss: 43.18465902257849\n",
      "Epoch 14 \t Batch 1400 \t Training Loss: 43.18985342706953\n",
      "Epoch 14 \t Batch 1450 \t Training Loss: 43.14800788090147\n",
      "Epoch 14 \t Batch 1500 \t Training Loss: 43.19662037531535\n",
      "Epoch 14 \t Batch 1550 \t Training Loss: 43.27287357084213\n",
      "Epoch 14 \t Batch 1600 \t Training Loss: 43.24107247591019\n",
      "Epoch 14 \t Batch 1650 \t Training Loss: 43.215252736409504\n",
      "Epoch 14 \t Batch 1700 \t Training Loss: 43.1948395762724\n",
      "Epoch 14 \t Batch 1750 \t Training Loss: 43.186875081743516\n",
      "Epoch 14 \t Batch 1800 \t Training Loss: 43.2028614086575\n",
      "Epoch 14 \t Batch 1850 \t Training Loss: 43.22671395276044\n",
      "Epoch 14 \t Batch 1900 \t Training Loss: 43.25177110872771\n",
      "Epoch 14 \t Batch 1950 \t Training Loss: 43.20625658181997\n",
      "Epoch 14 \t Batch 2000 \t Training Loss: 43.22947586536407\n",
      "Epoch 14 \t Batch 2050 \t Training Loss: 43.19935703091505\n",
      "Epoch 14 \t Batch 2100 \t Training Loss: 43.138310874757316\n",
      "Epoch 14 \t Batch 2150 \t Training Loss: 43.19731848871985\n",
      "Epoch 14 \t Batch 2200 \t Training Loss: 43.2180652366985\n",
      "Epoch 14 \t Batch 2250 \t Training Loss: 43.2218754196167\n",
      "Epoch 14 \t Batch 2300 \t Training Loss: 43.20737774226976\n",
      "Epoch 14 \t Batch 2350 \t Training Loss: 43.163775719987584\n",
      "Epoch 14 \t Batch 2400 \t Training Loss: 43.165185527006784\n",
      "Epoch 14 \t Batch 2450 \t Training Loss: 43.169043596228775\n",
      "Epoch 14 \t Batch 2500 \t Training Loss: 43.20488165283203\n",
      "Epoch 14 \t Batch 2550 \t Training Loss: 43.19383541855158\n",
      "Epoch 14 \t Batch 2600 \t Training Loss: 43.20348279219407\n",
      "Epoch 14 \t Batch 2650 \t Training Loss: 43.23161203348412\n",
      "Epoch 14 \t Batch 2700 \t Training Loss: 43.26094888757776\n",
      "Epoch 14 \t Batch 2750 \t Training Loss: 43.25411947007613\n",
      "Epoch 14 \t Batch 2800 \t Training Loss: 43.263139831679204\n",
      "Epoch 14 \t Batch 2850 \t Training Loss: 43.29629123821593\n",
      "Epoch 14 \t Batch 2900 \t Training Loss: 43.32712019229757\n",
      "Epoch 14 \t Batch 2950 \t Training Loss: 43.344379309961354\n",
      "Epoch 14 \t Batch 3000 \t Training Loss: 43.35443618520101\n",
      "Epoch 14 \t Batch 3050 \t Training Loss: 43.366391201331965\n",
      "Epoch 14 \t Batch 3100 \t Training Loss: 43.347623287323984\n",
      "Epoch 14 \t Batch 3150 \t Training Loss: 43.357006247384206\n",
      "Epoch 14 \t Batch 3200 \t Training Loss: 43.36737010717392\n",
      "Epoch 14 \t Batch 3250 \t Training Loss: 43.419796131427475\n",
      "Epoch 14 \t Batch 3300 \t Training Loss: 43.41189380472357\n",
      "Epoch 14 \t Batch 3350 \t Training Loss: 43.43149968560062\n",
      "Epoch 14 \t Batch 3400 \t Training Loss: 43.43531941582175\n",
      "Epoch 14 \t Batch 3450 \t Training Loss: 43.4468526547197\n",
      "Epoch 14 \t Batch 3500 \t Training Loss: 43.44539200047084\n",
      "Epoch 14 \t Batch 3550 \t Training Loss: 43.43819643799688\n",
      "Epoch 14 \t Batch 3600 \t Training Loss: 43.42845688343048\n",
      "Epoch 14 \t Batch 3650 \t Training Loss: 43.44860435120047\n",
      "Epoch 14 \t Batch 50 \t Validation Loss: 40.64215841293335\n",
      "Epoch 14 \t Batch 100 \t Validation Loss: 49.31630146980286\n",
      "Epoch 14 \t Batch 150 \t Validation Loss: 42.990337918599444\n",
      "Epoch 14 \t Batch 200 \t Validation Loss: 42.340306105613706\n",
      "Epoch 14 \t Batch 250 \t Validation Loss: 46.1895884399414\n",
      "Epoch 14 \t Batch 300 \t Validation Loss: 44.09329019546509\n",
      "Epoch 14 \t Batch 350 \t Validation Loss: 42.74182349886213\n",
      "Epoch 14 \t Batch 400 \t Validation Loss: 40.63764898061752\n",
      "Epoch 14 \t Batch 450 \t Validation Loss: 39.89531108856201\n",
      "Epoch 14 \t Batch 500 \t Validation Loss: 38.5628205871582\n",
      "Epoch 14 \t Batch 550 \t Validation Loss: 37.398799917047675\n",
      "Epoch 14 \t Batch 600 \t Validation Loss: 37.15246899922689\n",
      "Epoch 14 \t Batch 650 \t Validation Loss: 37.981411942702074\n",
      "Epoch 14 \t Batch 700 \t Validation Loss: 39.07254015581948\n",
      "Epoch 14 \t Batch 750 \t Validation Loss: 39.63038193448384\n",
      "Epoch 14 \t Batch 800 \t Validation Loss: 40.28691875040531\n",
      "Epoch 14 \t Batch 850 \t Validation Loss: 40.6330413016151\n",
      "Epoch 14 \t Batch 900 \t Validation Loss: 40.48948813703325\n",
      "Epoch 14 \t Batch 950 \t Validation Loss: 40.22011352689643\n",
      "Epoch 14 \t Batch 1000 \t Validation Loss: 41.47188245153427\n",
      "Epoch 14 \t Batch 1050 \t Validation Loss: 42.149149595896404\n",
      "Epoch 14 \t Batch 1100 \t Validation Loss: 42.58209003101695\n",
      "Epoch 14 \t Batch 1150 \t Validation Loss: 42.387992429318636\n",
      "Epoch 14 \t Batch 1200 \t Validation Loss: 42.93031489054362\n",
      "Epoch 14 \t Batch 1250 \t Validation Loss: 43.03675248908996\n",
      "Epoch 14 \t Batch 1300 \t Validation Loss: 43.12190293055314\n",
      "Epoch 14 \t Batch 1350 \t Validation Loss: 42.762403336630925\n",
      "Epoch 14 \t Batch 1400 \t Validation Loss: 42.51599749667304\n",
      "Epoch 14 \t Batch 1450 \t Validation Loss: 42.362591057152585\n",
      "Epoch 14 \t Batch 1500 \t Validation Loss: 42.38447605355581\n",
      "Epoch 14 \t Batch 1550 \t Validation Loss: 41.981537803834485\n",
      "Epoch 14 \t Batch 1600 \t Validation Loss: 41.646250087916854\n",
      "Epoch 14 \t Batch 1650 \t Validation Loss: 41.585610928679955\n",
      "Epoch 14 \t Batch 1700 \t Validation Loss: 41.39384037494659\n",
      "Epoch 14 \t Batch 1750 \t Validation Loss: 41.13001352664403\n",
      "Epoch 14 \t Batch 1800 \t Validation Loss: 40.97390225436953\n",
      "Epoch 14 \t Batch 1850 \t Validation Loss: 41.4303074081524\n",
      "Epoch 14 \t Batch 1900 \t Validation Loss: 41.575977864767374\n",
      "Epoch 14 \t Batch 1950 \t Validation Loss: 41.370481672286985\n",
      "Epoch 14 \t Batch 2000 \t Validation Loss: 41.210705777406694\n",
      "Epoch 14 \t Batch 2050 \t Validation Loss: 41.20841093644863\n",
      "Epoch 14 \t Batch 2100 \t Validation Loss: 41.00578768207913\n",
      "Epoch 14 \t Batch 2150 \t Validation Loss: 40.747715953782546\n",
      "Epoch 14 \t Batch 2200 \t Validation Loss: 40.50302165920084\n",
      "Epoch 14 \t Batch 2250 \t Validation Loss: 40.27911473316617\n",
      "Epoch 14 \t Batch 2300 \t Validation Loss: 39.96270601790884\n",
      "Epoch 14 \t Batch 2350 \t Validation Loss: 39.84731765564452\n",
      "Epoch 14 \t Batch 2400 \t Validation Loss: 39.89055798272292\n",
      "Epoch 14 \t Batch 2450 \t Validation Loss: 40.155010534597906\n",
      "Epoch 14 Training Loss: 43.46489331870459 Validation Loss: 40.29896781022673\n",
      "Epoch 14 completed\n",
      "Epoch 15 \t Batch 50 \t Training Loss: 41.24335083007813\n",
      "Epoch 15 \t Batch 100 \t Training Loss: 41.41922086715698\n",
      "Epoch 15 \t Batch 150 \t Training Loss: 42.01213385264079\n",
      "Epoch 15 \t Batch 200 \t Training Loss: 42.10961282730103\n",
      "Epoch 15 \t Batch 250 \t Training Loss: 42.36140601348877\n",
      "Epoch 15 \t Batch 300 \t Training Loss: 42.222601356506345\n",
      "Epoch 15 \t Batch 350 \t Training Loss: 41.92035425458636\n",
      "Epoch 15 \t Batch 400 \t Training Loss: 41.92111289024353\n",
      "Epoch 15 \t Batch 450 \t Training Loss: 41.914745792812774\n",
      "Epoch 15 \t Batch 500 \t Training Loss: 41.93706080245972\n",
      "Epoch 15 \t Batch 550 \t Training Loss: 42.03642641587691\n",
      "Epoch 15 \t Batch 600 \t Training Loss: 42.12349732398987\n",
      "Epoch 15 \t Batch 650 \t Training Loss: 42.167294153066784\n",
      "Epoch 15 \t Batch 700 \t Training Loss: 42.2110721206665\n",
      "Epoch 15 \t Batch 750 \t Training Loss: 42.22990375264486\n",
      "Epoch 15 \t Batch 800 \t Training Loss: 42.35707048654556\n",
      "Epoch 15 \t Batch 850 \t Training Loss: 42.420946565515855\n",
      "Epoch 15 \t Batch 900 \t Training Loss: 42.49110283109877\n",
      "Epoch 15 \t Batch 950 \t Training Loss: 42.5319617281462\n",
      "Epoch 15 \t Batch 1000 \t Training Loss: 42.524408309936526\n",
      "Epoch 15 \t Batch 1050 \t Training Loss: 42.52945584796724\n",
      "Epoch 15 \t Batch 1100 \t Training Loss: 42.641913218064744\n",
      "Epoch 15 \t Batch 1150 \t Training Loss: 42.62010767895242\n",
      "Epoch 15 \t Batch 1200 \t Training Loss: 42.65616407712301\n",
      "Epoch 15 \t Batch 1250 \t Training Loss: 42.67407921295166\n",
      "Epoch 15 \t Batch 1300 \t Training Loss: 42.71774876081027\n",
      "Epoch 15 \t Batch 1350 \t Training Loss: 42.75055095107467\n",
      "Epoch 15 \t Batch 1400 \t Training Loss: 42.74201540538243\n",
      "Epoch 15 \t Batch 1450 \t Training Loss: 42.74906758538608\n",
      "Epoch 15 \t Batch 1500 \t Training Loss: 42.74840032704671\n",
      "Epoch 15 \t Batch 1550 \t Training Loss: 42.771095964985506\n",
      "Epoch 15 \t Batch 1600 \t Training Loss: 42.75882614612579\n",
      "Epoch 15 \t Batch 1650 \t Training Loss: 42.7551785625111\n",
      "Epoch 15 \t Batch 1700 \t Training Loss: 42.81269522947424\n",
      "Epoch 15 \t Batch 1750 \t Training Loss: 42.83914699009487\n",
      "Epoch 15 \t Batch 1800 \t Training Loss: 42.82402886284722\n",
      "Epoch 15 \t Batch 1850 \t Training Loss: 42.84409831691433\n",
      "Epoch 15 \t Batch 1900 \t Training Loss: 42.87836637898495\n",
      "Epoch 15 \t Batch 1950 \t Training Loss: 42.90484110905574\n",
      "Epoch 15 \t Batch 2000 \t Training Loss: 42.90384784030914\n",
      "Epoch 15 \t Batch 2050 \t Training Loss: 42.92608066186672\n",
      "Epoch 15 \t Batch 2100 \t Training Loss: 42.917848646981376\n",
      "Epoch 15 \t Batch 2150 \t Training Loss: 42.95410371115042\n",
      "Epoch 15 \t Batch 2200 \t Training Loss: 42.994525345889\n",
      "Epoch 15 \t Batch 2250 \t Training Loss: 42.992432587517634\n",
      "Epoch 15 \t Batch 2300 \t Training Loss: 43.00323035115781\n",
      "Epoch 15 \t Batch 2350 \t Training Loss: 43.034661867669286\n",
      "Epoch 15 \t Batch 2400 \t Training Loss: 43.02693011045456\n",
      "Epoch 15 \t Batch 2450 \t Training Loss: 43.0494710579697\n",
      "Epoch 15 \t Batch 2500 \t Training Loss: 43.04422174148559\n",
      "Epoch 15 \t Batch 2550 \t Training Loss: 43.0623792962467\n",
      "Epoch 15 \t Batch 2600 \t Training Loss: 43.05910277366638\n",
      "Epoch 15 \t Batch 2650 \t Training Loss: 43.0544578537851\n",
      "Epoch 15 \t Batch 2700 \t Training Loss: 43.07960169332999\n",
      "Epoch 15 \t Batch 2750 \t Training Loss: 43.05654538310658\n",
      "Epoch 15 \t Batch 2800 \t Training Loss: 43.04546439443316\n",
      "Epoch 15 \t Batch 2850 \t Training Loss: 43.081163628561455\n",
      "Epoch 15 \t Batch 2900 \t Training Loss: 43.07848389987288\n",
      "Epoch 15 \t Batch 2950 \t Training Loss: 43.07707391318628\n",
      "Epoch 15 \t Batch 3000 \t Training Loss: 43.07726199277242\n",
      "Epoch 15 \t Batch 3050 \t Training Loss: 43.090229566605366\n",
      "Epoch 15 \t Batch 3100 \t Training Loss: 43.07599597869381\n",
      "Epoch 15 \t Batch 3150 \t Training Loss: 43.09016906798832\n",
      "Epoch 15 \t Batch 3200 \t Training Loss: 43.10065674722195\n",
      "Epoch 15 \t Batch 3250 \t Training Loss: 43.1057325867873\n",
      "Epoch 15 \t Batch 3300 \t Training Loss: 43.11478418812607\n",
      "Epoch 15 \t Batch 3350 \t Training Loss: 43.11562772722387\n",
      "Epoch 15 \t Batch 3400 \t Training Loss: 43.11104827936958\n",
      "Epoch 15 \t Batch 3450 \t Training Loss: 43.11353975987089\n",
      "Epoch 15 \t Batch 3500 \t Training Loss: 43.116444640568325\n",
      "Epoch 15 \t Batch 3550 \t Training Loss: 43.14022474369533\n",
      "Epoch 15 \t Batch 3600 \t Training Loss: 43.1489966445499\n",
      "Epoch 15 \t Batch 3650 \t Training Loss: 43.14859771780772\n",
      "Epoch 15 \t Batch 50 \t Validation Loss: 39.660385036468504\n",
      "Epoch 15 \t Batch 100 \t Validation Loss: 50.34862448692322\n",
      "Epoch 15 \t Batch 150 \t Validation Loss: 42.79849098523458\n",
      "Epoch 15 \t Batch 200 \t Validation Loss: 41.7367281126976\n",
      "Epoch 15 \t Batch 250 \t Validation Loss: 45.914550367355346\n",
      "Epoch 15 \t Batch 300 \t Validation Loss: 43.16389667669932\n",
      "Epoch 15 \t Batch 350 \t Validation Loss: 41.857183850152154\n",
      "Epoch 15 \t Batch 400 \t Validation Loss: 39.84904590964317\n",
      "Epoch 15 \t Batch 450 \t Validation Loss: 39.135414328045314\n",
      "Epoch 15 \t Batch 500 \t Validation Loss: 37.78388408756256\n",
      "Epoch 15 \t Batch 550 \t Validation Loss: 36.6410864387859\n",
      "Epoch 15 \t Batch 600 \t Validation Loss: 36.785642074743905\n",
      "Epoch 15 \t Batch 650 \t Validation Loss: 38.05477761855492\n",
      "Epoch 15 \t Batch 700 \t Validation Loss: 39.670253240721564\n",
      "Epoch 15 \t Batch 750 \t Validation Loss: 40.55732408841451\n",
      "Epoch 15 \t Batch 800 \t Validation Loss: 41.57718849718571\n",
      "Epoch 15 \t Batch 850 \t Validation Loss: 42.195766139871935\n",
      "Epoch 15 \t Batch 900 \t Validation Loss: 42.27468156019847\n",
      "Epoch 15 \t Batch 950 \t Validation Loss: 42.172477041043734\n",
      "Epoch 15 \t Batch 1000 \t Validation Loss: 43.72347098302841\n",
      "Epoch 15 \t Batch 1050 \t Validation Loss: 44.74857871418907\n",
      "Epoch 15 \t Batch 1100 \t Validation Loss: 45.44750960740176\n",
      "Epoch 15 \t Batch 1150 \t Validation Loss: 45.308143786969396\n",
      "Epoch 15 \t Batch 1200 \t Validation Loss: 46.05407056371371\n",
      "Epoch 15 \t Batch 1250 \t Validation Loss: 46.3044017791748\n",
      "Epoch 15 \t Batch 1300 \t Validation Loss: 46.38006323961111\n",
      "Epoch 15 \t Batch 1350 \t Validation Loss: 46.00962631861369\n",
      "Epoch 15 \t Batch 1400 \t Validation Loss: 45.77804555552346\n",
      "Epoch 15 \t Batch 1450 \t Validation Loss: 45.70387360474159\n",
      "Epoch 15 \t Batch 1500 \t Validation Loss: 45.789078109105425\n",
      "Epoch 15 \t Batch 1550 \t Validation Loss: 45.329104065433626\n",
      "Epoch 15 \t Batch 1600 \t Validation Loss: 44.981987051963806\n",
      "Epoch 15 \t Batch 1650 \t Validation Loss: 44.87613963502826\n",
      "Epoch 15 \t Batch 1700 \t Validation Loss: 44.639415498621325\n",
      "Epoch 15 \t Batch 1750 \t Validation Loss: 44.33972850472586\n",
      "Epoch 15 \t Batch 1800 \t Validation Loss: 44.231059867011176\n",
      "Epoch 15 \t Batch 1850 \t Validation Loss: 44.66642878351985\n",
      "Epoch 15 \t Batch 1900 \t Validation Loss: 44.784120558688514\n",
      "Epoch 15 \t Batch 1950 \t Validation Loss: 44.53298430271638\n",
      "Epoch 15 \t Batch 2000 \t Validation Loss: 44.378449432373046\n",
      "Epoch 15 \t Batch 2050 \t Validation Loss: 44.47128558972987\n",
      "Epoch 15 \t Batch 2100 \t Validation Loss: 44.269439124152775\n",
      "Epoch 15 \t Batch 2150 \t Validation Loss: 43.94227539905282\n",
      "Epoch 15 \t Batch 2200 \t Validation Loss: 43.61565860011361\n",
      "Epoch 15 \t Batch 2250 \t Validation Loss: 43.29472617255317\n",
      "Epoch 15 \t Batch 2300 \t Validation Loss: 42.88930954383767\n",
      "Epoch 15 \t Batch 2350 \t Validation Loss: 42.69139025617153\n",
      "Epoch 15 \t Batch 2400 \t Validation Loss: 42.68288696954648\n",
      "Epoch 15 \t Batch 2450 \t Validation Loss: 42.865657254141205\n",
      "Epoch 15 Training Loss: 43.15311500940354 Validation Loss: 42.978646258738905\n",
      "Epoch 15 completed\n",
      "Epoch 16 \t Batch 50 \t Training Loss: 40.63872665405273\n",
      "Epoch 16 \t Batch 100 \t Training Loss: 41.14380018234253\n",
      "Epoch 16 \t Batch 150 \t Training Loss: 41.79898298899332\n",
      "Epoch 16 \t Batch 200 \t Training Loss: 41.94772061347962\n",
      "Epoch 16 \t Batch 250 \t Training Loss: 41.745015785217284\n",
      "Epoch 16 \t Batch 300 \t Training Loss: 41.885360171000166\n",
      "Epoch 16 \t Batch 350 \t Training Loss: 42.20587346758161\n",
      "Epoch 16 \t Batch 400 \t Training Loss: 42.260849199295045\n",
      "Epoch 16 \t Batch 450 \t Training Loss: 42.196040636698406\n",
      "Epoch 16 \t Batch 500 \t Training Loss: 42.16774396514892\n",
      "Epoch 16 \t Batch 550 \t Training Loss: 42.131834064830436\n",
      "Epoch 16 \t Batch 600 \t Training Loss: 42.274658654530846\n",
      "Epoch 16 \t Batch 650 \t Training Loss: 42.15628235450158\n",
      "Epoch 16 \t Batch 700 \t Training Loss: 42.11056127003261\n",
      "Epoch 16 \t Batch 750 \t Training Loss: 42.09478624216715\n",
      "Epoch 16 \t Batch 800 \t Training Loss: 42.16476334810257\n",
      "Epoch 16 \t Batch 850 \t Training Loss: 42.188494125815\n",
      "Epoch 16 \t Batch 900 \t Training Loss: 42.167834972805444\n",
      "Epoch 16 \t Batch 950 \t Training Loss: 42.241213643927324\n",
      "Epoch 16 \t Batch 1000 \t Training Loss: 42.23615669441223\n",
      "Epoch 16 \t Batch 1050 \t Training Loss: 42.23849540165492\n",
      "Epoch 16 \t Batch 1100 \t Training Loss: 42.281228245821865\n",
      "Epoch 16 \t Batch 1150 \t Training Loss: 42.27781903971796\n",
      "Epoch 16 \t Batch 1200 \t Training Loss: 42.32165523370107\n",
      "Epoch 16 \t Batch 1250 \t Training Loss: 42.340169078063965\n",
      "Epoch 16 \t Batch 1300 \t Training Loss: 42.29512506338266\n",
      "Epoch 16 \t Batch 1350 \t Training Loss: 42.25441670311822\n",
      "Epoch 16 \t Batch 1400 \t Training Loss: 42.26700834955488\n",
      "Epoch 16 \t Batch 1450 \t Training Loss: 42.31790066160005\n",
      "Epoch 16 \t Batch 1500 \t Training Loss: 42.32744872538249\n",
      "Epoch 16 \t Batch 1550 \t Training Loss: 42.385101427878105\n",
      "Epoch 16 \t Batch 1600 \t Training Loss: 42.43883089661598\n",
      "Epoch 16 \t Batch 1650 \t Training Loss: 42.468616738752885\n",
      "Epoch 16 \t Batch 1700 \t Training Loss: 42.4867181037454\n",
      "Epoch 16 \t Batch 1750 \t Training Loss: 42.47008592551095\n",
      "Epoch 16 \t Batch 1800 \t Training Loss: 42.48418098979526\n",
      "Epoch 16 \t Batch 1850 \t Training Loss: 42.43945873775998\n",
      "Epoch 16 \t Batch 1900 \t Training Loss: 42.48334667406584\n",
      "Epoch 16 \t Batch 1950 \t Training Loss: 42.52012564145602\n",
      "Epoch 16 \t Batch 2000 \t Training Loss: 42.534572547912596\n",
      "Epoch 16 \t Batch 2050 \t Training Loss: 42.56281357369772\n",
      "Epoch 16 \t Batch 2100 \t Training Loss: 42.56535612197149\n",
      "Epoch 16 \t Batch 2150 \t Training Loss: 42.56343422290891\n",
      "Epoch 16 \t Batch 2200 \t Training Loss: 42.55838634664362\n",
      "Epoch 16 \t Batch 2250 \t Training Loss: 42.56123415544298\n",
      "Epoch 16 \t Batch 2300 \t Training Loss: 42.56103556011034\n",
      "Epoch 16 \t Batch 2350 \t Training Loss: 42.54850755894438\n",
      "Epoch 16 \t Batch 2400 \t Training Loss: 42.569699437618254\n",
      "Epoch 16 \t Batch 2450 \t Training Loss: 42.57652134253054\n",
      "Epoch 16 \t Batch 2500 \t Training Loss: 42.574413396453856\n",
      "Epoch 16 \t Batch 2550 \t Training Loss: 42.62427492852304\n",
      "Epoch 16 \t Batch 2600 \t Training Loss: 42.66699692212618\n",
      "Epoch 16 \t Batch 2650 \t Training Loss: 42.667981524917316\n",
      "Epoch 16 \t Batch 2700 \t Training Loss: 42.68723145732173\n",
      "Epoch 16 \t Batch 2750 \t Training Loss: 42.68773516498913\n",
      "Epoch 16 \t Batch 2800 \t Training Loss: 42.66405161312648\n",
      "Epoch 16 \t Batch 2850 \t Training Loss: 42.6646009779813\n",
      "Epoch 16 \t Batch 2900 \t Training Loss: 42.69018973054557\n",
      "Epoch 16 \t Batch 2950 \t Training Loss: 42.70787279484636\n",
      "Epoch 16 \t Batch 3000 \t Training Loss: 42.70514553070068\n",
      "Epoch 16 \t Batch 3050 \t Training Loss: 42.69100118480745\n",
      "Epoch 16 \t Batch 3100 \t Training Loss: 42.708831963692944\n",
      "Epoch 16 \t Batch 3150 \t Training Loss: 42.72244371262808\n",
      "Epoch 16 \t Batch 3200 \t Training Loss: 42.74309674501419\n",
      "Epoch 16 \t Batch 3250 \t Training Loss: 42.74967734527588\n",
      "Epoch 16 \t Batch 3300 \t Training Loss: 42.73023024125533\n",
      "Epoch 16 \t Batch 3350 \t Training Loss: 42.75901676064107\n",
      "Epoch 16 \t Batch 3400 \t Training Loss: 42.75192188543432\n",
      "Epoch 16 \t Batch 3450 \t Training Loss: 42.71646678869275\n",
      "Epoch 16 \t Batch 3500 \t Training Loss: 42.72512859889439\n",
      "Epoch 16 \t Batch 3550 \t Training Loss: 42.739180563268526\n",
      "Epoch 16 \t Batch 3600 \t Training Loss: 42.721362096998426\n",
      "Epoch 16 \t Batch 3650 \t Training Loss: 42.75197896304196\n",
      "Epoch 16 \t Batch 50 \t Validation Loss: 38.95221261978149\n",
      "Epoch 16 \t Batch 100 \t Validation Loss: 49.04690091133118\n",
      "Epoch 16 \t Batch 150 \t Validation Loss: 39.7669324016571\n",
      "Epoch 16 \t Batch 200 \t Validation Loss: 39.38392267227173\n",
      "Epoch 16 \t Batch 250 \t Validation Loss: 44.28281169128418\n",
      "Epoch 16 \t Batch 300 \t Validation Loss: 41.23954889774323\n",
      "Epoch 16 \t Batch 350 \t Validation Loss: 40.095365491594585\n",
      "Epoch 16 \t Batch 400 \t Validation Loss: 38.343202528953555\n",
      "Epoch 16 \t Batch 450 \t Validation Loss: 37.72117076026069\n",
      "Epoch 16 \t Batch 500 \t Validation Loss: 36.582647392272946\n",
      "Epoch 16 \t Batch 550 \t Validation Loss: 35.594956666773015\n",
      "Epoch 16 \t Batch 600 \t Validation Loss: 35.54113340218862\n",
      "Epoch 16 \t Batch 650 \t Validation Loss: 36.67774008384118\n",
      "Epoch 16 \t Batch 700 \t Validation Loss: 37.63193073885781\n",
      "Epoch 16 \t Batch 750 \t Validation Loss: 38.10949703152974\n",
      "Epoch 16 \t Batch 800 \t Validation Loss: 38.649741618037226\n",
      "Epoch 16 \t Batch 850 \t Validation Loss: 39.072483999027924\n",
      "Epoch 16 \t Batch 900 \t Validation Loss: 38.9448744503657\n",
      "Epoch 16 \t Batch 950 \t Validation Loss: 38.625741720199585\n",
      "Epoch 16 \t Batch 1000 \t Validation Loss: 39.81874822092056\n",
      "Epoch 16 \t Batch 1050 \t Validation Loss: 40.65245611190796\n",
      "Epoch 16 \t Batch 1100 \t Validation Loss: 41.16278055017645\n",
      "Epoch 16 \t Batch 1150 \t Validation Loss: 41.01628522458284\n",
      "Epoch 16 \t Batch 1200 \t Validation Loss: 41.49771777153015\n",
      "Epoch 16 \t Batch 1250 \t Validation Loss: 41.57911979866028\n",
      "Epoch 16 \t Batch 1300 \t Validation Loss: 41.74288203569559\n",
      "Epoch 16 \t Batch 1350 \t Validation Loss: 41.3929401535458\n",
      "Epoch 16 \t Batch 1400 \t Validation Loss: 41.21999840429851\n",
      "Epoch 16 \t Batch 1450 \t Validation Loss: 41.205365481869926\n",
      "Epoch 16 \t Batch 1500 \t Validation Loss: 41.27626288779577\n",
      "Epoch 16 \t Batch 1550 \t Validation Loss: 40.934795133990626\n",
      "Epoch 16 \t Batch 1600 \t Validation Loss: 40.70342836424708\n",
      "Epoch 16 \t Batch 1650 \t Validation Loss: 40.725457811500085\n",
      "Epoch 16 \t Batch 1700 \t Validation Loss: 40.53887641556123\n",
      "Epoch 16 \t Batch 1750 \t Validation Loss: 40.336113726888385\n",
      "Epoch 16 \t Batch 1800 \t Validation Loss: 40.40465634332763\n",
      "Epoch 16 \t Batch 1850 \t Validation Loss: 40.88312311520448\n",
      "Epoch 16 \t Batch 1900 \t Validation Loss: 41.0874866916004\n",
      "Epoch 16 \t Batch 1950 \t Validation Loss: 40.88092672140171\n",
      "Epoch 16 \t Batch 2000 \t Validation Loss: 40.788374778151514\n",
      "Epoch 16 \t Batch 2050 \t Validation Loss: 40.96725135558989\n",
      "Epoch 16 \t Batch 2100 \t Validation Loss: 40.85755962950843\n",
      "Epoch 16 \t Batch 2150 \t Validation Loss: 40.619713992961614\n",
      "Epoch 16 \t Batch 2200 \t Validation Loss: 40.38242003560066\n",
      "Epoch 16 \t Batch 2250 \t Validation Loss: 40.175056206491256\n",
      "Epoch 16 \t Batch 2300 \t Validation Loss: 39.879137036696726\n",
      "Epoch 16 \t Batch 2350 \t Validation Loss: 39.77941838934066\n",
      "Epoch 16 \t Batch 2400 \t Validation Loss: 39.821388203303016\n",
      "Epoch 16 \t Batch 2450 \t Validation Loss: 40.10951334992234\n",
      "Epoch 16 Training Loss: 42.746754658261196 Validation Loss: 40.29811117343314\n",
      "Epoch 16 completed\n",
      "Epoch 17 \t Batch 50 \t Training Loss: 41.62820400238037\n",
      "Epoch 17 \t Batch 100 \t Training Loss: 40.4287845993042\n",
      "Epoch 17 \t Batch 150 \t Training Loss: 40.94703623453776\n",
      "Epoch 17 \t Batch 200 \t Training Loss: 41.04085919380188\n",
      "Epoch 17 \t Batch 250 \t Training Loss: 41.22790242767334\n",
      "Epoch 17 \t Batch 300 \t Training Loss: 41.245305411020915\n",
      "Epoch 17 \t Batch 350 \t Training Loss: 41.159800409589494\n",
      "Epoch 17 \t Batch 400 \t Training Loss: 41.28292496681213\n",
      "Epoch 17 \t Batch 450 \t Training Loss: 41.21690820058187\n",
      "Epoch 17 \t Batch 500 \t Training Loss: 41.172220176696776\n",
      "Epoch 17 \t Batch 550 \t Training Loss: 41.206365699768064\n",
      "Epoch 17 \t Batch 600 \t Training Loss: 41.295446456273396\n",
      "Epoch 17 \t Batch 650 \t Training Loss: 41.49790892087496\n",
      "Epoch 17 \t Batch 700 \t Training Loss: 41.54748617717198\n",
      "Epoch 17 \t Batch 750 \t Training Loss: 41.71974890391032\n",
      "Epoch 17 \t Batch 800 \t Training Loss: 41.82230548858642\n",
      "Epoch 17 \t Batch 850 \t Training Loss: 41.897748000201055\n",
      "Epoch 17 \t Batch 900 \t Training Loss: 41.925196183522544\n",
      "Epoch 17 \t Batch 950 \t Training Loss: 42.01271989521227\n",
      "Epoch 17 \t Batch 1000 \t Training Loss: 41.94675374031067\n",
      "Epoch 17 \t Batch 1050 \t Training Loss: 41.96412735348656\n",
      "Epoch 17 \t Batch 1100 \t Training Loss: 42.05137344707142\n",
      "Epoch 17 \t Batch 1150 \t Training Loss: 42.07500281624172\n",
      "Epoch 17 \t Batch 1200 \t Training Loss: 42.001109642982485\n",
      "Epoch 17 \t Batch 1250 \t Training Loss: 42.02236831665039\n",
      "Epoch 17 \t Batch 1300 \t Training Loss: 41.99266595106859\n",
      "Epoch 17 \t Batch 1350 \t Training Loss: 41.960056936475965\n",
      "Epoch 17 \t Batch 1400 \t Training Loss: 42.01315420695713\n",
      "Epoch 17 \t Batch 1450 \t Training Loss: 42.00790346474483\n",
      "Epoch 17 \t Batch 1500 \t Training Loss: 42.07133818181356\n",
      "Epoch 17 \t Batch 1550 \t Training Loss: 42.06823791503906\n",
      "Epoch 17 \t Batch 1600 \t Training Loss: 42.05548415541649\n",
      "Epoch 17 \t Batch 1650 \t Training Loss: 42.103754028551506\n",
      "Epoch 17 \t Batch 1700 \t Training Loss: 42.1325153204974\n",
      "Epoch 17 \t Batch 1750 \t Training Loss: 42.144509450639994\n",
      "Epoch 17 \t Batch 1800 \t Training Loss: 42.14981853697035\n",
      "Epoch 17 \t Batch 1850 \t Training Loss: 42.170576137852024\n",
      "Epoch 17 \t Batch 1900 \t Training Loss: 42.16362525237234\n",
      "Epoch 17 \t Batch 1950 \t Training Loss: 42.19751185001471\n",
      "Epoch 17 \t Batch 2000 \t Training Loss: 42.19580555152893\n",
      "Epoch 17 \t Batch 2050 \t Training Loss: 42.23567644817073\n",
      "Epoch 17 \t Batch 2100 \t Training Loss: 42.255775835854664\n",
      "Epoch 17 \t Batch 2150 \t Training Loss: 42.27808547707491\n",
      "Epoch 17 \t Batch 2200 \t Training Loss: 42.29297493761236\n",
      "Epoch 17 \t Batch 2250 \t Training Loss: 42.288355867173934\n",
      "Epoch 17 \t Batch 2300 \t Training Loss: 42.34882383761199\n",
      "Epoch 17 \t Batch 2350 \t Training Loss: 42.34234272490156\n",
      "Epoch 17 \t Batch 2400 \t Training Loss: 42.32733978350957\n",
      "Epoch 17 \t Batch 2450 \t Training Loss: 42.337139644233545\n",
      "Epoch 17 \t Batch 2500 \t Training Loss: 42.318014310455325\n",
      "Epoch 17 \t Batch 2550 \t Training Loss: 42.325264683891746\n",
      "Epoch 17 \t Batch 2600 \t Training Loss: 42.34403350243202\n",
      "Epoch 17 \t Batch 2650 \t Training Loss: 42.34437419315554\n",
      "Epoch 17 \t Batch 2700 \t Training Loss: 42.3212808559559\n",
      "Epoch 17 \t Batch 2750 \t Training Loss: 42.31830127438632\n",
      "Epoch 17 \t Batch 2800 \t Training Loss: 42.31341975075858\n",
      "Epoch 17 \t Batch 2850 \t Training Loss: 42.308614869368704\n",
      "Epoch 17 \t Batch 2900 \t Training Loss: 42.34379547974159\n",
      "Epoch 17 \t Batch 2950 \t Training Loss: 42.3209141152592\n",
      "Epoch 17 \t Batch 3000 \t Training Loss: 42.334618123372394\n",
      "Epoch 17 \t Batch 3050 \t Training Loss: 42.333397799632586\n",
      "Epoch 17 \t Batch 3100 \t Training Loss: 42.33893449598743\n",
      "Epoch 17 \t Batch 3150 \t Training Loss: 42.3486225545974\n",
      "Epoch 17 \t Batch 3200 \t Training Loss: 42.33997945845127\n",
      "Epoch 17 \t Batch 3250 \t Training Loss: 42.33386384641207\n",
      "Epoch 17 \t Batch 3300 \t Training Loss: 42.32134349591804\n",
      "Epoch 17 \t Batch 3350 \t Training Loss: 42.343560871295075\n",
      "Epoch 17 \t Batch 3400 \t Training Loss: 42.35463482520159\n",
      "Epoch 17 \t Batch 3450 \t Training Loss: 42.34975625300753\n",
      "Epoch 17 \t Batch 3500 \t Training Loss: 42.35420654296875\n",
      "Epoch 17 \t Batch 3550 \t Training Loss: 42.356479378283865\n",
      "Epoch 17 \t Batch 3600 \t Training Loss: 42.35111526489258\n",
      "Epoch 17 \t Batch 3650 \t Training Loss: 42.35127448304059\n",
      "Epoch 17 \t Batch 50 \t Validation Loss: 36.04143637657165\n",
      "Epoch 17 \t Batch 100 \t Validation Loss: 45.14024678707123\n",
      "Epoch 17 \t Batch 150 \t Validation Loss: 38.99383340835571\n",
      "Epoch 17 \t Batch 200 \t Validation Loss: 39.503419103622434\n",
      "Epoch 17 \t Batch 250 \t Validation Loss: 43.729077659606936\n",
      "Epoch 17 \t Batch 300 \t Validation Loss: 41.546938910484315\n",
      "Epoch 17 \t Batch 350 \t Validation Loss: 41.41649632453918\n",
      "Epoch 17 \t Batch 400 \t Validation Loss: 39.82205054402351\n",
      "Epoch 17 \t Batch 450 \t Validation Loss: 39.25136644045512\n",
      "Epoch 17 \t Batch 500 \t Validation Loss: 38.042170409202576\n",
      "Epoch 17 \t Batch 550 \t Validation Loss: 37.01110138719732\n",
      "Epoch 17 \t Batch 600 \t Validation Loss: 36.820832259655\n",
      "Epoch 17 \t Batch 650 \t Validation Loss: 37.76294449219337\n",
      "Epoch 17 \t Batch 700 \t Validation Loss: 38.72624177455902\n",
      "Epoch 17 \t Batch 750 \t Validation Loss: 39.09677680015564\n",
      "Epoch 17 \t Batch 800 \t Validation Loss: 39.575443043112756\n",
      "Epoch 17 \t Batch 850 \t Validation Loss: 39.844242365781\n",
      "Epoch 17 \t Batch 900 \t Validation Loss: 39.706139034165275\n",
      "Epoch 17 \t Batch 950 \t Validation Loss: 39.42958794694198\n",
      "Epoch 17 \t Batch 1000 \t Validation Loss: 40.55172940301895\n",
      "Epoch 17 \t Batch 1050 \t Validation Loss: 41.238703043347314\n",
      "Epoch 17 \t Batch 1100 \t Validation Loss: 41.61186943184246\n",
      "Epoch 17 \t Batch 1150 \t Validation Loss: 41.434496454570606\n",
      "Epoch 17 \t Batch 1200 \t Validation Loss: 41.988862928946816\n",
      "Epoch 17 \t Batch 1250 \t Validation Loss: 42.08631185874939\n",
      "Epoch 17 \t Batch 1300 \t Validation Loss: 42.232219198300285\n",
      "Epoch 17 \t Batch 1350 \t Validation Loss: 41.88837426609463\n",
      "Epoch 17 \t Batch 1400 \t Validation Loss: 41.70088099718094\n",
      "Epoch 17 \t Batch 1450 \t Validation Loss: 41.75304734295812\n",
      "Epoch 17 \t Batch 1500 \t Validation Loss: 41.78162396176656\n",
      "Epoch 17 \t Batch 1550 \t Validation Loss: 41.41025050932361\n",
      "Epoch 17 \t Batch 1600 \t Validation Loss: 41.122192237973216\n",
      "Epoch 17 \t Batch 1650 \t Validation Loss: 41.10534957712347\n",
      "Epoch 17 \t Batch 1700 \t Validation Loss: 40.90618745803833\n",
      "Epoch 17 \t Batch 1750 \t Validation Loss: 40.68452105985369\n",
      "Epoch 17 \t Batch 1800 \t Validation Loss: 40.762111835479736\n",
      "Epoch 17 \t Batch 1850 \t Validation Loss: 41.22803322869378\n",
      "Epoch 17 \t Batch 1900 \t Validation Loss: 41.40879085741545\n",
      "Epoch 17 \t Batch 1950 \t Validation Loss: 41.19904687490219\n",
      "Epoch 17 \t Batch 2000 \t Validation Loss: 41.127206500053404\n",
      "Epoch 17 \t Batch 2050 \t Validation Loss: 41.402062488183745\n",
      "Epoch 17 \t Batch 2100 \t Validation Loss: 41.29524374961853\n",
      "Epoch 17 \t Batch 2150 \t Validation Loss: 41.08987984235897\n",
      "Epoch 17 \t Batch 2200 \t Validation Loss: 40.869993523684414\n",
      "Epoch 17 \t Batch 2250 \t Validation Loss: 40.68976102108426\n",
      "Epoch 17 \t Batch 2300 \t Validation Loss: 40.42397583215133\n",
      "Epoch 17 \t Batch 2350 \t Validation Loss: 40.35830247351464\n",
      "Epoch 17 \t Batch 2400 \t Validation Loss: 40.413728071053825\n",
      "Epoch 17 \t Batch 2450 \t Validation Loss: 40.70423409714991\n",
      "Epoch 17 Training Loss: 42.337292433045036 Validation Loss: 40.89948158256419\n",
      "Epoch 17 completed\n",
      "Epoch 18 \t Batch 50 \t Training Loss: 41.251085624694824\n",
      "Epoch 18 \t Batch 100 \t Training Loss: 40.76614158630371\n",
      "Epoch 18 \t Batch 150 \t Training Loss: 40.85459073384603\n",
      "Epoch 18 \t Batch 200 \t Training Loss: 40.91661605834961\n",
      "Epoch 18 \t Batch 250 \t Training Loss: 40.739924659729006\n",
      "Epoch 18 \t Batch 300 \t Training Loss: 40.684218934377036\n",
      "Epoch 18 \t Batch 350 \t Training Loss: 40.7193832724435\n",
      "Epoch 18 \t Batch 400 \t Training Loss: 40.733457646369935\n",
      "Epoch 18 \t Batch 450 \t Training Loss: 40.94358258565267\n",
      "Epoch 18 \t Batch 500 \t Training Loss: 41.05906111907959\n",
      "Epoch 18 \t Batch 550 \t Training Loss: 41.05269376928156\n",
      "Epoch 18 \t Batch 600 \t Training Loss: 41.173727671305336\n",
      "Epoch 18 \t Batch 650 \t Training Loss: 41.222325618450455\n",
      "Epoch 18 \t Batch 700 \t Training Loss: 41.29748533248901\n",
      "Epoch 18 \t Batch 750 \t Training Loss: 41.32658094533284\n",
      "Epoch 18 \t Batch 800 \t Training Loss: 41.30506552219391\n",
      "Epoch 18 \t Batch 850 \t Training Loss: 41.34322010489071\n",
      "Epoch 18 \t Batch 900 \t Training Loss: 41.34452440473768\n",
      "Epoch 18 \t Batch 950 \t Training Loss: 41.36177257136295\n",
      "Epoch 18 \t Batch 1000 \t Training Loss: 41.36018796348572\n",
      "Epoch 18 \t Batch 1050 \t Training Loss: 41.37382642836798\n",
      "Epoch 18 \t Batch 1100 \t Training Loss: 41.33217622410167\n",
      "Epoch 18 \t Batch 1150 \t Training Loss: 41.37576591657555\n",
      "Epoch 18 \t Batch 1200 \t Training Loss: 41.402023129463196\n",
      "Epoch 18 \t Batch 1250 \t Training Loss: 41.39694942626953\n",
      "Epoch 18 \t Batch 1300 \t Training Loss: 41.42815850184514\n",
      "Epoch 18 \t Batch 1350 \t Training Loss: 41.446506587840894\n",
      "Epoch 18 \t Batch 1400 \t Training Loss: 41.510776369912286\n",
      "Epoch 18 \t Batch 1450 \t Training Loss: 41.55377162275643\n",
      "Epoch 18 \t Batch 1500 \t Training Loss: 41.56835518391927\n",
      "Epoch 18 \t Batch 1550 \t Training Loss: 41.58742756628221\n",
      "Epoch 18 \t Batch 1600 \t Training Loss: 41.58295844912529\n",
      "Epoch 18 \t Batch 1650 \t Training Loss: 41.587027260751434\n",
      "Epoch 18 \t Batch 1700 \t Training Loss: 41.59150749767528\n",
      "Epoch 18 \t Batch 1750 \t Training Loss: 41.59973099735805\n",
      "Epoch 18 \t Batch 1800 \t Training Loss: 41.60945505142212\n",
      "Epoch 18 \t Batch 1850 \t Training Loss: 41.62640053826409\n",
      "Epoch 18 \t Batch 1900 \t Training Loss: 41.65707776922929\n",
      "Epoch 18 \t Batch 1950 \t Training Loss: 41.653504117329916\n",
      "Epoch 18 \t Batch 2000 \t Training Loss: 41.61347475337982\n",
      "Epoch 18 \t Batch 2050 \t Training Loss: 41.65903486298352\n",
      "Epoch 18 \t Batch 2100 \t Training Loss: 41.67076670056298\n",
      "Epoch 18 \t Batch 2150 \t Training Loss: 41.678674013891886\n",
      "Epoch 18 \t Batch 2200 \t Training Loss: 41.703821987672285\n",
      "Epoch 18 \t Batch 2250 \t Training Loss: 41.69933452267117\n",
      "Epoch 18 \t Batch 2300 \t Training Loss: 41.75283617848935\n",
      "Epoch 18 \t Batch 2350 \t Training Loss: 41.77349390557472\n",
      "Epoch 18 \t Batch 2400 \t Training Loss: 41.76248253345489\n",
      "Epoch 18 \t Batch 2450 \t Training Loss: 41.80564324203802\n",
      "Epoch 18 \t Batch 2500 \t Training Loss: 41.805243801116944\n",
      "Epoch 18 \t Batch 2550 \t Training Loss: 41.816413442574294\n",
      "Epoch 18 \t Batch 2600 \t Training Loss: 41.82450532839849\n",
      "Epoch 18 \t Batch 2650 \t Training Loss: 41.807091957668085\n",
      "Epoch 18 \t Batch 2700 \t Training Loss: 41.78785939534505\n",
      "Epoch 18 \t Batch 2750 \t Training Loss: 41.80124542791193\n",
      "Epoch 18 \t Batch 2800 \t Training Loss: 41.83888526371547\n",
      "Epoch 18 \t Batch 2850 \t Training Loss: 41.83230238730447\n",
      "Epoch 18 \t Batch 2900 \t Training Loss: 41.858024301200075\n",
      "Epoch 18 \t Batch 2950 \t Training Loss: 41.860398987592276\n",
      "Epoch 18 \t Batch 3000 \t Training Loss: 41.84345708529155\n",
      "Epoch 18 \t Batch 3050 \t Training Loss: 41.863794025983964\n",
      "Epoch 18 \t Batch 3100 \t Training Loss: 41.87039839160058\n",
      "Epoch 18 \t Batch 3150 \t Training Loss: 41.882243706233915\n",
      "Epoch 18 \t Batch 3200 \t Training Loss: 41.910218988060954\n",
      "Epoch 18 \t Batch 3250 \t Training Loss: 41.906461735652044\n",
      "Epoch 18 \t Batch 3300 \t Training Loss: 41.8932662674875\n",
      "Epoch 18 \t Batch 3350 \t Training Loss: 41.90550080939905\n",
      "Epoch 18 \t Batch 3400 \t Training Loss: 41.9153231789084\n",
      "Epoch 18 \t Batch 3450 \t Training Loss: 41.91829646345498\n",
      "Epoch 18 \t Batch 3500 \t Training Loss: 41.94300726481846\n",
      "Epoch 18 \t Batch 3550 \t Training Loss: 41.96790278958603\n",
      "Epoch 18 \t Batch 3600 \t Training Loss: 41.96053376939562\n",
      "Epoch 18 \t Batch 3650 \t Training Loss: 41.96654513685671\n",
      "Epoch 18 \t Batch 50 \t Validation Loss: 28.932873287200927\n",
      "Epoch 18 \t Batch 100 \t Validation Loss: 36.43840895652771\n",
      "Epoch 18 \t Batch 150 \t Validation Loss: 30.89626449584961\n",
      "Epoch 18 \t Batch 200 \t Validation Loss: 31.487464056015014\n",
      "Epoch 18 \t Batch 250 \t Validation Loss: 35.004553745269774\n",
      "Epoch 18 \t Batch 300 \t Validation Loss: 33.12311060587565\n",
      "Epoch 18 \t Batch 350 \t Validation Loss: 33.33001828329904\n",
      "Epoch 18 \t Batch 400 \t Validation Loss: 32.47165679216385\n",
      "Epoch 18 \t Batch 450 \t Validation Loss: 32.643632553948294\n",
      "Epoch 18 \t Batch 500 \t Validation Loss: 32.068401805877684\n",
      "Epoch 18 \t Batch 550 \t Validation Loss: 31.48042724609375\n",
      "Epoch 18 \t Batch 600 \t Validation Loss: 31.914767249425253\n",
      "Epoch 18 \t Batch 650 \t Validation Loss: 33.611780985318696\n",
      "Epoch 18 \t Batch 700 \t Validation Loss: 35.082243604660036\n",
      "Epoch 18 \t Batch 750 \t Validation Loss: 35.999201279958086\n",
      "Epoch 18 \t Batch 800 \t Validation Loss: 37.064736000299455\n",
      "Epoch 18 \t Batch 850 \t Validation Loss: 37.7348392497792\n",
      "Epoch 18 \t Batch 900 \t Validation Loss: 37.8714083480835\n",
      "Epoch 18 \t Batch 950 \t Validation Loss: 37.85751732575266\n",
      "Epoch 18 \t Batch 1000 \t Validation Loss: 39.368183053016665\n",
      "Epoch 18 \t Batch 1050 \t Validation Loss: 40.37002684638614\n",
      "Epoch 18 \t Batch 1100 \t Validation Loss: 41.03192355849526\n",
      "Epoch 18 \t Batch 1150 \t Validation Loss: 40.937471549821936\n",
      "Epoch 18 \t Batch 1200 \t Validation Loss: 41.71781297286351\n",
      "Epoch 18 \t Batch 1250 \t Validation Loss: 41.98893480987549\n",
      "Epoch 18 \t Batch 1300 \t Validation Loss: 42.224325444148135\n",
      "Epoch 18 \t Batch 1350 \t Validation Loss: 41.926902562600596\n",
      "Epoch 18 \t Batch 1400 \t Validation Loss: 41.820540513311116\n",
      "Epoch 18 \t Batch 1450 \t Validation Loss: 41.85756225783249\n",
      "Epoch 18 \t Batch 1500 \t Validation Loss: 41.971216803709666\n",
      "Epoch 18 \t Batch 1550 \t Validation Loss: 41.648893644732816\n",
      "Epoch 18 \t Batch 1600 \t Validation Loss: 41.44037668928504\n",
      "Epoch 18 \t Batch 1650 \t Validation Loss: 41.521437871383895\n",
      "Epoch 18 \t Batch 1700 \t Validation Loss: 41.40422469405567\n",
      "Epoch 18 \t Batch 1750 \t Validation Loss: 41.26291012832097\n",
      "Epoch 18 \t Batch 1800 \t Validation Loss: 41.433750679625405\n",
      "Epoch 18 \t Batch 1850 \t Validation Loss: 41.946699177896654\n",
      "Epoch 18 \t Batch 1900 \t Validation Loss: 42.14598412200024\n",
      "Epoch 18 \t Batch 1950 \t Validation Loss: 41.92896332557385\n",
      "Epoch 18 \t Batch 2000 \t Validation Loss: 41.85871569812298\n",
      "Epoch 18 \t Batch 2050 \t Validation Loss: 42.15473604888451\n",
      "Epoch 18 \t Batch 2100 \t Validation Loss: 42.04765896922066\n",
      "Epoch 18 \t Batch 2150 \t Validation Loss: 41.80565834078678\n",
      "Epoch 18 \t Batch 2200 \t Validation Loss: 41.543140289458364\n",
      "Epoch 18 \t Batch 2250 \t Validation Loss: 41.29026697487301\n",
      "Epoch 18 \t Batch 2300 \t Validation Loss: 40.94652342153632\n",
      "Epoch 18 \t Batch 2350 \t Validation Loss: 40.807047429389144\n",
      "Epoch 18 \t Batch 2400 \t Validation Loss: 40.8350990241766\n",
      "Epoch 18 \t Batch 2450 \t Validation Loss: 41.082542350730115\n",
      "Epoch 18 Training Loss: 41.970669091722826 Validation Loss: 41.22893865677443\n",
      "Epoch 18 completed\n",
      "Epoch 19 \t Batch 50 \t Training Loss: 40.044091033935544\n",
      "Epoch 19 \t Batch 100 \t Training Loss: 40.90488855361939\n",
      "Epoch 19 \t Batch 150 \t Training Loss: 41.083055381774905\n",
      "Epoch 19 \t Batch 200 \t Training Loss: 40.802448453903196\n",
      "Epoch 19 \t Batch 250 \t Training Loss: 40.46380109405518\n",
      "Epoch 19 \t Batch 300 \t Training Loss: 40.68756365458171\n",
      "Epoch 19 \t Batch 350 \t Training Loss: 40.688743024553574\n",
      "Epoch 19 \t Batch 400 \t Training Loss: 40.75968131542206\n",
      "Epoch 19 \t Batch 450 \t Training Loss: 40.9474044418335\n",
      "Epoch 19 \t Batch 500 \t Training Loss: 40.84687225723267\n",
      "Epoch 19 \t Batch 550 \t Training Loss: 40.9920205896551\n",
      "Epoch 19 \t Batch 600 \t Training Loss: 40.84517423629761\n",
      "Epoch 19 \t Batch 650 \t Training Loss: 40.93611035273625\n",
      "Epoch 19 \t Batch 700 \t Training Loss: 40.92949698311942\n",
      "Epoch 19 \t Batch 750 \t Training Loss: 40.89402903493245\n",
      "Epoch 19 \t Batch 800 \t Training Loss: 40.90835339784622\n",
      "Epoch 19 \t Batch 850 \t Training Loss: 40.858709400401395\n",
      "Epoch 19 \t Batch 900 \t Training Loss: 40.87522608651055\n",
      "Epoch 19 \t Batch 950 \t Training Loss: 40.86810248525519\n",
      "Epoch 19 \t Batch 1000 \t Training Loss: 40.918665197372434\n",
      "Epoch 19 \t Batch 1050 \t Training Loss: 40.9631519808088\n",
      "Epoch 19 \t Batch 1100 \t Training Loss: 40.980810933546586\n",
      "Epoch 19 \t Batch 1150 \t Training Loss: 40.97386636319368\n",
      "Epoch 19 \t Batch 1200 \t Training Loss: 41.016985567410785\n",
      "Epoch 19 \t Batch 1250 \t Training Loss: 41.076277400207516\n",
      "Epoch 19 \t Batch 1300 \t Training Loss: 41.08932810416589\n",
      "Epoch 19 \t Batch 1350 \t Training Loss: 41.102347725762264\n",
      "Epoch 19 \t Batch 1400 \t Training Loss: 41.17255938121251\n",
      "Epoch 19 \t Batch 1450 \t Training Loss: 41.12948464886895\n",
      "Epoch 19 \t Batch 1500 \t Training Loss: 41.18123946507772\n",
      "Epoch 19 \t Batch 1550 \t Training Loss: 41.21209315761443\n",
      "Epoch 19 \t Batch 1600 \t Training Loss: 41.22147905945778\n",
      "Epoch 19 \t Batch 1650 \t Training Loss: 41.24101515683261\n",
      "Epoch 19 \t Batch 1700 \t Training Loss: 41.2676418282004\n",
      "Epoch 19 \t Batch 1750 \t Training Loss: 41.30686345563616\n",
      "Epoch 19 \t Batch 1800 \t Training Loss: 41.30449541303847\n",
      "Epoch 19 \t Batch 1850 \t Training Loss: 41.30569797412769\n",
      "Epoch 19 \t Batch 1900 \t Training Loss: 41.318138135608876\n",
      "Epoch 19 \t Batch 1950 \t Training Loss: 41.312121026454825\n",
      "Epoch 19 \t Batch 2000 \t Training Loss: 41.355329069137575\n",
      "Epoch 19 \t Batch 2050 \t Training Loss: 41.33718891609006\n",
      "Epoch 19 \t Batch 2100 \t Training Loss: 41.36659551348005\n",
      "Epoch 19 \t Batch 2150 \t Training Loss: 41.34034660516783\n",
      "Epoch 19 \t Batch 2200 \t Training Loss: 41.35545604619113\n",
      "Epoch 19 \t Batch 2250 \t Training Loss: 41.34091202121311\n",
      "Epoch 19 \t Batch 2300 \t Training Loss: 41.339025126747465\n",
      "Epoch 19 \t Batch 2350 \t Training Loss: 41.380935288287226\n",
      "Epoch 19 \t Batch 2400 \t Training Loss: 41.351327555179594\n",
      "Epoch 19 \t Batch 2450 \t Training Loss: 41.359770714896065\n",
      "Epoch 19 \t Batch 2500 \t Training Loss: 41.376440481567386\n",
      "Epoch 19 \t Batch 2550 \t Training Loss: 41.37078034120447\n",
      "Epoch 19 \t Batch 2600 \t Training Loss: 41.364892542178815\n",
      "Epoch 19 \t Batch 2650 \t Training Loss: 41.39644887744256\n",
      "Epoch 19 \t Batch 2700 \t Training Loss: 41.405664612098974\n",
      "Epoch 19 \t Batch 2750 \t Training Loss: 41.388148655978114\n",
      "Epoch 19 \t Batch 2800 \t Training Loss: 41.40315035956247\n",
      "Epoch 19 \t Batch 2850 \t Training Loss: 41.40661284463447\n",
      "Epoch 19 \t Batch 2900 \t Training Loss: 41.388755430681954\n",
      "Epoch 19 \t Batch 2950 \t Training Loss: 41.41521697804079\n",
      "Epoch 19 \t Batch 3000 \t Training Loss: 41.413383955001834\n",
      "Epoch 19 \t Batch 3050 \t Training Loss: 41.401555038202005\n",
      "Epoch 19 \t Batch 3100 \t Training Loss: 41.434186625942104\n",
      "Epoch 19 \t Batch 3150 \t Training Loss: 41.45867839752682\n",
      "Epoch 19 \t Batch 3200 \t Training Loss: 41.48630179047584\n",
      "Epoch 19 \t Batch 3250 \t Training Loss: 41.48468265592135\n",
      "Epoch 19 \t Batch 3300 \t Training Loss: 41.47631500186342\n",
      "Epoch 19 \t Batch 3350 \t Training Loss: 41.48136007508235\n",
      "Epoch 19 \t Batch 3400 \t Training Loss: 41.48645220644334\n",
      "Epoch 19 \t Batch 3450 \t Training Loss: 41.48451865210049\n",
      "Epoch 19 \t Batch 3500 \t Training Loss: 41.51014888054984\n",
      "Epoch 19 \t Batch 3550 \t Training Loss: 41.50185361298037\n",
      "Epoch 19 \t Batch 3600 \t Training Loss: 41.52701730410258\n",
      "Epoch 19 \t Batch 3650 \t Training Loss: 41.528366509006446\n",
      "Epoch 19 \t Batch 50 \t Validation Loss: 24.763902177810667\n",
      "Epoch 19 \t Batch 100 \t Validation Loss: 32.30086787700653\n",
      "Epoch 19 \t Batch 150 \t Validation Loss: 27.627388763427735\n",
      "Epoch 19 \t Batch 200 \t Validation Loss: 27.8596351146698\n",
      "Epoch 19 \t Batch 250 \t Validation Loss: 30.674429500579834\n",
      "Epoch 19 \t Batch 300 \t Validation Loss: 28.97899903615316\n",
      "Epoch 19 \t Batch 350 \t Validation Loss: 29.607644544328963\n",
      "Epoch 19 \t Batch 400 \t Validation Loss: 29.20195804357529\n",
      "Epoch 19 \t Batch 450 \t Validation Loss: 29.82217214796278\n",
      "Epoch 19 \t Batch 500 \t Validation Loss: 29.498873510360717\n",
      "Epoch 19 \t Batch 550 \t Validation Loss: 29.21571120002053\n",
      "Epoch 19 \t Batch 600 \t Validation Loss: 29.838194375038146\n",
      "Epoch 19 \t Batch 650 \t Validation Loss: 31.696586079230677\n",
      "Epoch 19 \t Batch 700 \t Validation Loss: 33.580158263615196\n",
      "Epoch 19 \t Batch 750 \t Validation Loss: 34.621565199534096\n",
      "Epoch 19 \t Batch 800 \t Validation Loss: 35.89178124547005\n",
      "Epoch 19 \t Batch 850 \t Validation Loss: 36.61158883263083\n",
      "Epoch 19 \t Batch 900 \t Validation Loss: 36.808007656733196\n",
      "Epoch 19 \t Batch 950 \t Validation Loss: 36.85363922420301\n",
      "Epoch 19 \t Batch 1000 \t Validation Loss: 38.46296577739716\n",
      "Epoch 19 \t Batch 1050 \t Validation Loss: 39.478474546160015\n",
      "Epoch 19 \t Batch 1100 \t Validation Loss: 40.256970739364625\n",
      "Epoch 19 \t Batch 1150 \t Validation Loss: 40.284418798529586\n",
      "Epoch 19 \t Batch 1200 \t Validation Loss: 41.116450854142506\n",
      "Epoch 19 \t Batch 1250 \t Validation Loss: 41.396595892333984\n",
      "Epoch 19 \t Batch 1300 \t Validation Loss: 41.6645085144043\n",
      "Epoch 19 \t Batch 1350 \t Validation Loss: 41.396473772967305\n",
      "Epoch 19 \t Batch 1400 \t Validation Loss: 41.35645586967468\n",
      "Epoch 19 \t Batch 1450 \t Validation Loss: 41.487505694422225\n",
      "Epoch 19 \t Batch 1500 \t Validation Loss: 41.640280692418415\n",
      "Epoch 19 \t Batch 1550 \t Validation Loss: 41.28001593928183\n",
      "Epoch 19 \t Batch 1600 \t Validation Loss: 41.02221410751343\n",
      "Epoch 19 \t Batch 1650 \t Validation Loss: 41.09242179581613\n",
      "Epoch 19 \t Batch 1700 \t Validation Loss: 40.91468001029071\n",
      "Epoch 19 \t Batch 1750 \t Validation Loss: 40.70687412970407\n",
      "Epoch 19 \t Batch 1800 \t Validation Loss: 40.83149506462945\n",
      "Epoch 19 \t Batch 1850 \t Validation Loss: 41.297723347431905\n",
      "Epoch 19 \t Batch 1900 \t Validation Loss: 41.5084572862324\n",
      "Epoch 19 \t Batch 1950 \t Validation Loss: 41.29857609675481\n",
      "Epoch 19 \t Batch 2000 \t Validation Loss: 41.22489171695709\n",
      "Epoch 19 \t Batch 2050 \t Validation Loss: 41.49962537346816\n",
      "Epoch 19 \t Batch 2100 \t Validation Loss: 41.45246319407509\n",
      "Epoch 19 \t Batch 2150 \t Validation Loss: 41.30060505844826\n",
      "Epoch 19 \t Batch 2200 \t Validation Loss: 41.16124975247816\n",
      "Epoch 19 \t Batch 2250 \t Validation Loss: 41.021836764441595\n",
      "Epoch 19 \t Batch 2300 \t Validation Loss: 40.7981275450665\n",
      "Epoch 19 \t Batch 2350 \t Validation Loss: 40.751084802911635\n",
      "Epoch 19 \t Batch 2400 \t Validation Loss: 40.85815340161324\n",
      "Epoch 19 \t Batch 2450 \t Validation Loss: 41.17428092917618\n",
      "Epoch 19 Training Loss: 41.52322636929827 Validation Loss: 41.31202139134531\n",
      "Epoch 19 completed\n",
      "Epoch 20 \t Batch 50 \t Training Loss: 40.38414276123047\n",
      "Epoch 20 \t Batch 100 \t Training Loss: 41.10287195205689\n",
      "Epoch 20 \t Batch 150 \t Training Loss: 40.550970764160155\n",
      "Epoch 20 \t Batch 200 \t Training Loss: 40.06186444282532\n",
      "Epoch 20 \t Batch 250 \t Training Loss: 40.07949907684326\n",
      "Epoch 20 \t Batch 300 \t Training Loss: 39.81982395807902\n",
      "Epoch 20 \t Batch 350 \t Training Loss: 39.88338328225272\n",
      "Epoch 20 \t Batch 400 \t Training Loss: 39.96178568840027\n",
      "Epoch 20 \t Batch 450 \t Training Loss: 39.85583288828532\n",
      "Epoch 20 \t Batch 500 \t Training Loss: 39.891128101348876\n",
      "Epoch 20 \t Batch 550 \t Training Loss: 39.93130656155673\n",
      "Epoch 20 \t Batch 600 \t Training Loss: 39.97777466138204\n",
      "Epoch 20 \t Batch 650 \t Training Loss: 40.008901581397424\n",
      "Epoch 20 \t Batch 700 \t Training Loss: 40.093705986567905\n",
      "Epoch 20 \t Batch 750 \t Training Loss: 40.10698985290527\n",
      "Epoch 20 \t Batch 800 \t Training Loss: 40.197233498096466\n",
      "Epoch 20 \t Batch 850 \t Training Loss: 40.170904904533835\n",
      "Epoch 20 \t Batch 900 \t Training Loss: 40.234128665924075\n",
      "Epoch 20 \t Batch 950 \t Training Loss: 40.212547298230625\n",
      "Epoch 20 \t Batch 1000 \t Training Loss: 40.241800689697264\n",
      "Epoch 20 \t Batch 1050 \t Training Loss: 40.32178241911389\n",
      "Epoch 20 \t Batch 1100 \t Training Loss: 40.36180010015314\n",
      "Epoch 20 \t Batch 1150 \t Training Loss: 40.40939956001613\n",
      "Epoch 20 \t Batch 1200 \t Training Loss: 40.36562791188558\n",
      "Epoch 20 \t Batch 1250 \t Training Loss: 40.38608610687256\n",
      "Epoch 20 \t Batch 1300 \t Training Loss: 40.42766225374662\n",
      "Epoch 20 \t Batch 1350 \t Training Loss: 40.40598436708804\n",
      "Epoch 20 \t Batch 1400 \t Training Loss: 40.41068792615618\n",
      "Epoch 20 \t Batch 1450 \t Training Loss: 40.4148567502252\n",
      "Epoch 20 \t Batch 1500 \t Training Loss: 40.42251921971639\n",
      "Epoch 20 \t Batch 1550 \t Training Loss: 40.44755990428309\n",
      "Epoch 20 \t Batch 1600 \t Training Loss: 40.55877246499062\n",
      "Epoch 20 \t Batch 1650 \t Training Loss: 40.53802964297208\n",
      "Epoch 20 \t Batch 1700 \t Training Loss: 40.57316637824563\n",
      "Epoch 20 \t Batch 1750 \t Training Loss: 40.5051707850865\n",
      "Epoch 20 \t Batch 1800 \t Training Loss: 40.54644485261705\n",
      "Epoch 20 \t Batch 1850 \t Training Loss: 40.5597794414211\n",
      "Epoch 20 \t Batch 1900 \t Training Loss: 40.56188345758539\n",
      "Epoch 20 \t Batch 1950 \t Training Loss: 40.51850670154278\n",
      "Epoch 20 \t Batch 2000 \t Training Loss: 40.53445375537872\n",
      "Epoch 20 \t Batch 2050 \t Training Loss: 40.53189290860804\n",
      "Epoch 20 \t Batch 2100 \t Training Loss: 40.564665708996\n",
      "Epoch 20 \t Batch 2150 \t Training Loss: 40.59110976108285\n",
      "Epoch 20 \t Batch 2200 \t Training Loss: 40.624141712188724\n",
      "Epoch 20 \t Batch 2250 \t Training Loss: 40.637884704589844\n",
      "Epoch 20 \t Batch 2300 \t Training Loss: 40.70167427394701\n",
      "Epoch 20 \t Batch 2350 \t Training Loss: 40.723821061316954\n",
      "Epoch 20 \t Batch 2400 \t Training Loss: 40.731345717906954\n",
      "Epoch 20 \t Batch 2450 \t Training Loss: 40.68213975244639\n",
      "Epoch 20 \t Batch 2500 \t Training Loss: 40.69026765899658\n",
      "Epoch 20 \t Batch 2550 \t Training Loss: 40.72366813510072\n",
      "Epoch 20 \t Batch 2600 \t Training Loss: 40.74234105183528\n",
      "Epoch 20 \t Batch 2650 \t Training Loss: 40.765100837563566\n",
      "Epoch 20 \t Batch 2700 \t Training Loss: 40.791803190443254\n",
      "Epoch 20 \t Batch 2750 \t Training Loss: 40.82240965617787\n",
      "Epoch 20 \t Batch 2800 \t Training Loss: 40.82061966555459\n",
      "Epoch 20 \t Batch 2850 \t Training Loss: 40.82967519392047\n",
      "Epoch 20 \t Batch 2900 \t Training Loss: 40.83512268855654\n",
      "Epoch 20 \t Batch 2950 \t Training Loss: 40.835493172467764\n",
      "Epoch 20 \t Batch 3000 \t Training Loss: 40.84194558461507\n",
      "Epoch 20 \t Batch 3050 \t Training Loss: 40.85073799883733\n",
      "Epoch 20 \t Batch 3100 \t Training Loss: 40.85817649226035\n",
      "Epoch 20 \t Batch 3150 \t Training Loss: 40.85575128101167\n",
      "Epoch 20 \t Batch 3200 \t Training Loss: 40.849854909181595\n",
      "Epoch 20 \t Batch 3250 \t Training Loss: 40.896822485116815\n",
      "Epoch 20 \t Batch 3300 \t Training Loss: 40.92830436244155\n",
      "Epoch 20 \t Batch 3350 \t Training Loss: 40.921853406536044\n",
      "Epoch 20 \t Batch 3400 \t Training Loss: 40.92031931091758\n",
      "Epoch 20 \t Batch 3450 \t Training Loss: 40.92135444862255\n",
      "Epoch 20 \t Batch 3500 \t Training Loss: 40.953676265171595\n",
      "Epoch 20 \t Batch 3550 \t Training Loss: 40.97567903652997\n",
      "Epoch 20 \t Batch 3600 \t Training Loss: 40.99322423564063\n",
      "Epoch 20 \t Batch 3650 \t Training Loss: 41.02676276481315\n",
      "Epoch 20 \t Batch 50 \t Validation Loss: 25.429976806640624\n",
      "Epoch 20 \t Batch 100 \t Validation Loss: 31.13704013824463\n",
      "Epoch 20 \t Batch 150 \t Validation Loss: 27.478606815338136\n",
      "Epoch 20 \t Batch 200 \t Validation Loss: 27.454383854866027\n",
      "Epoch 20 \t Batch 250 \t Validation Loss: 29.36018152999878\n",
      "Epoch 20 \t Batch 300 \t Validation Loss: 28.002033200263977\n",
      "Epoch 20 \t Batch 350 \t Validation Loss: 28.77536268370492\n",
      "Epoch 20 \t Batch 400 \t Validation Loss: 28.453561621904374\n",
      "Epoch 20 \t Batch 450 \t Validation Loss: 29.025413902070788\n",
      "Epoch 20 \t Batch 500 \t Validation Loss: 28.72412254047394\n",
      "Epoch 20 \t Batch 550 \t Validation Loss: 28.494347236806696\n",
      "Epoch 20 \t Batch 600 \t Validation Loss: 29.00450302521388\n",
      "Epoch 20 \t Batch 650 \t Validation Loss: 30.470345113460834\n",
      "Epoch 20 \t Batch 700 \t Validation Loss: 31.82316940375737\n",
      "Epoch 20 \t Batch 750 \t Validation Loss: 32.68418996111552\n",
      "Epoch 20 \t Batch 800 \t Validation Loss: 33.620744414925575\n",
      "Epoch 20 \t Batch 850 \t Validation Loss: 34.30745701228871\n",
      "Epoch 20 \t Batch 900 \t Validation Loss: 34.486582469410365\n",
      "Epoch 20 \t Batch 950 \t Validation Loss: 34.514319523761145\n",
      "Epoch 20 \t Batch 1000 \t Validation Loss: 35.916646428585054\n",
      "Epoch 20 \t Batch 1050 \t Validation Loss: 36.90109163261595\n",
      "Epoch 20 \t Batch 1100 \t Validation Loss: 37.51988034920259\n",
      "Epoch 20 \t Batch 1150 \t Validation Loss: 37.49601660168689\n",
      "Epoch 20 \t Batch 1200 \t Validation Loss: 38.1345030194521\n",
      "Epoch 20 \t Batch 1250 \t Validation Loss: 38.383979747200016\n",
      "Epoch 20 \t Batch 1300 \t Validation Loss: 38.62793094873428\n",
      "Epoch 20 \t Batch 1350 \t Validation Loss: 38.44008924925769\n",
      "Epoch 20 \t Batch 1400 \t Validation Loss: 38.361618733576364\n",
      "Epoch 20 \t Batch 1450 \t Validation Loss: 38.401422006179544\n",
      "Epoch 20 \t Batch 1500 \t Validation Loss: 38.56534911632538\n",
      "Epoch 20 \t Batch 1550 \t Validation Loss: 38.346767726098335\n",
      "Epoch 20 \t Batch 1600 \t Validation Loss: 38.2624221816659\n",
      "Epoch 20 \t Batch 1650 \t Validation Loss: 38.366498123804725\n",
      "Epoch 20 \t Batch 1700 \t Validation Loss: 38.24817258582396\n",
      "Epoch 20 \t Batch 1750 \t Validation Loss: 38.11102863774981\n",
      "Epoch 20 \t Batch 1800 \t Validation Loss: 38.21219513760673\n",
      "Epoch 20 \t Batch 1850 \t Validation Loss: 38.739542616509105\n",
      "Epoch 20 \t Batch 1900 \t Validation Loss: 39.024348260728935\n",
      "Epoch 20 \t Batch 1950 \t Validation Loss: 38.878223571532814\n",
      "Epoch 20 \t Batch 2000 \t Validation Loss: 38.82600062584877\n",
      "Epoch 20 \t Batch 2050 \t Validation Loss: 39.02628288245783\n",
      "Epoch 20 \t Batch 2100 \t Validation Loss: 38.996608747981846\n",
      "Epoch 20 \t Batch 2150 \t Validation Loss: 38.86557572497878\n",
      "Epoch 20 \t Batch 2200 \t Validation Loss: 38.736915462884035\n",
      "Epoch 20 \t Batch 2250 \t Validation Loss: 38.63079689216614\n",
      "Epoch 20 \t Batch 2300 \t Validation Loss: 38.44159277045208\n",
      "Epoch 20 \t Batch 2350 \t Validation Loss: 38.43041430290709\n",
      "Epoch 20 \t Batch 2400 \t Validation Loss: 38.55304130633672\n",
      "Epoch 20 \t Batch 2450 \t Validation Loss: 38.90481677269449\n",
      "Epoch 20 Training Loss: 41.026134345367666 Validation Loss: 39.112044417625896\n",
      "Epoch 20 completed\n",
      "Epoch 21 \t Batch 50 \t Training Loss: 40.69597858428955\n",
      "Epoch 21 \t Batch 100 \t Training Loss: 39.5461057472229\n",
      "Epoch 21 \t Batch 150 \t Training Loss: 39.87305721282959\n",
      "Epoch 21 \t Batch 200 \t Training Loss: 39.69524309158325\n",
      "Epoch 21 \t Batch 250 \t Training Loss: 39.88490844726562\n",
      "Epoch 21 \t Batch 300 \t Training Loss: 39.97554690678914\n",
      "Epoch 21 \t Batch 350 \t Training Loss: 40.13245030539376\n",
      "Epoch 21 \t Batch 400 \t Training Loss: 40.03088454723358\n",
      "Epoch 21 \t Batch 450 \t Training Loss: 40.07134791056315\n",
      "Epoch 21 \t Batch 500 \t Training Loss: 40.01589194107056\n",
      "Epoch 21 \t Batch 550 \t Training Loss: 39.99001115625555\n",
      "Epoch 21 \t Batch 600 \t Training Loss: 40.03834468841553\n",
      "Epoch 21 \t Batch 650 \t Training Loss: 39.99493933164156\n",
      "Epoch 21 \t Batch 700 \t Training Loss: 39.94252913338797\n",
      "Epoch 21 \t Batch 750 \t Training Loss: 39.92541734313965\n",
      "Epoch 21 \t Batch 800 \t Training Loss: 39.85745537996292\n",
      "Epoch 21 \t Batch 850 \t Training Loss: 39.7694848094267\n",
      "Epoch 21 \t Batch 900 \t Training Loss: 39.74065275828043\n",
      "Epoch 21 \t Batch 950 \t Training Loss: 39.73935946815892\n",
      "Epoch 21 \t Batch 1000 \t Training Loss: 39.758110385894774\n",
      "Epoch 21 \t Batch 1050 \t Training Loss: 39.86043510981968\n",
      "Epoch 21 \t Batch 1100 \t Training Loss: 39.87451060208407\n",
      "Epoch 21 \t Batch 1150 \t Training Loss: 39.86889664691427\n",
      "Epoch 21 \t Batch 1200 \t Training Loss: 39.85681046168009\n",
      "Epoch 21 \t Batch 1250 \t Training Loss: 39.87404791412354\n",
      "Epoch 21 \t Batch 1300 \t Training Loss: 39.860167996333196\n",
      "Epoch 21 \t Batch 1350 \t Training Loss: 39.829818919146504\n",
      "Epoch 21 \t Batch 1400 \t Training Loss: 39.82075482504708\n",
      "Epoch 21 \t Batch 1450 \t Training Loss: 39.82974851279423\n",
      "Epoch 21 \t Batch 1500 \t Training Loss: 39.833588386535645\n",
      "Epoch 21 \t Batch 1550 \t Training Loss: 39.821859919640325\n",
      "Epoch 21 \t Batch 1600 \t Training Loss: 39.8385116314888\n",
      "Epoch 21 \t Batch 1650 \t Training Loss: 39.83966131730513\n",
      "Epoch 21 \t Batch 1700 \t Training Loss: 39.826421083562515\n",
      "Epoch 21 \t Batch 1750 \t Training Loss: 39.83064342389788\n",
      "Epoch 21 \t Batch 1800 \t Training Loss: 39.82302921507094\n",
      "Epoch 21 \t Batch 1850 \t Training Loss: 39.837811534984695\n",
      "Epoch 21 \t Batch 1900 \t Training Loss: 39.84442905426025\n",
      "Epoch 21 \t Batch 1950 \t Training Loss: 39.85316919571314\n",
      "Epoch 21 \t Batch 2000 \t Training Loss: 39.868168108940125\n",
      "Epoch 21 \t Batch 2050 \t Training Loss: 39.910335291420544\n",
      "Epoch 21 \t Batch 2100 \t Training Loss: 39.897151322137745\n",
      "Epoch 21 \t Batch 2150 \t Training Loss: 39.93107432609381\n",
      "Epoch 21 \t Batch 2200 \t Training Loss: 39.932868818803264\n",
      "Epoch 21 \t Batch 2250 \t Training Loss: 39.942389199998644\n",
      "Epoch 21 \t Batch 2300 \t Training Loss: 39.93299987958825\n",
      "Epoch 21 \t Batch 2350 \t Training Loss: 39.992032180136825\n",
      "Epoch 21 \t Batch 2400 \t Training Loss: 40.00374054352442\n",
      "Epoch 21 \t Batch 2450 \t Training Loss: 40.03095945630755\n",
      "Epoch 21 \t Batch 2500 \t Training Loss: 40.04120619659424\n",
      "Epoch 21 \t Batch 2550 \t Training Loss: 40.08175265368293\n",
      "Epoch 21 \t Batch 2600 \t Training Loss: 40.104365627582254\n",
      "Epoch 21 \t Batch 2650 \t Training Loss: 40.116396030929856\n",
      "Epoch 21 \t Batch 2700 \t Training Loss: 40.169687045768455\n",
      "Epoch 21 \t Batch 2750 \t Training Loss: 40.225512704329056\n",
      "Epoch 21 \t Batch 2800 \t Training Loss: 40.251927193232945\n",
      "Epoch 21 \t Batch 2850 \t Training Loss: 40.259548823038735\n",
      "Epoch 21 \t Batch 2900 \t Training Loss: 40.24963674019123\n",
      "Epoch 21 \t Batch 2950 \t Training Loss: 40.23958407515186\n",
      "Epoch 21 \t Batch 3000 \t Training Loss: 40.2638414293925\n",
      "Epoch 21 \t Batch 3050 \t Training Loss: 40.29945398424493\n",
      "Epoch 21 \t Batch 3100 \t Training Loss: 40.32208180273733\n",
      "Epoch 21 \t Batch 3150 \t Training Loss: 40.36806188371446\n",
      "Epoch 21 \t Batch 3200 \t Training Loss: 40.4136535847187\n",
      "Epoch 21 \t Batch 3250 \t Training Loss: 40.422782264122596\n",
      "Epoch 21 \t Batch 3300 \t Training Loss: 40.429453578717784\n",
      "Epoch 21 \t Batch 3350 \t Training Loss: 40.43891297525435\n",
      "Epoch 21 \t Batch 3400 \t Training Loss: 40.43183164259967\n",
      "Epoch 21 \t Batch 3450 \t Training Loss: 40.45713738040648\n",
      "Epoch 21 \t Batch 3500 \t Training Loss: 40.44471322740827\n",
      "Epoch 21 \t Batch 3550 \t Training Loss: 40.47676962839046\n",
      "Epoch 21 \t Batch 3600 \t Training Loss: 40.47382501443227\n",
      "Epoch 21 \t Batch 3650 \t Training Loss: 40.49142393347335\n",
      "Epoch 21 \t Batch 50 \t Validation Loss: 34.655700550079345\n",
      "Epoch 21 \t Batch 100 \t Validation Loss: 40.32234710693359\n",
      "Epoch 21 \t Batch 150 \t Validation Loss: 34.537284768422445\n",
      "Epoch 21 \t Batch 200 \t Validation Loss: 33.881391625404355\n",
      "Epoch 21 \t Batch 250 \t Validation Loss: 37.397026973724365\n",
      "Epoch 21 \t Batch 300 \t Validation Loss: 35.47534969329834\n",
      "Epoch 21 \t Batch 350 \t Validation Loss: 35.38119105747768\n",
      "Epoch 21 \t Batch 400 \t Validation Loss: 34.24673814058304\n",
      "Epoch 21 \t Batch 450 \t Validation Loss: 34.06211436589559\n",
      "Epoch 21 \t Batch 500 \t Validation Loss: 33.28274542427063\n",
      "Epoch 21 \t Batch 550 \t Validation Loss: 32.692217991568825\n",
      "Epoch 21 \t Batch 600 \t Validation Loss: 32.7612309662501\n",
      "Epoch 21 \t Batch 650 \t Validation Loss: 33.837416638594405\n",
      "Epoch 21 \t Batch 700 \t Validation Loss: 34.63772191456386\n",
      "Epoch 21 \t Batch 750 \t Validation Loss: 35.05595058695475\n",
      "Epoch 21 \t Batch 800 \t Validation Loss: 35.78786603569984\n",
      "Epoch 21 \t Batch 850 \t Validation Loss: 36.148004244636084\n",
      "Epoch 21 \t Batch 900 \t Validation Loss: 35.98067553838094\n",
      "Epoch 21 \t Batch 950 \t Validation Loss: 35.771009371908086\n",
      "Epoch 21 \t Batch 1000 \t Validation Loss: 36.8043525094986\n",
      "Epoch 21 \t Batch 1050 \t Validation Loss: 37.58402274290721\n",
      "Epoch 21 \t Batch 1100 \t Validation Loss: 37.94926804217425\n",
      "Epoch 21 \t Batch 1150 \t Validation Loss: 37.802409909497136\n",
      "Epoch 21 \t Batch 1200 \t Validation Loss: 38.29136190195879\n",
      "Epoch 21 \t Batch 1250 \t Validation Loss: 38.377641963768006\n",
      "Epoch 21 \t Batch 1300 \t Validation Loss: 38.6228352698913\n",
      "Epoch 21 \t Batch 1350 \t Validation Loss: 38.387236944657786\n",
      "Epoch 21 \t Batch 1400 \t Validation Loss: 38.30588943873133\n",
      "Epoch 21 \t Batch 1450 \t Validation Loss: 38.45791591562074\n",
      "Epoch 21 \t Batch 1500 \t Validation Loss: 38.56385641129812\n",
      "Epoch 21 \t Batch 1550 \t Validation Loss: 38.345629226623046\n",
      "Epoch 21 \t Batch 1600 \t Validation Loss: 38.300285131037235\n",
      "Epoch 21 \t Batch 1650 \t Validation Loss: 38.44876274542375\n",
      "Epoch 21 \t Batch 1700 \t Validation Loss: 38.386702183835645\n",
      "Epoch 21 \t Batch 1750 \t Validation Loss: 38.30547443689619\n",
      "Epoch 21 \t Batch 1800 \t Validation Loss: 38.50801561117172\n",
      "Epoch 21 \t Batch 1850 \t Validation Loss: 39.0577414267772\n",
      "Epoch 21 \t Batch 1900 \t Validation Loss: 39.361681794367335\n",
      "Epoch 21 \t Batch 1950 \t Validation Loss: 39.16441908249488\n",
      "Epoch 21 \t Batch 2000 \t Validation Loss: 39.09746269774437\n",
      "Epoch 21 \t Batch 2050 \t Validation Loss: 39.413101684989\n",
      "Epoch 21 \t Batch 2100 \t Validation Loss: 39.38635420867375\n",
      "Epoch 21 \t Batch 2150 \t Validation Loss: 39.245330301107366\n",
      "Epoch 21 \t Batch 2200 \t Validation Loss: 39.08688048947941\n",
      "Epoch 21 \t Batch 2250 \t Validation Loss: 38.93831798701816\n",
      "Epoch 21 \t Batch 2300 \t Validation Loss: 38.72209712785223\n",
      "Epoch 21 \t Batch 2350 \t Validation Loss: 38.708550114428746\n",
      "Epoch 21 \t Batch 2400 \t Validation Loss: 38.803404259383676\n",
      "Epoch 21 \t Batch 2450 \t Validation Loss: 39.156316773648165\n",
      "Epoch 21 Training Loss: 40.48959422917506 Validation Loss: 39.36110098263273\n",
      "Epoch 21 completed\n",
      "Epoch 22 \t Batch 50 \t Training Loss: 38.63005153656006\n",
      "Epoch 22 \t Batch 100 \t Training Loss: 39.06430667877197\n",
      "Epoch 22 \t Batch 150 \t Training Loss: 39.11084604899089\n",
      "Epoch 22 \t Batch 200 \t Training Loss: 39.18290813446045\n",
      "Epoch 22 \t Batch 250 \t Training Loss: 39.316172386169434\n",
      "Epoch 22 \t Batch 300 \t Training Loss: 39.326660022735595\n",
      "Epoch 22 \t Batch 350 \t Training Loss: 39.46716404506138\n",
      "Epoch 22 \t Batch 400 \t Training Loss: 39.357267003059384\n",
      "Epoch 22 \t Batch 450 \t Training Loss: 39.41129085116916\n",
      "Epoch 22 \t Batch 500 \t Training Loss: 39.512898693084715\n",
      "Epoch 22 \t Batch 550 \t Training Loss: 39.42397191827948\n",
      "Epoch 22 \t Batch 600 \t Training Loss: 39.503040068944294\n",
      "Epoch 22 \t Batch 650 \t Training Loss: 39.63285239586463\n",
      "Epoch 22 \t Batch 700 \t Training Loss: 39.58194137845721\n",
      "Epoch 22 \t Batch 750 \t Training Loss: 39.614749384562174\n",
      "Epoch 22 \t Batch 800 \t Training Loss: 39.540926678180696\n",
      "Epoch 22 \t Batch 850 \t Training Loss: 39.53085903392119\n",
      "Epoch 22 \t Batch 900 \t Training Loss: 39.52647033691406\n",
      "Epoch 22 \t Batch 950 \t Training Loss: 39.49381891551771\n",
      "Epoch 22 \t Batch 1000 \t Training Loss: 39.45757926368714\n",
      "Epoch 22 \t Batch 1050 \t Training Loss: 39.5496122887021\n",
      "Epoch 22 \t Batch 1100 \t Training Loss: 39.50716614289717\n",
      "Epoch 22 \t Batch 1150 \t Training Loss: 39.52353215756624\n",
      "Epoch 22 \t Batch 1200 \t Training Loss: 39.58666881243388\n",
      "Epoch 22 \t Batch 1250 \t Training Loss: 39.59707382965088\n",
      "Epoch 22 \t Batch 1300 \t Training Loss: 39.60411346435547\n",
      "Epoch 22 \t Batch 1350 \t Training Loss: 39.60300664689806\n",
      "Epoch 22 \t Batch 1400 \t Training Loss: 39.59793177332197\n",
      "Epoch 22 \t Batch 1450 \t Training Loss: 39.64476006738071\n",
      "Epoch 22 \t Batch 1500 \t Training Loss: 39.64752330271403\n",
      "Epoch 22 \t Batch 1550 \t Training Loss: 39.69248505253945\n",
      "Epoch 22 \t Batch 1600 \t Training Loss: 39.71108381748199\n",
      "Epoch 22 \t Batch 1650 \t Training Loss: 39.71704821499911\n",
      "Epoch 22 \t Batch 1700 \t Training Loss: 39.755679064357984\n",
      "Epoch 22 \t Batch 1750 \t Training Loss: 39.785712527683806\n",
      "Epoch 22 \t Batch 1800 \t Training Loss: 39.75073723157247\n",
      "Epoch 22 \t Batch 1850 \t Training Loss: 39.7332512551385\n",
      "Epoch 22 \t Batch 1900 \t Training Loss: 39.74170145436337\n",
      "Epoch 22 \t Batch 1950 \t Training Loss: 39.709524383544924\n",
      "Epoch 22 \t Batch 2000 \t Training Loss: 39.71237197303772\n",
      "Epoch 22 \t Batch 2050 \t Training Loss: 39.71303167017495\n",
      "Epoch 22 \t Batch 2100 \t Training Loss: 39.74046104431152\n",
      "Epoch 22 \t Batch 2150 \t Training Loss: 39.780608563977616\n",
      "Epoch 22 \t Batch 2200 \t Training Loss: 39.79600509643555\n",
      "Epoch 22 \t Batch 2250 \t Training Loss: 39.84513213857015\n",
      "Epoch 22 \t Batch 2300 \t Training Loss: 39.88851218265036\n",
      "Epoch 22 \t Batch 2350 \t Training Loss: 39.91186871548916\n",
      "Epoch 22 \t Batch 2400 \t Training Loss: 39.922715550263725\n",
      "Epoch 22 \t Batch 2450 \t Training Loss: 39.91242655929254\n",
      "Epoch 22 \t Batch 2500 \t Training Loss: 39.9238858581543\n",
      "Epoch 22 \t Batch 2550 \t Training Loss: 39.92287466161391\n",
      "Epoch 22 \t Batch 2600 \t Training Loss: 39.91454417302058\n",
      "Epoch 22 \t Batch 2650 \t Training Loss: 39.90785294010954\n",
      "Epoch 22 \t Batch 2700 \t Training Loss: 39.91439903824418\n",
      "Epoch 22 \t Batch 2750 \t Training Loss: 39.922919017444954\n",
      "Epoch 22 \t Batch 2800 \t Training Loss: 39.93336637565068\n",
      "Epoch 22 \t Batch 2850 \t Training Loss: 39.949863069099294\n",
      "Epoch 22 \t Batch 2900 \t Training Loss: 39.94665640074631\n",
      "Epoch 22 \t Batch 2950 \t Training Loss: 39.94479101795261\n",
      "Epoch 22 \t Batch 3000 \t Training Loss: 39.94445301691691\n",
      "Epoch 22 \t Batch 3050 \t Training Loss: 39.94015145161113\n",
      "Epoch 22 \t Batch 3100 \t Training Loss: 39.94611912819647\n",
      "Epoch 22 \t Batch 3150 \t Training Loss: 39.95192484840514\n",
      "Epoch 22 \t Batch 3200 \t Training Loss: 39.969274120926855\n",
      "Epoch 22 \t Batch 3250 \t Training Loss: 39.97789471435547\n",
      "Epoch 22 \t Batch 3300 \t Training Loss: 39.99366692571929\n",
      "Epoch 22 \t Batch 3350 \t Training Loss: 40.012114851083325\n",
      "Epoch 22 \t Batch 3400 \t Training Loss: 40.01332132283379\n",
      "Epoch 22 \t Batch 3450 \t Training Loss: 40.01448348667311\n",
      "Epoch 22 \t Batch 3500 \t Training Loss: 40.018819817134315\n",
      "Epoch 22 \t Batch 3550 \t Training Loss: 40.017847179359116\n",
      "Epoch 22 \t Batch 3600 \t Training Loss: 40.050624747806125\n",
      "Epoch 22 \t Batch 3650 \t Training Loss: 40.063156887733776\n",
      "Epoch 22 \t Batch 50 \t Validation Loss: 34.88339012145996\n",
      "Epoch 22 \t Batch 100 \t Validation Loss: 42.08623014450073\n",
      "Epoch 22 \t Batch 150 \t Validation Loss: 35.6678810373942\n",
      "Epoch 22 \t Batch 200 \t Validation Loss: 35.443689575195314\n",
      "Epoch 22 \t Batch 250 \t Validation Loss: 39.32235726928711\n",
      "Epoch 22 \t Batch 300 \t Validation Loss: 37.08594426472982\n",
      "Epoch 22 \t Batch 350 \t Validation Loss: 37.310875314985005\n",
      "Epoch 22 \t Batch 400 \t Validation Loss: 35.82552432775498\n",
      "Epoch 22 \t Batch 450 \t Validation Loss: 35.65406617694431\n",
      "Epoch 22 \t Batch 500 \t Validation Loss: 34.65749674987793\n",
      "Epoch 22 \t Batch 550 \t Validation Loss: 33.82623244892467\n",
      "Epoch 22 \t Batch 600 \t Validation Loss: 33.87926953951518\n",
      "Epoch 22 \t Batch 650 \t Validation Loss: 34.96075037736159\n",
      "Epoch 22 \t Batch 700 \t Validation Loss: 36.187878155027114\n",
      "Epoch 22 \t Batch 750 \t Validation Loss: 36.81469879023234\n",
      "Epoch 22 \t Batch 800 \t Validation Loss: 37.462331014871594\n",
      "Epoch 22 \t Batch 850 \t Validation Loss: 37.93042163512286\n",
      "Epoch 22 \t Batch 900 \t Validation Loss: 37.992334601084394\n",
      "Epoch 22 \t Batch 950 \t Validation Loss: 37.90634305652819\n",
      "Epoch 22 \t Batch 1000 \t Validation Loss: 39.12356953048706\n",
      "Epoch 22 \t Batch 1050 \t Validation Loss: 39.96234711964925\n",
      "Epoch 22 \t Batch 1100 \t Validation Loss: 40.44373968991366\n",
      "Epoch 22 \t Batch 1150 \t Validation Loss: 40.37328557885211\n",
      "Epoch 22 \t Batch 1200 \t Validation Loss: 41.00548908551534\n",
      "Epoch 22 \t Batch 1250 \t Validation Loss: 41.214792965316775\n",
      "Epoch 22 \t Batch 1300 \t Validation Loss: 41.38332154457386\n",
      "Epoch 22 \t Batch 1350 \t Validation Loss: 41.152385992474024\n",
      "Epoch 22 \t Batch 1400 \t Validation Loss: 40.9557283956664\n",
      "Epoch 22 \t Batch 1450 \t Validation Loss: 40.876720630711525\n",
      "Epoch 22 \t Batch 1500 \t Validation Loss: 40.98340658187866\n",
      "Epoch 22 \t Batch 1550 \t Validation Loss: 40.744206147963\n",
      "Epoch 22 \t Batch 1600 \t Validation Loss: 40.521918416023254\n",
      "Epoch 22 \t Batch 1650 \t Validation Loss: 40.57979706735322\n",
      "Epoch 22 \t Batch 1700 \t Validation Loss: 40.52471097048591\n",
      "Epoch 22 \t Batch 1750 \t Validation Loss: 40.35322154344831\n",
      "Epoch 22 \t Batch 1800 \t Validation Loss: 40.37585575845507\n",
      "Epoch 22 \t Batch 1850 \t Validation Loss: 40.89596096966718\n",
      "Epoch 22 \t Batch 1900 \t Validation Loss: 41.12271125592684\n",
      "Epoch 22 \t Batch 1950 \t Validation Loss: 40.97505369039682\n",
      "Epoch 22 \t Batch 2000 \t Validation Loss: 40.892064271450046\n",
      "Epoch 22 \t Batch 2050 \t Validation Loss: 40.98659734958556\n",
      "Epoch 22 \t Batch 2100 \t Validation Loss: 40.8973767167046\n",
      "Epoch 22 \t Batch 2150 \t Validation Loss: 40.71856599053671\n",
      "Epoch 22 \t Batch 2200 \t Validation Loss: 40.54282211130315\n",
      "Epoch 22 \t Batch 2250 \t Validation Loss: 40.36155908075968\n",
      "Epoch 22 \t Batch 2300 \t Validation Loss: 40.079426317733265\n",
      "Epoch 22 \t Batch 2350 \t Validation Loss: 40.007832852323006\n",
      "Epoch 22 \t Batch 2400 \t Validation Loss: 40.09714319437742\n",
      "Epoch 22 \t Batch 2450 \t Validation Loss: 40.399063904334085\n",
      "Epoch 22 Training Loss: 40.061423167537576 Validation Loss: 40.58838870763392\n",
      "Epoch 22 completed\n",
      "Epoch 23 \t Batch 50 \t Training Loss: 36.751212158203124\n",
      "Epoch 23 \t Batch 100 \t Training Loss: 37.975454845428466\n",
      "Epoch 23 \t Batch 150 \t Training Loss: 38.160664812723795\n",
      "Epoch 23 \t Batch 200 \t Training Loss: 37.872173395156864\n",
      "Epoch 23 \t Batch 250 \t Training Loss: 37.94819966125488\n",
      "Epoch 23 \t Batch 300 \t Training Loss: 38.17344924926758\n",
      "Epoch 23 \t Batch 350 \t Training Loss: 38.27704104287284\n",
      "Epoch 23 \t Batch 400 \t Training Loss: 38.281976509094235\n",
      "Epoch 23 \t Batch 450 \t Training Loss: 38.23193648868137\n",
      "Epoch 23 \t Batch 500 \t Training Loss: 38.35135791015625\n",
      "Epoch 23 \t Batch 550 \t Training Loss: 38.40199613397772\n",
      "Epoch 23 \t Batch 600 \t Training Loss: 38.44983938217163\n",
      "Epoch 23 \t Batch 650 \t Training Loss: 38.449219107994665\n",
      "Epoch 23 \t Batch 700 \t Training Loss: 38.408101103646416\n",
      "Epoch 23 \t Batch 750 \t Training Loss: 38.48943898773194\n",
      "Epoch 23 \t Batch 800 \t Training Loss: 38.469101963043215\n",
      "Epoch 23 \t Batch 850 \t Training Loss: 38.4266086511051\n",
      "Epoch 23 \t Batch 900 \t Training Loss: 38.43298737419976\n",
      "Epoch 23 \t Batch 950 \t Training Loss: 38.52776701676218\n",
      "Epoch 23 \t Batch 1000 \t Training Loss: 38.60988250923157\n",
      "Epoch 23 \t Batch 1050 \t Training Loss: 38.627808316548666\n",
      "Epoch 23 \t Batch 1100 \t Training Loss: 38.69326629812067\n",
      "Epoch 23 \t Batch 1150 \t Training Loss: 38.706533146733825\n",
      "Epoch 23 \t Batch 1200 \t Training Loss: 38.71690948486328\n",
      "Epoch 23 \t Batch 1250 \t Training Loss: 38.796279545593265\n",
      "Epoch 23 \t Batch 1300 \t Training Loss: 38.819629675058216\n",
      "Epoch 23 \t Batch 1350 \t Training Loss: 38.82263083846481\n",
      "Epoch 23 \t Batch 1400 \t Training Loss: 38.88251785687038\n",
      "Epoch 23 \t Batch 1450 \t Training Loss: 38.92091426454741\n",
      "Epoch 23 \t Batch 1500 \t Training Loss: 38.96131758753459\n",
      "Epoch 23 \t Batch 1550 \t Training Loss: 38.98178075728878\n",
      "Epoch 23 \t Batch 1600 \t Training Loss: 38.998955476284024\n",
      "Epoch 23 \t Batch 1650 \t Training Loss: 39.01049492229115\n",
      "Epoch 23 \t Batch 1700 \t Training Loss: 39.02026175779455\n",
      "Epoch 23 \t Batch 1750 \t Training Loss: 39.03003167179653\n",
      "Epoch 23 \t Batch 1800 \t Training Loss: 39.019905973010594\n",
      "Epoch 23 \t Batch 1850 \t Training Loss: 39.04446006053203\n",
      "Epoch 23 \t Batch 1900 \t Training Loss: 39.105306780965705\n",
      "Epoch 23 \t Batch 1950 \t Training Loss: 39.0895330282358\n",
      "Epoch 23 \t Batch 2000 \t Training Loss: 39.109053604125975\n",
      "Epoch 23 \t Batch 2050 \t Training Loss: 39.16515793125804\n",
      "Epoch 23 \t Batch 2100 \t Training Loss: 39.152858004797075\n",
      "Epoch 23 \t Batch 2150 \t Training Loss: 39.12651543195857\n",
      "Epoch 23 \t Batch 2200 \t Training Loss: 39.12942398591475\n",
      "Epoch 23 \t Batch 2250 \t Training Loss: 39.1596559753418\n",
      "Epoch 23 \t Batch 2300 \t Training Loss: 39.136901741027835\n",
      "Epoch 23 \t Batch 2350 \t Training Loss: 39.15640126086296\n",
      "Epoch 23 \t Batch 2400 \t Training Loss: 39.1647167690595\n",
      "Epoch 23 \t Batch 2450 \t Training Loss: 39.20011696796028\n",
      "Epoch 23 \t Batch 2500 \t Training Loss: 39.19243809356689\n",
      "Epoch 23 \t Batch 2550 \t Training Loss: 39.23655257804721\n",
      "Epoch 23 \t Batch 2600 \t Training Loss: 39.23233870799725\n",
      "Epoch 23 \t Batch 2650 \t Training Loss: 39.26861841381721\n",
      "Epoch 23 \t Batch 2700 \t Training Loss: 39.27242635585644\n",
      "Epoch 23 \t Batch 2750 \t Training Loss: 39.28180697978627\n",
      "Epoch 23 \t Batch 2800 \t Training Loss: 39.29510653563908\n",
      "Epoch 23 \t Batch 2850 \t Training Loss: 39.3023773039433\n",
      "Epoch 23 \t Batch 2900 \t Training Loss: 39.313677763445625\n",
      "Epoch 23 \t Batch 2950 \t Training Loss: 39.33067213543391\n",
      "Epoch 23 \t Batch 3000 \t Training Loss: 39.337270692189534\n",
      "Epoch 23 \t Batch 3050 \t Training Loss: 39.3394856624916\n",
      "Epoch 23 \t Batch 3100 \t Training Loss: 39.35218826970746\n",
      "Epoch 23 \t Batch 3150 \t Training Loss: 39.361456353929306\n",
      "Epoch 23 \t Batch 3200 \t Training Loss: 39.3896648311615\n",
      "Epoch 23 \t Batch 3250 \t Training Loss: 39.39439002110408\n",
      "Epoch 23 \t Batch 3300 \t Training Loss: 39.389384558706574\n",
      "Epoch 23 \t Batch 3350 \t Training Loss: 39.399068644509384\n",
      "Epoch 23 \t Batch 3400 \t Training Loss: 39.413123215507056\n",
      "Epoch 23 \t Batch 3450 \t Training Loss: 39.42941136180491\n",
      "Epoch 23 \t Batch 3500 \t Training Loss: 39.44786068834577\n",
      "Epoch 23 \t Batch 3550 \t Training Loss: 39.463563105623486\n",
      "Epoch 23 \t Batch 3600 \t Training Loss: 39.466446481280855\n",
      "Epoch 23 \t Batch 3650 \t Training Loss: 39.475347175598145\n",
      "Epoch 23 \t Batch 50 \t Validation Loss: 30.145754566192625\n",
      "Epoch 23 \t Batch 100 \t Validation Loss: 37.64566632270813\n",
      "Epoch 23 \t Batch 150 \t Validation Loss: 31.690517304738364\n",
      "Epoch 23 \t Batch 200 \t Validation Loss: 31.898806624412536\n",
      "Epoch 23 \t Batch 250 \t Validation Loss: 35.2449115600586\n",
      "Epoch 23 \t Batch 300 \t Validation Loss: 33.217522376378376\n",
      "Epoch 23 \t Batch 350 \t Validation Loss: 33.6945686503819\n",
      "Epoch 23 \t Batch 400 \t Validation Loss: 32.74251653432846\n",
      "Epoch 23 \t Batch 450 \t Validation Loss: 32.83016224119398\n",
      "Epoch 23 \t Batch 500 \t Validation Loss: 32.152737955093386\n",
      "Epoch 23 \t Batch 550 \t Validation Loss: 31.59613076990301\n",
      "Epoch 23 \t Batch 600 \t Validation Loss: 31.93985219637553\n",
      "Epoch 23 \t Batch 650 \t Validation Loss: 33.300792940579925\n",
      "Epoch 23 \t Batch 700 \t Validation Loss: 34.573430782045634\n",
      "Epoch 23 \t Batch 750 \t Validation Loss: 35.49217105865478\n",
      "Epoch 23 \t Batch 800 \t Validation Loss: 36.54002769827843\n",
      "Epoch 23 \t Batch 850 \t Validation Loss: 37.09424092348884\n",
      "Epoch 23 \t Batch 900 \t Validation Loss: 37.18684057235718\n",
      "Epoch 23 \t Batch 950 \t Validation Loss: 37.13946315263447\n",
      "Epoch 23 \t Batch 1000 \t Validation Loss: 38.5670346326828\n",
      "Epoch 23 \t Batch 1050 \t Validation Loss: 39.436299548830306\n",
      "Epoch 23 \t Batch 1100 \t Validation Loss: 39.95652771646326\n",
      "Epoch 23 \t Batch 1150 \t Validation Loss: 39.854362699674525\n",
      "Epoch 23 \t Batch 1200 \t Validation Loss: 40.524669487873716\n",
      "Epoch 23 \t Batch 1250 \t Validation Loss: 40.74346834449768\n",
      "Epoch 23 \t Batch 1300 \t Validation Loss: 40.93986153272482\n",
      "Epoch 23 \t Batch 1350 \t Validation Loss: 40.74180059962802\n",
      "Epoch 23 \t Batch 1400 \t Validation Loss: 40.603797060080936\n",
      "Epoch 23 \t Batch 1450 \t Validation Loss: 40.53576906171338\n",
      "Epoch 23 \t Batch 1500 \t Validation Loss: 40.64847006893158\n",
      "Epoch 23 \t Batch 1550 \t Validation Loss: 40.398368904667514\n",
      "Epoch 23 \t Batch 1600 \t Validation Loss: 40.267066473066805\n",
      "Epoch 23 \t Batch 1650 \t Validation Loss: 40.354391274018724\n",
      "Epoch 23 \t Batch 1700 \t Validation Loss: 40.267966662294725\n",
      "Epoch 23 \t Batch 1750 \t Validation Loss: 40.15266526985168\n",
      "Epoch 23 \t Batch 1800 \t Validation Loss: 40.189015584256914\n",
      "Epoch 23 \t Batch 1850 \t Validation Loss: 40.6775286625527\n",
      "Epoch 23 \t Batch 1900 \t Validation Loss: 40.91523782203072\n",
      "Epoch 23 \t Batch 1950 \t Validation Loss: 40.75834161880689\n",
      "Epoch 23 \t Batch 2000 \t Validation Loss: 40.67357233119011\n",
      "Epoch 23 \t Batch 2050 \t Validation Loss: 40.80766238166065\n",
      "Epoch 23 \t Batch 2100 \t Validation Loss: 40.77544064816974\n",
      "Epoch 23 \t Batch 2150 \t Validation Loss: 40.6453226504215\n",
      "Epoch 23 \t Batch 2200 \t Validation Loss: 40.49623835802078\n",
      "Epoch 23 \t Batch 2250 \t Validation Loss: 40.393930943171185\n",
      "Epoch 23 \t Batch 2300 \t Validation Loss: 40.20422626920369\n",
      "Epoch 23 \t Batch 2350 \t Validation Loss: 40.20543757712587\n",
      "Epoch 23 \t Batch 2400 \t Validation Loss: 40.29045949111382\n",
      "Epoch 23 \t Batch 2450 \t Validation Loss: 40.62200397053543\n",
      "Epoch 23 Training Loss: 39.47889152763974 Validation Loss: 40.837855174460195\n",
      "Epoch 23 completed\n",
      "Epoch 24 \t Batch 50 \t Training Loss: 39.934630699157715\n",
      "Epoch 24 \t Batch 100 \t Training Loss: 39.00047157287597\n",
      "Epoch 24 \t Batch 150 \t Training Loss: 38.43924950917562\n",
      "Epoch 24 \t Batch 200 \t Training Loss: 38.138671760559085\n",
      "Epoch 24 \t Batch 250 \t Training Loss: 38.16351996612549\n",
      "Epoch 24 \t Batch 300 \t Training Loss: 37.991212100982665\n",
      "Epoch 24 \t Batch 350 \t Training Loss: 38.13960645403181\n",
      "Epoch 24 \t Batch 400 \t Training Loss: 38.18102035045624\n",
      "Epoch 24 \t Batch 450 \t Training Loss: 38.182259763081866\n",
      "Epoch 24 \t Batch 500 \t Training Loss: 38.118828125\n",
      "Epoch 24 \t Batch 550 \t Training Loss: 38.0254530507868\n",
      "Epoch 24 \t Batch 600 \t Training Loss: 37.94234347343445\n",
      "Epoch 24 \t Batch 650 \t Training Loss: 38.04687988868127\n",
      "Epoch 24 \t Batch 700 \t Training Loss: 38.04692847933088\n",
      "Epoch 24 \t Batch 750 \t Training Loss: 37.954896260579424\n",
      "Epoch 24 \t Batch 800 \t Training Loss: 37.947166793346405\n",
      "Epoch 24 \t Batch 850 \t Training Loss: 37.99010746899773\n",
      "Epoch 24 \t Batch 900 \t Training Loss: 37.94724065144857\n",
      "Epoch 24 \t Batch 950 \t Training Loss: 38.012479651601694\n",
      "Epoch 24 \t Batch 1000 \t Training Loss: 38.00786832809448\n",
      "Epoch 24 \t Batch 1050 \t Training Loss: 37.993800548371816\n",
      "Epoch 24 \t Batch 1100 \t Training Loss: 38.00612861633301\n",
      "Epoch 24 \t Batch 1150 \t Training Loss: 38.082620071742845\n",
      "Epoch 24 \t Batch 1200 \t Training Loss: 38.0997472747167\n",
      "Epoch 24 \t Batch 1250 \t Training Loss: 38.16964574432373\n",
      "Epoch 24 \t Batch 1300 \t Training Loss: 38.256235825465275\n",
      "Epoch 24 \t Batch 1350 \t Training Loss: 38.25434356406883\n",
      "Epoch 24 \t Batch 1400 \t Training Loss: 38.28715175219944\n",
      "Epoch 24 \t Batch 1450 \t Training Loss: 38.29042711586788\n",
      "Epoch 24 \t Batch 1500 \t Training Loss: 38.293838245391846\n",
      "Epoch 24 \t Batch 1550 \t Training Loss: 38.351569747924806\n",
      "Epoch 24 \t Batch 1600 \t Training Loss: 38.43014410495758\n",
      "Epoch 24 \t Batch 1650 \t Training Loss: 38.41987482013124\n",
      "Epoch 24 \t Batch 1700 \t Training Loss: 38.43810506708482\n",
      "Epoch 24 \t Batch 1750 \t Training Loss: 38.44207258278983\n",
      "Epoch 24 \t Batch 1800 \t Training Loss: 38.44831706788805\n",
      "Epoch 24 \t Batch 1850 \t Training Loss: 38.47922522879936\n",
      "Epoch 24 \t Batch 1900 \t Training Loss: 38.48472460194638\n",
      "Epoch 24 \t Batch 1950 \t Training Loss: 38.48021664448274\n",
      "Epoch 24 \t Batch 2000 \t Training Loss: 38.498811838150026\n",
      "Epoch 24 \t Batch 2050 \t Training Loss: 38.49419788546678\n",
      "Epoch 24 \t Batch 2100 \t Training Loss: 38.52942618142991\n",
      "Epoch 24 \t Batch 2150 \t Training Loss: 38.54671458931856\n",
      "Epoch 24 \t Batch 2200 \t Training Loss: 38.53864007082853\n",
      "Epoch 24 \t Batch 2250 \t Training Loss: 38.60483589426676\n",
      "Epoch 24 \t Batch 2300 \t Training Loss: 38.62129075838172\n",
      "Epoch 24 \t Batch 2350 \t Training Loss: 38.638450668821946\n",
      "Epoch 24 \t Batch 2400 \t Training Loss: 38.64020191907883\n",
      "Epoch 24 \t Batch 2450 \t Training Loss: 38.6404272826837\n",
      "Epoch 24 \t Batch 2500 \t Training Loss: 38.686820112609865\n",
      "Epoch 24 \t Batch 2550 \t Training Loss: 38.66392342960133\n",
      "Epoch 24 \t Batch 2600 \t Training Loss: 38.70187587958116\n",
      "Epoch 24 \t Batch 2650 \t Training Loss: 38.723595072188466\n",
      "Epoch 24 \t Batch 2700 \t Training Loss: 38.72560715710675\n",
      "Epoch 24 \t Batch 2750 \t Training Loss: 38.75274329792369\n",
      "Epoch 24 \t Batch 2800 \t Training Loss: 38.728238716806686\n",
      "Epoch 24 \t Batch 2850 \t Training Loss: 38.73371691118207\n",
      "Epoch 24 \t Batch 2900 \t Training Loss: 38.75464562777815\n",
      "Epoch 24 \t Batch 2950 \t Training Loss: 38.77149807234942\n",
      "Epoch 24 \t Batch 3000 \t Training Loss: 38.76832620048523\n",
      "Epoch 24 \t Batch 3050 \t Training Loss: 38.7821996119765\n",
      "Epoch 24 \t Batch 3100 \t Training Loss: 38.80770936535251\n",
      "Epoch 24 \t Batch 3150 \t Training Loss: 38.80777600727384\n",
      "Epoch 24 \t Batch 3200 \t Training Loss: 38.82000420868397\n",
      "Epoch 24 \t Batch 3250 \t Training Loss: 38.82535490358793\n",
      "Epoch 24 \t Batch 3300 \t Training Loss: 38.830579069888955\n",
      "Epoch 24 \t Batch 3350 \t Training Loss: 38.83954739698723\n",
      "Epoch 24 \t Batch 3400 \t Training Loss: 38.87397233009338\n",
      "Epoch 24 \t Batch 3450 \t Training Loss: 38.87649915943975\n",
      "Epoch 24 \t Batch 3500 \t Training Loss: 38.890295469556534\n",
      "Epoch 24 \t Batch 3550 \t Training Loss: 38.90082227411404\n",
      "Epoch 24 \t Batch 3600 \t Training Loss: 38.92008860111237\n",
      "Epoch 24 \t Batch 3650 \t Training Loss: 38.91000069605161\n",
      "Epoch 24 \t Batch 50 \t Validation Loss: 32.04739810943604\n",
      "Epoch 24 \t Batch 100 \t Validation Loss: 40.261176109313965\n",
      "Epoch 24 \t Batch 150 \t Validation Loss: 35.17933940887451\n",
      "Epoch 24 \t Batch 200 \t Validation Loss: 35.112211570739746\n",
      "Epoch 24 \t Batch 250 \t Validation Loss: 38.31681827545166\n",
      "Epoch 24 \t Batch 300 \t Validation Loss: 36.429082260131835\n",
      "Epoch 24 \t Batch 350 \t Validation Loss: 36.662660443442206\n",
      "Epoch 24 \t Batch 400 \t Validation Loss: 35.6777366566658\n",
      "Epoch 24 \t Batch 450 \t Validation Loss: 35.745216295454235\n",
      "Epoch 24 \t Batch 500 \t Validation Loss: 35.08145536994934\n",
      "Epoch 24 \t Batch 550 \t Validation Loss: 34.40371326966719\n",
      "Epoch 24 \t Batch 600 \t Validation Loss: 34.51264115333557\n",
      "Epoch 24 \t Batch 650 \t Validation Loss: 35.83820284916804\n",
      "Epoch 24 \t Batch 700 \t Validation Loss: 37.008824159077236\n",
      "Epoch 24 \t Batch 750 \t Validation Loss: 37.75284878158569\n",
      "Epoch 24 \t Batch 800 \t Validation Loss: 38.64851030468941\n",
      "Epoch 24 \t Batch 850 \t Validation Loss: 39.161727391411276\n",
      "Epoch 24 \t Batch 900 \t Validation Loss: 39.12517820570204\n",
      "Epoch 24 \t Batch 950 \t Validation Loss: 38.93345458281668\n",
      "Epoch 24 \t Batch 1000 \t Validation Loss: 40.32217972278595\n",
      "Epoch 24 \t Batch 1050 \t Validation Loss: 41.18751750673567\n",
      "Epoch 24 \t Batch 1100 \t Validation Loss: 41.75267864660783\n",
      "Epoch 24 \t Batch 1150 \t Validation Loss: 41.59149770239125\n",
      "Epoch 24 \t Batch 1200 \t Validation Loss: 42.13737848997116\n",
      "Epoch 24 \t Batch 1250 \t Validation Loss: 42.28068478927612\n",
      "Epoch 24 \t Batch 1300 \t Validation Loss: 42.44472506082975\n",
      "Epoch 24 \t Batch 1350 \t Validation Loss: 42.142440484364826\n",
      "Epoch 24 \t Batch 1400 \t Validation Loss: 41.9838251876831\n",
      "Epoch 24 \t Batch 1450 \t Validation Loss: 41.96724053941924\n",
      "Epoch 24 \t Batch 1500 \t Validation Loss: 42.07776930809021\n",
      "Epoch 24 \t Batch 1550 \t Validation Loss: 41.76169141400245\n",
      "Epoch 24 \t Batch 1600 \t Validation Loss: 41.60490012586117\n",
      "Epoch 24 \t Batch 1650 \t Validation Loss: 41.66562335274436\n",
      "Epoch 24 \t Batch 1700 \t Validation Loss: 41.51836775330936\n",
      "Epoch 24 \t Batch 1750 \t Validation Loss: 41.35775703920637\n",
      "Epoch 24 \t Batch 1800 \t Validation Loss: 41.385040652487014\n",
      "Epoch 24 \t Batch 1850 \t Validation Loss: 41.84973663484728\n",
      "Epoch 24 \t Batch 1900 \t Validation Loss: 42.05898103362635\n",
      "Epoch 24 \t Batch 1950 \t Validation Loss: 41.839303249946006\n",
      "Epoch 24 \t Batch 2000 \t Validation Loss: 41.71428244256973\n",
      "Epoch 24 \t Batch 2050 \t Validation Loss: 41.85511753733565\n",
      "Epoch 24 \t Batch 2100 \t Validation Loss: 41.752835570289974\n",
      "Epoch 24 \t Batch 2150 \t Validation Loss: 41.55088213765344\n",
      "Epoch 24 \t Batch 2200 \t Validation Loss: 41.329202258803626\n",
      "Epoch 24 \t Batch 2250 \t Validation Loss: 41.143565349155004\n",
      "Epoch 24 \t Batch 2300 \t Validation Loss: 40.90150364502617\n",
      "Epoch 24 \t Batch 2350 \t Validation Loss: 40.829243641305474\n",
      "Epoch 24 \t Batch 2400 \t Validation Loss: 40.88342685858409\n",
      "Epoch 24 \t Batch 2450 \t Validation Loss: 41.176199085079894\n",
      "Epoch 24 Training Loss: 38.92535026616853 Validation Loss: 41.34003684389127\n",
      "Epoch 24 completed\n",
      "Epoch 25 \t Batch 50 \t Training Loss: 36.26203491210938\n",
      "Epoch 25 \t Batch 100 \t Training Loss: 36.69002056121826\n",
      "Epoch 25 \t Batch 150 \t Training Loss: 37.144369455973305\n",
      "Epoch 25 \t Batch 200 \t Training Loss: 37.27394665718079\n",
      "Epoch 25 \t Batch 250 \t Training Loss: 37.29171201324463\n",
      "Epoch 25 \t Batch 300 \t Training Loss: 37.31851603825887\n",
      "Epoch 25 \t Batch 350 \t Training Loss: 37.23359773908343\n",
      "Epoch 25 \t Batch 400 \t Training Loss: 37.28472312450409\n",
      "Epoch 25 \t Batch 450 \t Training Loss: 37.36273738013374\n",
      "Epoch 25 \t Batch 500 \t Training Loss: 37.369421062469485\n",
      "Epoch 25 \t Batch 550 \t Training Loss: 37.48213443062522\n",
      "Epoch 25 \t Batch 600 \t Training Loss: 37.53558531125387\n",
      "Epoch 25 \t Batch 650 \t Training Loss: 37.622427403376655\n",
      "Epoch 25 \t Batch 700 \t Training Loss: 37.654871752602716\n",
      "Epoch 25 \t Batch 750 \t Training Loss: 37.63754660797119\n",
      "Epoch 25 \t Batch 800 \t Training Loss: 37.67830809593201\n",
      "Epoch 25 \t Batch 850 \t Training Loss: 37.673591456693764\n",
      "Epoch 25 \t Batch 900 \t Training Loss: 37.842701263427735\n",
      "Epoch 25 \t Batch 950 \t Training Loss: 37.818036225971426\n",
      "Epoch 25 \t Batch 1000 \t Training Loss: 37.853800867080686\n",
      "Epoch 25 \t Batch 1050 \t Training Loss: 37.88755336943127\n",
      "Epoch 25 \t Batch 1100 \t Training Loss: 37.91987961335616\n",
      "Epoch 25 \t Batch 1150 \t Training Loss: 37.91213739312214\n",
      "Epoch 25 \t Batch 1200 \t Training Loss: 37.90668617407481\n",
      "Epoch 25 \t Batch 1250 \t Training Loss: 37.93437928771973\n",
      "Epoch 25 \t Batch 1300 \t Training Loss: 37.97385851639968\n",
      "Epoch 25 \t Batch 1350 \t Training Loss: 37.90918487972684\n",
      "Epoch 25 \t Batch 1400 \t Training Loss: 37.90709335190909\n",
      "Epoch 25 \t Batch 1450 \t Training Loss: 37.93914276780753\n",
      "Epoch 25 \t Batch 1500 \t Training Loss: 37.92696818796794\n",
      "Epoch 25 \t Batch 1550 \t Training Loss: 37.92435781540409\n",
      "Epoch 25 \t Batch 1600 \t Training Loss: 37.92129018068314\n",
      "Epoch 25 \t Batch 1650 \t Training Loss: 37.84937840086041\n",
      "Epoch 25 \t Batch 1700 \t Training Loss: 37.85520282072179\n",
      "Epoch 25 \t Batch 1750 \t Training Loss: 37.85658906228202\n",
      "Epoch 25 \t Batch 1800 \t Training Loss: 37.8869044801924\n",
      "Epoch 25 \t Batch 1850 \t Training Loss: 37.9015329154762\n",
      "Epoch 25 \t Batch 1900 \t Training Loss: 37.92558918200041\n",
      "Epoch 25 \t Batch 1950 \t Training Loss: 37.95855688046186\n",
      "Epoch 25 \t Batch 2000 \t Training Loss: 37.96626869106293\n",
      "Epoch 25 \t Batch 2050 \t Training Loss: 37.98105322395883\n",
      "Epoch 25 \t Batch 2100 \t Training Loss: 37.961076186043876\n",
      "Epoch 25 \t Batch 2150 \t Training Loss: 37.95921917227812\n",
      "Epoch 25 \t Batch 2200 \t Training Loss: 37.939871142127295\n",
      "Epoch 25 \t Batch 2250 \t Training Loss: 37.984949601067434\n",
      "Epoch 25 \t Batch 2300 \t Training Loss: 38.000928309896715\n",
      "Epoch 25 \t Batch 2350 \t Training Loss: 38.01453884692902\n",
      "Epoch 25 \t Batch 2400 \t Training Loss: 38.08084300279617\n",
      "Epoch 25 \t Batch 2450 \t Training Loss: 38.09822968385657\n",
      "Epoch 25 \t Batch 2500 \t Training Loss: 38.09750195083618\n",
      "Epoch 25 \t Batch 2550 \t Training Loss: 38.09784003463446\n",
      "Epoch 25 \t Batch 2600 \t Training Loss: 38.11780208147489\n",
      "Epoch 25 \t Batch 2650 \t Training Loss: 38.15117053265841\n",
      "Epoch 25 \t Batch 2700 \t Training Loss: 38.17386979350337\n",
      "Epoch 25 \t Batch 2750 \t Training Loss: 38.162974997086955\n",
      "Epoch 25 \t Batch 2800 \t Training Loss: 38.176458614213125\n",
      "Epoch 25 \t Batch 2850 \t Training Loss: 38.208431766576936\n",
      "Epoch 25 \t Batch 2900 \t Training Loss: 38.22300653786495\n",
      "Epoch 25 \t Batch 2950 \t Training Loss: 38.2594735174664\n",
      "Epoch 25 \t Batch 3000 \t Training Loss: 38.27151306597392\n",
      "Epoch 25 \t Batch 3050 \t Training Loss: 38.29778926349077\n",
      "Epoch 25 \t Batch 3100 \t Training Loss: 38.29684168784849\n",
      "Epoch 25 \t Batch 3150 \t Training Loss: 38.286652487497484\n",
      "Epoch 25 \t Batch 3200 \t Training Loss: 38.29588905930519\n",
      "Epoch 25 \t Batch 3250 \t Training Loss: 38.29153171011118\n",
      "Epoch 25 \t Batch 3300 \t Training Loss: 38.29995709216956\n",
      "Epoch 25 \t Batch 3350 \t Training Loss: 38.30465153480644\n",
      "Epoch 25 \t Batch 3400 \t Training Loss: 38.314199200237496\n",
      "Epoch 25 \t Batch 3450 \t Training Loss: 38.307495567764065\n",
      "Epoch 25 \t Batch 3500 \t Training Loss: 38.32556242752075\n",
      "Epoch 25 \t Batch 3550 \t Training Loss: 38.35792828304667\n",
      "Epoch 25 \t Batch 3600 \t Training Loss: 38.34922694312202\n",
      "Epoch 25 \t Batch 3650 \t Training Loss: 38.36655327470335\n",
      "Epoch 25 \t Batch 50 \t Validation Loss: 24.94072172164917\n",
      "Epoch 25 \t Batch 100 \t Validation Loss: 28.50816270828247\n",
      "Epoch 25 \t Batch 150 \t Validation Loss: 25.034960021972655\n",
      "Epoch 25 \t Batch 200 \t Validation Loss: 24.525933730602265\n",
      "Epoch 25 \t Batch 250 \t Validation Loss: 27.120133459091186\n",
      "Epoch 25 \t Batch 300 \t Validation Loss: 25.92856199423472\n",
      "Epoch 25 \t Batch 350 \t Validation Loss: 26.813101546423777\n",
      "Epoch 25 \t Batch 400 \t Validation Loss: 26.666446269750594\n",
      "Epoch 25 \t Batch 450 \t Validation Loss: 27.419606580734253\n",
      "Epoch 25 \t Batch 500 \t Validation Loss: 27.347932122230528\n",
      "Epoch 25 \t Batch 550 \t Validation Loss: 27.148383368578823\n",
      "Epoch 25 \t Batch 600 \t Validation Loss: 27.752013130982718\n",
      "Epoch 25 \t Batch 650 \t Validation Loss: 29.43203275900621\n",
      "Epoch 25 \t Batch 700 \t Validation Loss: 31.23959724221911\n",
      "Epoch 25 \t Batch 750 \t Validation Loss: 32.48682649167379\n",
      "Epoch 25 \t Batch 800 \t Validation Loss: 33.77589652240276\n",
      "Epoch 25 \t Batch 850 \t Validation Loss: 34.582269325817336\n",
      "Epoch 25 \t Batch 900 \t Validation Loss: 34.87569166766273\n",
      "Epoch 25 \t Batch 950 \t Validation Loss: 35.045640099676035\n",
      "Epoch 25 \t Batch 1000 \t Validation Loss: 36.69873687314987\n",
      "Epoch 25 \t Batch 1050 \t Validation Loss: 37.76634737423488\n",
      "Epoch 25 \t Batch 1100 \t Validation Loss: 38.53416700883345\n",
      "Epoch 25 \t Batch 1150 \t Validation Loss: 38.53021537614905\n",
      "Epoch 25 \t Batch 1200 \t Validation Loss: 39.32759036064148\n",
      "Epoch 25 \t Batch 1250 \t Validation Loss: 39.67863214874268\n",
      "Epoch 25 \t Batch 1300 \t Validation Loss: 39.92615973619314\n",
      "Epoch 25 \t Batch 1350 \t Validation Loss: 39.73370731283117\n",
      "Epoch 25 \t Batch 1400 \t Validation Loss: 39.63505713122232\n",
      "Epoch 25 \t Batch 1450 \t Validation Loss: 39.60775645617781\n",
      "Epoch 25 \t Batch 1500 \t Validation Loss: 39.79907379817963\n",
      "Epoch 25 \t Batch 1550 \t Validation Loss: 39.514815176379294\n",
      "Epoch 25 \t Batch 1600 \t Validation Loss: 39.320560060441494\n",
      "Epoch 25 \t Batch 1650 \t Validation Loss: 39.36478741703611\n",
      "Epoch 25 \t Batch 1700 \t Validation Loss: 39.248956598113566\n",
      "Epoch 25 \t Batch 1750 \t Validation Loss: 39.09038988140651\n",
      "Epoch 25 \t Batch 1800 \t Validation Loss: 39.091765613290995\n",
      "Epoch 25 \t Batch 1850 \t Validation Loss: 39.61702707831924\n",
      "Epoch 25 \t Batch 1900 \t Validation Loss: 39.86198735362605\n",
      "Epoch 25 \t Batch 1950 \t Validation Loss: 39.7206155696282\n",
      "Epoch 25 \t Batch 2000 \t Validation Loss: 39.6342351167202\n",
      "Epoch 25 \t Batch 2050 \t Validation Loss: 39.706572117456574\n",
      "Epoch 25 \t Batch 2100 \t Validation Loss: 39.573309618404934\n",
      "Epoch 25 \t Batch 2150 \t Validation Loss: 39.38700777475224\n",
      "Epoch 25 \t Batch 2200 \t Validation Loss: 39.19785001776435\n",
      "Epoch 25 \t Batch 2250 \t Validation Loss: 39.01625725407071\n",
      "Epoch 25 \t Batch 2300 \t Validation Loss: 38.75924366463786\n",
      "Epoch 25 \t Batch 2350 \t Validation Loss: 38.695203410716765\n",
      "Epoch 25 \t Batch 2400 \t Validation Loss: 38.79318788776795\n",
      "Epoch 25 \t Batch 2450 \t Validation Loss: 39.12413087017682\n",
      "Epoch 25 Training Loss: 38.36186734410643 Validation Loss: 39.30417557341325\n",
      "Epoch 25 completed\n",
      "Epoch 26 \t Batch 50 \t Training Loss: 39.553305587768556\n",
      "Epoch 26 \t Batch 100 \t Training Loss: 37.9540251159668\n",
      "Epoch 26 \t Batch 150 \t Training Loss: 37.59711578369141\n",
      "Epoch 26 \t Batch 200 \t Training Loss: 37.29095449447632\n",
      "Epoch 26 \t Batch 250 \t Training Loss: 37.32846472930908\n",
      "Epoch 26 \t Batch 300 \t Training Loss: 37.188373800913496\n",
      "Epoch 26 \t Batch 350 \t Training Loss: 37.10948463984898\n",
      "Epoch 26 \t Batch 400 \t Training Loss: 37.13812882900238\n",
      "Epoch 26 \t Batch 450 \t Training Loss: 37.246415286593965\n",
      "Epoch 26 \t Batch 500 \t Training Loss: 37.22358140945435\n",
      "Epoch 26 \t Batch 550 \t Training Loss: 37.255006152066315\n",
      "Epoch 26 \t Batch 600 \t Training Loss: 37.18307462374369\n",
      "Epoch 26 \t Batch 650 \t Training Loss: 37.20881382575402\n",
      "Epoch 26 \t Batch 700 \t Training Loss: 37.25850170135498\n",
      "Epoch 26 \t Batch 750 \t Training Loss: 37.31461050923665\n",
      "Epoch 26 \t Batch 800 \t Training Loss: 37.22349001884461\n",
      "Epoch 26 \t Batch 850 \t Training Loss: 37.178620473076315\n",
      "Epoch 26 \t Batch 900 \t Training Loss: 37.23942913691203\n",
      "Epoch 26 \t Batch 950 \t Training Loss: 37.2791746400532\n",
      "Epoch 26 \t Batch 1000 \t Training Loss: 37.28926675796509\n",
      "Epoch 26 \t Batch 1050 \t Training Loss: 37.31200066521054\n",
      "Epoch 26 \t Batch 1100 \t Training Loss: 37.33314910021695\n",
      "Epoch 26 \t Batch 1150 \t Training Loss: 37.3552496735946\n",
      "Epoch 26 \t Batch 1200 \t Training Loss: 37.352923092842104\n",
      "Epoch 26 \t Batch 1250 \t Training Loss: 37.38941072998047\n",
      "Epoch 26 \t Batch 1300 \t Training Loss: 37.374657798180216\n",
      "Epoch 26 \t Batch 1350 \t Training Loss: 37.39799653088605\n",
      "Epoch 26 \t Batch 1400 \t Training Loss: 37.3719034739903\n",
      "Epoch 26 \t Batch 1450 \t Training Loss: 37.407223412086225\n",
      "Epoch 26 \t Batch 1500 \t Training Loss: 37.401602001190184\n",
      "Epoch 26 \t Batch 1550 \t Training Loss: 37.4161141204834\n",
      "Epoch 26 \t Batch 1600 \t Training Loss: 37.383054674863814\n",
      "Epoch 26 \t Batch 1650 \t Training Loss: 37.40684713883834\n",
      "Epoch 26 \t Batch 1700 \t Training Loss: 37.4184404406828\n",
      "Epoch 26 \t Batch 1750 \t Training Loss: 37.47623203059605\n",
      "Epoch 26 \t Batch 1800 \t Training Loss: 37.430339687135486\n",
      "Epoch 26 \t Batch 1850 \t Training Loss: 37.46897068126781\n",
      "Epoch 26 \t Batch 1900 \t Training Loss: 37.495539116106535\n",
      "Epoch 26 \t Batch 1950 \t Training Loss: 37.48083753732534\n",
      "Epoch 26 \t Batch 2000 \t Training Loss: 37.484823399543764\n",
      "Epoch 26 \t Batch 2050 \t Training Loss: 37.50656140397235\n",
      "Epoch 26 \t Batch 2100 \t Training Loss: 37.4966042700268\n",
      "Epoch 26 \t Batch 2150 \t Training Loss: 37.48340101641278\n",
      "Epoch 26 \t Batch 2200 \t Training Loss: 37.46373069329695\n",
      "Epoch 26 \t Batch 2250 \t Training Loss: 37.44317723507351\n",
      "Epoch 26 \t Batch 2300 \t Training Loss: 37.46890052214913\n",
      "Epoch 26 \t Batch 2350 \t Training Loss: 37.496980482060856\n",
      "Epoch 26 \t Batch 2400 \t Training Loss: 37.52461630185445\n",
      "Epoch 26 \t Batch 2450 \t Training Loss: 37.52650665594607\n",
      "Epoch 26 \t Batch 2500 \t Training Loss: 37.519765341949466\n",
      "Epoch 26 \t Batch 2550 \t Training Loss: 37.543817158867334\n",
      "Epoch 26 \t Batch 2600 \t Training Loss: 37.54045880170969\n",
      "Epoch 26 \t Batch 2650 \t Training Loss: 37.534165981940504\n",
      "Epoch 26 \t Batch 2700 \t Training Loss: 37.535299622571024\n",
      "Epoch 26 \t Batch 2750 \t Training Loss: 37.517855696938256\n",
      "Epoch 26 \t Batch 2800 \t Training Loss: 37.51741556848798\n",
      "Epoch 26 \t Batch 2850 \t Training Loss: 37.519017942328205\n",
      "Epoch 26 \t Batch 2900 \t Training Loss: 37.52290454272566\n",
      "Epoch 26 \t Batch 2950 \t Training Loss: 37.53084614575919\n",
      "Epoch 26 \t Batch 3000 \t Training Loss: 37.54428790283203\n",
      "Epoch 26 \t Batch 3050 \t Training Loss: 37.56940767632156\n",
      "Epoch 26 \t Batch 3100 \t Training Loss: 37.60509117126465\n",
      "Epoch 26 \t Batch 3150 \t Training Loss: 37.60804060496981\n",
      "Epoch 26 \t Batch 3200 \t Training Loss: 37.615414024591445\n",
      "Epoch 26 \t Batch 3250 \t Training Loss: 37.625489351125864\n",
      "Epoch 26 \t Batch 3300 \t Training Loss: 37.62079757228042\n",
      "Epoch 26 \t Batch 3350 \t Training Loss: 37.636903425757566\n",
      "Epoch 26 \t Batch 3400 \t Training Loss: 37.65019104172202\n",
      "Epoch 26 \t Batch 3450 \t Training Loss: 37.661540731347124\n",
      "Epoch 26 \t Batch 3500 \t Training Loss: 37.67760902241298\n",
      "Epoch 26 \t Batch 3550 \t Training Loss: 37.68534858972254\n",
      "Epoch 26 \t Batch 3600 \t Training Loss: 37.69593961291843\n",
      "Epoch 26 \t Batch 3650 \t Training Loss: 37.69514384648571\n",
      "Epoch 26 \t Batch 50 \t Validation Loss: 21.790990133285522\n",
      "Epoch 26 \t Batch 100 \t Validation Loss: 26.861558175086977\n",
      "Epoch 26 \t Batch 150 \t Validation Loss: 24.134932006200156\n",
      "Epoch 26 \t Batch 200 \t Validation Loss: 23.312464473247527\n",
      "Epoch 26 \t Batch 250 \t Validation Loss: 25.402995763778687\n",
      "Epoch 26 \t Batch 300 \t Validation Loss: 24.452739067077637\n",
      "Epoch 26 \t Batch 350 \t Validation Loss: 25.61197605950492\n",
      "Epoch 26 \t Batch 400 \t Validation Loss: 25.53035937309265\n",
      "Epoch 26 \t Batch 450 \t Validation Loss: 26.419376536475287\n",
      "Epoch 26 \t Batch 500 \t Validation Loss: 26.40307765388489\n",
      "Epoch 26 \t Batch 550 \t Validation Loss: 26.284050079692495\n",
      "Epoch 26 \t Batch 600 \t Validation Loss: 26.98684393088023\n",
      "Epoch 26 \t Batch 650 \t Validation Loss: 28.65407983486469\n",
      "Epoch 26 \t Batch 700 \t Validation Loss: 30.58711520263127\n",
      "Epoch 26 \t Batch 750 \t Validation Loss: 31.854251963933308\n",
      "Epoch 26 \t Batch 800 \t Validation Loss: 33.0679357033968\n",
      "Epoch 26 \t Batch 850 \t Validation Loss: 33.88752797014573\n",
      "Epoch 26 \t Batch 900 \t Validation Loss: 34.18884217951033\n",
      "Epoch 26 \t Batch 950 \t Validation Loss: 34.36666082532782\n",
      "Epoch 26 \t Batch 1000 \t Validation Loss: 36.01905943727493\n",
      "Epoch 26 \t Batch 1050 \t Validation Loss: 37.02197109858195\n",
      "Epoch 26 \t Batch 1100 \t Validation Loss: 37.824129366441205\n",
      "Epoch 26 \t Batch 1150 \t Validation Loss: 37.91295592349508\n",
      "Epoch 26 \t Batch 1200 \t Validation Loss: 38.72477705081304\n",
      "Epoch 26 \t Batch 1250 \t Validation Loss: 39.09765355033875\n",
      "Epoch 26 \t Batch 1300 \t Validation Loss: 39.36708012470832\n",
      "Epoch 26 \t Batch 1350 \t Validation Loss: 39.20926734323855\n",
      "Epoch 26 \t Batch 1400 \t Validation Loss: 39.11158943619047\n",
      "Epoch 26 \t Batch 1450 \t Validation Loss: 39.06810234497333\n",
      "Epoch 26 \t Batch 1500 \t Validation Loss: 39.23816004737218\n",
      "Epoch 26 \t Batch 1550 \t Validation Loss: 39.02298449224041\n",
      "Epoch 26 \t Batch 1600 \t Validation Loss: 38.87034372076392\n",
      "Epoch 26 \t Batch 1650 \t Validation Loss: 38.930617362947174\n",
      "Epoch 26 \t Batch 1700 \t Validation Loss: 38.89692405013477\n",
      "Epoch 26 \t Batch 1750 \t Validation Loss: 38.774034823145186\n",
      "Epoch 26 \t Batch 1800 \t Validation Loss: 38.73903344459004\n",
      "Epoch 26 \t Batch 1850 \t Validation Loss: 39.32190514010352\n",
      "Epoch 26 \t Batch 1900 \t Validation Loss: 39.59349469523681\n",
      "Epoch 26 \t Batch 1950 \t Validation Loss: 39.474799134547894\n",
      "Epoch 26 \t Batch 2000 \t Validation Loss: 39.3824710777998\n",
      "Epoch 26 \t Batch 2050 \t Validation Loss: 39.46088489288237\n",
      "Epoch 26 \t Batch 2100 \t Validation Loss: 39.343144637857165\n",
      "Epoch 26 \t Batch 2150 \t Validation Loss: 39.177128023435905\n",
      "Epoch 26 \t Batch 2200 \t Validation Loss: 39.02081419587135\n",
      "Epoch 26 \t Batch 2250 \t Validation Loss: 38.84965971702999\n",
      "Epoch 26 \t Batch 2300 \t Validation Loss: 38.56948068774265\n",
      "Epoch 26 \t Batch 2350 \t Validation Loss: 38.515238661157326\n",
      "Epoch 26 \t Batch 2400 \t Validation Loss: 38.633274909555915\n",
      "Epoch 26 \t Batch 2450 \t Validation Loss: 38.95753591449893\n",
      "Epoch 26 Training Loss: 37.6961376784022 Validation Loss: 39.14908480518437\n",
      "Epoch 26 completed\n",
      "Epoch 27 \t Batch 50 \t Training Loss: 35.72859912872315\n",
      "Epoch 27 \t Batch 100 \t Training Loss: 35.798948745727536\n",
      "Epoch 27 \t Batch 150 \t Training Loss: 35.90301115671794\n",
      "Epoch 27 \t Batch 200 \t Training Loss: 35.58254940032959\n",
      "Epoch 27 \t Batch 250 \t Training Loss: 35.69739869689941\n",
      "Epoch 27 \t Batch 300 \t Training Loss: 35.57203118006388\n",
      "Epoch 27 \t Batch 350 \t Training Loss: 35.60356576102121\n",
      "Epoch 27 \t Batch 400 \t Training Loss: 35.52167920589447\n",
      "Epoch 27 \t Batch 450 \t Training Loss: 35.71615822686089\n",
      "Epoch 27 \t Batch 500 \t Training Loss: 35.68201675796509\n",
      "Epoch 27 \t Batch 550 \t Training Loss: 35.8985528529774\n",
      "Epoch 27 \t Batch 600 \t Training Loss: 35.92783453941345\n",
      "Epoch 27 \t Batch 650 \t Training Loss: 35.9656142777663\n",
      "Epoch 27 \t Batch 700 \t Training Loss: 35.979399212428504\n",
      "Epoch 27 \t Batch 750 \t Training Loss: 36.07966555023194\n",
      "Epoch 27 \t Batch 800 \t Training Loss: 36.11070641040802\n",
      "Epoch 27 \t Batch 850 \t Training Loss: 36.18892107795266\n",
      "Epoch 27 \t Batch 900 \t Training Loss: 36.27155948638916\n",
      "Epoch 27 \t Batch 950 \t Training Loss: 36.35906592118113\n",
      "Epoch 27 \t Batch 1000 \t Training Loss: 36.39007077980042\n",
      "Epoch 27 \t Batch 1050 \t Training Loss: 36.40117766244071\n",
      "Epoch 27 \t Batch 1100 \t Training Loss: 36.38534583872015\n",
      "Epoch 27 \t Batch 1150 \t Training Loss: 36.41470830005148\n",
      "Epoch 27 \t Batch 1200 \t Training Loss: 36.4487243827184\n",
      "Epoch 27 \t Batch 1250 \t Training Loss: 36.469870851135255\n",
      "Epoch 27 \t Batch 1300 \t Training Loss: 36.53229066848755\n",
      "Epoch 27 \t Batch 1350 \t Training Loss: 36.54085269362838\n",
      "Epoch 27 \t Batch 1400 \t Training Loss: 36.548966650281635\n",
      "Epoch 27 \t Batch 1450 \t Training Loss: 36.52799052534432\n",
      "Epoch 27 \t Batch 1500 \t Training Loss: 36.56977394358317\n",
      "Epoch 27 \t Batch 1550 \t Training Loss: 36.53426358499835\n",
      "Epoch 27 \t Batch 1600 \t Training Loss: 36.54081764101982\n",
      "Epoch 27 \t Batch 1650 \t Training Loss: 36.53253089442398\n",
      "Epoch 27 \t Batch 1700 \t Training Loss: 36.580439588883344\n",
      "Epoch 27 \t Batch 1750 \t Training Loss: 36.54448148018973\n",
      "Epoch 27 \t Batch 1800 \t Training Loss: 36.585976159837514\n",
      "Epoch 27 \t Batch 1850 \t Training Loss: 36.63145592766839\n",
      "Epoch 27 \t Batch 1900 \t Training Loss: 36.65017020576879\n",
      "Epoch 27 \t Batch 1950 \t Training Loss: 36.63616407541128\n",
      "Epoch 27 \t Batch 2000 \t Training Loss: 36.63094944477081\n",
      "Epoch 27 \t Batch 2050 \t Training Loss: 36.61827022459449\n",
      "Epoch 27 \t Batch 2100 \t Training Loss: 36.61103177388509\n",
      "Epoch 27 \t Batch 2150 \t Training Loss: 36.63552596336187\n",
      "Epoch 27 \t Batch 2200 \t Training Loss: 36.651909349614925\n",
      "Epoch 27 \t Batch 2250 \t Training Loss: 36.6591385506524\n",
      "Epoch 27 \t Batch 2300 \t Training Loss: 36.66820737838745\n",
      "Epoch 27 \t Batch 2350 \t Training Loss: 36.67394296849027\n",
      "Epoch 27 \t Batch 2400 \t Training Loss: 36.67777241706848\n",
      "Epoch 27 \t Batch 2450 \t Training Loss: 36.68573736852529\n",
      "Epoch 27 \t Batch 2500 \t Training Loss: 36.7045280418396\n",
      "Epoch 27 \t Batch 2550 \t Training Loss: 36.71049748514213\n",
      "Epoch 27 \t Batch 2600 \t Training Loss: 36.71711258668166\n",
      "Epoch 27 \t Batch 2650 \t Training Loss: 36.73655401697699\n",
      "Epoch 27 \t Batch 2700 \t Training Loss: 36.729635378519696\n",
      "Epoch 27 \t Batch 2750 \t Training Loss: 36.73021507887407\n",
      "Epoch 27 \t Batch 2800 \t Training Loss: 36.754552683149065\n",
      "Epoch 27 \t Batch 2850 \t Training Loss: 36.77140499315764\n",
      "Epoch 27 \t Batch 2900 \t Training Loss: 36.806961302263986\n",
      "Epoch 27 \t Batch 2950 \t Training Loss: 36.80799056424933\n",
      "Epoch 27 \t Batch 3000 \t Training Loss: 36.80984242248535\n",
      "Epoch 27 \t Batch 3050 \t Training Loss: 36.809331158497294\n",
      "Epoch 27 \t Batch 3100 \t Training Loss: 36.850139708980436\n",
      "Epoch 27 \t Batch 3150 \t Training Loss: 36.858506002880276\n",
      "Epoch 27 \t Batch 3200 \t Training Loss: 36.87997105896473\n",
      "Epoch 27 \t Batch 3250 \t Training Loss: 36.88882157721886\n",
      "Epoch 27 \t Batch 3300 \t Training Loss: 36.89829925941699\n",
      "Epoch 27 \t Batch 3350 \t Training Loss: 36.887038803669945\n",
      "Epoch 27 \t Batch 3400 \t Training Loss: 36.88960300838246\n",
      "Epoch 27 \t Batch 3450 \t Training Loss: 36.91031113223753\n",
      "Epoch 27 \t Batch 3500 \t Training Loss: 36.91740385872977\n",
      "Epoch 27 \t Batch 3550 \t Training Loss: 36.906192301360655\n",
      "Epoch 27 \t Batch 3600 \t Training Loss: 36.906044693522986\n",
      "Epoch 27 \t Batch 3650 \t Training Loss: 36.92205225539534\n",
      "Epoch 27 \t Batch 50 \t Validation Loss: 18.83558044433594\n",
      "Epoch 27 \t Batch 100 \t Validation Loss: 23.551143369674683\n",
      "Epoch 27 \t Batch 150 \t Validation Loss: 21.41647334098816\n",
      "Epoch 27 \t Batch 200 \t Validation Loss: 20.947914626598358\n",
      "Epoch 27 \t Batch 250 \t Validation Loss: 22.4318560962677\n",
      "Epoch 27 \t Batch 300 \t Validation Loss: 21.74941534678141\n",
      "Epoch 27 \t Batch 350 \t Validation Loss: 23.52970376423427\n",
      "Epoch 27 \t Batch 400 \t Validation Loss: 23.863258378505705\n",
      "Epoch 27 \t Batch 450 \t Validation Loss: 24.94435864130656\n",
      "Epoch 27 \t Batch 500 \t Validation Loss: 25.067183629989625\n",
      "Epoch 27 \t Batch 550 \t Validation Loss: 25.09979202964089\n",
      "Epoch 27 \t Batch 600 \t Validation Loss: 25.686125992139182\n",
      "Epoch 27 \t Batch 650 \t Validation Loss: 27.137928708883432\n",
      "Epoch 27 \t Batch 700 \t Validation Loss: 28.62330487251282\n",
      "Epoch 27 \t Batch 750 \t Validation Loss: 29.670874624888103\n",
      "Epoch 27 \t Batch 800 \t Validation Loss: 30.578938500881193\n",
      "Epoch 27 \t Batch 850 \t Validation Loss: 31.349095922357897\n",
      "Epoch 27 \t Batch 900 \t Validation Loss: 31.60080457104577\n",
      "Epoch 27 \t Batch 950 \t Validation Loss: 31.68705772851643\n",
      "Epoch 27 \t Batch 1000 \t Validation Loss: 33.150302486896514\n",
      "Epoch 27 \t Batch 1050 \t Validation Loss: 34.109072377341136\n",
      "Epoch 27 \t Batch 1100 \t Validation Loss: 34.80649424813011\n",
      "Epoch 27 \t Batch 1150 \t Validation Loss: 34.8436087119061\n",
      "Epoch 27 \t Batch 1200 \t Validation Loss: 35.42977323532104\n",
      "Epoch 27 \t Batch 1250 \t Validation Loss: 35.727735250854494\n",
      "Epoch 27 \t Batch 1300 \t Validation Loss: 35.99084173862751\n",
      "Epoch 27 \t Batch 1350 \t Validation Loss: 35.87105678911562\n",
      "Epoch 27 \t Batch 1400 \t Validation Loss: 35.77638317653111\n",
      "Epoch 27 \t Batch 1450 \t Validation Loss: 35.71369227113395\n",
      "Epoch 27 \t Batch 1500 \t Validation Loss: 35.93704415957133\n",
      "Epoch 27 \t Batch 1550 \t Validation Loss: 35.756592638569494\n",
      "Epoch 27 \t Batch 1600 \t Validation Loss: 35.648011002540585\n",
      "Epoch 27 \t Batch 1650 \t Validation Loss: 35.730905678488995\n",
      "Epoch 27 \t Batch 1700 \t Validation Loss: 35.678757481855506\n",
      "Epoch 27 \t Batch 1750 \t Validation Loss: 35.55896671840123\n",
      "Epoch 27 \t Batch 1800 \t Validation Loss: 35.50627581119537\n",
      "Epoch 27 \t Batch 1850 \t Validation Loss: 36.07438549093298\n",
      "Epoch 27 \t Batch 1900 \t Validation Loss: 36.39583852516977\n",
      "Epoch 27 \t Batch 1950 \t Validation Loss: 36.284906525734144\n",
      "Epoch 27 \t Batch 2000 \t Validation Loss: 36.19543030977249\n",
      "Epoch 27 \t Batch 2050 \t Validation Loss: 36.17614367787431\n",
      "Epoch 27 \t Batch 2100 \t Validation Loss: 36.066928929828464\n",
      "Epoch 27 \t Batch 2150 \t Validation Loss: 35.98131329425546\n",
      "Epoch 27 \t Batch 2200 \t Validation Loss: 35.895470633506775\n",
      "Epoch 27 \t Batch 2250 \t Validation Loss: 35.809825926038954\n",
      "Epoch 27 \t Batch 2300 \t Validation Loss: 35.62130727892337\n",
      "Epoch 27 \t Batch 2350 \t Validation Loss: 35.66263291744475\n",
      "Epoch 27 \t Batch 2400 \t Validation Loss: 35.82558522502581\n",
      "Epoch 27 \t Batch 2450 \t Validation Loss: 36.20021495040582\n",
      "Epoch 27 Training Loss: 36.9168739594446 Validation Loss: 36.393757871993174\n",
      "Epoch 27 completed\n",
      "Epoch 28 \t Batch 50 \t Training Loss: 35.133357009887696\n",
      "Epoch 28 \t Batch 100 \t Training Loss: 34.731845951080324\n",
      "Epoch 28 \t Batch 150 \t Training Loss: 35.36291032155355\n",
      "Epoch 28 \t Batch 200 \t Training Loss: 35.24353948593139\n",
      "Epoch 28 \t Batch 250 \t Training Loss: 35.657706893920896\n",
      "Epoch 28 \t Batch 300 \t Training Loss: 35.58700490951538\n",
      "Epoch 28 \t Batch 350 \t Training Loss: 35.70025599343436\n",
      "Epoch 28 \t Batch 400 \t Training Loss: 35.640874104499815\n",
      "Epoch 28 \t Batch 450 \t Training Loss: 35.5295405154758\n",
      "Epoch 28 \t Batch 500 \t Training Loss: 35.536052700042724\n",
      "Epoch 28 \t Batch 550 \t Training Loss: 35.4062116726962\n",
      "Epoch 28 \t Batch 600 \t Training Loss: 35.44165070851644\n",
      "Epoch 28 \t Batch 650 \t Training Loss: 35.48193119635949\n",
      "Epoch 28 \t Batch 700 \t Training Loss: 35.46058536529541\n",
      "Epoch 28 \t Batch 750 \t Training Loss: 35.50368060557047\n",
      "Epoch 28 \t Batch 800 \t Training Loss: 35.47761277675629\n",
      "Epoch 28 \t Batch 850 \t Training Loss: 35.51986811693977\n",
      "Epoch 28 \t Batch 900 \t Training Loss: 35.66200929005941\n",
      "Epoch 28 \t Batch 950 \t Training Loss: 35.644951717979026\n",
      "Epoch 28 \t Batch 1000 \t Training Loss: 35.58645120429993\n",
      "Epoch 28 \t Batch 1050 \t Training Loss: 35.67520064944313\n",
      "Epoch 28 \t Batch 1100 \t Training Loss: 35.74641749295321\n",
      "Epoch 28 \t Batch 1150 \t Training Loss: 35.710973008197286\n",
      "Epoch 28 \t Batch 1200 \t Training Loss: 35.671945104599\n",
      "Epoch 28 \t Batch 1250 \t Training Loss: 35.75226142120361\n",
      "Epoch 28 \t Batch 1300 \t Training Loss: 35.7480974476154\n",
      "Epoch 28 \t Batch 1350 \t Training Loss: 35.76288989031757\n",
      "Epoch 28 \t Batch 1400 \t Training Loss: 35.74177518572126\n",
      "Epoch 28 \t Batch 1450 \t Training Loss: 35.753558679778\n",
      "Epoch 28 \t Batch 1500 \t Training Loss: 35.754203379313154\n",
      "Epoch 28 \t Batch 1550 \t Training Loss: 35.75391344008907\n",
      "Epoch 28 \t Batch 1600 \t Training Loss: 35.801868578195574\n",
      "Epoch 28 \t Batch 1650 \t Training Loss: 35.772626660664876\n",
      "Epoch 28 \t Batch 1700 \t Training Loss: 35.83765127967386\n",
      "Epoch 28 \t Batch 1750 \t Training Loss: 35.86088270677839\n",
      "Epoch 28 \t Batch 1800 \t Training Loss: 35.86424347983466\n",
      "Epoch 28 \t Batch 1850 \t Training Loss: 35.88303489169559\n",
      "Epoch 28 \t Batch 1900 \t Training Loss: 35.89483914023951\n",
      "Epoch 28 \t Batch 1950 \t Training Loss: 35.9016240525857\n",
      "Epoch 28 \t Batch 2000 \t Training Loss: 35.89175837135315\n",
      "Epoch 28 \t Batch 2050 \t Training Loss: 35.90178211863448\n",
      "Epoch 28 \t Batch 2100 \t Training Loss: 35.92404449372064\n",
      "Epoch 28 \t Batch 2150 \t Training Loss: 35.94625119941179\n",
      "Epoch 28 \t Batch 2200 \t Training Loss: 35.96711647033691\n",
      "Epoch 28 \t Batch 2250 \t Training Loss: 35.94738177320692\n",
      "Epoch 28 \t Batch 2300 \t Training Loss: 35.93986755205238\n",
      "Epoch 28 \t Batch 2350 \t Training Loss: 35.952071494244514\n",
      "Epoch 28 \t Batch 2400 \t Training Loss: 35.96085730711619\n",
      "Epoch 28 \t Batch 2450 \t Training Loss: 35.958669572946974\n",
      "Epoch 28 \t Batch 2500 \t Training Loss: 35.942198349761966\n",
      "Epoch 28 \t Batch 2550 \t Training Loss: 35.951209078022075\n",
      "Epoch 28 \t Batch 2600 \t Training Loss: 35.94452271828285\n",
      "Epoch 28 \t Batch 2650 \t Training Loss: 35.96979481319212\n",
      "Epoch 28 \t Batch 2700 \t Training Loss: 35.97393871024803\n",
      "Epoch 28 \t Batch 2750 \t Training Loss: 36.02988566936146\n",
      "Epoch 28 \t Batch 2800 \t Training Loss: 36.0297990989685\n",
      "Epoch 28 \t Batch 2850 \t Training Loss: 36.024402449758426\n",
      "Epoch 28 \t Batch 2900 \t Training Loss: 36.06826676204287\n",
      "Epoch 28 \t Batch 2950 \t Training Loss: 36.08165112188307\n",
      "Epoch 28 \t Batch 3000 \t Training Loss: 36.10962303543091\n",
      "Epoch 28 \t Batch 3050 \t Training Loss: 36.12141713251833\n",
      "Epoch 28 \t Batch 3100 \t Training Loss: 36.11503198623657\n",
      "Epoch 28 \t Batch 3150 \t Training Loss: 36.10245599958632\n",
      "Epoch 28 \t Batch 3200 \t Training Loss: 36.113129066228865\n",
      "Epoch 28 \t Batch 3250 \t Training Loss: 36.13140311431885\n",
      "Epoch 28 \t Batch 3300 \t Training Loss: 36.134501445654664\n",
      "Epoch 28 \t Batch 3350 \t Training Loss: 36.17017190449273\n",
      "Epoch 28 \t Batch 3400 \t Training Loss: 36.16068368014167\n",
      "Epoch 28 \t Batch 3450 \t Training Loss: 36.181704983365705\n",
      "Epoch 28 \t Batch 3500 \t Training Loss: 36.19079308537074\n",
      "Epoch 28 \t Batch 3550 \t Training Loss: 36.21471981263497\n",
      "Epoch 28 \t Batch 3600 \t Training Loss: 36.22120938565996\n",
      "Epoch 28 \t Batch 3650 \t Training Loss: 36.247245850236446\n",
      "Epoch 28 \t Batch 50 \t Validation Loss: 40.438709354400636\n",
      "Epoch 28 \t Batch 100 \t Validation Loss: 52.764490079879764\n",
      "Epoch 28 \t Batch 150 \t Validation Loss: 43.46778758366903\n",
      "Epoch 28 \t Batch 200 \t Validation Loss: 43.01125700473786\n",
      "Epoch 28 \t Batch 250 \t Validation Loss: 47.74919556045532\n",
      "Epoch 28 \t Batch 300 \t Validation Loss: 44.74797011693319\n",
      "Epoch 28 \t Batch 350 \t Validation Loss: 43.022793001447404\n",
      "Epoch 28 \t Batch 400 \t Validation Loss: 40.933803288936616\n",
      "Epoch 28 \t Batch 450 \t Validation Loss: 40.10460397932265\n",
      "Epoch 28 \t Batch 500 \t Validation Loss: 38.70559285926819\n",
      "Epoch 28 \t Batch 550 \t Validation Loss: 37.56985564838756\n",
      "Epoch 28 \t Batch 600 \t Validation Loss: 37.22308798313141\n",
      "Epoch 28 \t Batch 650 \t Validation Loss: 38.01948775218084\n",
      "Epoch 28 \t Batch 700 \t Validation Loss: 39.042999225343976\n",
      "Epoch 28 \t Batch 750 \t Validation Loss: 39.457741283416745\n",
      "Epoch 28 \t Batch 800 \t Validation Loss: 40.003005135059354\n",
      "Epoch 28 \t Batch 850 \t Validation Loss: 40.29869353799259\n",
      "Epoch 28 \t Batch 900 \t Validation Loss: 40.15331557485792\n",
      "Epoch 28 \t Batch 950 \t Validation Loss: 39.87715534712139\n",
      "Epoch 28 \t Batch 1000 \t Validation Loss: 41.04319481563568\n",
      "Epoch 28 \t Batch 1050 \t Validation Loss: 41.73398939813887\n",
      "Epoch 28 \t Batch 1100 \t Validation Loss: 42.18026819619266\n",
      "Epoch 28 \t Batch 1150 \t Validation Loss: 41.96821331811988\n",
      "Epoch 28 \t Batch 1200 \t Validation Loss: 42.411864111820854\n",
      "Epoch 28 \t Batch 1250 \t Validation Loss: 42.48003974685669\n",
      "Epoch 28 \t Batch 1300 \t Validation Loss: 42.55529952782851\n",
      "Epoch 28 \t Batch 1350 \t Validation Loss: 42.24479021354958\n",
      "Epoch 28 \t Batch 1400 \t Validation Loss: 42.004281408446175\n",
      "Epoch 28 \t Batch 1450 \t Validation Loss: 41.9196063554698\n",
      "Epoch 28 \t Batch 1500 \t Validation Loss: 41.978041574637096\n",
      "Epoch 28 \t Batch 1550 \t Validation Loss: 41.63284049264846\n",
      "Epoch 28 \t Batch 1600 \t Validation Loss: 41.4087439610064\n",
      "Epoch 28 \t Batch 1650 \t Validation Loss: 41.379536105213745\n",
      "Epoch 28 \t Batch 1700 \t Validation Loss: 41.19959803426967\n",
      "Epoch 28 \t Batch 1750 \t Validation Loss: 41.00513102899279\n",
      "Epoch 28 \t Batch 1800 \t Validation Loss: 41.02048564871152\n",
      "Epoch 28 \t Batch 1850 \t Validation Loss: 41.47106272684561\n",
      "Epoch 28 \t Batch 1900 \t Validation Loss: 41.690536314939195\n",
      "Epoch 28 \t Batch 1950 \t Validation Loss: 41.448137488854236\n",
      "Epoch 28 \t Batch 2000 \t Validation Loss: 41.317740490078926\n",
      "Epoch 28 \t Batch 2050 \t Validation Loss: 41.464968255322155\n",
      "Epoch 28 \t Batch 2100 \t Validation Loss: 41.37813902321316\n",
      "Epoch 28 \t Batch 2150 \t Validation Loss: 41.210496714946835\n",
      "Epoch 28 \t Batch 2200 \t Validation Loss: 41.03751468192447\n",
      "Epoch 28 \t Batch 2250 \t Validation Loss: 40.899676854981315\n",
      "Epoch 28 \t Batch 2300 \t Validation Loss: 40.65413612106572\n",
      "Epoch 28 \t Batch 2350 \t Validation Loss: 40.63849762358564\n",
      "Epoch 28 \t Batch 2400 \t Validation Loss: 40.727222702403864\n",
      "Epoch 28 \t Batch 2450 \t Validation Loss: 41.050003141967615\n",
      "Epoch 28 Training Loss: 36.27126752277237 Validation Loss: 41.236369569483514\n",
      "Epoch 28 completed\n",
      "Epoch 29 \t Batch 50 \t Training Loss: 33.3928258895874\n",
      "Epoch 29 \t Batch 100 \t Training Loss: 33.74557767868042\n",
      "Epoch 29 \t Batch 150 \t Training Loss: 33.995779558817546\n",
      "Epoch 29 \t Batch 200 \t Training Loss: 34.14017942428589\n",
      "Epoch 29 \t Batch 250 \t Training Loss: 34.18933795928955\n",
      "Epoch 29 \t Batch 300 \t Training Loss: 34.38019392649333\n",
      "Epoch 29 \t Batch 350 \t Training Loss: 34.48991715022496\n",
      "Epoch 29 \t Batch 400 \t Training Loss: 34.38461885929107\n",
      "Epoch 29 \t Batch 450 \t Training Loss: 34.392961218092175\n",
      "Epoch 29 \t Batch 500 \t Training Loss: 34.518009391784666\n",
      "Epoch 29 \t Batch 550 \t Training Loss: 34.62715031016957\n",
      "Epoch 29 \t Batch 600 \t Training Loss: 34.694823888142906\n",
      "Epoch 29 \t Batch 650 \t Training Loss: 34.66518851353572\n",
      "Epoch 29 \t Batch 700 \t Training Loss: 34.75687686647688\n",
      "Epoch 29 \t Batch 750 \t Training Loss: 34.7534085261027\n",
      "Epoch 29 \t Batch 800 \t Training Loss: 34.828841865062714\n",
      "Epoch 29 \t Batch 850 \t Training Loss: 34.899088513991416\n",
      "Epoch 29 \t Batch 900 \t Training Loss: 34.98991025924683\n",
      "Epoch 29 \t Batch 950 \t Training Loss: 35.0415082048115\n",
      "Epoch 29 \t Batch 1000 \t Training Loss: 35.045297351837156\n",
      "Epoch 29 \t Batch 1050 \t Training Loss: 35.01690416971842\n",
      "Epoch 29 \t Batch 1100 \t Training Loss: 35.00934195605191\n",
      "Epoch 29 \t Batch 1150 \t Training Loss: 35.02695202537205\n",
      "Epoch 29 \t Batch 1200 \t Training Loss: 35.06802269140879\n",
      "Epoch 29 \t Batch 1250 \t Training Loss: 35.11615461578369\n",
      "Epoch 29 \t Batch 1300 \t Training Loss: 35.217688446044924\n",
      "Epoch 29 \t Batch 1350 \t Training Loss: 35.23053599039714\n",
      "Epoch 29 \t Batch 1400 \t Training Loss: 35.26089080538068\n",
      "Epoch 29 \t Batch 1450 \t Training Loss: 35.208978438541806\n",
      "Epoch 29 \t Batch 1500 \t Training Loss: 35.25533259328206\n",
      "Epoch 29 \t Batch 1550 \t Training Loss: 35.24235970158731\n",
      "Epoch 29 \t Batch 1600 \t Training Loss: 35.234952228069304\n",
      "Epoch 29 \t Batch 1650 \t Training Loss: 35.25631861600009\n",
      "Epoch 29 \t Batch 1700 \t Training Loss: 35.280892077053295\n",
      "Epoch 29 \t Batch 1750 \t Training Loss: 35.33539604513986\n",
      "Epoch 29 \t Batch 1800 \t Training Loss: 35.332315912246706\n",
      "Epoch 29 \t Batch 1850 \t Training Loss: 35.33353064150424\n",
      "Epoch 29 \t Batch 1900 \t Training Loss: 35.359517983888324\n",
      "Epoch 29 \t Batch 1950 \t Training Loss: 35.35649660257193\n",
      "Epoch 29 \t Batch 2000 \t Training Loss: 35.362472764015195\n",
      "Epoch 29 \t Batch 2050 \t Training Loss: 35.3765814646279\n",
      "Epoch 29 \t Batch 2100 \t Training Loss: 35.41440492357526\n",
      "Epoch 29 \t Batch 2150 \t Training Loss: 35.42634250286014\n",
      "Epoch 29 \t Batch 2200 \t Training Loss: 35.401782403425734\n",
      "Epoch 29 \t Batch 2250 \t Training Loss: 35.39487834676107\n",
      "Epoch 29 \t Batch 2300 \t Training Loss: 35.413916628464406\n",
      "Epoch 29 \t Batch 2350 \t Training Loss: 35.428749194855385\n",
      "Epoch 29 \t Batch 2400 \t Training Loss: 35.43348433653514\n",
      "Epoch 29 \t Batch 2450 \t Training Loss: 35.44238144777259\n",
      "Epoch 29 \t Batch 2500 \t Training Loss: 35.456527781677245\n",
      "Epoch 29 \t Batch 2550 \t Training Loss: 35.45150940091002\n",
      "Epoch 29 \t Batch 2600 \t Training Loss: 35.46239658942589\n",
      "Epoch 29 \t Batch 2650 \t Training Loss: 35.482248173119885\n",
      "Epoch 29 \t Batch 2700 \t Training Loss: 35.47232828776042\n",
      "Epoch 29 \t Batch 2750 \t Training Loss: 35.47127907631614\n",
      "Epoch 29 \t Batch 2800 \t Training Loss: 35.477332309314185\n",
      "Epoch 29 \t Batch 2850 \t Training Loss: 35.46956627628259\n",
      "Epoch 29 \t Batch 2900 \t Training Loss: 35.46842522259416\n",
      "Epoch 29 \t Batch 2950 \t Training Loss: 35.48362449969275\n",
      "Epoch 29 \t Batch 3000 \t Training Loss: 35.480167176564535\n",
      "Epoch 29 \t Batch 3050 \t Training Loss: 35.489108034978145\n",
      "Epoch 29 \t Batch 3100 \t Training Loss: 35.509456993841354\n",
      "Epoch 29 \t Batch 3150 \t Training Loss: 35.53036093454512\n",
      "Epoch 29 \t Batch 3200 \t Training Loss: 35.527216222882274\n",
      "Epoch 29 \t Batch 3250 \t Training Loss: 35.51582187300462\n",
      "Epoch 29 \t Batch 3300 \t Training Loss: 35.54169806278113\n",
      "Epoch 29 \t Batch 3350 \t Training Loss: 35.520966421098855\n",
      "Epoch 29 \t Batch 3400 \t Training Loss: 35.51852628876181\n",
      "Epoch 29 \t Batch 3450 \t Training Loss: 35.51618480682373\n",
      "Epoch 29 \t Batch 3500 \t Training Loss: 35.50436536952427\n",
      "Epoch 29 \t Batch 3550 \t Training Loss: 35.49797587703651\n",
      "Epoch 29 \t Batch 3600 \t Training Loss: 35.504003046883476\n",
      "Epoch 29 \t Batch 3650 \t Training Loss: 35.508898025408186\n",
      "Epoch 29 \t Batch 50 \t Validation Loss: 22.91456413269043\n",
      "Epoch 29 \t Batch 100 \t Validation Loss: 27.241635208129882\n",
      "Epoch 29 \t Batch 150 \t Validation Loss: 25.12279354095459\n",
      "Epoch 29 \t Batch 200 \t Validation Loss: 25.014987630844118\n",
      "Epoch 29 \t Batch 250 \t Validation Loss: 26.888112670898437\n",
      "Epoch 29 \t Batch 300 \t Validation Loss: 26.180158308347067\n",
      "Epoch 29 \t Batch 350 \t Validation Loss: 27.16421268054417\n",
      "Epoch 29 \t Batch 400 \t Validation Loss: 26.940435007810592\n",
      "Epoch 29 \t Batch 450 \t Validation Loss: 27.694436852137247\n",
      "Epoch 29 \t Batch 500 \t Validation Loss: 27.486805747032165\n",
      "Epoch 29 \t Batch 550 \t Validation Loss: 27.301263896768745\n",
      "Epoch 29 \t Batch 600 \t Validation Loss: 27.783654367923738\n",
      "Epoch 29 \t Batch 650 \t Validation Loss: 29.245070025370673\n",
      "Epoch 29 \t Batch 700 \t Validation Loss: 30.846585438592093\n",
      "Epoch 29 \t Batch 750 \t Validation Loss: 31.852230447133383\n",
      "Epoch 29 \t Batch 800 \t Validation Loss: 32.81505398988724\n",
      "Epoch 29 \t Batch 850 \t Validation Loss: 33.502840314752916\n",
      "Epoch 29 \t Batch 900 \t Validation Loss: 33.64253357569377\n",
      "Epoch 29 \t Batch 950 \t Validation Loss: 33.67514576460186\n",
      "Epoch 29 \t Batch 1000 \t Validation Loss: 35.109437735557556\n",
      "Epoch 29 \t Batch 1050 \t Validation Loss: 36.016790661584764\n",
      "Epoch 29 \t Batch 1100 \t Validation Loss: 36.66980133403431\n",
      "Epoch 29 \t Batch 1150 \t Validation Loss: 36.72900831305462\n",
      "Epoch 29 \t Batch 1200 \t Validation Loss: 37.41699928283691\n",
      "Epoch 29 \t Batch 1250 \t Validation Loss: 37.665242461776735\n",
      "Epoch 29 \t Batch 1300 \t Validation Loss: 37.89777444289281\n",
      "Epoch 29 \t Batch 1350 \t Validation Loss: 37.74720991381893\n",
      "Epoch 29 \t Batch 1400 \t Validation Loss: 37.64125769989831\n",
      "Epoch 29 \t Batch 1450 \t Validation Loss: 37.618870258002445\n",
      "Epoch 29 \t Batch 1500 \t Validation Loss: 37.7845882914861\n",
      "Epoch 29 \t Batch 1550 \t Validation Loss: 37.59557046367276\n",
      "Epoch 29 \t Batch 1600 \t Validation Loss: 37.494821525514126\n",
      "Epoch 29 \t Batch 1650 \t Validation Loss: 37.57918069463788\n",
      "Epoch 29 \t Batch 1700 \t Validation Loss: 37.52403930467718\n",
      "Epoch 29 \t Batch 1750 \t Validation Loss: 37.41140409878322\n",
      "Epoch 29 \t Batch 1800 \t Validation Loss: 37.4039058025678\n",
      "Epoch 29 \t Batch 1850 \t Validation Loss: 37.96733245720734\n",
      "Epoch 29 \t Batch 1900 \t Validation Loss: 38.24357848393289\n",
      "Epoch 29 \t Batch 1950 \t Validation Loss: 38.11289756359198\n",
      "Epoch 29 \t Batch 2000 \t Validation Loss: 38.01222318291664\n",
      "Epoch 29 \t Batch 2050 \t Validation Loss: 38.096973806939474\n",
      "Epoch 29 \t Batch 2100 \t Validation Loss: 38.05411666166215\n",
      "Epoch 29 \t Batch 2150 \t Validation Loss: 37.93884615010993\n",
      "Epoch 29 \t Batch 2200 \t Validation Loss: 37.82486183448271\n",
      "Epoch 29 \t Batch 2250 \t Validation Loss: 37.709372686809964\n",
      "Epoch 29 \t Batch 2300 \t Validation Loss: 37.48995287480562\n",
      "Epoch 29 \t Batch 2350 \t Validation Loss: 37.48212621120696\n",
      "Epoch 29 \t Batch 2400 \t Validation Loss: 37.63080065449079\n",
      "Epoch 29 \t Batch 2450 \t Validation Loss: 37.998778314785085\n",
      "Epoch 29 Training Loss: 35.521999831808046 Validation Loss: 38.212783733358634\n",
      "Epoch 29 completed\n",
      "Epoch 30 \t Batch 50 \t Training Loss: 33.54045341491699\n",
      "Epoch 30 \t Batch 100 \t Training Loss: 33.69325326919556\n",
      "Epoch 30 \t Batch 150 \t Training Loss: 33.54693756103516\n",
      "Epoch 30 \t Batch 200 \t Training Loss: 33.57174140930176\n",
      "Epoch 30 \t Batch 250 \t Training Loss: 33.57393728637695\n",
      "Epoch 30 \t Batch 300 \t Training Loss: 33.56217749913534\n",
      "Epoch 30 \t Batch 350 \t Training Loss: 33.734092570713585\n",
      "Epoch 30 \t Batch 400 \t Training Loss: 33.744034571647646\n",
      "Epoch 30 \t Batch 450 \t Training Loss: 33.66836398230659\n",
      "Epoch 30 \t Batch 500 \t Training Loss: 33.6590941734314\n",
      "Epoch 30 \t Batch 550 \t Training Loss: 33.66702310735529\n",
      "Epoch 30 \t Batch 600 \t Training Loss: 33.749108683268226\n",
      "Epoch 30 \t Batch 650 \t Training Loss: 33.76256377293513\n",
      "Epoch 30 \t Batch 700 \t Training Loss: 33.894570955548964\n",
      "Epoch 30 \t Batch 750 \t Training Loss: 33.95127828725179\n",
      "Epoch 30 \t Batch 800 \t Training Loss: 34.016686148643494\n",
      "Epoch 30 \t Batch 850 \t Training Loss: 34.01938088809742\n",
      "Epoch 30 \t Batch 900 \t Training Loss: 33.944968422783745\n",
      "Epoch 30 \t Batch 950 \t Training Loss: 34.039001599362024\n",
      "Epoch 30 \t Batch 1000 \t Training Loss: 34.04960694885254\n",
      "Epoch 30 \t Batch 1050 \t Training Loss: 34.033518947419665\n",
      "Epoch 30 \t Batch 1100 \t Training Loss: 34.03688905715942\n",
      "Epoch 30 \t Batch 1150 \t Training Loss: 34.00673310155454\n",
      "Epoch 30 \t Batch 1200 \t Training Loss: 34.03618048350016\n",
      "Epoch 30 \t Batch 1250 \t Training Loss: 34.03173008270264\n",
      "Epoch 30 \t Batch 1300 \t Training Loss: 34.009860778221714\n",
      "Epoch 30 \t Batch 1350 \t Training Loss: 34.01643283561424\n",
      "Epoch 30 \t Batch 1400 \t Training Loss: 34.05398272923061\n",
      "Epoch 30 \t Batch 1450 \t Training Loss: 34.03003690916916\n",
      "Epoch 30 \t Batch 1500 \t Training Loss: 34.044306640625\n",
      "Epoch 30 \t Batch 1550 \t Training Loss: 34.07787704836937\n",
      "Epoch 30 \t Batch 1600 \t Training Loss: 34.06283417105675\n",
      "Epoch 30 \t Batch 1650 \t Training Loss: 34.0857802882339\n",
      "Epoch 30 \t Batch 1700 \t Training Loss: 34.096955778458536\n",
      "Epoch 30 \t Batch 1750 \t Training Loss: 34.113923706054685\n",
      "Epoch 30 \t Batch 1800 \t Training Loss: 34.12097894986471\n",
      "Epoch 30 \t Batch 1850 \t Training Loss: 34.1334666566591\n",
      "Epoch 30 \t Batch 1900 \t Training Loss: 34.15375095869366\n",
      "Epoch 30 \t Batch 1950 \t Training Loss: 34.2038653593797\n",
      "Epoch 30 \t Batch 2000 \t Training Loss: 34.23519556808472\n",
      "Epoch 30 \t Batch 2050 \t Training Loss: 34.251060640288564\n",
      "Epoch 30 \t Batch 2100 \t Training Loss: 34.297673185439336\n",
      "Epoch 30 \t Batch 2150 \t Training Loss: 34.328776447384854\n",
      "Epoch 30 \t Batch 2200 \t Training Loss: 34.317580932270396\n",
      "Epoch 30 \t Batch 2250 \t Training Loss: 34.36623929765489\n",
      "Epoch 30 \t Batch 2300 \t Training Loss: 34.400233853381614\n",
      "Epoch 30 \t Batch 2350 \t Training Loss: 34.39200627590748\n",
      "Epoch 30 \t Batch 2400 \t Training Loss: 34.405733176867166\n",
      "Epoch 30 \t Batch 2450 \t Training Loss: 34.428191476160166\n",
      "Epoch 30 \t Batch 2500 \t Training Loss: 34.459298726654055\n",
      "Epoch 30 \t Batch 2550 \t Training Loss: 34.50539996128456\n",
      "Epoch 30 \t Batch 2600 \t Training Loss: 34.49515432577867\n",
      "Epoch 30 \t Batch 2650 \t Training Loss: 34.51955505155168\n",
      "Epoch 30 \t Batch 2700 \t Training Loss: 34.5448247718811\n",
      "Epoch 30 \t Batch 2750 \t Training Loss: 34.548292404868384\n",
      "Epoch 30 \t Batch 2800 \t Training Loss: 34.56395390646798\n",
      "Epoch 30 \t Batch 2850 \t Training Loss: 34.599899468003656\n",
      "Epoch 30 \t Batch 2900 \t Training Loss: 34.62084597883553\n",
      "Epoch 30 \t Batch 2950 \t Training Loss: 34.619726724786275\n",
      "Epoch 30 \t Batch 3000 \t Training Loss: 34.63644392712911\n",
      "Epoch 30 \t Batch 3050 \t Training Loss: 34.644118463484965\n",
      "Epoch 30 \t Batch 3100 \t Training Loss: 34.656426139339324\n",
      "Epoch 30 \t Batch 3150 \t Training Loss: 34.673604336693174\n",
      "Epoch 30 \t Batch 3200 \t Training Loss: 34.694668874144554\n",
      "Epoch 30 \t Batch 3250 \t Training Loss: 34.71012561915471\n",
      "Epoch 30 \t Batch 3300 \t Training Loss: 34.72112277637829\n",
      "Epoch 30 \t Batch 3350 \t Training Loss: 34.75002975976289\n",
      "Epoch 30 \t Batch 3400 \t Training Loss: 34.76462147712707\n",
      "Epoch 30 \t Batch 3450 \t Training Loss: 34.791394973423174\n",
      "Epoch 30 \t Batch 3500 \t Training Loss: 34.79579391479492\n",
      "Epoch 30 \t Batch 3550 \t Training Loss: 34.79237640595772\n",
      "Epoch 30 \t Batch 3600 \t Training Loss: 34.82178462293413\n",
      "Epoch 30 \t Batch 3650 \t Training Loss: 34.850284654800205\n",
      "Epoch 30 \t Batch 50 \t Validation Loss: 23.453410663604735\n",
      "Epoch 30 \t Batch 100 \t Validation Loss: 27.1530016708374\n",
      "Epoch 30 \t Batch 150 \t Validation Loss: 25.739570503234862\n",
      "Epoch 30 \t Batch 200 \t Validation Loss: 25.197200264930725\n",
      "Epoch 30 \t Batch 250 \t Validation Loss: 27.14438955307007\n",
      "Epoch 30 \t Batch 300 \t Validation Loss: 26.633527714411418\n",
      "Epoch 30 \t Batch 350 \t Validation Loss: 27.92303372519357\n",
      "Epoch 30 \t Batch 400 \t Validation Loss: 27.761378486156463\n",
      "Epoch 30 \t Batch 450 \t Validation Loss: 28.43694096883138\n",
      "Epoch 30 \t Batch 500 \t Validation Loss: 28.235170993804932\n",
      "Epoch 30 \t Batch 550 \t Validation Loss: 28.063174813010477\n",
      "Epoch 30 \t Batch 600 \t Validation Loss: 28.325612767537436\n",
      "Epoch 30 \t Batch 650 \t Validation Loss: 29.391646863497222\n",
      "Epoch 30 \t Batch 700 \t Validation Loss: 30.518856506347657\n",
      "Epoch 30 \t Batch 750 \t Validation Loss: 31.162532051086426\n",
      "Epoch 30 \t Batch 800 \t Validation Loss: 31.78366204738617\n",
      "Epoch 30 \t Batch 850 \t Validation Loss: 32.29606815674726\n",
      "Epoch 30 \t Batch 900 \t Validation Loss: 32.323747107187906\n",
      "Epoch 30 \t Batch 950 \t Validation Loss: 32.26600451519615\n",
      "Epoch 30 \t Batch 1000 \t Validation Loss: 33.37269177150726\n",
      "Epoch 30 \t Batch 1050 \t Validation Loss: 34.180906750361125\n",
      "Epoch 30 \t Batch 1100 \t Validation Loss: 34.62425101843747\n",
      "Epoch 30 \t Batch 1150 \t Validation Loss: 34.60599418805993\n",
      "Epoch 30 \t Batch 1200 \t Validation Loss: 35.12814372380574\n",
      "Epoch 30 \t Batch 1250 \t Validation Loss: 35.3333367980957\n",
      "Epoch 30 \t Batch 1300 \t Validation Loss: 35.55180905342102\n",
      "Epoch 30 \t Batch 1350 \t Validation Loss: 35.41335367485329\n",
      "Epoch 30 \t Batch 1400 \t Validation Loss: 35.28767925671169\n",
      "Epoch 30 \t Batch 1450 \t Validation Loss: 35.274296025901\n",
      "Epoch 30 \t Batch 1500 \t Validation Loss: 35.42369955682754\n",
      "Epoch 30 \t Batch 1550 \t Validation Loss: 35.27286285261954\n",
      "Epoch 30 \t Batch 1600 \t Validation Loss: 35.2336596660316\n",
      "Epoch 30 \t Batch 1650 \t Validation Loss: 35.354586720755606\n",
      "Epoch 30 \t Batch 1700 \t Validation Loss: 35.31325966652702\n",
      "Epoch 30 \t Batch 1750 \t Validation Loss: 35.23977601773398\n",
      "Epoch 30 \t Batch 1800 \t Validation Loss: 35.26902670211262\n",
      "Epoch 30 \t Batch 1850 \t Validation Loss: 35.81803520086649\n",
      "Epoch 30 \t Batch 1900 \t Validation Loss: 36.17180330565101\n",
      "Epoch 30 \t Batch 1950 \t Validation Loss: 36.03504459662315\n",
      "Epoch 30 \t Batch 2000 \t Validation Loss: 35.94828101861477\n",
      "Epoch 30 \t Batch 2050 \t Validation Loss: 36.03426430109071\n",
      "Epoch 30 \t Batch 2100 \t Validation Loss: 36.00419835351762\n",
      "Epoch 30 \t Batch 2150 \t Validation Loss: 35.916413409543594\n",
      "Epoch 30 \t Batch 2200 \t Validation Loss: 35.808897214694454\n",
      "Epoch 30 \t Batch 2250 \t Validation Loss: 35.736864906628924\n",
      "Epoch 30 \t Batch 2300 \t Validation Loss: 35.587595820737924\n",
      "Epoch 30 \t Batch 2350 \t Validation Loss: 35.636721853398264\n",
      "Epoch 30 \t Batch 2400 \t Validation Loss: 35.80671543250481\n",
      "Epoch 30 \t Batch 2450 \t Validation Loss: 36.20851653867838\n",
      "Epoch 30 Training Loss: 34.84967093015315 Validation Loss: 36.39546831058605\n",
      "Epoch 30 completed\n",
      "Epoch 31 \t Batch 50 \t Training Loss: 33.22792800903321\n",
      "Epoch 31 \t Batch 100 \t Training Loss: 32.28144323348999\n",
      "Epoch 31 \t Batch 150 \t Training Loss: 33.05084447224935\n",
      "Epoch 31 \t Batch 200 \t Training Loss: 33.159116172790526\n",
      "Epoch 31 \t Batch 250 \t Training Loss: 33.325664207458495\n",
      "Epoch 31 \t Batch 300 \t Training Loss: 33.27645814259847\n",
      "Epoch 31 \t Batch 350 \t Training Loss: 33.27929609571184\n",
      "Epoch 31 \t Batch 400 \t Training Loss: 33.14233957767487\n",
      "Epoch 31 \t Batch 450 \t Training Loss: 33.12587747361925\n",
      "Epoch 31 \t Batch 500 \t Training Loss: 32.97909841537476\n",
      "Epoch 31 \t Batch 550 \t Training Loss: 33.007153264825995\n",
      "Epoch 31 \t Batch 600 \t Training Loss: 33.06076420466105\n",
      "Epoch 31 \t Batch 650 \t Training Loss: 33.066816329956055\n",
      "Epoch 31 \t Batch 700 \t Training Loss: 33.074037481035504\n",
      "Epoch 31 \t Batch 750 \t Training Loss: 33.063400723775224\n",
      "Epoch 31 \t Batch 800 \t Training Loss: 33.096593055725094\n",
      "Epoch 31 \t Batch 850 \t Training Loss: 33.05796993031221\n",
      "Epoch 31 \t Batch 900 \t Training Loss: 33.11052220026652\n",
      "Epoch 31 \t Batch 950 \t Training Loss: 33.13455869574296\n",
      "Epoch 31 \t Batch 1000 \t Training Loss: 33.20742583656311\n",
      "Epoch 31 \t Batch 1050 \t Training Loss: 33.23950002761114\n",
      "Epoch 31 \t Batch 1100 \t Training Loss: 33.25244174610485\n",
      "Epoch 31 \t Batch 1150 \t Training Loss: 33.332238286889115\n",
      "Epoch 31 \t Batch 1200 \t Training Loss: 33.331730381647745\n",
      "Epoch 31 \t Batch 1250 \t Training Loss: 33.40474972991943\n",
      "Epoch 31 \t Batch 1300 \t Training Loss: 33.40036677433894\n",
      "Epoch 31 \t Batch 1350 \t Training Loss: 33.364034823664916\n",
      "Epoch 31 \t Batch 1400 \t Training Loss: 33.3859563936506\n",
      "Epoch 31 \t Batch 1450 \t Training Loss: 33.353644994538406\n",
      "Epoch 31 \t Batch 1500 \t Training Loss: 33.34508193206787\n",
      "Epoch 31 \t Batch 1550 \t Training Loss: 33.346896249094314\n",
      "Epoch 31 \t Batch 1600 \t Training Loss: 33.375883070230486\n",
      "Epoch 31 \t Batch 1650 \t Training Loss: 33.389500963615646\n",
      "Epoch 31 \t Batch 1700 \t Training Loss: 33.451537418365476\n",
      "Epoch 31 \t Batch 1750 \t Training Loss: 33.463635930742534\n",
      "Epoch 31 \t Batch 1800 \t Training Loss: 33.47612029393514\n",
      "Epoch 31 \t Batch 1850 \t Training Loss: 33.49162581366462\n",
      "Epoch 31 \t Batch 1900 \t Training Loss: 33.52909194745516\n",
      "Epoch 31 \t Batch 1950 \t Training Loss: 33.56485333858392\n",
      "Epoch 31 \t Batch 2000 \t Training Loss: 33.55981992149353\n",
      "Epoch 31 \t Batch 2050 \t Training Loss: 33.557161480973406\n",
      "Epoch 31 \t Batch 2100 \t Training Loss: 33.52173871903192\n",
      "Epoch 31 \t Batch 2150 \t Training Loss: 33.54571149338123\n",
      "Epoch 31 \t Batch 2200 \t Training Loss: 33.542600923885\n",
      "Epoch 31 \t Batch 2250 \t Training Loss: 33.5632072168986\n",
      "Epoch 31 \t Batch 2300 \t Training Loss: 33.591949996948244\n",
      "Epoch 31 \t Batch 2350 \t Training Loss: 33.58064570812469\n",
      "Epoch 31 \t Batch 2400 \t Training Loss: 33.59376835902532\n",
      "Epoch 31 \t Batch 2450 \t Training Loss: 33.58193207487768\n",
      "Epoch 31 \t Batch 2500 \t Training Loss: 33.60701442489624\n",
      "Epoch 31 \t Batch 2550 \t Training Loss: 33.608740457272994\n",
      "Epoch 31 \t Batch 2600 \t Training Loss: 33.6294496521583\n",
      "Epoch 31 \t Batch 2650 \t Training Loss: 33.66489856719971\n",
      "Epoch 31 \t Batch 2700 \t Training Loss: 33.69953227361043\n",
      "Epoch 31 \t Batch 2750 \t Training Loss: 33.697940374200996\n",
      "Epoch 31 \t Batch 2800 \t Training Loss: 33.71910586970193\n",
      "Epoch 31 \t Batch 2850 \t Training Loss: 33.74327462380393\n",
      "Epoch 31 \t Batch 2900 \t Training Loss: 33.75496975800087\n",
      "Epoch 31 \t Batch 2950 \t Training Loss: 33.78434901027356\n",
      "Epoch 31 \t Batch 3000 \t Training Loss: 33.792938171386716\n",
      "Epoch 31 \t Batch 3050 \t Training Loss: 33.82296627232286\n",
      "Epoch 31 \t Batch 3100 \t Training Loss: 33.83862359816028\n",
      "Epoch 31 \t Batch 3150 \t Training Loss: 33.85906214396159\n",
      "Epoch 31 \t Batch 3200 \t Training Loss: 33.87530513167381\n",
      "Epoch 31 \t Batch 3250 \t Training Loss: 33.87307548875075\n",
      "Epoch 31 \t Batch 3300 \t Training Loss: 33.87985065402407\n",
      "Epoch 31 \t Batch 3350 \t Training Loss: 33.88248853598068\n",
      "Epoch 31 \t Batch 3400 \t Training Loss: 33.89869965833776\n",
      "Epoch 31 \t Batch 3450 \t Training Loss: 33.91546098792035\n",
      "Epoch 31 \t Batch 3500 \t Training Loss: 33.93634236962455\n",
      "Epoch 31 \t Batch 3550 \t Training Loss: 33.952818494984804\n",
      "Epoch 31 \t Batch 3600 \t Training Loss: 33.963010216818915\n",
      "Epoch 31 \t Batch 3650 \t Training Loss: 33.95350326120037\n",
      "Epoch 31 \t Batch 50 \t Validation Loss: 42.48577548980713\n",
      "Epoch 31 \t Batch 100 \t Validation Loss: 51.496900691986085\n",
      "Epoch 31 \t Batch 150 \t Validation Loss: 44.50505025545756\n",
      "Epoch 31 \t Batch 200 \t Validation Loss: 44.07760873556137\n",
      "Epoch 31 \t Batch 250 \t Validation Loss: 48.56461065483093\n",
      "Epoch 31 \t Batch 300 \t Validation Loss: 46.150145848592125\n",
      "Epoch 31 \t Batch 350 \t Validation Loss: 44.76542708260673\n",
      "Epoch 31 \t Batch 400 \t Validation Loss: 42.32932911396026\n",
      "Epoch 31 \t Batch 450 \t Validation Loss: 41.2776994281345\n",
      "Epoch 31 \t Batch 500 \t Validation Loss: 39.73750011062622\n",
      "Epoch 31 \t Batch 550 \t Validation Loss: 38.44006858478893\n",
      "Epoch 31 \t Batch 600 \t Validation Loss: 37.85575767199198\n",
      "Epoch 31 \t Batch 650 \t Validation Loss: 38.19694896991437\n",
      "Epoch 31 \t Batch 700 \t Validation Loss: 38.92485854557582\n",
      "Epoch 31 \t Batch 750 \t Validation Loss: 39.09309199015299\n",
      "Epoch 31 \t Batch 800 \t Validation Loss: 39.26251593708992\n",
      "Epoch 31 \t Batch 850 \t Validation Loss: 39.408615122402416\n",
      "Epoch 31 \t Batch 900 \t Validation Loss: 39.20306979073418\n",
      "Epoch 31 \t Batch 950 \t Validation Loss: 38.950220974370055\n",
      "Epoch 31 \t Batch 1000 \t Validation Loss: 39.94268514347076\n",
      "Epoch 31 \t Batch 1050 \t Validation Loss: 40.580636154129394\n",
      "Epoch 31 \t Batch 1100 \t Validation Loss: 40.902797537500206\n",
      "Epoch 31 \t Batch 1150 \t Validation Loss: 40.657729500273\n",
      "Epoch 31 \t Batch 1200 \t Validation Loss: 41.0586842050155\n",
      "Epoch 31 \t Batch 1250 \t Validation Loss: 41.18748488636017\n",
      "Epoch 31 \t Batch 1300 \t Validation Loss: 41.22324852961761\n",
      "Epoch 31 \t Batch 1350 \t Validation Loss: 40.899303030967715\n",
      "Epoch 31 \t Batch 1400 \t Validation Loss: 40.607811438867024\n",
      "Epoch 31 \t Batch 1450 \t Validation Loss: 40.38636545658112\n",
      "Epoch 31 \t Batch 1500 \t Validation Loss: 40.428136847496035\n",
      "Epoch 31 \t Batch 1550 \t Validation Loss: 40.09408379616276\n",
      "Epoch 31 \t Batch 1600 \t Validation Loss: 39.865220722258094\n",
      "Epoch 31 \t Batch 1650 \t Validation Loss: 39.83984180999525\n",
      "Epoch 31 \t Batch 1700 \t Validation Loss: 39.66916227088255\n",
      "Epoch 31 \t Batch 1750 \t Validation Loss: 39.46668937601362\n",
      "Epoch 31 \t Batch 1800 \t Validation Loss: 39.3255168000857\n",
      "Epoch 31 \t Batch 1850 \t Validation Loss: 39.74947529741236\n",
      "Epoch 31 \t Batch 1900 \t Validation Loss: 39.978780382808885\n",
      "Epoch 31 \t Batch 1950 \t Validation Loss: 39.76781795721788\n",
      "Epoch 31 \t Batch 2000 \t Validation Loss: 39.58301228690147\n",
      "Epoch 31 \t Batch 2050 \t Validation Loss: 39.523988364149886\n",
      "Epoch 31 \t Batch 2100 \t Validation Loss: 39.4100609216236\n",
      "Epoch 31 \t Batch 2150 \t Validation Loss: 39.225438974957136\n",
      "Epoch 31 \t Batch 2200 \t Validation Loss: 39.048145733746615\n",
      "Epoch 31 \t Batch 2250 \t Validation Loss: 38.885989640553795\n",
      "Epoch 31 \t Batch 2300 \t Validation Loss: 38.63035616791767\n",
      "Epoch 31 \t Batch 2350 \t Validation Loss: 38.580901644280615\n",
      "Epoch 31 \t Batch 2400 \t Validation Loss: 38.68251178979874\n",
      "Epoch 31 \t Batch 2450 \t Validation Loss: 39.015068598572086\n",
      "Epoch 31 Training Loss: 33.96482227412799 Validation Loss: 39.2001208494623\n",
      "Epoch 31 completed\n",
      "Epoch 32 \t Batch 50 \t Training Loss: 33.15259765625\n",
      "Epoch 32 \t Batch 100 \t Training Loss: 33.000185337066654\n",
      "Epoch 32 \t Batch 150 \t Training Loss: 32.74941883087158\n",
      "Epoch 32 \t Batch 200 \t Training Loss: 32.8551133441925\n",
      "Epoch 32 \t Batch 250 \t Training Loss: 32.764437423706056\n",
      "Epoch 32 \t Batch 300 \t Training Loss: 32.6610945892334\n",
      "Epoch 32 \t Batch 350 \t Training Loss: 32.751093308585034\n",
      "Epoch 32 \t Batch 400 \t Training Loss: 32.61081874370575\n",
      "Epoch 32 \t Batch 450 \t Training Loss: 32.58696854485406\n",
      "Epoch 32 \t Batch 500 \t Training Loss: 32.60108055114746\n",
      "Epoch 32 \t Batch 550 \t Training Loss: 32.577820171009414\n",
      "Epoch 32 \t Batch 600 \t Training Loss: 32.620793641408284\n",
      "Epoch 32 \t Batch 650 \t Training Loss: 32.65280198904184\n",
      "Epoch 32 \t Batch 700 \t Training Loss: 32.546523396628245\n",
      "Epoch 32 \t Batch 750 \t Training Loss: 32.533494285583494\n",
      "Epoch 32 \t Batch 800 \t Training Loss: 32.536542403697965\n",
      "Epoch 32 \t Batch 850 \t Training Loss: 32.53907154307646\n",
      "Epoch 32 \t Batch 900 \t Training Loss: 32.552675592634415\n",
      "Epoch 32 \t Batch 950 \t Training Loss: 32.57848050569233\n",
      "Epoch 32 \t Batch 1000 \t Training Loss: 32.58038716888428\n",
      "Epoch 32 \t Batch 1050 \t Training Loss: 32.672008732387\n",
      "Epoch 32 \t Batch 1100 \t Training Loss: 32.659167693745005\n",
      "Epoch 32 \t Batch 1150 \t Training Loss: 32.64636330811874\n",
      "Epoch 32 \t Batch 1200 \t Training Loss: 32.64016361713409\n",
      "Epoch 32 \t Batch 1250 \t Training Loss: 32.68666065216065\n",
      "Epoch 32 \t Batch 1300 \t Training Loss: 32.70463540150569\n",
      "Epoch 32 \t Batch 1350 \t Training Loss: 32.718454928927954\n",
      "Epoch 32 \t Batch 1400 \t Training Loss: 32.7253021131243\n",
      "Epoch 32 \t Batch 1450 \t Training Loss: 32.741947751538504\n",
      "Epoch 32 \t Batch 1500 \t Training Loss: 32.769377928415935\n",
      "Epoch 32 \t Batch 1550 \t Training Loss: 32.80716147268972\n",
      "Epoch 32 \t Batch 1600 \t Training Loss: 32.825869500637054\n",
      "Epoch 32 \t Batch 1650 \t Training Loss: 32.855733012575094\n",
      "Epoch 32 \t Batch 1700 \t Training Loss: 32.89409043816959\n",
      "Epoch 32 \t Batch 1750 \t Training Loss: 32.92052864619664\n",
      "Epoch 32 \t Batch 1800 \t Training Loss: 32.959497957229615\n",
      "Epoch 32 \t Batch 1850 \t Training Loss: 32.98404630815661\n",
      "Epoch 32 \t Batch 1900 \t Training Loss: 32.96282551514475\n",
      "Epoch 32 \t Batch 1950 \t Training Loss: 32.94945374806722\n",
      "Epoch 32 \t Batch 2000 \t Training Loss: 32.96875985240936\n",
      "Epoch 32 \t Batch 2050 \t Training Loss: 32.99322995534757\n",
      "Epoch 32 \t Batch 2100 \t Training Loss: 33.03999560038249\n",
      "Epoch 32 \t Batch 2150 \t Training Loss: 33.046131134920344\n",
      "Epoch 32 \t Batch 2200 \t Training Loss: 33.05787607019598\n",
      "Epoch 32 \t Batch 2250 \t Training Loss: 33.080999098036024\n",
      "Epoch 32 \t Batch 2300 \t Training Loss: 33.09458131541376\n",
      "Epoch 32 \t Batch 2350 \t Training Loss: 33.12771382027484\n",
      "Epoch 32 \t Batch 2400 \t Training Loss: 33.14015089432399\n",
      "Epoch 32 \t Batch 2450 \t Training Loss: 33.13843452531464\n",
      "Epoch 32 \t Batch 2500 \t Training Loss: 33.11936976699829\n",
      "Epoch 32 \t Batch 2550 \t Training Loss: 33.148784613515815\n",
      "Epoch 32 \t Batch 2600 \t Training Loss: 33.16410459224994\n",
      "Epoch 32 \t Batch 2650 \t Training Loss: 33.16411072713024\n",
      "Epoch 32 \t Batch 2700 \t Training Loss: 33.205848956637915\n",
      "Epoch 32 \t Batch 2750 \t Training Loss: 33.20699949715354\n",
      "Epoch 32 \t Batch 2800 \t Training Loss: 33.209383288111006\n",
      "Epoch 32 \t Batch 2850 \t Training Loss: 33.21695917162979\n",
      "Epoch 32 \t Batch 2900 \t Training Loss: 33.22131939855115\n",
      "Epoch 32 \t Batch 2950 \t Training Loss: 33.23954100980597\n",
      "Epoch 32 \t Batch 3000 \t Training Loss: 33.24601515833537\n",
      "Epoch 32 \t Batch 3050 \t Training Loss: 33.22318763920518\n",
      "Epoch 32 \t Batch 3100 \t Training Loss: 33.22804023681148\n",
      "Epoch 32 \t Batch 3150 \t Training Loss: 33.227784639994304\n",
      "Epoch 32 \t Batch 3200 \t Training Loss: 33.25354083657265\n",
      "Epoch 32 \t Batch 3250 \t Training Loss: 33.26281948852539\n",
      "Epoch 32 \t Batch 3300 \t Training Loss: 33.27511881163626\n",
      "Epoch 32 \t Batch 3350 \t Training Loss: 33.28906934311141\n",
      "Epoch 32 \t Batch 3400 \t Training Loss: 33.30260269950418\n",
      "Epoch 32 \t Batch 3450 \t Training Loss: 33.31056625034498\n",
      "Epoch 32 \t Batch 3500 \t Training Loss: 33.31359003448486\n",
      "Epoch 32 \t Batch 3550 \t Training Loss: 33.316191672741525\n",
      "Epoch 32 \t Batch 3600 \t Training Loss: 33.327362207306756\n",
      "Epoch 32 \t Batch 3650 \t Training Loss: 33.35487978634769\n",
      "Epoch 32 \t Batch 50 \t Validation Loss: 23.929702072143556\n",
      "Epoch 32 \t Batch 100 \t Validation Loss: 29.244268321990965\n",
      "Epoch 32 \t Batch 150 \t Validation Loss: 26.657465686798094\n",
      "Epoch 32 \t Batch 200 \t Validation Loss: 26.006165738105775\n",
      "Epoch 32 \t Batch 250 \t Validation Loss: 28.18127699661255\n",
      "Epoch 32 \t Batch 300 \t Validation Loss: 26.918138750394185\n",
      "Epoch 32 \t Batch 350 \t Validation Loss: 27.784711212430683\n",
      "Epoch 32 \t Batch 400 \t Validation Loss: 27.65708569407463\n",
      "Epoch 32 \t Batch 450 \t Validation Loss: 28.356487256156072\n",
      "Epoch 32 \t Batch 500 \t Validation Loss: 28.240228306770323\n",
      "Epoch 32 \t Batch 550 \t Validation Loss: 28.063607958880336\n",
      "Epoch 32 \t Batch 600 \t Validation Loss: 28.453251791795093\n",
      "Epoch 32 \t Batch 650 \t Validation Loss: 29.756338425416214\n",
      "Epoch 32 \t Batch 700 \t Validation Loss: 31.161690264429364\n",
      "Epoch 32 \t Batch 750 \t Validation Loss: 32.06294583829244\n",
      "Epoch 32 \t Batch 800 \t Validation Loss: 32.89753159582615\n",
      "Epoch 32 \t Batch 850 \t Validation Loss: 33.55817643950967\n",
      "Epoch 32 \t Batch 900 \t Validation Loss: 33.76073633988698\n",
      "Epoch 32 \t Batch 950 \t Validation Loss: 33.80891572299757\n",
      "Epoch 32 \t Batch 1000 \t Validation Loss: 35.23272954416275\n",
      "Epoch 32 \t Batch 1050 \t Validation Loss: 36.15285706247602\n",
      "Epoch 32 \t Batch 1100 \t Validation Loss: 36.77433837153695\n",
      "Epoch 32 \t Batch 1150 \t Validation Loss: 36.77484482433485\n",
      "Epoch 32 \t Batch 1200 \t Validation Loss: 37.394227593342464\n",
      "Epoch 32 \t Batch 1250 \t Validation Loss: 37.69334807434082\n",
      "Epoch 32 \t Batch 1300 \t Validation Loss: 37.913929403011615\n",
      "Epoch 32 \t Batch 1350 \t Validation Loss: 37.78486893194693\n",
      "Epoch 32 \t Batch 1400 \t Validation Loss: 37.65625374930246\n",
      "Epoch 32 \t Batch 1450 \t Validation Loss: 37.53763062575768\n",
      "Epoch 32 \t Batch 1500 \t Validation Loss: 37.71482277520498\n",
      "Epoch 32 \t Batch 1550 \t Validation Loss: 37.5185975933075\n",
      "Epoch 32 \t Batch 1600 \t Validation Loss: 37.43662411659956\n",
      "Epoch 32 \t Batch 1650 \t Validation Loss: 37.50883457039342\n",
      "Epoch 32 \t Batch 1700 \t Validation Loss: 37.43364525935229\n",
      "Epoch 32 \t Batch 1750 \t Validation Loss: 37.31583263533456\n",
      "Epoch 32 \t Batch 1800 \t Validation Loss: 37.23558830393685\n",
      "Epoch 32 \t Batch 1850 \t Validation Loss: 37.78097322489764\n",
      "Epoch 32 \t Batch 1900 \t Validation Loss: 38.09680485850886\n",
      "Epoch 32 \t Batch 1950 \t Validation Loss: 37.98254103464958\n",
      "Epoch 32 \t Batch 2000 \t Validation Loss: 37.88417417645454\n",
      "Epoch 32 \t Batch 2050 \t Validation Loss: 37.8510441424207\n",
      "Epoch 32 \t Batch 2100 \t Validation Loss: 37.749616827964786\n",
      "Epoch 32 \t Batch 2150 \t Validation Loss: 37.63234614505324\n",
      "Epoch 32 \t Batch 2200 \t Validation Loss: 37.5042098624056\n",
      "Epoch 32 \t Batch 2250 \t Validation Loss: 37.363493238449095\n",
      "Epoch 32 \t Batch 2300 \t Validation Loss: 37.121730363679966\n",
      "Epoch 32 \t Batch 2350 \t Validation Loss: 37.1044363805081\n",
      "Epoch 32 \t Batch 2400 \t Validation Loss: 37.2572727851073\n",
      "Epoch 32 \t Batch 2450 \t Validation Loss: 37.59444622098183\n",
      "Epoch 32 Training Loss: 33.3688296855731 Validation Loss: 37.76961283230936\n",
      "Epoch 32 completed\n",
      "Epoch 33 \t Batch 50 \t Training Loss: 32.99936527252197\n",
      "Epoch 33 \t Batch 100 \t Training Loss: 32.57053516387939\n",
      "Epoch 33 \t Batch 150 \t Training Loss: 32.16201279958089\n",
      "Epoch 33 \t Batch 200 \t Training Loss: 31.80354803085327\n",
      "Epoch 33 \t Batch 250 \t Training Loss: 31.754957153320312\n",
      "Epoch 33 \t Batch 300 \t Training Loss: 31.814961630503337\n",
      "Epoch 33 \t Batch 350 \t Training Loss: 31.773573995317733\n",
      "Epoch 33 \t Batch 400 \t Training Loss: 31.645575957298277\n",
      "Epoch 33 \t Batch 450 \t Training Loss: 31.696882336934408\n",
      "Epoch 33 \t Batch 500 \t Training Loss: 31.6734875869751\n",
      "Epoch 33 \t Batch 550 \t Training Loss: 31.620681207830255\n",
      "Epoch 33 \t Batch 600 \t Training Loss: 31.83638022740682\n",
      "Epoch 33 \t Batch 650 \t Training Loss: 31.784600155170146\n",
      "Epoch 33 \t Batch 700 \t Training Loss: 31.755281693594796\n",
      "Epoch 33 \t Batch 750 \t Training Loss: 31.81353793589274\n",
      "Epoch 33 \t Batch 800 \t Training Loss: 31.8403409075737\n",
      "Epoch 33 \t Batch 850 \t Training Loss: 31.85647575378418\n",
      "Epoch 33 \t Batch 900 \t Training Loss: 31.814968465169272\n",
      "Epoch 33 \t Batch 950 \t Training Loss: 31.77740812201249\n",
      "Epoch 33 \t Batch 1000 \t Training Loss: 31.766367124557494\n",
      "Epoch 33 \t Batch 1050 \t Training Loss: 31.749826389494395\n",
      "Epoch 33 \t Batch 1100 \t Training Loss: 31.826415316841818\n",
      "Epoch 33 \t Batch 1150 \t Training Loss: 31.864372452445654\n",
      "Epoch 33 \t Batch 1200 \t Training Loss: 31.898119686444602\n",
      "Epoch 33 \t Batch 1250 \t Training Loss: 31.877601055908205\n",
      "Epoch 33 \t Batch 1300 \t Training Loss: 31.96548731290377\n",
      "Epoch 33 \t Batch 1350 \t Training Loss: 31.98849553567392\n",
      "Epoch 33 \t Batch 1400 \t Training Loss: 31.993096648624967\n",
      "Epoch 33 \t Batch 1450 \t Training Loss: 32.050498667749864\n",
      "Epoch 33 \t Batch 1500 \t Training Loss: 32.08567783228556\n",
      "Epoch 33 \t Batch 1550 \t Training Loss: 32.06804220384167\n",
      "Epoch 33 \t Batch 1600 \t Training Loss: 32.04794383406639\n",
      "Epoch 33 \t Batch 1650 \t Training Loss: 32.04402238441236\n",
      "Epoch 33 \t Batch 1700 \t Training Loss: 32.036326921126424\n",
      "Epoch 33 \t Batch 1750 \t Training Loss: 32.06547240883963\n",
      "Epoch 33 \t Batch 1800 \t Training Loss: 32.06457022772895\n",
      "Epoch 33 \t Batch 1850 \t Training Loss: 32.102518555924696\n",
      "Epoch 33 \t Batch 1900 \t Training Loss: 32.09629465303923\n",
      "Epoch 33 \t Batch 1950 \t Training Loss: 32.092690311334074\n",
      "Epoch 33 \t Batch 2000 \t Training Loss: 32.0862093667984\n",
      "Epoch 33 \t Batch 2050 \t Training Loss: 32.088151130676266\n",
      "Epoch 33 \t Batch 2100 \t Training Loss: 32.10654380525862\n",
      "Epoch 33 \t Batch 2150 \t Training Loss: 32.10820192292679\n",
      "Epoch 33 \t Batch 2200 \t Training Loss: 32.13076659549366\n",
      "Epoch 33 \t Batch 2250 \t Training Loss: 32.135983085632326\n",
      "Epoch 33 \t Batch 2300 \t Training Loss: 32.16454142031462\n",
      "Epoch 33 \t Batch 2350 \t Training Loss: 32.1667487075481\n",
      "Epoch 33 \t Batch 2400 \t Training Loss: 32.1748323837916\n",
      "Epoch 33 \t Batch 2450 \t Training Loss: 32.16969481876918\n",
      "Epoch 33 \t Batch 2500 \t Training Loss: 32.201959059906\n",
      "Epoch 33 \t Batch 2550 \t Training Loss: 32.22756958157409\n",
      "Epoch 33 \t Batch 2600 \t Training Loss: 32.26261078174298\n",
      "Epoch 33 \t Batch 2650 \t Training Loss: 32.24989567162856\n",
      "Epoch 33 \t Batch 2700 \t Training Loss: 32.24740637037489\n",
      "Epoch 33 \t Batch 2750 \t Training Loss: 32.25628500158137\n",
      "Epoch 33 \t Batch 2800 \t Training Loss: 32.2679508468083\n",
      "Epoch 33 \t Batch 2850 \t Training Loss: 32.279691695497746\n",
      "Epoch 33 \t Batch 2900 \t Training Loss: 32.30812255070127\n",
      "Epoch 33 \t Batch 2950 \t Training Loss: 32.31532705791926\n",
      "Epoch 33 \t Batch 3000 \t Training Loss: 32.29835756746928\n",
      "Epoch 33 \t Batch 3050 \t Training Loss: 32.3017927607552\n",
      "Epoch 33 \t Batch 3100 \t Training Loss: 32.328197415259574\n",
      "Epoch 33 \t Batch 3150 \t Training Loss: 32.329837882632305\n",
      "Epoch 33 \t Batch 3200 \t Training Loss: 32.32411398351192\n",
      "Epoch 33 \t Batch 3250 \t Training Loss: 32.340062004676234\n",
      "Epoch 33 \t Batch 3300 \t Training Loss: 32.35162605632435\n",
      "Epoch 33 \t Batch 3350 \t Training Loss: 32.37766959688557\n",
      "Epoch 33 \t Batch 3400 \t Training Loss: 32.406381704667034\n",
      "Epoch 33 \t Batch 3450 \t Training Loss: 32.418656824637154\n",
      "Epoch 33 \t Batch 3500 \t Training Loss: 32.429855102539065\n",
      "Epoch 33 \t Batch 3550 \t Training Loss: 32.41759022457499\n",
      "Epoch 33 \t Batch 3600 \t Training Loss: 32.437315635151336\n",
      "Epoch 33 \t Batch 3650 \t Training Loss: 32.458451693809195\n",
      "Epoch 33 \t Batch 50 \t Validation Loss: 25.535768184661865\n",
      "Epoch 33 \t Batch 100 \t Validation Loss: 30.88325813293457\n",
      "Epoch 33 \t Batch 150 \t Validation Loss: 27.50408061345418\n",
      "Epoch 33 \t Batch 200 \t Validation Loss: 26.731581892967224\n",
      "Epoch 33 \t Batch 250 \t Validation Loss: 29.283007453918458\n",
      "Epoch 33 \t Batch 300 \t Validation Loss: 28.143171631495157\n",
      "Epoch 33 \t Batch 350 \t Validation Loss: 29.266057308741978\n",
      "Epoch 33 \t Batch 400 \t Validation Loss: 28.8014009141922\n",
      "Epoch 33 \t Batch 450 \t Validation Loss: 29.316290285322403\n",
      "Epoch 33 \t Batch 500 \t Validation Loss: 28.962477340698243\n",
      "Epoch 33 \t Batch 550 \t Validation Loss: 28.665129587000067\n",
      "Epoch 33 \t Batch 600 \t Validation Loss: 29.04470773220062\n",
      "Epoch 33 \t Batch 650 \t Validation Loss: 30.37230605345506\n",
      "Epoch 33 \t Batch 700 \t Validation Loss: 31.763711534227642\n",
      "Epoch 33 \t Batch 750 \t Validation Loss: 32.688848274230956\n",
      "Epoch 33 \t Batch 800 \t Validation Loss: 33.746779890060424\n",
      "Epoch 33 \t Batch 850 \t Validation Loss: 34.32373687968535\n",
      "Epoch 33 \t Batch 900 \t Validation Loss: 34.41915171305339\n",
      "Epoch 33 \t Batch 950 \t Validation Loss: 34.443704819930225\n",
      "Epoch 33 \t Batch 1000 \t Validation Loss: 35.8371640663147\n",
      "Epoch 33 \t Batch 1050 \t Validation Loss: 36.760157864434376\n",
      "Epoch 33 \t Batch 1100 \t Validation Loss: 37.37017282789404\n",
      "Epoch 33 \t Batch 1150 \t Validation Loss: 37.30732757983\n",
      "Epoch 33 \t Batch 1200 \t Validation Loss: 37.91590202490489\n",
      "Epoch 33 \t Batch 1250 \t Validation Loss: 38.167569108581546\n",
      "Epoch 33 \t Batch 1300 \t Validation Loss: 38.389507959072404\n",
      "Epoch 33 \t Batch 1350 \t Validation Loss: 38.19993971153542\n",
      "Epoch 33 \t Batch 1400 \t Validation Loss: 38.06305903366634\n",
      "Epoch 33 \t Batch 1450 \t Validation Loss: 38.042524656098465\n",
      "Epoch 33 \t Batch 1500 \t Validation Loss: 38.22635557556152\n",
      "Epoch 33 \t Batch 1550 \t Validation Loss: 37.994214691039055\n",
      "Epoch 33 \t Batch 1600 \t Validation Loss: 37.893577434420585\n",
      "Epoch 33 \t Batch 1650 \t Validation Loss: 37.963790603984485\n",
      "Epoch 33 \t Batch 1700 \t Validation Loss: 37.881435130063224\n",
      "Epoch 33 \t Batch 1750 \t Validation Loss: 37.74491842923845\n",
      "Epoch 33 \t Batch 1800 \t Validation Loss: 37.801394357681275\n",
      "Epoch 33 \t Batch 1850 \t Validation Loss: 38.299665364445865\n",
      "Epoch 33 \t Batch 1900 \t Validation Loss: 38.5957078431782\n",
      "Epoch 33 \t Batch 1950 \t Validation Loss: 38.44156022683168\n",
      "Epoch 33 \t Batch 2000 \t Validation Loss: 38.351834834575655\n",
      "Epoch 33 \t Batch 2050 \t Validation Loss: 38.46426974738517\n",
      "Epoch 33 \t Batch 2100 \t Validation Loss: 38.39115887006124\n",
      "Epoch 33 \t Batch 2150 \t Validation Loss: 38.31647336959839\n",
      "Epoch 33 \t Batch 2200 \t Validation Loss: 38.21871147719297\n",
      "Epoch 33 \t Batch 2250 \t Validation Loss: 38.112000290340845\n",
      "Epoch 33 \t Batch 2300 \t Validation Loss: 37.92648404556772\n",
      "Epoch 33 \t Batch 2350 \t Validation Loss: 37.95207372036386\n",
      "Epoch 33 \t Batch 2400 \t Validation Loss: 38.11656317134698\n",
      "Epoch 33 \t Batch 2450 \t Validation Loss: 38.486832348375906\n",
      "Epoch 33 Training Loss: 32.45769659955473 Validation Loss: 38.68937946275457\n",
      "Epoch 33 completed\n",
      "Epoch 34 \t Batch 50 \t Training Loss: 31.568236846923828\n",
      "Epoch 34 \t Batch 100 \t Training Loss: 32.16027414321899\n",
      "Epoch 34 \t Batch 150 \t Training Loss: 31.463496119181315\n",
      "Epoch 34 \t Batch 200 \t Training Loss: 30.9996124458313\n",
      "Epoch 34 \t Batch 250 \t Training Loss: 30.852908264160156\n",
      "Epoch 34 \t Batch 300 \t Training Loss: 30.80333017985026\n",
      "Epoch 34 \t Batch 350 \t Training Loss: 30.83628585270473\n",
      "Epoch 34 \t Batch 400 \t Training Loss: 30.721210951805116\n",
      "Epoch 34 \t Batch 450 \t Training Loss: 30.763076404995388\n",
      "Epoch 34 \t Batch 500 \t Training Loss: 30.676141788482667\n",
      "Epoch 34 \t Batch 550 \t Training Loss: 30.699429567510432\n",
      "Epoch 34 \t Batch 600 \t Training Loss: 30.769533802668253\n",
      "Epoch 34 \t Batch 650 \t Training Loss: 30.84483347379244\n",
      "Epoch 34 \t Batch 700 \t Training Loss: 30.850039196014404\n",
      "Epoch 34 \t Batch 750 \t Training Loss: 30.908150337219237\n",
      "Epoch 34 \t Batch 800 \t Training Loss: 30.89239561319351\n",
      "Epoch 34 \t Batch 850 \t Training Loss: 30.889421261058136\n",
      "Epoch 34 \t Batch 900 \t Training Loss: 30.92840165456136\n",
      "Epoch 34 \t Batch 950 \t Training Loss: 30.990230678759122\n",
      "Epoch 34 \t Batch 1000 \t Training Loss: 30.995738458633422\n",
      "Epoch 34 \t Batch 1050 \t Training Loss: 30.95824685777937\n",
      "Epoch 34 \t Batch 1100 \t Training Loss: 30.98298672242598\n",
      "Epoch 34 \t Batch 1150 \t Training Loss: 30.986826354317042\n",
      "Epoch 34 \t Batch 1200 \t Training Loss: 31.054802769025166\n",
      "Epoch 34 \t Batch 1250 \t Training Loss: 31.048711329650878\n",
      "Epoch 34 \t Batch 1300 \t Training Loss: 31.049737179095928\n",
      "Epoch 34 \t Batch 1350 \t Training Loss: 31.03387239244249\n",
      "Epoch 34 \t Batch 1400 \t Training Loss: 31.051993980407715\n",
      "Epoch 34 \t Batch 1450 \t Training Loss: 31.023307706898656\n",
      "Epoch 34 \t Batch 1500 \t Training Loss: 31.03936702601115\n",
      "Epoch 34 \t Batch 1550 \t Training Loss: 31.045838458153508\n",
      "Epoch 34 \t Batch 1600 \t Training Loss: 31.05124518752098\n",
      "Epoch 34 \t Batch 1650 \t Training Loss: 31.050902254509204\n",
      "Epoch 34 \t Batch 1700 \t Training Loss: 31.035560292636646\n",
      "Epoch 34 \t Batch 1750 \t Training Loss: 31.064633174351282\n",
      "Epoch 34 \t Batch 1800 \t Training Loss: 31.040208849377102\n",
      "Epoch 34 \t Batch 1850 \t Training Loss: 31.049413970741067\n",
      "Epoch 34 \t Batch 1900 \t Training Loss: 31.08173857437937\n",
      "Epoch 34 \t Batch 1950 \t Training Loss: 31.076324702531863\n",
      "Epoch 34 \t Batch 2000 \t Training Loss: 31.106954168319703\n",
      "Epoch 34 \t Batch 2050 \t Training Loss: 31.11702647511552\n",
      "Epoch 34 \t Batch 2100 \t Training Loss: 31.16995657057989\n",
      "Epoch 34 \t Batch 2150 \t Training Loss: 31.209770529990973\n",
      "Epoch 34 \t Batch 2200 \t Training Loss: 31.22574496529319\n",
      "Epoch 34 \t Batch 2250 \t Training Loss: 31.225297437032065\n",
      "Epoch 34 \t Batch 2300 \t Training Loss: 31.237048443504\n",
      "Epoch 34 \t Batch 2350 \t Training Loss: 31.246073971200495\n",
      "Epoch 34 \t Batch 2400 \t Training Loss: 31.27238854487737\n",
      "Epoch 34 \t Batch 2450 \t Training Loss: 31.286805405130192\n",
      "Epoch 34 \t Batch 2500 \t Training Loss: 31.282143103790283\n",
      "Epoch 34 \t Batch 2550 \t Training Loss: 31.282930411544502\n",
      "Epoch 34 \t Batch 2600 \t Training Loss: 31.290256186998807\n",
      "Epoch 34 \t Batch 2650 \t Training Loss: 31.303910845630575\n",
      "Epoch 34 \t Batch 2700 \t Training Loss: 31.31096637937758\n",
      "Epoch 34 \t Batch 2750 \t Training Loss: 31.340965086503463\n",
      "Epoch 34 \t Batch 2800 \t Training Loss: 31.350738797187805\n",
      "Epoch 34 \t Batch 2850 \t Training Loss: 31.35763198450992\n",
      "Epoch 34 \t Batch 2900 \t Training Loss: 31.353933317907924\n",
      "Epoch 34 \t Batch 2950 \t Training Loss: 31.34527191485389\n",
      "Epoch 34 \t Batch 3000 \t Training Loss: 31.356179569244386\n",
      "Epoch 34 \t Batch 3050 \t Training Loss: 31.36454060163654\n",
      "Epoch 34 \t Batch 3100 \t Training Loss: 31.387341323975594\n",
      "Epoch 34 \t Batch 3150 \t Training Loss: 31.408573934161474\n",
      "Epoch 34 \t Batch 3200 \t Training Loss: 31.40857327759266\n",
      "Epoch 34 \t Batch 3250 \t Training Loss: 31.427716011047362\n",
      "Epoch 34 \t Batch 3300 \t Training Loss: 31.440730745142158\n",
      "Epoch 34 \t Batch 3350 \t Training Loss: 31.437130642221934\n",
      "Epoch 34 \t Batch 3400 \t Training Loss: 31.45566825025222\n",
      "Epoch 34 \t Batch 3450 \t Training Loss: 31.45366669641025\n",
      "Epoch 34 \t Batch 3500 \t Training Loss: 31.46828321674892\n",
      "Epoch 34 \t Batch 3550 \t Training Loss: 31.48695028439374\n",
      "Epoch 34 \t Batch 3600 \t Training Loss: 31.50682981861962\n",
      "Epoch 34 \t Batch 3650 \t Training Loss: 31.524708355942817\n",
      "Epoch 34 \t Batch 50 \t Validation Loss: 15.61425775527954\n",
      "Epoch 34 \t Batch 100 \t Validation Loss: 18.430845432281494\n",
      "Epoch 34 \t Batch 150 \t Validation Loss: 17.889794273376467\n",
      "Epoch 34 \t Batch 200 \t Validation Loss: 16.92711954832077\n",
      "Epoch 34 \t Batch 250 \t Validation Loss: 17.877365434646606\n",
      "Epoch 34 \t Batch 300 \t Validation Loss: 17.589727431933085\n",
      "Epoch 34 \t Batch 350 \t Validation Loss: 19.53572822025844\n",
      "Epoch 34 \t Batch 400 \t Validation Loss: 20.269313201904296\n",
      "Epoch 34 \t Batch 450 \t Validation Loss: 21.780314252641467\n",
      "Epoch 34 \t Batch 500 \t Validation Loss: 22.182285362243654\n",
      "Epoch 34 \t Batch 550 \t Validation Loss: 22.51649136283181\n",
      "Epoch 34 \t Batch 600 \t Validation Loss: 23.4076757589976\n",
      "Epoch 34 \t Batch 650 \t Validation Loss: 25.056604018578163\n",
      "Epoch 34 \t Batch 700 \t Validation Loss: 26.866661530903407\n",
      "Epoch 34 \t Batch 750 \t Validation Loss: 28.044452878316243\n",
      "Epoch 34 \t Batch 800 \t Validation Loss: 29.254436511993408\n",
      "Epoch 34 \t Batch 850 \t Validation Loss: 29.99490645128138\n",
      "Epoch 34 \t Batch 900 \t Validation Loss: 30.330673606130812\n",
      "Epoch 34 \t Batch 950 \t Validation Loss: 30.58184902843676\n",
      "Epoch 34 \t Batch 1000 \t Validation Loss: 32.123345278263095\n",
      "Epoch 34 \t Batch 1050 \t Validation Loss: 33.18407608804249\n",
      "Epoch 34 \t Batch 1100 \t Validation Loss: 33.923098731907935\n",
      "Epoch 34 \t Batch 1150 \t Validation Loss: 34.026530657229216\n",
      "Epoch 34 \t Batch 1200 \t Validation Loss: 34.840118877887726\n",
      "Epoch 34 \t Batch 1250 \t Validation Loss: 35.232619276046755\n",
      "Epoch 34 \t Batch 1300 \t Validation Loss: 35.53193244090447\n",
      "Epoch 34 \t Batch 1350 \t Validation Loss: 35.45321042202137\n",
      "Epoch 34 \t Batch 1400 \t Validation Loss: 35.404109237194064\n",
      "Epoch 34 \t Batch 1450 \t Validation Loss: 35.40880383557287\n",
      "Epoch 34 \t Batch 1500 \t Validation Loss: 35.65118386967977\n",
      "Epoch 34 \t Batch 1550 \t Validation Loss: 35.492402302526656\n",
      "Epoch 34 \t Batch 1600 \t Validation Loss: 35.47789390265942\n",
      "Epoch 34 \t Batch 1650 \t Validation Loss: 35.62241486636075\n",
      "Epoch 34 \t Batch 1700 \t Validation Loss: 35.598237957673916\n",
      "Epoch 34 \t Batch 1750 \t Validation Loss: 35.52644812665667\n",
      "Epoch 34 \t Batch 1800 \t Validation Loss: 35.60457994143168\n",
      "Epoch 34 \t Batch 1850 \t Validation Loss: 36.176788269249165\n",
      "Epoch 34 \t Batch 1900 \t Validation Loss: 36.53569346879658\n",
      "Epoch 34 \t Batch 1950 \t Validation Loss: 36.425979853409984\n",
      "Epoch 34 \t Batch 2000 \t Validation Loss: 36.37248572587967\n",
      "Epoch 34 \t Batch 2050 \t Validation Loss: 36.498343162071414\n",
      "Epoch 34 \t Batch 2100 \t Validation Loss: 36.49997008641561\n",
      "Epoch 34 \t Batch 2150 \t Validation Loss: 36.45272060216859\n",
      "Epoch 34 \t Batch 2200 \t Validation Loss: 36.39227000149813\n",
      "Epoch 34 \t Batch 2250 \t Validation Loss: 36.351790666368274\n",
      "Epoch 34 \t Batch 2300 \t Validation Loss: 36.238392278733464\n",
      "Epoch 34 \t Batch 2350 \t Validation Loss: 36.307127111211734\n",
      "Epoch 34 \t Batch 2400 \t Validation Loss: 36.489545907477535\n",
      "Epoch 34 \t Batch 2450 \t Validation Loss: 36.89317870655838\n",
      "Epoch 34 Training Loss: 31.537776991741218 Validation Loss: 37.07335584997744\n",
      "Epoch 34 completed\n",
      "Epoch 35 \t Batch 50 \t Training Loss: 29.719173507690428\n",
      "Epoch 35 \t Batch 100 \t Training Loss: 30.289969787597656\n",
      "Epoch 35 \t Batch 150 \t Training Loss: 30.382060203552246\n",
      "Epoch 35 \t Batch 200 \t Training Loss: 30.37725333213806\n",
      "Epoch 35 \t Batch 250 \t Training Loss: 30.173450889587404\n",
      "Epoch 35 \t Batch 300 \t Training Loss: 30.07723293940226\n",
      "Epoch 35 \t Batch 350 \t Training Loss: 30.026956111363003\n",
      "Epoch 35 \t Batch 400 \t Training Loss: 30.07416666507721\n",
      "Epoch 35 \t Batch 450 \t Training Loss: 30.04190106709798\n",
      "Epoch 35 \t Batch 500 \t Training Loss: 30.040552043914794\n",
      "Epoch 35 \t Batch 550 \t Training Loss: 30.047780959389428\n",
      "Epoch 35 \t Batch 600 \t Training Loss: 30.059237337112428\n",
      "Epoch 35 \t Batch 650 \t Training Loss: 30.132024735670825\n",
      "Epoch 35 \t Batch 700 \t Training Loss: 30.081009028298514\n",
      "Epoch 35 \t Batch 750 \t Training Loss: 29.99441973876953\n",
      "Epoch 35 \t Batch 800 \t Training Loss: 30.099023821353914\n",
      "Epoch 35 \t Batch 850 \t Training Loss: 30.018657872817094\n",
      "Epoch 35 \t Batch 900 \t Training Loss: 30.07372003979153\n",
      "Epoch 35 \t Batch 950 \t Training Loss: 30.136128254940637\n",
      "Epoch 35 \t Batch 1000 \t Training Loss: 30.234831413269042\n",
      "Epoch 35 \t Batch 1050 \t Training Loss: 30.258871857779365\n",
      "Epoch 35 \t Batch 1100 \t Training Loss: 30.250003982890735\n",
      "Epoch 35 \t Batch 1150 \t Training Loss: 30.2907097692075\n",
      "Epoch 35 \t Batch 1200 \t Training Loss: 30.278086110750834\n",
      "Epoch 35 \t Batch 1250 \t Training Loss: 30.296714765930176\n",
      "Epoch 35 \t Batch 1300 \t Training Loss: 30.280921487074632\n",
      "Epoch 35 \t Batch 1350 \t Training Loss: 30.27597481621636\n",
      "Epoch 35 \t Batch 1400 \t Training Loss: 30.28283781869071\n",
      "Epoch 35 \t Batch 1450 \t Training Loss: 30.271741852595888\n",
      "Epoch 35 \t Batch 1500 \t Training Loss: 30.26566330973307\n",
      "Epoch 35 \t Batch 1550 \t Training Loss: 30.26535769308767\n",
      "Epoch 35 \t Batch 1600 \t Training Loss: 30.250986169576645\n",
      "Epoch 35 \t Batch 1650 \t Training Loss: 30.294788829919064\n",
      "Epoch 35 \t Batch 1700 \t Training Loss: 30.36474571676815\n",
      "Epoch 35 \t Batch 1750 \t Training Loss: 30.358764310564315\n",
      "Epoch 35 \t Batch 1800 \t Training Loss: 30.363130024804008\n",
      "Epoch 35 \t Batch 1850 \t Training Loss: 30.357752133962272\n",
      "Epoch 35 \t Batch 1900 \t Training Loss: 30.38495376586914\n",
      "Epoch 35 \t Batch 1950 \t Training Loss: 30.384276836101826\n",
      "Epoch 35 \t Batch 2000 \t Training Loss: 30.386934642791747\n",
      "Epoch 35 \t Batch 2050 \t Training Loss: 30.399947519069766\n",
      "Epoch 35 \t Batch 2100 \t Training Loss: 30.398981507619222\n",
      "Epoch 35 \t Batch 2150 \t Training Loss: 30.401559708173885\n",
      "Epoch 35 \t Batch 2200 \t Training Loss: 30.39086891607805\n",
      "Epoch 35 \t Batch 2250 \t Training Loss: 30.370889385647242\n",
      "Epoch 35 \t Batch 2300 \t Training Loss: 30.397454739446225\n",
      "Epoch 35 \t Batch 2350 \t Training Loss: 30.420986811049442\n",
      "Epoch 35 \t Batch 2400 \t Training Loss: 30.411572104295093\n",
      "Epoch 35 \t Batch 2450 \t Training Loss: 30.410503610104932\n",
      "Epoch 35 \t Batch 2500 \t Training Loss: 30.401347589111328\n",
      "Epoch 35 \t Batch 2550 \t Training Loss: 30.399842209909476\n",
      "Epoch 35 \t Batch 2600 \t Training Loss: 30.4144412950369\n",
      "Epoch 35 \t Batch 2650 \t Training Loss: 30.42325219784143\n",
      "Epoch 35 \t Batch 2700 \t Training Loss: 30.428733341075755\n",
      "Epoch 35 \t Batch 2750 \t Training Loss: 30.434822577043015\n",
      "Epoch 35 \t Batch 2800 \t Training Loss: 30.446743925639563\n",
      "Epoch 35 \t Batch 2850 \t Training Loss: 30.440183034528765\n",
      "Epoch 35 \t Batch 2900 \t Training Loss: 30.462639115432214\n",
      "Epoch 35 \t Batch 2950 \t Training Loss: 30.479431981878765\n",
      "Epoch 35 \t Batch 3000 \t Training Loss: 30.519470061620076\n",
      "Epoch 35 \t Batch 3050 \t Training Loss: 30.530242911166834\n",
      "Epoch 35 \t Batch 3100 \t Training Loss: 30.5422835411564\n",
      "Epoch 35 \t Batch 3150 \t Training Loss: 30.57060352688744\n",
      "Epoch 35 \t Batch 3200 \t Training Loss: 30.58314069926739\n",
      "Epoch 35 \t Batch 3250 \t Training Loss: 30.604886377187874\n",
      "Epoch 35 \t Batch 3300 \t Training Loss: 30.625050029176656\n",
      "Epoch 35 \t Batch 3350 \t Training Loss: 30.642377811830436\n",
      "Epoch 35 \t Batch 3400 \t Training Loss: 30.656789873908547\n",
      "Epoch 35 \t Batch 3450 \t Training Loss: 30.665716907114223\n",
      "Epoch 35 \t Batch 3500 \t Training Loss: 30.6765592515128\n",
      "Epoch 35 \t Batch 3550 \t Training Loss: 30.7051065654486\n",
      "Epoch 35 \t Batch 3600 \t Training Loss: 30.712887954182094\n",
      "Epoch 35 \t Batch 3650 \t Training Loss: 30.73771669570714\n",
      "Epoch 35 \t Batch 50 \t Validation Loss: 31.196440448760985\n",
      "Epoch 35 \t Batch 100 \t Validation Loss: 38.92937443733215\n",
      "Epoch 35 \t Batch 150 \t Validation Loss: 33.80864350001017\n",
      "Epoch 35 \t Batch 200 \t Validation Loss: 33.23748554944992\n",
      "Epoch 35 \t Batch 250 \t Validation Loss: 36.503079320907595\n",
      "Epoch 35 \t Batch 300 \t Validation Loss: 34.58398923714956\n",
      "Epoch 35 \t Batch 350 \t Validation Loss: 34.70193664414542\n",
      "Epoch 35 \t Batch 400 \t Validation Loss: 33.624275320768355\n",
      "Epoch 35 \t Batch 450 \t Validation Loss: 33.66164963722229\n",
      "Epoch 35 \t Batch 500 \t Validation Loss: 32.93708460521698\n",
      "Epoch 35 \t Batch 550 \t Validation Loss: 32.26509135939858\n",
      "Epoch 35 \t Batch 600 \t Validation Loss: 32.35244222243627\n",
      "Epoch 35 \t Batch 650 \t Validation Loss: 33.450170026192296\n",
      "Epoch 35 \t Batch 700 \t Validation Loss: 34.6472375195367\n",
      "Epoch 35 \t Batch 750 \t Validation Loss: 35.36779914410909\n",
      "Epoch 35 \t Batch 800 \t Validation Loss: 36.03066599547863\n",
      "Epoch 35 \t Batch 850 \t Validation Loss: 36.56614301962011\n",
      "Epoch 35 \t Batch 900 \t Validation Loss: 36.6343067905638\n",
      "Epoch 35 \t Batch 950 \t Validation Loss: 36.56831651537042\n",
      "Epoch 35 \t Batch 1000 \t Validation Loss: 37.927456357479095\n",
      "Epoch 35 \t Batch 1050 \t Validation Loss: 38.808289901642574\n",
      "Epoch 35 \t Batch 1100 \t Validation Loss: 39.38579592141238\n",
      "Epoch 35 \t Batch 1150 \t Validation Loss: 39.2714352010644\n",
      "Epoch 35 \t Batch 1200 \t Validation Loss: 39.843470607598626\n",
      "Epoch 35 \t Batch 1250 \t Validation Loss: 40.07327984695434\n",
      "Epoch 35 \t Batch 1300 \t Validation Loss: 40.24890262677119\n",
      "Epoch 35 \t Batch 1350 \t Validation Loss: 40.026796642586035\n",
      "Epoch 35 \t Batch 1400 \t Validation Loss: 39.85103641305651\n",
      "Epoch 35 \t Batch 1450 \t Validation Loss: 39.751139874293884\n",
      "Epoch 35 \t Batch 1500 \t Validation Loss: 39.90069026533762\n",
      "Epoch 35 \t Batch 1550 \t Validation Loss: 39.63505974769592\n",
      "Epoch 35 \t Batch 1600 \t Validation Loss: 39.47736652463674\n",
      "Epoch 35 \t Batch 1650 \t Validation Loss: 39.51909752036586\n",
      "Epoch 35 \t Batch 1700 \t Validation Loss: 39.418116449187785\n",
      "Epoch 35 \t Batch 1750 \t Validation Loss: 39.2771102444785\n",
      "Epoch 35 \t Batch 1800 \t Validation Loss: 39.23823647154702\n",
      "Epoch 35 \t Batch 1850 \t Validation Loss: 39.736363276662054\n",
      "Epoch 35 \t Batch 1900 \t Validation Loss: 39.99341036018572\n",
      "Epoch 35 \t Batch 1950 \t Validation Loss: 39.81901981720558\n",
      "Epoch 35 \t Batch 2000 \t Validation Loss: 39.69800789666176\n",
      "Epoch 35 \t Batch 2050 \t Validation Loss: 39.77340035717662\n",
      "Epoch 35 \t Batch 2100 \t Validation Loss: 39.63208511965615\n",
      "Epoch 35 \t Batch 2150 \t Validation Loss: 39.440271380890245\n",
      "Epoch 35 \t Batch 2200 \t Validation Loss: 39.24919495560906\n",
      "Epoch 35 \t Batch 2250 \t Validation Loss: 39.064994532903036\n",
      "Epoch 35 \t Batch 2300 \t Validation Loss: 38.78959019795708\n",
      "Epoch 35 \t Batch 2350 \t Validation Loss: 38.71842077265394\n",
      "Epoch 35 \t Batch 2400 \t Validation Loss: 38.816718563735485\n",
      "Epoch 35 \t Batch 2450 \t Validation Loss: 39.10949117553477\n",
      "Epoch 35 Training Loss: 30.736349229594225 Validation Loss: 39.281605722164954\n",
      "Epoch 35 completed\n",
      "Epoch 36 \t Batch 50 \t Training Loss: 29.250876274108887\n",
      "Epoch 36 \t Batch 100 \t Training Loss: 29.558209743499756\n",
      "Epoch 36 \t Batch 150 \t Training Loss: 29.23290454864502\n",
      "Epoch 36 \t Batch 200 \t Training Loss: 29.004618406295776\n",
      "Epoch 36 \t Batch 250 \t Training Loss: 29.05372343444824\n",
      "Epoch 36 \t Batch 300 \t Training Loss: 29.123087011973062\n",
      "Epoch 36 \t Batch 350 \t Training Loss: 29.197764625549315\n",
      "Epoch 36 \t Batch 400 \t Training Loss: 29.36679415225983\n",
      "Epoch 36 \t Batch 450 \t Training Loss: 29.424697223239473\n",
      "Epoch 36 \t Batch 500 \t Training Loss: 29.31158275604248\n",
      "Epoch 36 \t Batch 550 \t Training Loss: 29.30229656566273\n",
      "Epoch 36 \t Batch 600 \t Training Loss: 29.280585762659708\n",
      "Epoch 36 \t Batch 650 \t Training Loss: 29.22628229874831\n",
      "Epoch 36 \t Batch 700 \t Training Loss: 29.237225080217634\n",
      "Epoch 36 \t Batch 750 \t Training Loss: 29.241462328592934\n",
      "Epoch 36 \t Batch 800 \t Training Loss: 29.27382637500763\n",
      "Epoch 36 \t Batch 850 \t Training Loss: 29.25753454096177\n",
      "Epoch 36 \t Batch 900 \t Training Loss: 29.28797038184272\n",
      "Epoch 36 \t Batch 950 \t Training Loss: 29.290516750938014\n",
      "Epoch 36 \t Batch 1000 \t Training Loss: 29.288215614318847\n",
      "Epoch 36 \t Batch 1050 \t Training Loss: 29.287917658487956\n",
      "Epoch 36 \t Batch 1100 \t Training Loss: 29.263275618119675\n",
      "Epoch 36 \t Batch 1150 \t Training Loss: 29.278894666588826\n",
      "Epoch 36 \t Batch 1200 \t Training Loss: 29.249044987360637\n",
      "Epoch 36 \t Batch 1250 \t Training Loss: 29.275444540405275\n",
      "Epoch 36 \t Batch 1300 \t Training Loss: 29.288852314582236\n",
      "Epoch 36 \t Batch 1350 \t Training Loss: 29.317816146568017\n",
      "Epoch 36 \t Batch 1400 \t Training Loss: 29.276462621688843\n",
      "Epoch 36 \t Batch 1450 \t Training Loss: 29.281871948242188\n",
      "Epoch 36 \t Batch 1500 \t Training Loss: 29.253632244110108\n",
      "Epoch 36 \t Batch 1550 \t Training Loss: 29.280949304642217\n",
      "Epoch 36 \t Batch 1600 \t Training Loss: 29.250211584568024\n",
      "Epoch 36 \t Batch 1650 \t Training Loss: 29.248590332956024\n",
      "Epoch 36 \t Batch 1700 \t Training Loss: 29.23482353995828\n",
      "Epoch 36 \t Batch 1750 \t Training Loss: 29.28321784537179\n",
      "Epoch 36 \t Batch 1800 \t Training Loss: 29.31071321487427\n",
      "Epoch 36 \t Batch 1850 \t Training Loss: 29.34521354675293\n",
      "Epoch 36 \t Batch 1900 \t Training Loss: 29.351173800418252\n",
      "Epoch 36 \t Batch 1950 \t Training Loss: 29.35651751200358\n",
      "Epoch 36 \t Batch 2000 \t Training Loss: 29.390387140274047\n",
      "Epoch 36 \t Batch 2050 \t Training Loss: 29.41714537922929\n",
      "Epoch 36 \t Batch 2100 \t Training Loss: 29.429164881933303\n",
      "Epoch 36 \t Batch 2150 \t Training Loss: 29.483389409087426\n",
      "Epoch 36 \t Batch 2200 \t Training Loss: 29.517424015565354\n",
      "Epoch 36 \t Batch 2250 \t Training Loss: 29.51885117594401\n",
      "Epoch 36 \t Batch 2300 \t Training Loss: 29.548646171403966\n",
      "Epoch 36 \t Batch 2350 \t Training Loss: 29.59202743286782\n",
      "Epoch 36 \t Batch 2400 \t Training Loss: 29.601084478696187\n",
      "Epoch 36 \t Batch 2450 \t Training Loss: 29.627104584051636\n",
      "Epoch 36 \t Batch 2500 \t Training Loss: 29.649300659942625\n",
      "Epoch 36 \t Batch 2550 \t Training Loss: 29.673150068544874\n",
      "Epoch 36 \t Batch 2600 \t Training Loss: 29.695454060481143\n",
      "Epoch 36 \t Batch 2650 \t Training Loss: 29.700430571358158\n",
      "Epoch 36 \t Batch 2700 \t Training Loss: 29.70876216535215\n",
      "Epoch 36 \t Batch 2750 \t Training Loss: 29.685187601956454\n",
      "Epoch 36 \t Batch 2800 \t Training Loss: 29.705210071972438\n",
      "Epoch 36 \t Batch 2850 \t Training Loss: 29.729831579107987\n",
      "Epoch 36 \t Batch 2900 \t Training Loss: 29.73636365495879\n",
      "Epoch 36 \t Batch 2950 \t Training Loss: 29.73950130785926\n",
      "Epoch 36 \t Batch 3000 \t Training Loss: 29.756583375930788\n",
      "Epoch 36 \t Batch 3050 \t Training Loss: 29.77717619786497\n",
      "Epoch 36 \t Batch 3100 \t Training Loss: 29.799799397376276\n",
      "Epoch 36 \t Batch 3150 \t Training Loss: 29.834095073881603\n",
      "Epoch 36 \t Batch 3200 \t Training Loss: 29.84302292406559\n",
      "Epoch 36 \t Batch 3250 \t Training Loss: 29.825458797748272\n",
      "Epoch 36 \t Batch 3300 \t Training Loss: 29.829100394393457\n",
      "Epoch 36 \t Batch 3350 \t Training Loss: 29.838625455827856\n",
      "Epoch 36 \t Batch 3400 \t Training Loss: 29.866484128166647\n",
      "Epoch 36 \t Batch 3450 \t Training Loss: 29.88943208279817\n",
      "Epoch 36 \t Batch 3500 \t Training Loss: 29.904858939579555\n",
      "Epoch 36 \t Batch 3550 \t Training Loss: 29.918966558214645\n",
      "Epoch 36 \t Batch 3600 \t Training Loss: 29.92540230645074\n",
      "Epoch 36 \t Batch 3650 \t Training Loss: 29.929045090348755\n",
      "Epoch 36 \t Batch 50 \t Validation Loss: 19.138831310272217\n",
      "Epoch 36 \t Batch 100 \t Validation Loss: 21.650258417129518\n",
      "Epoch 36 \t Batch 150 \t Validation Loss: 20.37388317743937\n",
      "Epoch 36 \t Batch 200 \t Validation Loss: 19.89447400569916\n",
      "Epoch 36 \t Batch 250 \t Validation Loss: 21.129632896423338\n",
      "Epoch 36 \t Batch 300 \t Validation Loss: 20.771315383911134\n",
      "Epoch 36 \t Batch 350 \t Validation Loss: 22.621007360730854\n",
      "Epoch 36 \t Batch 400 \t Validation Loss: 23.24890919446945\n",
      "Epoch 36 \t Batch 450 \t Validation Loss: 24.64697966893514\n",
      "Epoch 36 \t Batch 500 \t Validation Loss: 25.016488918304443\n",
      "Epoch 36 \t Batch 550 \t Validation Loss: 25.21552441857078\n",
      "Epoch 36 \t Batch 600 \t Validation Loss: 25.928881589571635\n",
      "Epoch 36 \t Batch 650 \t Validation Loss: 27.55592023702768\n",
      "Epoch 36 \t Batch 700 \t Validation Loss: 29.439125849860055\n",
      "Epoch 36 \t Batch 750 \t Validation Loss: 30.64092952219645\n",
      "Epoch 36 \t Batch 800 \t Validation Loss: 31.81778256416321\n",
      "Epoch 36 \t Batch 850 \t Validation Loss: 32.594994557324576\n",
      "Epoch 36 \t Batch 900 \t Validation Loss: 32.88096621089512\n",
      "Epoch 36 \t Batch 950 \t Validation Loss: 33.07661928979974\n",
      "Epoch 36 \t Batch 1000 \t Validation Loss: 34.707510197639465\n",
      "Epoch 36 \t Batch 1050 \t Validation Loss: 35.80736298061552\n",
      "Epoch 36 \t Batch 1100 \t Validation Loss: 36.597547530260954\n",
      "Epoch 36 \t Batch 1150 \t Validation Loss: 36.69225353406823\n",
      "Epoch 36 \t Batch 1200 \t Validation Loss: 37.51832602183024\n",
      "Epoch 36 \t Batch 1250 \t Validation Loss: 37.88285870552063\n",
      "Epoch 36 \t Batch 1300 \t Validation Loss: 38.17090076409853\n",
      "Epoch 36 \t Batch 1350 \t Validation Loss: 38.03978942411917\n",
      "Epoch 36 \t Batch 1400 \t Validation Loss: 37.96839088814599\n",
      "Epoch 36 \t Batch 1450 \t Validation Loss: 37.979660875715055\n",
      "Epoch 36 \t Batch 1500 \t Validation Loss: 38.19001344045003\n",
      "Epoch 36 \t Batch 1550 \t Validation Loss: 37.99613521698983\n",
      "Epoch 36 \t Batch 1600 \t Validation Loss: 37.86403268933296\n",
      "Epoch 36 \t Batch 1650 \t Validation Loss: 37.9597838083903\n",
      "Epoch 36 \t Batch 1700 \t Validation Loss: 37.934828039057116\n",
      "Epoch 36 \t Batch 1750 \t Validation Loss: 37.84950238364083\n",
      "Epoch 36 \t Batch 1800 \t Validation Loss: 37.92118403169844\n",
      "Epoch 36 \t Batch 1850 \t Validation Loss: 38.46105211464135\n",
      "Epoch 36 \t Batch 1900 \t Validation Loss: 38.760435304641724\n",
      "Epoch 36 \t Batch 1950 \t Validation Loss: 38.62812455299573\n",
      "Epoch 36 \t Batch 2000 \t Validation Loss: 38.568793647766114\n",
      "Epoch 36 \t Batch 2050 \t Validation Loss: 38.718865202927006\n",
      "Epoch 36 \t Batch 2100 \t Validation Loss: 38.657394922347294\n",
      "Epoch 36 \t Batch 2150 \t Validation Loss: 38.534375537606174\n",
      "Epoch 36 \t Batch 2200 \t Validation Loss: 38.40473436962475\n",
      "Epoch 36 \t Batch 2250 \t Validation Loss: 38.28488058259752\n",
      "Epoch 36 \t Batch 2300 \t Validation Loss: 38.061059828633844\n",
      "Epoch 36 \t Batch 2350 \t Validation Loss: 38.061441149610154\n",
      "Epoch 36 \t Batch 2400 \t Validation Loss: 38.202198485533394\n",
      "Epoch 36 \t Batch 2450 \t Validation Loss: 38.54944762755414\n",
      "Epoch 36 Training Loss: 29.933002645082794 Validation Loss: 38.733020555276376\n",
      "Epoch 36 completed\n",
      "Epoch 37 \t Batch 50 \t Training Loss: 27.852798881530763\n",
      "Epoch 37 \t Batch 100 \t Training Loss: 27.793265800476075\n",
      "Epoch 37 \t Batch 150 \t Training Loss: 27.976480827331542\n",
      "Epoch 37 \t Batch 200 \t Training Loss: 27.991690425872804\n",
      "Epoch 37 \t Batch 250 \t Training Loss: 28.34029069519043\n",
      "Epoch 37 \t Batch 300 \t Training Loss: 28.31028205235799\n",
      "Epoch 37 \t Batch 350 \t Training Loss: 28.583227860586984\n",
      "Epoch 37 \t Batch 400 \t Training Loss: 28.505527281761168\n",
      "Epoch 37 \t Batch 450 \t Training Loss: 28.40343044281006\n",
      "Epoch 37 \t Batch 500 \t Training Loss: 28.29950820541382\n",
      "Epoch 37 \t Batch 550 \t Training Loss: 28.212284025712446\n",
      "Epoch 37 \t Batch 600 \t Training Loss: 28.277073952356975\n",
      "Epoch 37 \t Batch 650 \t Training Loss: 28.282017581646258\n",
      "Epoch 37 \t Batch 700 \t Training Loss: 28.231750548226493\n",
      "Epoch 37 \t Batch 750 \t Training Loss: 28.200456301371258\n",
      "Epoch 37 \t Batch 800 \t Training Loss: 28.153334159851074\n",
      "Epoch 37 \t Batch 850 \t Training Loss: 28.21426306107465\n",
      "Epoch 37 \t Batch 900 \t Training Loss: 28.217207677629258\n",
      "Epoch 37 \t Batch 950 \t Training Loss: 28.254430756819875\n",
      "Epoch 37 \t Batch 1000 \t Training Loss: 28.33739154243469\n",
      "Epoch 37 \t Batch 1050 \t Training Loss: 28.35170387631371\n",
      "Epoch 37 \t Batch 1100 \t Training Loss: 28.33814274007624\n",
      "Epoch 37 \t Batch 1150 \t Training Loss: 28.32696750972582\n",
      "Epoch 37 \t Batch 1200 \t Training Loss: 28.427911667823793\n",
      "Epoch 37 \t Batch 1250 \t Training Loss: 28.45525789337158\n",
      "Epoch 37 \t Batch 1300 \t Training Loss: 28.492267824319693\n",
      "Epoch 37 \t Batch 1350 \t Training Loss: 28.56914434786196\n",
      "Epoch 37 \t Batch 1400 \t Training Loss: 28.57541752542768\n",
      "Epoch 37 \t Batch 1450 \t Training Loss: 28.543498689059554\n",
      "Epoch 37 \t Batch 1500 \t Training Loss: 28.563486057281494\n",
      "Epoch 37 \t Batch 1550 \t Training Loss: 28.63416504029305\n",
      "Epoch 37 \t Batch 1600 \t Training Loss: 28.670990236997604\n",
      "Epoch 37 \t Batch 1650 \t Training Loss: 28.685958444999926\n",
      "Epoch 37 \t Batch 1700 \t Training Loss: 28.668379564846262\n",
      "Epoch 37 \t Batch 1750 \t Training Loss: 28.68357925088065\n",
      "Epoch 37 \t Batch 1800 \t Training Loss: 28.670935182571412\n",
      "Epoch 37 \t Batch 1850 \t Training Loss: 28.684040650032664\n",
      "Epoch 37 \t Batch 1900 \t Training Loss: 28.66889520845915\n",
      "Epoch 37 \t Batch 1950 \t Training Loss: 28.70387623615754\n",
      "Epoch 37 \t Batch 2000 \t Training Loss: 28.73487260055542\n",
      "Epoch 37 \t Batch 2050 \t Training Loss: 28.745613835962807\n",
      "Epoch 37 \t Batch 2100 \t Training Loss: 28.75170850481306\n",
      "Epoch 37 \t Batch 2150 \t Training Loss: 28.7719255819986\n",
      "Epoch 37 \t Batch 2200 \t Training Loss: 28.790639667510987\n",
      "Epoch 37 \t Batch 2250 \t Training Loss: 28.805413936191133\n",
      "Epoch 37 \t Batch 2300 \t Training Loss: 28.801987308004627\n",
      "Epoch 37 \t Batch 2350 \t Training Loss: 28.811738848584763\n",
      "Epoch 37 \t Batch 2400 \t Training Loss: 28.850994431177774\n",
      "Epoch 37 \t Batch 2450 \t Training Loss: 28.843441580947566\n",
      "Epoch 37 \t Batch 2500 \t Training Loss: 28.86577739868164\n",
      "Epoch 37 \t Batch 2550 \t Training Loss: 28.89984572765874\n",
      "Epoch 37 \t Batch 2600 \t Training Loss: 28.910715718636148\n",
      "Epoch 37 \t Batch 2650 \t Training Loss: 28.923481473382914\n",
      "Epoch 37 \t Batch 2700 \t Training Loss: 28.924030734874584\n",
      "Epoch 37 \t Batch 2750 \t Training Loss: 28.923808054837313\n",
      "Epoch 37 \t Batch 2800 \t Training Loss: 28.93872067110879\n",
      "Epoch 37 \t Batch 2850 \t Training Loss: 28.933089435644316\n",
      "Epoch 37 \t Batch 2900 \t Training Loss: 28.939901617642107\n",
      "Epoch 37 \t Batch 2950 \t Training Loss: 28.935641290697\n",
      "Epoch 37 \t Batch 3000 \t Training Loss: 28.962535313924153\n",
      "Epoch 37 \t Batch 3050 \t Training Loss: 28.97254187537021\n",
      "Epoch 37 \t Batch 3100 \t Training Loss: 28.98145911801246\n",
      "Epoch 37 \t Batch 3150 \t Training Loss: 28.980192165072\n",
      "Epoch 37 \t Batch 3200 \t Training Loss: 28.989317750930788\n",
      "Epoch 37 \t Batch 3250 \t Training Loss: 28.995617612398586\n",
      "Epoch 37 \t Batch 3300 \t Training Loss: 29.026023083889122\n",
      "Epoch 37 \t Batch 3350 \t Training Loss: 29.031128254790804\n",
      "Epoch 37 \t Batch 3400 \t Training Loss: 29.057029783024507\n",
      "Epoch 37 \t Batch 3450 \t Training Loss: 29.063404772661734\n",
      "Epoch 37 \t Batch 3500 \t Training Loss: 29.075075278690882\n",
      "Epoch 37 \t Batch 3550 \t Training Loss: 29.070553126267985\n",
      "Epoch 37 \t Batch 3600 \t Training Loss: 29.081228714519078\n",
      "Epoch 37 \t Batch 3650 \t Training Loss: 29.086543975203003\n",
      "Epoch 37 \t Batch 50 \t Validation Loss: 17.8594322013855\n",
      "Epoch 37 \t Batch 100 \t Validation Loss: 20.898917808532715\n",
      "Epoch 37 \t Batch 150 \t Validation Loss: 20.04728920618693\n",
      "Epoch 37 \t Batch 200 \t Validation Loss: 19.368302671909333\n",
      "Epoch 37 \t Batch 250 \t Validation Loss: 20.595850522994994\n",
      "Epoch 37 \t Batch 300 \t Validation Loss: 20.219705014228822\n",
      "Epoch 37 \t Batch 350 \t Validation Loss: 21.9805710043226\n",
      "Epoch 37 \t Batch 400 \t Validation Loss: 22.33065418601036\n",
      "Epoch 37 \t Batch 450 \t Validation Loss: 23.52461694929335\n",
      "Epoch 37 \t Batch 500 \t Validation Loss: 23.682873185157774\n",
      "Epoch 37 \t Batch 550 \t Validation Loss: 23.87819259036671\n",
      "Epoch 37 \t Batch 600 \t Validation Loss: 24.614799989859264\n",
      "Epoch 37 \t Batch 650 \t Validation Loss: 26.179085585520816\n",
      "Epoch 37 \t Batch 700 \t Validation Loss: 27.67451332296644\n",
      "Epoch 37 \t Batch 750 \t Validation Loss: 28.81293227068583\n",
      "Epoch 37 \t Batch 800 \t Validation Loss: 29.82791970908642\n",
      "Epoch 37 \t Batch 850 \t Validation Loss: 30.61817241388209\n",
      "Epoch 37 \t Batch 900 \t Validation Loss: 30.872690102789136\n",
      "Epoch 37 \t Batch 950 \t Validation Loss: 31.0291727362181\n",
      "Epoch 37 \t Batch 1000 \t Validation Loss: 32.55218925237656\n",
      "Epoch 37 \t Batch 1050 \t Validation Loss: 33.60606233596802\n",
      "Epoch 37 \t Batch 1100 \t Validation Loss: 34.279710109884086\n",
      "Epoch 37 \t Batch 1150 \t Validation Loss: 34.31103363368822\n",
      "Epoch 37 \t Batch 1200 \t Validation Loss: 34.95984299023946\n",
      "Epoch 37 \t Batch 1250 \t Validation Loss: 35.24448514213562\n",
      "Epoch 37 \t Batch 1300 \t Validation Loss: 35.52982609712161\n",
      "Epoch 37 \t Batch 1350 \t Validation Loss: 35.457430076952335\n",
      "Epoch 37 \t Batch 1400 \t Validation Loss: 35.390660216127124\n",
      "Epoch 37 \t Batch 1450 \t Validation Loss: 35.38705769473109\n",
      "Epoch 37 \t Batch 1500 \t Validation Loss: 35.642275052388506\n",
      "Epoch 37 \t Batch 1550 \t Validation Loss: 35.49666623515468\n",
      "Epoch 37 \t Batch 1600 \t Validation Loss: 35.48857722222805\n",
      "Epoch 37 \t Batch 1650 \t Validation Loss: 35.620223292148474\n",
      "Epoch 37 \t Batch 1700 \t Validation Loss: 35.59299989419825\n",
      "Epoch 37 \t Batch 1750 \t Validation Loss: 35.5586640020098\n",
      "Epoch 37 \t Batch 1800 \t Validation Loss: 35.61083094067044\n",
      "Epoch 37 \t Batch 1850 \t Validation Loss: 36.18289600269215\n",
      "Epoch 37 \t Batch 1900 \t Validation Loss: 36.555792833629404\n",
      "Epoch 37 \t Batch 1950 \t Validation Loss: 36.4504915193411\n",
      "Epoch 37 \t Batch 2000 \t Validation Loss: 36.386382036685944\n",
      "Epoch 37 \t Batch 2050 \t Validation Loss: 36.46511540296601\n",
      "Epoch 37 \t Batch 2100 \t Validation Loss: 36.46096324057806\n",
      "Epoch 37 \t Batch 2150 \t Validation Loss: 36.42502181873765\n",
      "Epoch 37 \t Batch 2200 \t Validation Loss: 36.364090545394205\n",
      "Epoch 37 \t Batch 2250 \t Validation Loss: 36.30584331173367\n",
      "Epoch 37 \t Batch 2300 \t Validation Loss: 36.16338618423628\n",
      "Epoch 37 \t Batch 2350 \t Validation Loss: 36.22927650147296\n",
      "Epoch 37 \t Batch 2400 \t Validation Loss: 36.42820506433646\n",
      "Epoch 37 \t Batch 2450 \t Validation Loss: 36.83404018149084\n",
      "Epoch 37 Training Loss: 29.080165975876437 Validation Loss: 37.036797956793336\n",
      "Epoch 37 completed\n",
      "Epoch 38 \t Batch 50 \t Training Loss: 28.56004337310791\n",
      "Epoch 38 \t Batch 100 \t Training Loss: 27.450695152282716\n",
      "Epoch 38 \t Batch 150 \t Training Loss: 27.792624346415202\n",
      "Epoch 38 \t Batch 200 \t Training Loss: 27.61666558265686\n",
      "Epoch 38 \t Batch 250 \t Training Loss: 27.525537803649904\n",
      "Epoch 38 \t Batch 300 \t Training Loss: 27.707556177775064\n",
      "Epoch 38 \t Batch 350 \t Training Loss: 27.652306415012905\n",
      "Epoch 38 \t Batch 400 \t Training Loss: 27.622579278945924\n",
      "Epoch 38 \t Batch 450 \t Training Loss: 27.615872209337024\n",
      "Epoch 38 \t Batch 500 \t Training Loss: 27.72263878631592\n",
      "Epoch 38 \t Batch 550 \t Training Loss: 27.823372320695356\n",
      "Epoch 38 \t Batch 600 \t Training Loss: 27.82839009284973\n",
      "Epoch 38 \t Batch 650 \t Training Loss: 27.788363694411057\n",
      "Epoch 38 \t Batch 700 \t Training Loss: 27.83729137965611\n",
      "Epoch 38 \t Batch 750 \t Training Loss: 27.806516939798993\n",
      "Epoch 38 \t Batch 800 \t Training Loss: 27.781320061683655\n",
      "Epoch 38 \t Batch 850 \t Training Loss: 27.82631597855512\n",
      "Epoch 38 \t Batch 900 \t Training Loss: 27.714038976033528\n",
      "Epoch 38 \t Batch 950 \t Training Loss: 27.675430307890238\n",
      "Epoch 38 \t Batch 1000 \t Training Loss: 27.694491134643556\n",
      "Epoch 38 \t Batch 1050 \t Training Loss: 27.785239639282228\n",
      "Epoch 38 \t Batch 1100 \t Training Loss: 27.83206949407404\n",
      "Epoch 38 \t Batch 1150 \t Training Loss: 27.868863538659138\n",
      "Epoch 38 \t Batch 1200 \t Training Loss: 27.90140653292338\n",
      "Epoch 38 \t Batch 1250 \t Training Loss: 27.93116700439453\n",
      "Epoch 38 \t Batch 1300 \t Training Loss: 27.928837171701286\n",
      "Epoch 38 \t Batch 1350 \t Training Loss: 27.900513037222403\n",
      "Epoch 38 \t Batch 1400 \t Training Loss: 27.90690431731088\n",
      "Epoch 38 \t Batch 1450 \t Training Loss: 27.933928872470197\n",
      "Epoch 38 \t Batch 1500 \t Training Loss: 27.9225290184021\n",
      "Epoch 38 \t Batch 1550 \t Training Loss: 28.03529706278155\n",
      "Epoch 38 \t Batch 1600 \t Training Loss: 28.055603774785997\n",
      "Epoch 38 \t Batch 1650 \t Training Loss: 28.08687208695845\n",
      "Epoch 38 \t Batch 1700 \t Training Loss: 28.10672201941995\n",
      "Epoch 38 \t Batch 1750 \t Training Loss: 28.10073386165074\n",
      "Epoch 38 \t Batch 1800 \t Training Loss: 28.104224333233304\n",
      "Epoch 38 \t Batch 1850 \t Training Loss: 28.130153600331898\n",
      "Epoch 38 \t Batch 1900 \t Training Loss: 28.144066542575235\n",
      "Epoch 38 \t Batch 1950 \t Training Loss: 28.131084821652145\n",
      "Epoch 38 \t Batch 2000 \t Training Loss: 28.16627546405792\n",
      "Epoch 38 \t Batch 2050 \t Training Loss: 28.162387973506277\n",
      "Epoch 38 \t Batch 2100 \t Training Loss: 28.167212701525006\n",
      "Epoch 38 \t Batch 2150 \t Training Loss: 28.1455085035812\n",
      "Epoch 38 \t Batch 2200 \t Training Loss: 28.144002869345925\n",
      "Epoch 38 \t Batch 2250 \t Training Loss: 28.160105998569065\n",
      "Epoch 38 \t Batch 2300 \t Training Loss: 28.156693122697913\n",
      "Epoch 38 \t Batch 2350 \t Training Loss: 28.17319996935256\n",
      "Epoch 38 \t Batch 2400 \t Training Loss: 28.194714380900066\n",
      "Epoch 38 \t Batch 2450 \t Training Loss: 28.17231148778176\n",
      "Epoch 38 \t Batch 2500 \t Training Loss: 28.193856760406494\n",
      "Epoch 38 \t Batch 2550 \t Training Loss: 28.214913855159985\n",
      "Epoch 38 \t Batch 2600 \t Training Loss: 28.214218248954186\n",
      "Epoch 38 \t Batch 2650 \t Training Loss: 28.207058125621867\n",
      "Epoch 38 \t Batch 2700 \t Training Loss: 28.198059418996174\n",
      "Epoch 38 \t Batch 2750 \t Training Loss: 28.19261067685214\n",
      "Epoch 38 \t Batch 2800 \t Training Loss: 28.176609334264484\n",
      "Epoch 38 \t Batch 2850 \t Training Loss: 28.17750556745027\n",
      "Epoch 38 \t Batch 2900 \t Training Loss: 28.1855115587958\n",
      "Epoch 38 \t Batch 2950 \t Training Loss: 28.218155585466807\n",
      "Epoch 38 \t Batch 3000 \t Training Loss: 28.224901043574015\n",
      "Epoch 38 \t Batch 3050 \t Training Loss: 28.224269379162397\n",
      "Epoch 38 \t Batch 3100 \t Training Loss: 28.23487380612281\n",
      "Epoch 38 \t Batch 3150 \t Training Loss: 28.241255604577443\n",
      "Epoch 38 \t Batch 3200 \t Training Loss: 28.263998568058014\n",
      "Epoch 38 \t Batch 3250 \t Training Loss: 28.286746126615085\n",
      "Epoch 38 \t Batch 3300 \t Training Loss: 28.307876353986334\n",
      "Epoch 38 \t Batch 3350 \t Training Loss: 28.315070090792073\n",
      "Epoch 38 \t Batch 3400 \t Training Loss: 28.331564016342163\n",
      "Epoch 38 \t Batch 3450 \t Training Loss: 28.31880537337151\n",
      "Epoch 38 \t Batch 3500 \t Training Loss: 28.316718490055628\n",
      "Epoch 38 \t Batch 3550 \t Training Loss: 28.329326779002876\n",
      "Epoch 38 \t Batch 3600 \t Training Loss: 28.33902987321218\n",
      "Epoch 38 \t Batch 3650 \t Training Loss: 28.324786619421555\n",
      "Epoch 38 \t Batch 50 \t Validation Loss: 20.21067617416382\n",
      "Epoch 38 \t Batch 100 \t Validation Loss: 24.219740552902223\n",
      "Epoch 38 \t Batch 150 \t Validation Loss: 22.407289994557697\n",
      "Epoch 38 \t Batch 200 \t Validation Loss: 22.151129570007324\n",
      "Epoch 38 \t Batch 250 \t Validation Loss: 23.66650786972046\n",
      "Epoch 38 \t Batch 300 \t Validation Loss: 23.110030307769776\n",
      "Epoch 38 \t Batch 350 \t Validation Loss: 24.855795394352505\n",
      "Epoch 38 \t Batch 400 \t Validation Loss: 25.046257581710815\n",
      "Epoch 38 \t Batch 450 \t Validation Loss: 26.095453406439887\n",
      "Epoch 38 \t Batch 500 \t Validation Loss: 26.132606660842896\n",
      "Epoch 38 \t Batch 550 \t Validation Loss: 26.132180063074287\n",
      "Epoch 38 \t Batch 600 \t Validation Loss: 26.66620182355245\n",
      "Epoch 38 \t Batch 650 \t Validation Loss: 28.042394969646747\n",
      "Epoch 38 \t Batch 700 \t Validation Loss: 29.756754621778217\n",
      "Epoch 38 \t Batch 750 \t Validation Loss: 30.89516324742635\n",
      "Epoch 38 \t Batch 800 \t Validation Loss: 31.9329815864563\n",
      "Epoch 38 \t Batch 850 \t Validation Loss: 32.64827227424173\n",
      "Epoch 38 \t Batch 900 \t Validation Loss: 32.878522415161136\n",
      "Epoch 38 \t Batch 950 \t Validation Loss: 33.05041444276509\n",
      "Epoch 38 \t Batch 1000 \t Validation Loss: 34.60842331123352\n",
      "Epoch 38 \t Batch 1050 \t Validation Loss: 35.637401333309356\n",
      "Epoch 38 \t Batch 1100 \t Validation Loss: 36.39936264688318\n",
      "Epoch 38 \t Batch 1150 \t Validation Loss: 36.43950619863427\n",
      "Epoch 38 \t Batch 1200 \t Validation Loss: 37.17059339563052\n",
      "Epoch 38 \t Batch 1250 \t Validation Loss: 37.515988187789915\n",
      "Epoch 38 \t Batch 1300 \t Validation Loss: 37.75460366872641\n",
      "Epoch 38 \t Batch 1350 \t Validation Loss: 37.61454719790706\n",
      "Epoch 38 \t Batch 1400 \t Validation Loss: 37.48218403850283\n",
      "Epoch 38 \t Batch 1450 \t Validation Loss: 37.35010647905284\n",
      "Epoch 38 \t Batch 1500 \t Validation Loss: 37.539001727104186\n",
      "Epoch 38 \t Batch 1550 \t Validation Loss: 37.344934249693345\n",
      "Epoch 38 \t Batch 1600 \t Validation Loss: 37.217009990513326\n",
      "Epoch 38 \t Batch 1650 \t Validation Loss: 37.25639885671211\n",
      "Epoch 38 \t Batch 1700 \t Validation Loss: 37.20962176070494\n",
      "Epoch 38 \t Batch 1750 \t Validation Loss: 37.077379663467404\n",
      "Epoch 38 \t Batch 1800 \t Validation Loss: 36.979760904047225\n",
      "Epoch 38 \t Batch 1850 \t Validation Loss: 37.51715114155331\n",
      "Epoch 38 \t Batch 1900 \t Validation Loss: 37.83219164572264\n",
      "Epoch 38 \t Batch 1950 \t Validation Loss: 37.718071551934266\n",
      "Epoch 38 \t Batch 2000 \t Validation Loss: 37.59861648440361\n",
      "Epoch 38 \t Batch 2050 \t Validation Loss: 37.52521924158422\n",
      "Epoch 38 \t Batch 2100 \t Validation Loss: 37.421180242583866\n",
      "Epoch 38 \t Batch 2150 \t Validation Loss: 37.31111446358437\n",
      "Epoch 38 \t Batch 2200 \t Validation Loss: 37.21079861272465\n",
      "Epoch 38 \t Batch 2250 \t Validation Loss: 37.10292385673523\n",
      "Epoch 38 \t Batch 2300 \t Validation Loss: 36.902367031055945\n",
      "Epoch 38 \t Batch 2350 \t Validation Loss: 36.91664030014201\n",
      "Epoch 38 \t Batch 2400 \t Validation Loss: 37.081349505384765\n",
      "Epoch 38 \t Batch 2450 \t Validation Loss: 37.45087341016653\n",
      "Epoch 38 Training Loss: 28.331460085403958 Validation Loss: 37.642039217151606\n",
      "Epoch 38 completed\n",
      "Epoch 39 \t Batch 50 \t Training Loss: 28.117221221923828\n",
      "Epoch 39 \t Batch 100 \t Training Loss: 27.1577667427063\n",
      "Epoch 39 \t Batch 150 \t Training Loss: 27.447237078348795\n",
      "Epoch 39 \t Batch 200 \t Training Loss: 27.256527643203736\n",
      "Epoch 39 \t Batch 250 \t Training Loss: 27.024928634643555\n",
      "Epoch 39 \t Batch 300 \t Training Loss: 27.024376799265543\n",
      "Epoch 39 \t Batch 350 \t Training Loss: 26.83115298679897\n",
      "Epoch 39 \t Batch 400 \t Training Loss: 26.800362644195555\n",
      "Epoch 39 \t Batch 450 \t Training Loss: 26.726760864257812\n",
      "Epoch 39 \t Batch 500 \t Training Loss: 26.633050605773924\n",
      "Epoch 39 \t Batch 550 \t Training Loss: 26.62191265106201\n",
      "Epoch 39 \t Batch 600 \t Training Loss: 26.70251792271932\n",
      "Epoch 39 \t Batch 650 \t Training Loss: 26.75321114760179\n",
      "Epoch 39 \t Batch 700 \t Training Loss: 26.75136141368321\n",
      "Epoch 39 \t Batch 750 \t Training Loss: 26.763051902770997\n",
      "Epoch 39 \t Batch 800 \t Training Loss: 26.782703118324278\n",
      "Epoch 39 \t Batch 850 \t Training Loss: 26.7471934150247\n",
      "Epoch 39 \t Batch 900 \t Training Loss: 26.792487076653373\n",
      "Epoch 39 \t Batch 950 \t Training Loss: 26.80681706679495\n",
      "Epoch 39 \t Batch 1000 \t Training Loss: 26.805352853775023\n",
      "Epoch 39 \t Batch 1050 \t Training Loss: 26.762397969563803\n",
      "Epoch 39 \t Batch 1100 \t Training Loss: 26.814742905009876\n",
      "Epoch 39 \t Batch 1150 \t Training Loss: 26.83296068440313\n",
      "Epoch 39 \t Batch 1200 \t Training Loss: 26.808710521062213\n",
      "Epoch 39 \t Batch 1250 \t Training Loss: 26.802419816589357\n",
      "Epoch 39 \t Batch 1300 \t Training Loss: 26.81041911491981\n",
      "Epoch 39 \t Batch 1350 \t Training Loss: 26.871301691973652\n",
      "Epoch 39 \t Batch 1400 \t Training Loss: 26.852504880087718\n",
      "Epoch 39 \t Batch 1450 \t Training Loss: 26.814847758062953\n",
      "Epoch 39 \t Batch 1500 \t Training Loss: 26.83913244374593\n",
      "Epoch 39 \t Batch 1550 \t Training Loss: 26.877475291836646\n",
      "Epoch 39 \t Batch 1600 \t Training Loss: 26.938078730106355\n",
      "Epoch 39 \t Batch 1650 \t Training Loss: 26.954177784775244\n",
      "Epoch 39 \t Batch 1700 \t Training Loss: 26.94333190020393\n",
      "Epoch 39 \t Batch 1750 \t Training Loss: 26.928164116995674\n",
      "Epoch 39 \t Batch 1800 \t Training Loss: 26.90930308977763\n",
      "Epoch 39 \t Batch 1850 \t Training Loss: 26.91653327117095\n",
      "Epoch 39 \t Batch 1900 \t Training Loss: 26.95975710417095\n",
      "Epoch 39 \t Batch 1950 \t Training Loss: 26.951439993442634\n",
      "Epoch 39 \t Batch 2000 \t Training Loss: 26.98922145652771\n",
      "Epoch 39 \t Batch 2050 \t Training Loss: 26.9924728281905\n",
      "Epoch 39 \t Batch 2100 \t Training Loss: 27.025194844745453\n",
      "Epoch 39 \t Batch 2150 \t Training Loss: 27.019701239120128\n",
      "Epoch 39 \t Batch 2200 \t Training Loss: 27.028768372969193\n",
      "Epoch 39 \t Batch 2250 \t Training Loss: 27.061469061109754\n",
      "Epoch 39 \t Batch 2300 \t Training Loss: 27.060748885610828\n",
      "Epoch 39 \t Batch 2350 \t Training Loss: 27.06670620208091\n",
      "Epoch 39 \t Batch 2400 \t Training Loss: 27.096319338480633\n",
      "Epoch 39 \t Batch 2450 \t Training Loss: 27.097737750617824\n",
      "Epoch 39 \t Batch 2500 \t Training Loss: 27.11917399368286\n",
      "Epoch 39 \t Batch 2550 \t Training Loss: 27.169490088668525\n",
      "Epoch 39 \t Batch 2600 \t Training Loss: 27.175267226145817\n",
      "Epoch 39 \t Batch 2650 \t Training Loss: 27.151881597267007\n",
      "Epoch 39 \t Batch 2700 \t Training Loss: 27.157865700191923\n",
      "Epoch 39 \t Batch 2750 \t Training Loss: 27.17760934864391\n",
      "Epoch 39 \t Batch 2800 \t Training Loss: 27.204745368957518\n",
      "Epoch 39 \t Batch 2850 \t Training Loss: 27.223836807117127\n",
      "Epoch 39 \t Batch 2900 \t Training Loss: 27.22618623996603\n",
      "Epoch 39 \t Batch 2950 \t Training Loss: 27.24070870932886\n",
      "Epoch 39 \t Batch 3000 \t Training Loss: 27.26008162625631\n",
      "Epoch 39 \t Batch 3050 \t Training Loss: 27.283639486344136\n",
      "Epoch 39 \t Batch 3100 \t Training Loss: 27.31789735424903\n",
      "Epoch 39 \t Batch 3150 \t Training Loss: 27.32921965947227\n",
      "Epoch 39 \t Batch 3200 \t Training Loss: 27.345546471476556\n",
      "Epoch 39 \t Batch 3250 \t Training Loss: 27.356923985407903\n",
      "Epoch 39 \t Batch 3300 \t Training Loss: 27.364018446026428\n",
      "Epoch 39 \t Batch 3350 \t Training Loss: 27.372852466640186\n",
      "Epoch 39 \t Batch 3400 \t Training Loss: 27.371155345580156\n",
      "Epoch 39 \t Batch 3450 \t Training Loss: 27.376456114727517\n",
      "Epoch 39 \t Batch 3500 \t Training Loss: 27.380811458587647\n",
      "Epoch 39 \t Batch 3550 \t Training Loss: 27.391703558371102\n",
      "Epoch 39 \t Batch 3600 \t Training Loss: 27.398334963056776\n",
      "Epoch 39 \t Batch 3650 \t Training Loss: 27.420161755705532\n",
      "Epoch 39 \t Batch 50 \t Validation Loss: 22.966299648284913\n",
      "Epoch 39 \t Batch 100 \t Validation Loss: 28.45473747253418\n",
      "Epoch 39 \t Batch 150 \t Validation Loss: 26.405430717468263\n",
      "Epoch 39 \t Batch 200 \t Validation Loss: 25.51524734735489\n",
      "Epoch 39 \t Batch 250 \t Validation Loss: 27.239671411514284\n",
      "Epoch 39 \t Batch 300 \t Validation Loss: 26.412779506047567\n",
      "Epoch 39 \t Batch 350 \t Validation Loss: 27.830591561453684\n",
      "Epoch 39 \t Batch 400 \t Validation Loss: 27.681601927280425\n",
      "Epoch 39 \t Batch 450 \t Validation Loss: 28.37489268620809\n",
      "Epoch 39 \t Batch 500 \t Validation Loss: 28.143997064590454\n",
      "Epoch 39 \t Batch 550 \t Validation Loss: 27.963325607993387\n",
      "Epoch 39 \t Batch 600 \t Validation Loss: 28.44820421854655\n",
      "Epoch 39 \t Batch 650 \t Validation Loss: 29.771318388718825\n",
      "Epoch 39 \t Batch 700 \t Validation Loss: 31.393160934448243\n",
      "Epoch 39 \t Batch 750 \t Validation Loss: 32.43220265833537\n",
      "Epoch 39 \t Batch 800 \t Validation Loss: 33.533303421735766\n",
      "Epoch 39 \t Batch 850 \t Validation Loss: 34.18509352852316\n",
      "Epoch 39 \t Batch 900 \t Validation Loss: 34.37678964614868\n",
      "Epoch 39 \t Batch 950 \t Validation Loss: 34.499592428709335\n",
      "Epoch 39 \t Batch 1000 \t Validation Loss: 36.0371660528183\n",
      "Epoch 39 \t Batch 1050 \t Validation Loss: 37.02484189714704\n",
      "Epoch 39 \t Batch 1100 \t Validation Loss: 37.71315338741649\n",
      "Epoch 39 \t Batch 1150 \t Validation Loss: 37.73157103331193\n",
      "Epoch 39 \t Batch 1200 \t Validation Loss: 38.46518133004506\n",
      "Epoch 39 \t Batch 1250 \t Validation Loss: 38.762105486679076\n",
      "Epoch 39 \t Batch 1300 \t Validation Loss: 38.96412606202639\n",
      "Epoch 39 \t Batch 1350 \t Validation Loss: 38.78872170095091\n",
      "Epoch 39 \t Batch 1400 \t Validation Loss: 38.66423216853823\n",
      "Epoch 39 \t Batch 1450 \t Validation Loss: 38.63065451227386\n",
      "Epoch 39 \t Batch 1500 \t Validation Loss: 38.81703028043111\n",
      "Epoch 39 \t Batch 1550 \t Validation Loss: 38.57350057724983\n",
      "Epoch 39 \t Batch 1600 \t Validation Loss: 38.43831037342548\n",
      "Epoch 39 \t Batch 1650 \t Validation Loss: 38.47602883367828\n",
      "Epoch 39 \t Batch 1700 \t Validation Loss: 38.360769451926735\n",
      "Epoch 39 \t Batch 1750 \t Validation Loss: 38.234146187918526\n",
      "Epoch 39 \t Batch 1800 \t Validation Loss: 38.22462913831075\n",
      "Epoch 39 \t Batch 1850 \t Validation Loss: 38.71314398791339\n",
      "Epoch 39 \t Batch 1900 \t Validation Loss: 39.010038434078815\n",
      "Epoch 39 \t Batch 1950 \t Validation Loss: 38.84585235889141\n",
      "Epoch 39 \t Batch 2000 \t Validation Loss: 38.73310733032226\n",
      "Epoch 39 \t Batch 2050 \t Validation Loss: 38.77919280866297\n",
      "Epoch 39 \t Batch 2100 \t Validation Loss: 38.733765042168756\n",
      "Epoch 39 \t Batch 2150 \t Validation Loss: 38.6439949696563\n",
      "Epoch 39 \t Batch 2200 \t Validation Loss: 38.565312964266\n",
      "Epoch 39 \t Batch 2250 \t Validation Loss: 38.49694553671943\n",
      "Epoch 39 \t Batch 2300 \t Validation Loss: 38.31718352815379\n",
      "Epoch 39 \t Batch 2350 \t Validation Loss: 38.36233802105518\n",
      "Epoch 39 \t Batch 2400 \t Validation Loss: 38.5286711593469\n",
      "Epoch 39 \t Batch 2450 \t Validation Loss: 38.91894835608346\n",
      "Epoch 39 Training Loss: 27.420809851347943 Validation Loss: 39.132387790974086\n",
      "Epoch 39 completed\n",
      "Epoch 40 \t Batch 50 \t Training Loss: 27.768224754333495\n",
      "Epoch 40 \t Batch 100 \t Training Loss: 27.298550968170165\n",
      "Epoch 40 \t Batch 150 \t Training Loss: 26.794561297098795\n",
      "Epoch 40 \t Batch 200 \t Training Loss: 26.33505796432495\n",
      "Epoch 40 \t Batch 250 \t Training Loss: 26.526124549865724\n",
      "Epoch 40 \t Batch 300 \t Training Loss: 26.251112429300942\n",
      "Epoch 40 \t Batch 350 \t Training Loss: 26.21450474330357\n",
      "Epoch 40 \t Batch 400 \t Training Loss: 26.199235291481017\n",
      "Epoch 40 \t Batch 450 \t Training Loss: 26.084905344645183\n",
      "Epoch 40 \t Batch 500 \t Training Loss: 26.065104141235352\n",
      "Epoch 40 \t Batch 550 \t Training Loss: 26.059930371371184\n",
      "Epoch 40 \t Batch 600 \t Training Loss: 25.95862042427063\n",
      "Epoch 40 \t Batch 650 \t Training Loss: 25.912162296588605\n",
      "Epoch 40 \t Batch 700 \t Training Loss: 25.953047978537423\n",
      "Epoch 40 \t Batch 750 \t Training Loss: 25.936481503804526\n",
      "Epoch 40 \t Batch 800 \t Training Loss: 25.973122658729555\n",
      "Epoch 40 \t Batch 850 \t Training Loss: 26.04704389011159\n",
      "Epoch 40 \t Batch 900 \t Training Loss: 26.05617223315769\n",
      "Epoch 40 \t Batch 950 \t Training Loss: 26.067225084806743\n",
      "Epoch 40 \t Batch 1000 \t Training Loss: 26.11637512397766\n",
      "Epoch 40 \t Batch 1050 \t Training Loss: 26.182582633608863\n",
      "Epoch 40 \t Batch 1100 \t Training Loss: 26.182581972642378\n",
      "Epoch 40 \t Batch 1150 \t Training Loss: 26.171041017615277\n",
      "Epoch 40 \t Batch 1200 \t Training Loss: 26.177062643369037\n",
      "Epoch 40 \t Batch 1250 \t Training Loss: 26.162795164489747\n",
      "Epoch 40 \t Batch 1300 \t Training Loss: 26.226822603665866\n",
      "Epoch 40 \t Batch 1350 \t Training Loss: 26.240864168802897\n",
      "Epoch 40 \t Batch 1400 \t Training Loss: 26.266495703288488\n",
      "Epoch 40 \t Batch 1450 \t Training Loss: 26.291242207494275\n",
      "Epoch 40 \t Batch 1500 \t Training Loss: 26.333665927886962\n",
      "Epoch 40 \t Batch 1550 \t Training Loss: 26.3624901593116\n",
      "Epoch 40 \t Batch 1600 \t Training Loss: 26.36822124838829\n",
      "Epoch 40 \t Batch 1650 \t Training Loss: 26.412256017742735\n",
      "Epoch 40 \t Batch 1700 \t Training Loss: 26.404145838793585\n",
      "Epoch 40 \t Batch 1750 \t Training Loss: 26.386907432556153\n",
      "Epoch 40 \t Batch 1800 \t Training Loss: 26.3947048441569\n",
      "Epoch 40 \t Batch 1850 \t Training Loss: 26.36248589386811\n",
      "Epoch 40 \t Batch 1900 \t Training Loss: 26.372773654335422\n",
      "Epoch 40 \t Batch 1950 \t Training Loss: 26.423496220906575\n",
      "Epoch 40 \t Batch 2000 \t Training Loss: 26.432090762138365\n",
      "Epoch 40 \t Batch 2050 \t Training Loss: 26.41661409331531\n",
      "Epoch 40 \t Batch 2100 \t Training Loss: 26.42698699133737\n",
      "Epoch 40 \t Batch 2150 \t Training Loss: 26.440993325876637\n",
      "Epoch 40 \t Batch 2200 \t Training Loss: 26.44274387879805\n",
      "Epoch 40 \t Batch 2250 \t Training Loss: 26.430391509162057\n",
      "Epoch 40 \t Batch 2300 \t Training Loss: 26.427318004939867\n",
      "Epoch 40 \t Batch 2350 \t Training Loss: 26.434762787514543\n",
      "Epoch 40 \t Batch 2400 \t Training Loss: 26.45091963370641\n",
      "Epoch 40 \t Batch 2450 \t Training Loss: 26.469231474740166\n",
      "Epoch 40 \t Batch 2500 \t Training Loss: 26.46205188522339\n",
      "Epoch 40 \t Batch 2550 \t Training Loss: 26.484638839422487\n",
      "Epoch 40 \t Batch 2600 \t Training Loss: 26.493513533518865\n",
      "Epoch 40 \t Batch 2650 \t Training Loss: 26.505235431599168\n",
      "Epoch 40 \t Batch 2700 \t Training Loss: 26.50076042316578\n",
      "Epoch 40 \t Batch 2750 \t Training Loss: 26.534989967346192\n",
      "Epoch 40 \t Batch 2800 \t Training Loss: 26.523130234990802\n",
      "Epoch 40 \t Batch 2850 \t Training Loss: 26.53506016179135\n",
      "Epoch 40 \t Batch 2900 \t Training Loss: 26.553776188554433\n",
      "Epoch 40 \t Batch 2950 \t Training Loss: 26.56883940292617\n",
      "Epoch 40 \t Batch 3000 \t Training Loss: 26.554928990046182\n",
      "Epoch 40 \t Batch 3050 \t Training Loss: 26.55499472883881\n",
      "Epoch 40 \t Batch 3100 \t Training Loss: 26.560661173789732\n",
      "Epoch 40 \t Batch 3150 \t Training Loss: 26.5693060435946\n",
      "Epoch 40 \t Batch 3200 \t Training Loss: 26.567472572922707\n",
      "Epoch 40 \t Batch 3250 \t Training Loss: 26.574030190101038\n",
      "Epoch 40 \t Batch 3300 \t Training Loss: 26.557146487958505\n",
      "Epoch 40 \t Batch 3350 \t Training Loss: 26.556721073549184\n",
      "Epoch 40 \t Batch 3400 \t Training Loss: 26.580714386771707\n",
      "Epoch 40 \t Batch 3450 \t Training Loss: 26.570698286968728\n",
      "Epoch 40 \t Batch 3500 \t Training Loss: 26.56728748212542\n",
      "Epoch 40 \t Batch 3550 \t Training Loss: 26.564889209371216\n",
      "Epoch 40 \t Batch 3600 \t Training Loss: 26.572977705001833\n",
      "Epoch 40 \t Batch 3650 \t Training Loss: 26.580644290349255\n",
      "Epoch 40 \t Batch 50 \t Validation Loss: 24.999479904174805\n",
      "Epoch 40 \t Batch 100 \t Validation Loss: 30.773040618896484\n",
      "Epoch 40 \t Batch 150 \t Validation Loss: 28.765900630950927\n",
      "Epoch 40 \t Batch 200 \t Validation Loss: 27.588330063819885\n",
      "Epoch 40 \t Batch 250 \t Validation Loss: 29.25291046524048\n",
      "Epoch 40 \t Batch 300 \t Validation Loss: 28.26799176375071\n",
      "Epoch 40 \t Batch 350 \t Validation Loss: 29.258808142798287\n",
      "Epoch 40 \t Batch 400 \t Validation Loss: 28.885268613100052\n",
      "Epoch 40 \t Batch 450 \t Validation Loss: 29.46538339720832\n",
      "Epoch 40 \t Batch 500 \t Validation Loss: 29.164072226524354\n",
      "Epoch 40 \t Batch 550 \t Validation Loss: 28.89798542802984\n",
      "Epoch 40 \t Batch 600 \t Validation Loss: 29.20928447643916\n",
      "Epoch 40 \t Batch 650 \t Validation Loss: 30.432428256548366\n",
      "Epoch 40 \t Batch 700 \t Validation Loss: 31.570595465387616\n",
      "Epoch 40 \t Batch 750 \t Validation Loss: 32.2130395450592\n",
      "Epoch 40 \t Batch 800 \t Validation Loss: 32.87209410727024\n",
      "Epoch 40 \t Batch 850 \t Validation Loss: 33.38441445911632\n",
      "Epoch 40 \t Batch 900 \t Validation Loss: 33.469097621175976\n",
      "Epoch 40 \t Batch 950 \t Validation Loss: 33.43322257343092\n",
      "Epoch 40 \t Batch 1000 \t Validation Loss: 34.57668507909775\n",
      "Epoch 40 \t Batch 1050 \t Validation Loss: 35.480978647867836\n",
      "Epoch 40 \t Batch 1100 \t Validation Loss: 35.964598614952784\n",
      "Epoch 40 \t Batch 1150 \t Validation Loss: 35.92854794792507\n",
      "Epoch 40 \t Batch 1200 \t Validation Loss: 36.51888852675756\n",
      "Epoch 40 \t Batch 1250 \t Validation Loss: 36.73497994575501\n",
      "Epoch 40 \t Batch 1300 \t Validation Loss: 37.015637882306024\n",
      "Epoch 40 \t Batch 1350 \t Validation Loss: 36.84169235335456\n",
      "Epoch 40 \t Batch 1400 \t Validation Loss: 36.76277527298246\n",
      "Epoch 40 \t Batch 1450 \t Validation Loss: 36.799360596558145\n",
      "Epoch 40 \t Batch 1500 \t Validation Loss: 36.96068119955063\n",
      "Epoch 40 \t Batch 1550 \t Validation Loss: 36.79157886397454\n",
      "Epoch 40 \t Batch 1600 \t Validation Loss: 36.73461421564221\n",
      "Epoch 40 \t Batch 1650 \t Validation Loss: 36.84073100422368\n",
      "Epoch 40 \t Batch 1700 \t Validation Loss: 36.80436190843582\n",
      "Epoch 40 \t Batch 1750 \t Validation Loss: 36.74501748234885\n",
      "Epoch 40 \t Batch 1800 \t Validation Loss: 36.82145670268271\n",
      "Epoch 40 \t Batch 1850 \t Validation Loss: 37.38848425903836\n",
      "Epoch 40 \t Batch 1900 \t Validation Loss: 37.71603754256901\n",
      "Epoch 40 \t Batch 1950 \t Validation Loss: 37.56688472026433\n",
      "Epoch 40 \t Batch 2000 \t Validation Loss: 37.4981836925745\n",
      "Epoch 40 \t Batch 2050 \t Validation Loss: 37.64310286138116\n",
      "Epoch 40 \t Batch 2100 \t Validation Loss: 37.582809584594905\n",
      "Epoch 40 \t Batch 2150 \t Validation Loss: 37.47200292287871\n",
      "Epoch 40 \t Batch 2200 \t Validation Loss: 37.352781369577755\n",
      "Epoch 40 \t Batch 2250 \t Validation Loss: 37.22899977503882\n",
      "Epoch 40 \t Batch 2300 \t Validation Loss: 37.03198906058851\n",
      "Epoch 40 \t Batch 2350 \t Validation Loss: 37.01084310480889\n",
      "Epoch 40 \t Batch 2400 \t Validation Loss: 37.15550345927477\n",
      "Epoch 40 \t Batch 2450 \t Validation Loss: 37.51830726458102\n",
      "Epoch 40 Training Loss: 26.580345902084133 Validation Loss: 37.71345061750768\n",
      "Epoch 40 completed\n",
      "Epoch 41 \t Batch 50 \t Training Loss: 25.105394134521486\n",
      "Epoch 41 \t Batch 100 \t Training Loss: 25.96815402984619\n",
      "Epoch 41 \t Batch 150 \t Training Loss: 25.72633940378825\n",
      "Epoch 41 \t Batch 200 \t Training Loss: 26.020821828842163\n",
      "Epoch 41 \t Batch 250 \t Training Loss: 25.947854042053223\n",
      "Epoch 41 \t Batch 300 \t Training Loss: 25.808687680562336\n",
      "Epoch 41 \t Batch 350 \t Training Loss: 25.904506710597445\n",
      "Epoch 41 \t Batch 400 \t Training Loss: 25.7301944732666\n",
      "Epoch 41 \t Batch 450 \t Training Loss: 25.704058312310114\n",
      "Epoch 41 \t Batch 500 \t Training Loss: 25.70542573928833\n",
      "Epoch 41 \t Batch 550 \t Training Loss: 25.644759459062055\n",
      "Epoch 41 \t Batch 600 \t Training Loss: 25.55777825991313\n",
      "Epoch 41 \t Batch 650 \t Training Loss: 25.51426291832557\n",
      "Epoch 41 \t Batch 700 \t Training Loss: 25.41341335569109\n",
      "Epoch 41 \t Batch 750 \t Training Loss: 25.401592679341633\n",
      "Epoch 41 \t Batch 800 \t Training Loss: 25.41846806049347\n",
      "Epoch 41 \t Batch 850 \t Training Loss: 25.40251735014074\n",
      "Epoch 41 \t Batch 900 \t Training Loss: 25.37253215154012\n",
      "Epoch 41 \t Batch 950 \t Training Loss: 25.402693407159102\n",
      "Epoch 41 \t Batch 1000 \t Training Loss: 25.408706327438356\n",
      "Epoch 41 \t Batch 1050 \t Training Loss: 25.38864701770601\n",
      "Epoch 41 \t Batch 1100 \t Training Loss: 25.340903785011985\n",
      "Epoch 41 \t Batch 1150 \t Training Loss: 25.322994159200917\n",
      "Epoch 41 \t Batch 1200 \t Training Loss: 25.336254698435464\n",
      "Epoch 41 \t Batch 1250 \t Training Loss: 25.32877546081543\n",
      "Epoch 41 \t Batch 1300 \t Training Loss: 25.408408839886004\n",
      "Epoch 41 \t Batch 1350 \t Training Loss: 25.444190288119845\n",
      "Epoch 41 \t Batch 1400 \t Training Loss: 25.438897395815168\n",
      "Epoch 41 \t Batch 1450 \t Training Loss: 25.456008629634464\n",
      "Epoch 41 \t Batch 1500 \t Training Loss: 25.4853751856486\n",
      "Epoch 41 \t Batch 1550 \t Training Loss: 25.503932771990375\n",
      "Epoch 41 \t Batch 1600 \t Training Loss: 25.495485447645187\n",
      "Epoch 41 \t Batch 1650 \t Training Loss: 25.478847019311154\n",
      "Epoch 41 \t Batch 1700 \t Training Loss: 25.52070831186631\n",
      "Epoch 41 \t Batch 1750 \t Training Loss: 25.529581261771067\n",
      "Epoch 41 \t Batch 1800 \t Training Loss: 25.550536816914875\n",
      "Epoch 41 \t Batch 1850 \t Training Loss: 25.563446723319387\n",
      "Epoch 41 \t Batch 1900 \t Training Loss: 25.620826651924535\n",
      "Epoch 41 \t Batch 1950 \t Training Loss: 25.60880871503781\n",
      "Epoch 41 \t Batch 2000 \t Training Loss: 25.59803964519501\n",
      "Epoch 41 \t Batch 2050 \t Training Loss: 25.604641567323267\n",
      "Epoch 41 \t Batch 2100 \t Training Loss: 25.623459483555386\n",
      "Epoch 41 \t Batch 2150 \t Training Loss: 25.598978805541993\n",
      "Epoch 41 \t Batch 2200 \t Training Loss: 25.590540390881625\n",
      "Epoch 41 \t Batch 2250 \t Training Loss: 25.575085695054796\n",
      "Epoch 41 \t Batch 2300 \t Training Loss: 25.581287293641463\n",
      "Epoch 41 \t Batch 2350 \t Training Loss: 25.57916649270565\n",
      "Epoch 41 \t Batch 2400 \t Training Loss: 25.58351431528727\n",
      "Epoch 41 \t Batch 2450 \t Training Loss: 25.585223347410864\n",
      "Epoch 41 \t Batch 2500 \t Training Loss: 25.613639945983888\n",
      "Epoch 41 \t Batch 2550 \t Training Loss: 25.607076249216117\n",
      "Epoch 41 \t Batch 2600 \t Training Loss: 25.61845663070679\n",
      "Epoch 41 \t Batch 2650 \t Training Loss: 25.63121171339503\n",
      "Epoch 41 \t Batch 2700 \t Training Loss: 25.635502683145027\n",
      "Epoch 41 \t Batch 2750 \t Training Loss: 25.658787291093304\n",
      "Epoch 41 \t Batch 2800 \t Training Loss: 25.662024376051768\n",
      "Epoch 41 \t Batch 2850 \t Training Loss: 25.67262643178304\n",
      "Epoch 41 \t Batch 2900 \t Training Loss: 25.66679020980309\n",
      "Epoch 41 \t Batch 2950 \t Training Loss: 25.676037361662267\n",
      "Epoch 41 \t Batch 3000 \t Training Loss: 25.69789864985148\n",
      "Epoch 41 \t Batch 3050 \t Training Loss: 25.72143883689505\n",
      "Epoch 41 \t Batch 3100 \t Training Loss: 25.728235334580944\n",
      "Epoch 41 \t Batch 3150 \t Training Loss: 25.739156838068887\n",
      "Epoch 41 \t Batch 3200 \t Training Loss: 25.755910888314247\n",
      "Epoch 41 \t Batch 3250 \t Training Loss: 25.764892979548527\n",
      "Epoch 41 \t Batch 3300 \t Training Loss: 25.770418294270833\n",
      "Epoch 41 \t Batch 3350 \t Training Loss: 25.78652129557595\n",
      "Epoch 41 \t Batch 3400 \t Training Loss: 25.79143866931691\n",
      "Epoch 41 \t Batch 3450 \t Training Loss: 25.815991848378943\n",
      "Epoch 41 \t Batch 3500 \t Training Loss: 25.831590544019427\n",
      "Epoch 41 \t Batch 3550 \t Training Loss: 25.84483985954607\n",
      "Epoch 41 \t Batch 3600 \t Training Loss: 25.834917419221664\n",
      "Epoch 41 \t Batch 3650 \t Training Loss: 25.847899586925767\n",
      "Epoch 41 \t Batch 50 \t Validation Loss: 15.16427206993103\n",
      "Epoch 41 \t Batch 100 \t Validation Loss: 20.024719014167786\n",
      "Epoch 41 \t Batch 150 \t Validation Loss: 18.442970606486004\n",
      "Epoch 41 \t Batch 200 \t Validation Loss: 18.197819221019746\n",
      "Epoch 41 \t Batch 250 \t Validation Loss: 19.366324655532836\n",
      "Epoch 41 \t Batch 300 \t Validation Loss: 18.839980923334757\n",
      "Epoch 41 \t Batch 350 \t Validation Loss: 20.903168369020733\n",
      "Epoch 41 \t Batch 400 \t Validation Loss: 21.50649532198906\n",
      "Epoch 41 \t Batch 450 \t Validation Loss: 22.899660113652548\n",
      "Epoch 41 \t Batch 500 \t Validation Loss: 23.2079362077713\n",
      "Epoch 41 \t Batch 550 \t Validation Loss: 23.473016689473933\n",
      "Epoch 41 \t Batch 600 \t Validation Loss: 24.333316943645478\n",
      "Epoch 41 \t Batch 650 \t Validation Loss: 26.130814083539523\n",
      "Epoch 41 \t Batch 700 \t Validation Loss: 28.084995079040528\n",
      "Epoch 41 \t Batch 750 \t Validation Loss: 29.357822324117024\n",
      "Epoch 41 \t Batch 800 \t Validation Loss: 30.721973648071287\n",
      "Epoch 41 \t Batch 850 \t Validation Loss: 31.51842451207778\n",
      "Epoch 41 \t Batch 900 \t Validation Loss: 31.84797976175944\n",
      "Epoch 41 \t Batch 950 \t Validation Loss: 32.05597287228233\n",
      "Epoch 41 \t Batch 1000 \t Validation Loss: 33.687696744918824\n",
      "Epoch 41 \t Batch 1050 \t Validation Loss: 34.73975139935811\n",
      "Epoch 41 \t Batch 1100 \t Validation Loss: 35.505370406671005\n",
      "Epoch 41 \t Batch 1150 \t Validation Loss: 35.60432983232581\n",
      "Epoch 41 \t Batch 1200 \t Validation Loss: 36.44075613498688\n",
      "Epoch 41 \t Batch 1250 \t Validation Loss: 36.7890958026886\n",
      "Epoch 41 \t Batch 1300 \t Validation Loss: 37.092120881080625\n",
      "Epoch 41 \t Batch 1350 \t Validation Loss: 36.99385451458119\n",
      "Epoch 41 \t Batch 1400 \t Validation Loss: 36.94885808706284\n",
      "Epoch 41 \t Batch 1450 \t Validation Loss: 36.96650938066943\n",
      "Epoch 41 \t Batch 1500 \t Validation Loss: 37.20812434101105\n",
      "Epoch 41 \t Batch 1550 \t Validation Loss: 37.05428891489583\n",
      "Epoch 41 \t Batch 1600 \t Validation Loss: 37.01284922748804\n",
      "Epoch 41 \t Batch 1650 \t Validation Loss: 37.13001355460196\n",
      "Epoch 41 \t Batch 1700 \t Validation Loss: 37.128649938527275\n",
      "Epoch 41 \t Batch 1750 \t Validation Loss: 37.0611326487405\n",
      "Epoch 41 \t Batch 1800 \t Validation Loss: 37.18113779094484\n",
      "Epoch 41 \t Batch 1850 \t Validation Loss: 37.72051417505419\n",
      "Epoch 41 \t Batch 1900 \t Validation Loss: 38.05392412311152\n",
      "Epoch 41 \t Batch 1950 \t Validation Loss: 37.92861263739757\n",
      "Epoch 41 \t Batch 2000 \t Validation Loss: 37.867169955015186\n",
      "Epoch 41 \t Batch 2050 \t Validation Loss: 38.01089045850242\n",
      "Epoch 41 \t Batch 2100 \t Validation Loss: 38.00268394947052\n",
      "Epoch 41 \t Batch 2150 \t Validation Loss: 37.91519161423972\n",
      "Epoch 41 \t Batch 2200 \t Validation Loss: 37.84360974853689\n",
      "Epoch 41 \t Batch 2250 \t Validation Loss: 37.76605145200094\n",
      "Epoch 41 \t Batch 2300 \t Validation Loss: 37.59782942315807\n",
      "Epoch 41 \t Batch 2350 \t Validation Loss: 37.63911910199104\n",
      "Epoch 41 \t Batch 2400 \t Validation Loss: 37.80156539678573\n",
      "Epoch 41 \t Batch 2450 \t Validation Loss: 38.19094677555318\n",
      "Epoch 41 Training Loss: 25.848578015226575 Validation Loss: 38.383609028992716\n",
      "Epoch 41 completed\n",
      "Epoch 42 \t Batch 50 \t Training Loss: 26.103903427124024\n",
      "Epoch 42 \t Batch 100 \t Training Loss: 25.586956596374513\n",
      "Epoch 42 \t Batch 150 \t Training Loss: 25.173847490946454\n",
      "Epoch 42 \t Batch 200 \t Training Loss: 24.740714502334594\n",
      "Epoch 42 \t Batch 250 \t Training Loss: 24.792255645751954\n",
      "Epoch 42 \t Batch 300 \t Training Loss: 24.725786762237547\n",
      "Epoch 42 \t Batch 350 \t Training Loss: 24.60383505140032\n",
      "Epoch 42 \t Batch 400 \t Training Loss: 24.68098783969879\n",
      "Epoch 42 \t Batch 450 \t Training Loss: 24.69022441864014\n",
      "Epoch 42 \t Batch 500 \t Training Loss: 24.653314559936522\n",
      "Epoch 42 \t Batch 550 \t Training Loss: 24.672065922130237\n",
      "Epoch 42 \t Batch 600 \t Training Loss: 24.600520102183022\n",
      "Epoch 42 \t Batch 650 \t Training Loss: 24.549062696603627\n",
      "Epoch 42 \t Batch 700 \t Training Loss: 24.56132699421474\n",
      "Epoch 42 \t Batch 750 \t Training Loss: 24.484483484903972\n",
      "Epoch 42 \t Batch 800 \t Training Loss: 24.38710339784622\n",
      "Epoch 42 \t Batch 850 \t Training Loss: 24.373737867018757\n",
      "Epoch 42 \t Batch 900 \t Training Loss: 24.36241275363498\n",
      "Epoch 42 \t Batch 950 \t Training Loss: 24.371858002512077\n",
      "Epoch 42 \t Batch 1000 \t Training Loss: 24.360766677856446\n",
      "Epoch 42 \t Batch 1050 \t Training Loss: 24.36534252166748\n",
      "Epoch 42 \t Batch 1100 \t Training Loss: 24.392472896575928\n",
      "Epoch 42 \t Batch 1150 \t Training Loss: 24.402747407996138\n",
      "Epoch 42 \t Batch 1200 \t Training Loss: 24.3732941309611\n",
      "Epoch 42 \t Batch 1250 \t Training Loss: 24.38167988586426\n",
      "Epoch 42 \t Batch 1300 \t Training Loss: 24.367824959388145\n",
      "Epoch 42 \t Batch 1350 \t Training Loss: 24.378576150117098\n",
      "Epoch 42 \t Batch 1400 \t Training Loss: 24.390027223314558\n",
      "Epoch 42 \t Batch 1450 \t Training Loss: 24.442558877879176\n",
      "Epoch 42 \t Batch 1500 \t Training Loss: 24.45685032526652\n",
      "Epoch 42 \t Batch 1550 \t Training Loss: 24.496344179953297\n",
      "Epoch 42 \t Batch 1600 \t Training Loss: 24.545672850608824\n",
      "Epoch 42 \t Batch 1650 \t Training Loss: 24.59168720360958\n",
      "Epoch 42 \t Batch 1700 \t Training Loss: 24.55396374534158\n",
      "Epoch 42 \t Batch 1750 \t Training Loss: 24.585980863298687\n",
      "Epoch 42 \t Batch 1800 \t Training Loss: 24.603853698306615\n",
      "Epoch 42 \t Batch 1850 \t Training Loss: 24.61386503889754\n",
      "Epoch 42 \t Batch 1900 \t Training Loss: 24.65568337591071\n",
      "Epoch 42 \t Batch 1950 \t Training Loss: 24.63736958625989\n",
      "Epoch 42 \t Batch 2000 \t Training Loss: 24.656162937164307\n",
      "Epoch 42 \t Batch 2050 \t Training Loss: 24.657153020719203\n",
      "Epoch 42 \t Batch 2100 \t Training Loss: 24.6684776197161\n",
      "Epoch 42 \t Batch 2150 \t Training Loss: 24.701894527701445\n",
      "Epoch 42 \t Batch 2200 \t Training Loss: 24.71072952964089\n",
      "Epoch 42 \t Batch 2250 \t Training Loss: 24.694600863138835\n",
      "Epoch 42 \t Batch 2300 \t Training Loss: 24.714683159537937\n",
      "Epoch 42 \t Batch 2350 \t Training Loss: 24.747842957516934\n",
      "Epoch 42 \t Batch 2400 \t Training Loss: 24.772793109416963\n",
      "Epoch 42 \t Batch 2450 \t Training Loss: 24.76338315224161\n",
      "Epoch 42 \t Batch 2500 \t Training Loss: 24.74200104293823\n",
      "Epoch 42 \t Batch 2550 \t Training Loss: 24.750533875858082\n",
      "Epoch 42 \t Batch 2600 \t Training Loss: 24.782022756429818\n",
      "Epoch 42 \t Batch 2650 \t Training Loss: 24.799376466139307\n",
      "Epoch 42 \t Batch 2700 \t Training Loss: 24.813701422656024\n",
      "Epoch 42 \t Batch 2750 \t Training Loss: 24.82289736036821\n",
      "Epoch 42 \t Batch 2800 \t Training Loss: 24.847563126427787\n",
      "Epoch 42 \t Batch 2850 \t Training Loss: 24.874510401675575\n",
      "Epoch 42 \t Batch 2900 \t Training Loss: 24.87942641093813\n",
      "Epoch 42 \t Batch 2950 \t Training Loss: 24.877383511430125\n",
      "Epoch 42 \t Batch 3000 \t Training Loss: 24.897894189198812\n",
      "Epoch 42 \t Batch 3050 \t Training Loss: 24.91205729562728\n",
      "Epoch 42 \t Batch 3100 \t Training Loss: 24.901962247048655\n",
      "Epoch 42 \t Batch 3150 \t Training Loss: 24.931779871743824\n",
      "Epoch 42 \t Batch 3200 \t Training Loss: 24.926308224201204\n",
      "Epoch 42 \t Batch 3250 \t Training Loss: 24.945011729313777\n",
      "Epoch 42 \t Batch 3300 \t Training Loss: 24.973235690376974\n",
      "Epoch 42 \t Batch 3350 \t Training Loss: 24.9753735425579\n",
      "Epoch 42 \t Batch 3400 \t Training Loss: 24.984981150346645\n",
      "Epoch 42 \t Batch 3450 \t Training Loss: 24.990470826522163\n",
      "Epoch 42 \t Batch 3500 \t Training Loss: 24.99542248916626\n",
      "Epoch 42 \t Batch 3550 \t Training Loss: 25.005832492667185\n",
      "Epoch 42 \t Batch 3600 \t Training Loss: 25.0134105708864\n",
      "Epoch 42 \t Batch 3650 \t Training Loss: 25.01950876627883\n",
      "Epoch 42 \t Batch 50 \t Validation Loss: 18.62740954399109\n",
      "Epoch 42 \t Batch 100 \t Validation Loss: 23.252255907058714\n",
      "Epoch 42 \t Batch 150 \t Validation Loss: 22.028562428156533\n",
      "Epoch 42 \t Batch 200 \t Validation Loss: 21.13115874528885\n",
      "Epoch 42 \t Batch 250 \t Validation Loss: 22.56288473510742\n",
      "Epoch 42 \t Batch 300 \t Validation Loss: 22.15647093931834\n",
      "Epoch 42 \t Batch 350 \t Validation Loss: 23.990010532651628\n",
      "Epoch 42 \t Batch 400 \t Validation Loss: 24.349570142030714\n",
      "Epoch 42 \t Batch 450 \t Validation Loss: 25.49936042679681\n",
      "Epoch 42 \t Batch 500 \t Validation Loss: 25.71653135585785\n",
      "Epoch 42 \t Batch 550 \t Validation Loss: 25.86530021233992\n",
      "Epoch 42 \t Batch 600 \t Validation Loss: 26.540730526447295\n",
      "Epoch 42 \t Batch 650 \t Validation Loss: 28.1053667648022\n",
      "Epoch 42 \t Batch 700 \t Validation Loss: 29.441310297421047\n",
      "Epoch 42 \t Batch 750 \t Validation Loss: 30.29014674695333\n",
      "Epoch 42 \t Batch 800 \t Validation Loss: 31.148968350291252\n",
      "Epoch 42 \t Batch 850 \t Validation Loss: 31.712634526982026\n",
      "Epoch 42 \t Batch 900 \t Validation Loss: 31.889142855008444\n",
      "Epoch 42 \t Batch 950 \t Validation Loss: 32.00282739589089\n",
      "Epoch 42 \t Batch 1000 \t Validation Loss: 33.29417989969254\n",
      "Epoch 42 \t Batch 1050 \t Validation Loss: 34.278545709791636\n",
      "Epoch 42 \t Batch 1100 \t Validation Loss: 34.86653044310483\n",
      "Epoch 42 \t Batch 1150 \t Validation Loss: 34.872323199977046\n",
      "Epoch 42 \t Batch 1200 \t Validation Loss: 35.56319630424181\n",
      "Epoch 42 \t Batch 1250 \t Validation Loss: 35.845650020217896\n",
      "Epoch 42 \t Batch 1300 \t Validation Loss: 36.18275711242969\n",
      "Epoch 42 \t Batch 1350 \t Validation Loss: 36.06804144859314\n",
      "Epoch 42 \t Batch 1400 \t Validation Loss: 36.06260258436203\n",
      "Epoch 42 \t Batch 1450 \t Validation Loss: 36.230010848538626\n",
      "Epoch 42 \t Batch 1500 \t Validation Loss: 36.44863958136241\n",
      "Epoch 42 \t Batch 1550 \t Validation Loss: 36.32799907038289\n",
      "Epoch 42 \t Batch 1600 \t Validation Loss: 36.36613787740469\n",
      "Epoch 42 \t Batch 1650 \t Validation Loss: 36.53685363971826\n",
      "Epoch 42 \t Batch 1700 \t Validation Loss: 36.565087191357335\n",
      "Epoch 42 \t Batch 1750 \t Validation Loss: 36.53689117731367\n",
      "Epoch 42 \t Batch 1800 \t Validation Loss: 36.75355364984936\n",
      "Epoch 42 \t Batch 1850 \t Validation Loss: 37.36172752870096\n",
      "Epoch 42 \t Batch 1900 \t Validation Loss: 37.72497786145461\n",
      "Epoch 42 \t Batch 1950 \t Validation Loss: 37.58328965529417\n",
      "Epoch 42 \t Batch 2000 \t Validation Loss: 37.562558019399646\n",
      "Epoch 42 \t Batch 2050 \t Validation Loss: 37.84866442680359\n",
      "Epoch 42 \t Batch 2100 \t Validation Loss: 37.894513118607655\n",
      "Epoch 42 \t Batch 2150 \t Validation Loss: 37.808491893812665\n",
      "Epoch 42 \t Batch 2200 \t Validation Loss: 37.68207161621614\n",
      "Epoch 42 \t Batch 2250 \t Validation Loss: 37.592333571751915\n",
      "Epoch 42 \t Batch 2300 \t Validation Loss: 37.4332816949098\n",
      "Epoch 42 \t Batch 2350 \t Validation Loss: 37.46084679623868\n",
      "Epoch 42 \t Batch 2400 \t Validation Loss: 37.61772409200668\n",
      "Epoch 42 \t Batch 2450 \t Validation Loss: 37.99484685586423\n",
      "Epoch 42 Training Loss: 25.02261442706525 Validation Loss: 38.19679673886918\n",
      "Epoch 42 completed\n",
      "Epoch 43 \t Batch 50 \t Training Loss: 24.930907440185546\n",
      "Epoch 43 \t Batch 100 \t Training Loss: 25.10117586135864\n",
      "Epoch 43 \t Batch 150 \t Training Loss: 24.512665723164876\n",
      "Epoch 43 \t Batch 200 \t Training Loss: 24.20694703102112\n",
      "Epoch 43 \t Batch 250 \t Training Loss: 23.947076332092283\n",
      "Epoch 43 \t Batch 300 \t Training Loss: 23.995655295054117\n",
      "Epoch 43 \t Batch 350 \t Training Loss: 24.033672746930804\n",
      "Epoch 43 \t Batch 400 \t Training Loss: 23.985549211502075\n",
      "Epoch 43 \t Batch 450 \t Training Loss: 23.977633764478895\n",
      "Epoch 43 \t Batch 500 \t Training Loss: 23.994644500732424\n",
      "Epoch 43 \t Batch 550 \t Training Loss: 23.923973026275636\n",
      "Epoch 43 \t Batch 600 \t Training Loss: 23.881209966341654\n",
      "Epoch 43 \t Batch 650 \t Training Loss: 23.939569474733794\n",
      "Epoch 43 \t Batch 700 \t Training Loss: 23.84806466647557\n",
      "Epoch 43 \t Batch 750 \t Training Loss: 23.839779966990154\n",
      "Epoch 43 \t Batch 800 \t Training Loss: 23.84879304289818\n",
      "Epoch 43 \t Batch 850 \t Training Loss: 23.851620046952192\n",
      "Epoch 43 \t Batch 900 \t Training Loss: 23.82590832286411\n",
      "Epoch 43 \t Batch 950 \t Training Loss: 23.834743767788535\n",
      "Epoch 43 \t Batch 1000 \t Training Loss: 23.870480435371398\n",
      "Epoch 43 \t Batch 1050 \t Training Loss: 23.913904165540423\n",
      "Epoch 43 \t Batch 1100 \t Training Loss: 23.95153538357128\n",
      "Epoch 43 \t Batch 1150 \t Training Loss: 23.945961830305016\n",
      "Epoch 43 \t Batch 1200 \t Training Loss: 23.9873047709465\n",
      "Epoch 43 \t Batch 1250 \t Training Loss: 24.009509670257568\n",
      "Epoch 43 \t Batch 1300 \t Training Loss: 24.051305780410768\n",
      "Epoch 43 \t Batch 1350 \t Training Loss: 24.04976068072849\n",
      "Epoch 43 \t Batch 1400 \t Training Loss: 24.03287993362972\n",
      "Epoch 43 \t Batch 1450 \t Training Loss: 24.036420071700523\n",
      "Epoch 43 \t Batch 1500 \t Training Loss: 24.047550349553426\n",
      "Epoch 43 \t Batch 1550 \t Training Loss: 24.009500676431962\n",
      "Epoch 43 \t Batch 1600 \t Training Loss: 23.992422841191292\n",
      "Epoch 43 \t Batch 1650 \t Training Loss: 24.00427857196692\n",
      "Epoch 43 \t Batch 1700 \t Training Loss: 24.039753108866073\n",
      "Epoch 43 \t Batch 1750 \t Training Loss: 24.02053133882795\n",
      "Epoch 43 \t Batch 1800 \t Training Loss: 24.009033765263027\n",
      "Epoch 43 \t Batch 1850 \t Training Loss: 24.006086441761738\n",
      "Epoch 43 \t Batch 1900 \t Training Loss: 24.053529844284057\n",
      "Epoch 43 \t Batch 1950 \t Training Loss: 24.0851345262772\n",
      "Epoch 43 \t Batch 2000 \t Training Loss: 24.10449793100357\n",
      "Epoch 43 \t Batch 2050 \t Training Loss: 24.105528938944747\n",
      "Epoch 43 \t Batch 2100 \t Training Loss: 24.125671128318423\n",
      "Epoch 43 \t Batch 2150 \t Training Loss: 24.128202534609063\n",
      "Epoch 43 \t Batch 2200 \t Training Loss: 24.130584962584756\n",
      "Epoch 43 \t Batch 2250 \t Training Loss: 24.149517220815024\n",
      "Epoch 43 \t Batch 2300 \t Training Loss: 24.159479397898135\n",
      "Epoch 43 \t Batch 2350 \t Training Loss: 24.15631636964514\n",
      "Epoch 43 \t Batch 2400 \t Training Loss: 24.166148410240808\n",
      "Epoch 43 \t Batch 2450 \t Training Loss: 24.17276199224044\n",
      "Epoch 43 \t Batch 2500 \t Training Loss: 24.183591585159302\n",
      "Epoch 43 \t Batch 2550 \t Training Loss: 24.20500457801071\n",
      "Epoch 43 \t Batch 2600 \t Training Loss: 24.20346682878641\n",
      "Epoch 43 \t Batch 2650 \t Training Loss: 24.220418565498207\n",
      "Epoch 43 \t Batch 2700 \t Training Loss: 24.227515962741993\n",
      "Epoch 43 \t Batch 2750 \t Training Loss: 24.256147501512007\n",
      "Epoch 43 \t Batch 2800 \t Training Loss: 24.268935302666254\n",
      "Epoch 43 \t Batch 2850 \t Training Loss: 24.257992686556097\n",
      "Epoch 43 \t Batch 2900 \t Training Loss: 24.24482616457446\n",
      "Epoch 43 \t Batch 2950 \t Training Loss: 24.26083780418008\n",
      "Epoch 43 \t Batch 3000 \t Training Loss: 24.25566939576467\n",
      "Epoch 43 \t Batch 3050 \t Training Loss: 24.252006557964886\n",
      "Epoch 43 \t Batch 3100 \t Training Loss: 24.250990930680306\n",
      "Epoch 43 \t Batch 3150 \t Training Loss: 24.25136494076441\n",
      "Epoch 43 \t Batch 3200 \t Training Loss: 24.278838844597338\n",
      "Epoch 43 \t Batch 3250 \t Training Loss: 24.290247081169714\n",
      "Epoch 43 \t Batch 3300 \t Training Loss: 24.292143149809405\n",
      "Epoch 43 \t Batch 3350 \t Training Loss: 24.29160735144544\n",
      "Epoch 43 \t Batch 3400 \t Training Loss: 24.290313836546506\n",
      "Epoch 43 \t Batch 3450 \t Training Loss: 24.308188731704934\n",
      "Epoch 43 \t Batch 3500 \t Training Loss: 24.31880401638576\n",
      "Epoch 43 \t Batch 3550 \t Training Loss: 24.3231439378228\n",
      "Epoch 43 \t Batch 3600 \t Training Loss: 24.31781327009201\n",
      "Epoch 43 \t Batch 3650 \t Training Loss: 24.316459721343158\n",
      "Epoch 43 \t Batch 50 \t Validation Loss: 14.063834466934203\n",
      "Epoch 43 \t Batch 100 \t Validation Loss: 17.40537609100342\n",
      "Epoch 43 \t Batch 150 \t Validation Loss: 16.710895179112754\n",
      "Epoch 43 \t Batch 200 \t Validation Loss: 16.01462233543396\n",
      "Epoch 43 \t Batch 250 \t Validation Loss: 17.206452171325683\n",
      "Epoch 43 \t Batch 300 \t Validation Loss: 16.87657581647237\n",
      "Epoch 43 \t Batch 350 \t Validation Loss: 19.403344440460206\n",
      "Epoch 43 \t Batch 400 \t Validation Loss: 20.2110715508461\n",
      "Epoch 43 \t Batch 450 \t Validation Loss: 21.70976710213555\n",
      "Epoch 43 \t Batch 500 \t Validation Loss: 22.090001832962034\n",
      "Epoch 43 \t Batch 550 \t Validation Loss: 22.389275887229225\n",
      "Epoch 43 \t Batch 600 \t Validation Loss: 23.505108375549316\n",
      "Epoch 43 \t Batch 650 \t Validation Loss: 25.522479796776405\n",
      "Epoch 43 \t Batch 700 \t Validation Loss: 27.70845521109445\n",
      "Epoch 43 \t Batch 750 \t Validation Loss: 29.14348415629069\n",
      "Epoch 43 \t Batch 800 \t Validation Loss: 30.44572537302971\n",
      "Epoch 43 \t Batch 850 \t Validation Loss: 31.49865628410788\n",
      "Epoch 43 \t Batch 900 \t Validation Loss: 31.978388625250922\n",
      "Epoch 43 \t Batch 950 \t Validation Loss: 32.25601382104974\n",
      "Epoch 43 \t Batch 1000 \t Validation Loss: 34.02406146717072\n",
      "Epoch 43 \t Batch 1050 \t Validation Loss: 35.20073071752276\n",
      "Epoch 43 \t Batch 1100 \t Validation Loss: 36.09613250949166\n",
      "Epoch 43 \t Batch 1150 \t Validation Loss: 36.253768771627676\n",
      "Epoch 43 \t Batch 1200 \t Validation Loss: 37.13800778865814\n",
      "Epoch 43 \t Batch 1250 \t Validation Loss: 37.600019673919675\n",
      "Epoch 43 \t Batch 1300 \t Validation Loss: 37.92786158635066\n",
      "Epoch 43 \t Batch 1350 \t Validation Loss: 37.814032849205866\n",
      "Epoch 43 \t Batch 1400 \t Validation Loss: 37.784794671194895\n",
      "Epoch 43 \t Batch 1450 \t Validation Loss: 37.89330973658068\n",
      "Epoch 43 \t Batch 1500 \t Validation Loss: 38.14940934276581\n",
      "Epoch 43 \t Batch 1550 \t Validation Loss: 37.92988898523392\n",
      "Epoch 43 \t Batch 1600 \t Validation Loss: 37.81702706247568\n",
      "Epoch 43 \t Batch 1650 \t Validation Loss: 37.896435392264166\n",
      "Epoch 43 \t Batch 1700 \t Validation Loss: 37.846519414396845\n",
      "Epoch 43 \t Batch 1750 \t Validation Loss: 37.71040443774632\n",
      "Epoch 43 \t Batch 1800 \t Validation Loss: 37.765413859420356\n",
      "Epoch 43 \t Batch 1850 \t Validation Loss: 38.304997909906746\n",
      "Epoch 43 \t Batch 1900 \t Validation Loss: 38.603994698524474\n",
      "Epoch 43 \t Batch 1950 \t Validation Loss: 38.49520818881499\n",
      "Epoch 43 \t Batch 2000 \t Validation Loss: 38.450385075807574\n",
      "Epoch 43 \t Batch 2050 \t Validation Loss: 38.58683703329505\n",
      "Epoch 43 \t Batch 2100 \t Validation Loss: 38.5191470579874\n",
      "Epoch 43 \t Batch 2150 \t Validation Loss: 38.41077532191609\n",
      "Epoch 43 \t Batch 2200 \t Validation Loss: 38.29165566942908\n",
      "Epoch 43 \t Batch 2250 \t Validation Loss: 38.1702370077769\n",
      "Epoch 43 \t Batch 2300 \t Validation Loss: 37.95655177862748\n",
      "Epoch 43 \t Batch 2350 \t Validation Loss: 37.965331194857335\n",
      "Epoch 43 \t Batch 2400 \t Validation Loss: 38.13092116713524\n",
      "Epoch 43 \t Batch 2450 \t Validation Loss: 38.51070443951354\n",
      "Epoch 43 Training Loss: 24.321851049930736 Validation Loss: 38.7009561886261\n",
      "Epoch 43 completed\n",
      "Epoch 44 \t Batch 50 \t Training Loss: 23.825435485839844\n",
      "Epoch 44 \t Batch 100 \t Training Loss: 23.22573329925537\n",
      "Epoch 44 \t Batch 150 \t Training Loss: 22.91340991973877\n",
      "Epoch 44 \t Batch 200 \t Training Loss: 23.201480045318604\n",
      "Epoch 44 \t Batch 250 \t Training Loss: 23.38750257110596\n",
      "Epoch 44 \t Batch 300 \t Training Loss: 22.943692677815754\n",
      "Epoch 44 \t Batch 350 \t Training Loss: 22.912127974373952\n",
      "Epoch 44 \t Batch 400 \t Training Loss: 23.03653120994568\n",
      "Epoch 44 \t Batch 450 \t Training Loss: 22.969290612538657\n",
      "Epoch 44 \t Batch 500 \t Training Loss: 23.0877509021759\n",
      "Epoch 44 \t Batch 550 \t Training Loss: 23.007267587835138\n",
      "Epoch 44 \t Batch 600 \t Training Loss: 22.969870576858522\n",
      "Epoch 44 \t Batch 650 \t Training Loss: 22.949434180626504\n",
      "Epoch 44 \t Batch 700 \t Training Loss: 22.953121814727783\n",
      "Epoch 44 \t Batch 750 \t Training Loss: 22.899373425801596\n",
      "Epoch 44 \t Batch 800 \t Training Loss: 22.919262330532074\n",
      "Epoch 44 \t Batch 850 \t Training Loss: 22.92133892059326\n",
      "Epoch 44 \t Batch 900 \t Training Loss: 22.963133345709906\n",
      "Epoch 44 \t Batch 950 \t Training Loss: 22.97354366904811\n",
      "Epoch 44 \t Batch 1000 \t Training Loss: 23.020027732849123\n",
      "Epoch 44 \t Batch 1050 \t Training Loss: 23.083261831374397\n",
      "Epoch 44 \t Batch 1100 \t Training Loss: 23.085313011516224\n",
      "Epoch 44 \t Batch 1150 \t Training Loss: 23.103958794137707\n",
      "Epoch 44 \t Batch 1200 \t Training Loss: 23.08883871157964\n",
      "Epoch 44 \t Batch 1250 \t Training Loss: 23.052252297210693\n",
      "Epoch 44 \t Batch 1300 \t Training Loss: 23.05193203485929\n",
      "Epoch 44 \t Batch 1350 \t Training Loss: 23.05321756080345\n",
      "Epoch 44 \t Batch 1400 \t Training Loss: 23.117140812873842\n",
      "Epoch 44 \t Batch 1450 \t Training Loss: 23.12375469536617\n",
      "Epoch 44 \t Batch 1500 \t Training Loss: 23.133549062728882\n",
      "Epoch 44 \t Batch 1550 \t Training Loss: 23.140054133630567\n",
      "Epoch 44 \t Batch 1600 \t Training Loss: 23.188862971663475\n",
      "Epoch 44 \t Batch 1650 \t Training Loss: 23.158710940389923\n",
      "Epoch 44 \t Batch 1700 \t Training Loss: 23.166445153741275\n",
      "Epoch 44 \t Batch 1750 \t Training Loss: 23.175763382502964\n",
      "Epoch 44 \t Batch 1800 \t Training Loss: 23.188575293752884\n",
      "Epoch 44 \t Batch 1850 \t Training Loss: 23.222905352308942\n",
      "Epoch 44 \t Batch 1900 \t Training Loss: 23.246772293793526\n",
      "Epoch 44 \t Batch 1950 \t Training Loss: 23.2752811280275\n",
      "Epoch 44 \t Batch 2000 \t Training Loss: 23.258259857654572\n",
      "Epoch 44 \t Batch 2050 \t Training Loss: 23.23041687709529\n",
      "Epoch 44 \t Batch 2100 \t Training Loss: 23.235738255182902\n",
      "Epoch 44 \t Batch 2150 \t Training Loss: 23.24283141069634\n",
      "Epoch 44 \t Batch 2200 \t Training Loss: 23.271419815150175\n",
      "Epoch 44 \t Batch 2250 \t Training Loss: 23.26233225377401\n",
      "Epoch 44 \t Batch 2300 \t Training Loss: 23.275832402187845\n",
      "Epoch 44 \t Batch 2350 \t Training Loss: 23.281254622682614\n",
      "Epoch 44 \t Batch 2400 \t Training Loss: 23.29927670677503\n",
      "Epoch 44 \t Batch 2450 \t Training Loss: 23.292559411574384\n",
      "Epoch 44 \t Batch 2500 \t Training Loss: 23.318635610580444\n",
      "Epoch 44 \t Batch 2550 \t Training Loss: 23.32654966952754\n",
      "Epoch 44 \t Batch 2600 \t Training Loss: 23.356155990087068\n",
      "Epoch 44 \t Batch 2650 \t Training Loss: 23.36382372046417\n",
      "Epoch 44 \t Batch 2700 \t Training Loss: 23.364082089530097\n",
      "Epoch 44 \t Batch 2750 \t Training Loss: 23.365960757515648\n",
      "Epoch 44 \t Batch 2800 \t Training Loss: 23.374927865777696\n",
      "Epoch 44 \t Batch 2850 \t Training Loss: 23.390343958202163\n",
      "Epoch 44 \t Batch 2900 \t Training Loss: 23.387891062703627\n",
      "Epoch 44 \t Batch 2950 \t Training Loss: 23.397134294994807\n",
      "Epoch 44 \t Batch 3000 \t Training Loss: 23.40033854516347\n",
      "Epoch 44 \t Batch 3050 \t Training Loss: 23.391014878632593\n",
      "Epoch 44 \t Batch 3100 \t Training Loss: 23.398904861634776\n",
      "Epoch 44 \t Batch 3150 \t Training Loss: 23.424075334034267\n",
      "Epoch 44 \t Batch 3200 \t Training Loss: 23.422502852976322\n",
      "Epoch 44 \t Batch 3250 \t Training Loss: 23.41881926404513\n",
      "Epoch 44 \t Batch 3300 \t Training Loss: 23.432473366766263\n",
      "Epoch 44 \t Batch 3350 \t Training Loss: 23.44427138342786\n",
      "Epoch 44 \t Batch 3400 \t Training Loss: 23.45171857581419\n",
      "Epoch 44 \t Batch 3450 \t Training Loss: 23.470878947990528\n",
      "Epoch 44 \t Batch 3500 \t Training Loss: 23.48781251335144\n",
      "Epoch 44 \t Batch 3550 \t Training Loss: 23.494820052939403\n",
      "Epoch 44 \t Batch 3600 \t Training Loss: 23.49540930112203\n",
      "Epoch 44 \t Batch 3650 \t Training Loss: 23.504878713529404\n",
      "Epoch 44 \t Batch 50 \t Validation Loss: 14.8707679271698\n",
      "Epoch 44 \t Batch 100 \t Validation Loss: 18.49273365020752\n",
      "Epoch 44 \t Batch 150 \t Validation Loss: 17.254758014678956\n",
      "Epoch 44 \t Batch 200 \t Validation Loss: 16.72974121809006\n",
      "Epoch 44 \t Batch 250 \t Validation Loss: 18.091489625930787\n",
      "Epoch 44 \t Batch 300 \t Validation Loss: 17.721906622250874\n",
      "Epoch 44 \t Batch 350 \t Validation Loss: 19.70053991998945\n",
      "Epoch 44 \t Batch 400 \t Validation Loss: 20.319067579507827\n",
      "Epoch 44 \t Batch 450 \t Validation Loss: 21.738083940082127\n",
      "Epoch 44 \t Batch 500 \t Validation Loss: 22.09182478427887\n",
      "Epoch 44 \t Batch 550 \t Validation Loss: 22.354121739647606\n",
      "Epoch 44 \t Batch 600 \t Validation Loss: 23.39008243640264\n",
      "Epoch 44 \t Batch 650 \t Validation Loss: 25.327086557975182\n",
      "Epoch 44 \t Batch 700 \t Validation Loss: 27.329833854266575\n",
      "Epoch 44 \t Batch 750 \t Validation Loss: 28.77245631980896\n",
      "Epoch 44 \t Batch 800 \t Validation Loss: 30.19381758749485\n",
      "Epoch 44 \t Batch 850 \t Validation Loss: 31.206656357821295\n",
      "Epoch 44 \t Batch 900 \t Validation Loss: 31.626116302808125\n",
      "Epoch 44 \t Batch 950 \t Validation Loss: 31.863562190909136\n",
      "Epoch 44 \t Batch 1000 \t Validation Loss: 33.585801693439485\n",
      "Epoch 44 \t Batch 1050 \t Validation Loss: 34.68059668132237\n",
      "Epoch 44 \t Batch 1100 \t Validation Loss: 35.52680509914052\n",
      "Epoch 44 \t Batch 1150 \t Validation Loss: 35.640088670979374\n",
      "Epoch 44 \t Batch 1200 \t Validation Loss: 36.45378706852595\n",
      "Epoch 44 \t Batch 1250 \t Validation Loss: 36.85035288734436\n",
      "Epoch 44 \t Batch 1300 \t Validation Loss: 37.19196989976443\n",
      "Epoch 44 \t Batch 1350 \t Validation Loss: 37.091590082027295\n",
      "Epoch 44 \t Batch 1400 \t Validation Loss: 37.04755426781518\n",
      "Epoch 44 \t Batch 1450 \t Validation Loss: 37.09989225880853\n",
      "Epoch 44 \t Batch 1500 \t Validation Loss: 37.34861105918884\n",
      "Epoch 44 \t Batch 1550 \t Validation Loss: 37.16562501661239\n",
      "Epoch 44 \t Batch 1600 \t Validation Loss: 37.08459525108337\n",
      "Epoch 44 \t Batch 1650 \t Validation Loss: 37.17059222943855\n",
      "Epoch 44 \t Batch 1700 \t Validation Loss: 37.162893806906304\n",
      "Epoch 44 \t Batch 1750 \t Validation Loss: 37.05476734106881\n",
      "Epoch 44 \t Batch 1800 \t Validation Loss: 37.08044297059377\n",
      "Epoch 44 \t Batch 1850 \t Validation Loss: 37.64422305493741\n",
      "Epoch 44 \t Batch 1900 \t Validation Loss: 37.969064679898715\n",
      "Epoch 44 \t Batch 1950 \t Validation Loss: 37.85385774905865\n",
      "Epoch 44 \t Batch 2000 \t Validation Loss: 37.788046592712405\n",
      "Epoch 44 \t Batch 2050 \t Validation Loss: 37.89989601926106\n",
      "Epoch 44 \t Batch 2100 \t Validation Loss: 37.82297231946673\n",
      "Epoch 44 \t Batch 2150 \t Validation Loss: 37.71594163406727\n",
      "Epoch 44 \t Batch 2200 \t Validation Loss: 37.598060942563144\n",
      "Epoch 44 \t Batch 2250 \t Validation Loss: 37.47538996844821\n",
      "Epoch 44 \t Batch 2300 \t Validation Loss: 37.25554568757182\n",
      "Epoch 44 \t Batch 2350 \t Validation Loss: 37.25189435766098\n",
      "Epoch 44 \t Batch 2400 \t Validation Loss: 37.39868360211452\n",
      "Epoch 44 \t Batch 2450 \t Validation Loss: 37.76022103747543\n",
      "Epoch 44 Training Loss: 23.514189405555683 Validation Loss: 37.95406566956988\n",
      "Epoch 44 completed\n",
      "Epoch 45 \t Batch 50 \t Training Loss: 22.638329601287843\n",
      "Epoch 45 \t Batch 100 \t Training Loss: 22.495958585739135\n",
      "Epoch 45 \t Batch 150 \t Training Loss: 22.69304397583008\n",
      "Epoch 45 \t Batch 200 \t Training Loss: 22.71562420845032\n",
      "Epoch 45 \t Batch 250 \t Training Loss: 22.471148639678955\n",
      "Epoch 45 \t Batch 300 \t Training Loss: 22.537863632837933\n",
      "Epoch 45 \t Batch 350 \t Training Loss: 22.31616854258946\n",
      "Epoch 45 \t Batch 400 \t Training Loss: 22.279331185817718\n",
      "Epoch 45 \t Batch 450 \t Training Loss: 22.2388664733039\n",
      "Epoch 45 \t Batch 500 \t Training Loss: 22.259921113967895\n",
      "Epoch 45 \t Batch 550 \t Training Loss: 22.265646606792103\n",
      "Epoch 45 \t Batch 600 \t Training Loss: 22.25423193772634\n",
      "Epoch 45 \t Batch 650 \t Training Loss: 22.200234115307147\n",
      "Epoch 45 \t Batch 700 \t Training Loss: 22.23339950289045\n",
      "Epoch 45 \t Batch 750 \t Training Loss: 22.302028689066567\n",
      "Epoch 45 \t Batch 800 \t Training Loss: 22.279202884435655\n",
      "Epoch 45 \t Batch 850 \t Training Loss: 22.37719435186947\n",
      "Epoch 45 \t Batch 900 \t Training Loss: 22.45339928627014\n",
      "Epoch 45 \t Batch 950 \t Training Loss: 22.490197676608435\n",
      "Epoch 45 \t Batch 1000 \t Training Loss: 22.510538460731507\n",
      "Epoch 45 \t Batch 1050 \t Training Loss: 22.53251038051787\n",
      "Epoch 45 \t Batch 1100 \t Training Loss: 22.562432483326305\n",
      "Epoch 45 \t Batch 1150 \t Training Loss: 22.589548135840374\n",
      "Epoch 45 \t Batch 1200 \t Training Loss: 22.567602780659993\n",
      "Epoch 45 \t Batch 1250 \t Training Loss: 22.544803067016602\n",
      "Epoch 45 \t Batch 1300 \t Training Loss: 22.519220074873704\n",
      "Epoch 45 \t Batch 1350 \t Training Loss: 22.508135904382776\n",
      "Epoch 45 \t Batch 1400 \t Training Loss: 22.497111998966762\n",
      "Epoch 45 \t Batch 1450 \t Training Loss: 22.51217262925773\n",
      "Epoch 45 \t Batch 1500 \t Training Loss: 22.50538434855143\n",
      "Epoch 45 \t Batch 1550 \t Training Loss: 22.516584589558263\n",
      "Epoch 45 \t Batch 1600 \t Training Loss: 22.51563268184662\n",
      "Epoch 45 \t Batch 1650 \t Training Loss: 22.49840540683631\n",
      "Epoch 45 \t Batch 1700 \t Training Loss: 22.51385715148028\n",
      "Epoch 45 \t Batch 1750 \t Training Loss: 22.527777898515975\n",
      "Epoch 45 \t Batch 1800 \t Training Loss: 22.5158996825748\n",
      "Epoch 45 \t Batch 1850 \t Training Loss: 22.519046015352817\n",
      "Epoch 45 \t Batch 1900 \t Training Loss: 22.543189846841912\n",
      "Epoch 45 \t Batch 1950 \t Training Loss: 22.54648083613469\n",
      "Epoch 45 \t Batch 2000 \t Training Loss: 22.520651462078096\n",
      "Epoch 45 \t Batch 2050 \t Training Loss: 22.531104387888096\n",
      "Epoch 45 \t Batch 2100 \t Training Loss: 22.556004036040534\n",
      "Epoch 45 \t Batch 2150 \t Training Loss: 22.544267157177593\n",
      "Epoch 45 \t Batch 2200 \t Training Loss: 22.561469660238785\n",
      "Epoch 45 \t Batch 2250 \t Training Loss: 22.56557588450114\n",
      "Epoch 45 \t Batch 2300 \t Training Loss: 22.595541964821194\n",
      "Epoch 45 \t Batch 2350 \t Training Loss: 22.607609197738324\n",
      "Epoch 45 \t Batch 2400 \t Training Loss: 22.62264258782069\n",
      "Epoch 45 \t Batch 2450 \t Training Loss: 22.63301580429077\n",
      "Epoch 45 \t Batch 2500 \t Training Loss: 22.650278589248657\n",
      "Epoch 45 \t Batch 2550 \t Training Loss: 22.656359678904217\n",
      "Epoch 45 \t Batch 2600 \t Training Loss: 22.66214702459482\n",
      "Epoch 45 \t Batch 2650 \t Training Loss: 22.6770333826317\n",
      "Epoch 45 \t Batch 2700 \t Training Loss: 22.680803859851977\n",
      "Epoch 45 \t Batch 2750 \t Training Loss: 22.683676743594084\n",
      "Epoch 45 \t Batch 2800 \t Training Loss: 22.68666677713394\n",
      "Epoch 45 \t Batch 2850 \t Training Loss: 22.67411511571784\n",
      "Epoch 45 \t Batch 2900 \t Training Loss: 22.674680566130014\n",
      "Epoch 45 \t Batch 2950 \t Training Loss: 22.662108863895224\n",
      "Epoch 45 \t Batch 3000 \t Training Loss: 22.65135756778717\n",
      "Epoch 45 \t Batch 3050 \t Training Loss: 22.650342944098302\n",
      "Epoch 45 \t Batch 3100 \t Training Loss: 22.662830938216178\n",
      "Epoch 45 \t Batch 3150 \t Training Loss: 22.671866562555707\n",
      "Epoch 45 \t Batch 3200 \t Training Loss: 22.681468478739262\n",
      "Epoch 45 \t Batch 3250 \t Training Loss: 22.70179256468553\n",
      "Epoch 45 \t Batch 3300 \t Training Loss: 22.709645049644237\n",
      "Epoch 45 \t Batch 3350 \t Training Loss: 22.723297839093565\n",
      "Epoch 45 \t Batch 3400 \t Training Loss: 22.727712872729583\n",
      "Epoch 45 \t Batch 3450 \t Training Loss: 22.73864556049955\n",
      "Epoch 45 \t Batch 3500 \t Training Loss: 22.747953152520317\n",
      "Epoch 45 \t Batch 3550 \t Training Loss: 22.75705855866553\n",
      "Epoch 45 \t Batch 3600 \t Training Loss: 22.76675130499734\n",
      "Epoch 45 \t Batch 3650 \t Training Loss: 22.78467058286275\n",
      "Epoch 45 \t Batch 50 \t Validation Loss: 19.45525430679321\n",
      "Epoch 45 \t Batch 100 \t Validation Loss: 24.64079842567444\n",
      "Epoch 45 \t Batch 150 \t Validation Loss: 22.257573591868084\n",
      "Epoch 45 \t Batch 200 \t Validation Loss: 21.552090005874632\n",
      "Epoch 45 \t Batch 250 \t Validation Loss: 23.3248939037323\n",
      "Epoch 45 \t Batch 300 \t Validation Loss: 22.656421386400858\n",
      "Epoch 45 \t Batch 350 \t Validation Loss: 24.315696650913782\n",
      "Epoch 45 \t Batch 400 \t Validation Loss: 24.44343881249428\n",
      "Epoch 45 \t Batch 450 \t Validation Loss: 25.52495077662998\n",
      "Epoch 45 \t Batch 500 \t Validation Loss: 25.515963291168212\n",
      "Epoch 45 \t Batch 550 \t Validation Loss: 25.57307461825284\n",
      "Epoch 45 \t Batch 600 \t Validation Loss: 26.32973042488098\n",
      "Epoch 45 \t Batch 650 \t Validation Loss: 27.981748931591326\n",
      "Epoch 45 \t Batch 700 \t Validation Loss: 29.774601330075946\n",
      "Epoch 45 \t Batch 750 \t Validation Loss: 30.98420361963908\n",
      "Epoch 45 \t Batch 800 \t Validation Loss: 32.22366200327873\n",
      "Epoch 45 \t Batch 850 \t Validation Loss: 33.01005846584545\n",
      "Epoch 45 \t Batch 900 \t Validation Loss: 33.30990382088555\n",
      "Epoch 45 \t Batch 950 \t Validation Loss: 33.47776832379793\n",
      "Epoch 45 \t Batch 1000 \t Validation Loss: 35.08823816490173\n",
      "Epoch 45 \t Batch 1050 \t Validation Loss: 36.10129354295277\n",
      "Epoch 45 \t Batch 1100 \t Validation Loss: 36.86783210407604\n",
      "Epoch 45 \t Batch 1150 \t Validation Loss: 36.91559145098147\n",
      "Epoch 45 \t Batch 1200 \t Validation Loss: 37.68488689780235\n",
      "Epoch 45 \t Batch 1250 \t Validation Loss: 38.02065230293274\n",
      "Epoch 45 \t Batch 1300 \t Validation Loss: 38.30740175503951\n",
      "Epoch 45 \t Batch 1350 \t Validation Loss: 38.16758006590384\n",
      "Epoch 45 \t Batch 1400 \t Validation Loss: 38.09695945433208\n",
      "Epoch 45 \t Batch 1450 \t Validation Loss: 38.176381871124796\n",
      "Epoch 45 \t Batch 1500 \t Validation Loss: 38.38109968503316\n",
      "Epoch 45 \t Batch 1550 \t Validation Loss: 38.168816944860644\n",
      "Epoch 45 \t Batch 1600 \t Validation Loss: 38.074011563658715\n",
      "Epoch 45 \t Batch 1650 \t Validation Loss: 38.14715223196781\n",
      "Epoch 45 \t Batch 1700 \t Validation Loss: 38.0763862323761\n",
      "Epoch 45 \t Batch 1750 \t Validation Loss: 37.98964097649711\n",
      "Epoch 45 \t Batch 1800 \t Validation Loss: 38.05502617094252\n",
      "Epoch 45 \t Batch 1850 \t Validation Loss: 38.55202995609593\n",
      "Epoch 45 \t Batch 1900 \t Validation Loss: 38.87094851142482\n",
      "Epoch 45 \t Batch 1950 \t Validation Loss: 38.73334296740018\n",
      "Epoch 45 \t Batch 2000 \t Validation Loss: 38.64806608533859\n",
      "Epoch 45 \t Batch 2050 \t Validation Loss: 38.777250062430774\n",
      "Epoch 45 \t Batch 2100 \t Validation Loss: 38.758073060171945\n",
      "Epoch 45 \t Batch 2150 \t Validation Loss: 38.6918791757628\n",
      "Epoch 45 \t Batch 2200 \t Validation Loss: 38.618200918544424\n",
      "Epoch 45 \t Batch 2250 \t Validation Loss: 38.564058284335665\n",
      "Epoch 45 \t Batch 2300 \t Validation Loss: 38.41281069879947\n",
      "Epoch 45 \t Batch 2350 \t Validation Loss: 38.48243010257153\n",
      "Epoch 45 \t Batch 2400 \t Validation Loss: 38.64594957431157\n",
      "Epoch 45 \t Batch 2450 \t Validation Loss: 39.0357147870745\n",
      "Epoch 45 Training Loss: 22.791208461950752 Validation Loss: 39.21957721957913\n",
      "Epoch 45 completed\n",
      "Epoch 46 \t Batch 50 \t Training Loss: 22.65175916671753\n",
      "Epoch 46 \t Batch 100 \t Training Loss: 22.01541042327881\n",
      "Epoch 46 \t Batch 150 \t Training Loss: 22.04871115366618\n",
      "Epoch 46 \t Batch 200 \t Training Loss: 21.68872164726257\n",
      "Epoch 46 \t Batch 250 \t Training Loss: 21.69779756164551\n",
      "Epoch 46 \t Batch 300 \t Training Loss: 21.592401021321614\n",
      "Epoch 46 \t Batch 350 \t Training Loss: 21.529775589534214\n",
      "Epoch 46 \t Batch 400 \t Training Loss: 21.56282922029495\n",
      "Epoch 46 \t Batch 450 \t Training Loss: 21.63193732579549\n",
      "Epoch 46 \t Batch 500 \t Training Loss: 21.684311323165893\n",
      "Epoch 46 \t Batch 550 \t Training Loss: 21.68155519138683\n",
      "Epoch 46 \t Batch 600 \t Training Loss: 21.63789665063222\n",
      "Epoch 46 \t Batch 650 \t Training Loss: 21.73955484390259\n",
      "Epoch 46 \t Batch 700 \t Training Loss: 21.765102130344935\n",
      "Epoch 46 \t Batch 750 \t Training Loss: 21.833861817677814\n",
      "Epoch 46 \t Batch 800 \t Training Loss: 21.75387244939804\n",
      "Epoch 46 \t Batch 850 \t Training Loss: 21.766909566767076\n",
      "Epoch 46 \t Batch 900 \t Training Loss: 21.77249512778388\n",
      "Epoch 46 \t Batch 950 \t Training Loss: 21.750747426685535\n",
      "Epoch 46 \t Batch 1000 \t Training Loss: 21.782960061073304\n",
      "Epoch 46 \t Batch 1050 \t Training Loss: 21.803755348750524\n",
      "Epoch 46 \t Batch 1100 \t Training Loss: 21.81437894040888\n",
      "Epoch 46 \t Batch 1150 \t Training Loss: 21.83605216731196\n",
      "Epoch 46 \t Batch 1200 \t Training Loss: 21.842193922996522\n",
      "Epoch 46 \t Batch 1250 \t Training Loss: 21.836106157684327\n",
      "Epoch 46 \t Batch 1300 \t Training Loss: 21.826016052686253\n",
      "Epoch 46 \t Batch 1350 \t Training Loss: 21.79948223043371\n",
      "Epoch 46 \t Batch 1400 \t Training Loss: 21.800548648153033\n",
      "Epoch 46 \t Batch 1450 \t Training Loss: 21.838262729644775\n",
      "Epoch 46 \t Batch 1500 \t Training Loss: 21.87237102953593\n",
      "Epoch 46 \t Batch 1550 \t Training Loss: 21.84986611950782\n",
      "Epoch 46 \t Batch 1600 \t Training Loss: 21.90842570722103\n",
      "Epoch 46 \t Batch 1650 \t Training Loss: 21.922171950484767\n",
      "Epoch 46 \t Batch 1700 \t Training Loss: 21.953968881158268\n",
      "Epoch 46 \t Batch 1750 \t Training Loss: 21.939863302503312\n",
      "Epoch 46 \t Batch 1800 \t Training Loss: 21.92601712597741\n",
      "Epoch 46 \t Batch 1850 \t Training Loss: 21.90616181502471\n",
      "Epoch 46 \t Batch 1900 \t Training Loss: 21.934871781499762\n",
      "Epoch 46 \t Batch 1950 \t Training Loss: 21.9858182765276\n",
      "Epoch 46 \t Batch 2000 \t Training Loss: 21.976872474193573\n",
      "Epoch 46 \t Batch 2050 \t Training Loss: 21.969221904568556\n",
      "Epoch 46 \t Batch 2100 \t Training Loss: 21.988202332087926\n",
      "Epoch 46 \t Batch 2150 \t Training Loss: 22.015718437904535\n",
      "Epoch 46 \t Batch 2200 \t Training Loss: 22.014286453073677\n",
      "Epoch 46 \t Batch 2250 \t Training Loss: 22.02140827094184\n",
      "Epoch 46 \t Batch 2300 \t Training Loss: 22.00574905063795\n",
      "Epoch 46 \t Batch 2350 \t Training Loss: 21.987600031507775\n",
      "Epoch 46 \t Batch 2400 \t Training Loss: 21.98707334637642\n",
      "Epoch 46 \t Batch 2450 \t Training Loss: 21.992465199451058\n",
      "Epoch 46 \t Batch 2500 \t Training Loss: 21.999973434829712\n",
      "Epoch 46 \t Batch 2550 \t Training Loss: 22.010687770843507\n",
      "Epoch 46 \t Batch 2600 \t Training Loss: 22.01026014951559\n",
      "Epoch 46 \t Batch 2650 \t Training Loss: 22.036507408214064\n",
      "Epoch 46 \t Batch 2700 \t Training Loss: 22.028073330985176\n",
      "Epoch 46 \t Batch 2750 \t Training Loss: 22.01948768823797\n",
      "Epoch 46 \t Batch 2800 \t Training Loss: 22.015127464021955\n",
      "Epoch 46 \t Batch 2850 \t Training Loss: 22.0330518675687\n",
      "Epoch 46 \t Batch 2900 \t Training Loss: 22.056856198146427\n",
      "Epoch 46 \t Batch 2950 \t Training Loss: 22.066725369469594\n",
      "Epoch 46 \t Batch 3000 \t Training Loss: 22.065666973749796\n",
      "Epoch 46 \t Batch 3050 \t Training Loss: 22.11725330727999\n",
      "Epoch 46 \t Batch 3100 \t Training Loss: 22.138691729883995\n",
      "Epoch 46 \t Batch 3150 \t Training Loss: 22.150620465354315\n",
      "Epoch 46 \t Batch 3200 \t Training Loss: 22.154257069826127\n",
      "Epoch 46 \t Batch 3250 \t Training Loss: 22.158460048968976\n",
      "Epoch 46 \t Batch 3300 \t Training Loss: 22.153005284685076\n",
      "Epoch 46 \t Batch 3350 \t Training Loss: 22.16202440432648\n",
      "Epoch 46 \t Batch 3400 \t Training Loss: 22.164612885643454\n",
      "Epoch 46 \t Batch 3450 \t Training Loss: 22.17109328062638\n",
      "Epoch 46 \t Batch 3500 \t Training Loss: 22.15440792955671\n",
      "Epoch 46 \t Batch 3550 \t Training Loss: 22.14598480385794\n",
      "Epoch 46 \t Batch 3600 \t Training Loss: 22.157647143469916\n",
      "Epoch 46 \t Batch 3650 \t Training Loss: 22.160893289226376\n",
      "Epoch 46 \t Batch 50 \t Validation Loss: 22.677382745742797\n",
      "Epoch 46 \t Batch 100 \t Validation Loss: 29.308681073188783\n",
      "Epoch 46 \t Batch 150 \t Validation Loss: 26.087177254358927\n",
      "Epoch 46 \t Batch 200 \t Validation Loss: 25.885197083950043\n",
      "Epoch 46 \t Batch 250 \t Validation Loss: 27.920685819625856\n",
      "Epoch 46 \t Batch 300 \t Validation Loss: 26.509747834205626\n",
      "Epoch 46 \t Batch 350 \t Validation Loss: 27.815418018613542\n",
      "Epoch 46 \t Batch 400 \t Validation Loss: 27.87328174352646\n",
      "Epoch 46 \t Batch 450 \t Validation Loss: 28.731516367594402\n",
      "Epoch 46 \t Batch 500 \t Validation Loss: 28.633148429870605\n",
      "Epoch 46 \t Batch 550 \t Validation Loss: 28.439284146048806\n",
      "Epoch 46 \t Batch 600 \t Validation Loss: 28.819836988449097\n",
      "Epoch 46 \t Batch 650 \t Validation Loss: 30.152210661081167\n",
      "Epoch 46 \t Batch 700 \t Validation Loss: 31.580955576896667\n",
      "Epoch 46 \t Batch 750 \t Validation Loss: 32.55961259905497\n",
      "Epoch 46 \t Batch 800 \t Validation Loss: 33.45751481354237\n",
      "Epoch 46 \t Batch 850 \t Validation Loss: 34.144434874478506\n",
      "Epoch 46 \t Batch 900 \t Validation Loss: 34.28924296538035\n",
      "Epoch 46 \t Batch 950 \t Validation Loss: 34.28420136552108\n",
      "Epoch 46 \t Batch 1000 \t Validation Loss: 35.72767991876602\n",
      "Epoch 46 \t Batch 1050 \t Validation Loss: 36.6992851298196\n",
      "Epoch 46 \t Batch 1100 \t Validation Loss: 37.360834734223104\n",
      "Epoch 46 \t Batch 1150 \t Validation Loss: 37.343835935178014\n",
      "Epoch 46 \t Batch 1200 \t Validation Loss: 37.93728364467621\n",
      "Epoch 46 \t Batch 1250 \t Validation Loss: 38.2097997844696\n",
      "Epoch 46 \t Batch 1300 \t Validation Loss: 38.424255683972284\n",
      "Epoch 46 \t Batch 1350 \t Validation Loss: 38.251194338268704\n",
      "Epoch 46 \t Batch 1400 \t Validation Loss: 38.1005759038244\n",
      "Epoch 46 \t Batch 1450 \t Validation Loss: 38.082406603385664\n",
      "Epoch 46 \t Batch 1500 \t Validation Loss: 38.27266244697571\n",
      "Epoch 46 \t Batch 1550 \t Validation Loss: 38.05100424551195\n",
      "Epoch 46 \t Batch 1600 \t Validation Loss: 37.93733638942242\n",
      "Epoch 46 \t Batch 1650 \t Validation Loss: 37.98532330830892\n",
      "Epoch 46 \t Batch 1700 \t Validation Loss: 37.931341207728664\n",
      "Epoch 46 \t Batch 1750 \t Validation Loss: 37.81301288332258\n",
      "Epoch 46 \t Batch 1800 \t Validation Loss: 37.821280172665915\n",
      "Epoch 46 \t Batch 1850 \t Validation Loss: 38.346670643058985\n",
      "Epoch 46 \t Batch 1900 \t Validation Loss: 38.637969404019806\n",
      "Epoch 46 \t Batch 1950 \t Validation Loss: 38.48179184400119\n",
      "Epoch 46 \t Batch 2000 \t Validation Loss: 38.35933312702179\n",
      "Epoch 46 \t Batch 2050 \t Validation Loss: 38.42873907275316\n",
      "Epoch 46 \t Batch 2100 \t Validation Loss: 38.365604580470496\n",
      "Epoch 46 \t Batch 2150 \t Validation Loss: 38.24678655402605\n",
      "Epoch 46 \t Batch 2200 \t Validation Loss: 38.13317720413208\n",
      "Epoch 46 \t Batch 2250 \t Validation Loss: 38.02388378185696\n",
      "Epoch 46 \t Batch 2300 \t Validation Loss: 37.8002280578406\n",
      "Epoch 46 \t Batch 2350 \t Validation Loss: 37.80114150625594\n",
      "Epoch 46 \t Batch 2400 \t Validation Loss: 37.95714522073666\n",
      "Epoch 46 \t Batch 2450 \t Validation Loss: 38.32411653917663\n",
      "Epoch 46 Training Loss: 22.16505613529825 Validation Loss: 38.50477905677898\n",
      "Epoch 46 completed\n",
      "Epoch 47 \t Batch 50 \t Training Loss: 21.920988082885742\n",
      "Epoch 47 \t Batch 100 \t Training Loss: 21.687037057876587\n",
      "Epoch 47 \t Batch 150 \t Training Loss: 21.637033723195394\n",
      "Epoch 47 \t Batch 200 \t Training Loss: 21.579335560798643\n",
      "Epoch 47 \t Batch 250 \t Training Loss: 21.632542064666747\n",
      "Epoch 47 \t Batch 300 \t Training Loss: 21.535959380467734\n",
      "Epoch 47 \t Batch 350 \t Training Loss: 21.520905426570348\n",
      "Epoch 47 \t Batch 400 \t Training Loss: 21.678757240772246\n",
      "Epoch 47 \t Batch 450 \t Training Loss: 21.615039846632214\n",
      "Epoch 47 \t Batch 500 \t Training Loss: 21.721226165771483\n",
      "Epoch 47 \t Batch 550 \t Training Loss: 21.66037048513239\n",
      "Epoch 47 \t Batch 600 \t Training Loss: 21.700490616162618\n",
      "Epoch 47 \t Batch 650 \t Training Loss: 21.66612622187688\n",
      "Epoch 47 \t Batch 700 \t Training Loss: 21.654387802396503\n",
      "Epoch 47 \t Batch 750 \t Training Loss: 21.5221122080485\n",
      "Epoch 47 \t Batch 800 \t Training Loss: 21.541753885746\n",
      "Epoch 47 \t Batch 850 \t Training Loss: 21.615998504863065\n",
      "Epoch 47 \t Batch 900 \t Training Loss: 21.593234377966986\n",
      "Epoch 47 \t Batch 950 \t Training Loss: 21.60165819067704\n",
      "Epoch 47 \t Batch 1000 \t Training Loss: 21.59941521549225\n",
      "Epoch 47 \t Batch 1050 \t Training Loss: 21.640296513693674\n",
      "Epoch 47 \t Batch 1100 \t Training Loss: 21.625124631361526\n",
      "Epoch 47 \t Batch 1150 \t Training Loss: 21.583460489356\n",
      "Epoch 47 \t Batch 1200 \t Training Loss: 21.570060307184857\n",
      "Epoch 47 \t Batch 1250 \t Training Loss: 21.558807624816893\n",
      "Epoch 47 \t Batch 1300 \t Training Loss: 21.571908192267784\n",
      "Epoch 47 \t Batch 1350 \t Training Loss: 21.584456122363054\n",
      "Epoch 47 \t Batch 1400 \t Training Loss: 21.598445446150645\n",
      "Epoch 47 \t Batch 1450 \t Training Loss: 21.630800486597522\n",
      "Epoch 47 \t Batch 1500 \t Training Loss: 21.66085761642456\n",
      "Epoch 47 \t Batch 1550 \t Training Loss: 21.638390412484444\n",
      "Epoch 47 \t Batch 1600 \t Training Loss: 21.66814541757107\n",
      "Epoch 47 \t Batch 1650 \t Training Loss: 21.64874251567956\n",
      "Epoch 47 \t Batch 1700 \t Training Loss: 21.641926409216488\n",
      "Epoch 47 \t Batch 1750 \t Training Loss: 21.59666403579712\n",
      "Epoch 47 \t Batch 1800 \t Training Loss: 21.605076535012987\n",
      "Epoch 47 \t Batch 1850 \t Training Loss: 21.617263778995824\n",
      "Epoch 47 \t Batch 1900 \t Training Loss: 21.62111561825401\n",
      "Epoch 47 \t Batch 1950 \t Training Loss: 21.593027293376434\n",
      "Epoch 47 \t Batch 2000 \t Training Loss: 21.592426308631897\n",
      "Epoch 47 \t Batch 2050 \t Training Loss: 21.585641289222533\n",
      "Epoch 47 \t Batch 2100 \t Training Loss: 21.600671048391433\n",
      "Epoch 47 \t Batch 2150 \t Training Loss: 21.597194778087527\n",
      "Epoch 47 \t Batch 2200 \t Training Loss: 21.602828936143354\n",
      "Epoch 47 \t Batch 2250 \t Training Loss: 21.58845248243544\n",
      "Epoch 47 \t Batch 2300 \t Training Loss: 21.57571039697398\n",
      "Epoch 47 \t Batch 2350 \t Training Loss: 21.57096809062552\n",
      "Epoch 47 \t Batch 2400 \t Training Loss: 21.55827858686447\n",
      "Epoch 47 \t Batch 2450 \t Training Loss: 21.560700369075853\n",
      "Epoch 47 \t Batch 2500 \t Training Loss: 21.554232791519166\n",
      "Epoch 47 \t Batch 2550 \t Training Loss: 21.57016881082572\n",
      "Epoch 47 \t Batch 2600 \t Training Loss: 21.58958835345048\n",
      "Epoch 47 \t Batch 2650 \t Training Loss: 21.58669802575741\n",
      "Epoch 47 \t Batch 2700 \t Training Loss: 21.592398719434385\n",
      "Epoch 47 \t Batch 2750 \t Training Loss: 21.603170042904942\n",
      "Epoch 47 \t Batch 2800 \t Training Loss: 21.60708620582308\n",
      "Epoch 47 \t Batch 2850 \t Training Loss: 21.598555684340628\n",
      "Epoch 47 \t Batch 2900 \t Training Loss: 21.59332062260858\n",
      "Epoch 47 \t Batch 2950 \t Training Loss: 21.61248865386187\n",
      "Epoch 47 \t Batch 3000 \t Training Loss: 21.62372211488088\n",
      "Epoch 47 \t Batch 3050 \t Training Loss: 21.619575181241895\n",
      "Epoch 47 \t Batch 3100 \t Training Loss: 21.633664883951987\n",
      "Epoch 47 \t Batch 3150 \t Training Loss: 21.639968651362828\n",
      "Epoch 47 \t Batch 3200 \t Training Loss: 21.640152975022794\n",
      "Epoch 47 \t Batch 3250 \t Training Loss: 21.647287796900823\n",
      "Epoch 47 \t Batch 3300 \t Training Loss: 21.64803711948973\n",
      "Epoch 47 \t Batch 3350 \t Training Loss: 21.656429213054146\n",
      "Epoch 47 \t Batch 3400 \t Training Loss: 21.645388334218193\n",
      "Epoch 47 \t Batch 3450 \t Training Loss: 21.65649622903354\n",
      "Epoch 47 \t Batch 3500 \t Training Loss: 21.66069144466945\n",
      "Epoch 47 \t Batch 3550 \t Training Loss: 21.662043987059256\n",
      "Epoch 47 \t Batch 3600 \t Training Loss: 21.65891049835417\n",
      "Epoch 47 \t Batch 3650 \t Training Loss: 21.666096285206\n",
      "Epoch 47 \t Batch 50 \t Validation Loss: 17.51797498703003\n",
      "Epoch 47 \t Batch 100 \t Validation Loss: 23.439987564086913\n",
      "Epoch 47 \t Batch 150 \t Validation Loss: 21.23283667564392\n",
      "Epoch 47 \t Batch 200 \t Validation Loss: 21.530537285804748\n",
      "Epoch 47 \t Batch 250 \t Validation Loss: 23.12239803123474\n",
      "Epoch 47 \t Batch 300 \t Validation Loss: 22.21382804393768\n",
      "Epoch 47 \t Batch 350 \t Validation Loss: 24.372574396133423\n",
      "Epoch 47 \t Batch 400 \t Validation Loss: 24.775223792791365\n",
      "Epoch 47 \t Batch 450 \t Validation Loss: 25.951113803651598\n",
      "Epoch 47 \t Batch 500 \t Validation Loss: 26.0880852022171\n",
      "Epoch 47 \t Batch 550 \t Validation Loss: 26.169350113435225\n",
      "Epoch 47 \t Batch 600 \t Validation Loss: 26.81595095872879\n",
      "Epoch 47 \t Batch 650 \t Validation Loss: 28.328894781699546\n",
      "Epoch 47 \t Batch 700 \t Validation Loss: 30.025648636817934\n",
      "Epoch 47 \t Batch 750 \t Validation Loss: 31.264517422358196\n",
      "Epoch 47 \t Batch 800 \t Validation Loss: 32.39768607914448\n",
      "Epoch 47 \t Batch 850 \t Validation Loss: 33.24467035461875\n",
      "Epoch 47 \t Batch 900 \t Validation Loss: 33.54608616669973\n",
      "Epoch 47 \t Batch 950 \t Validation Loss: 33.67941977902463\n",
      "Epoch 47 \t Batch 1000 \t Validation Loss: 35.33112964105606\n",
      "Epoch 47 \t Batch 1050 \t Validation Loss: 36.38972999708993\n",
      "Epoch 47 \t Batch 1100 \t Validation Loss: 37.17982231790369\n",
      "Epoch 47 \t Batch 1150 \t Validation Loss: 37.18009909588358\n",
      "Epoch 47 \t Batch 1200 \t Validation Loss: 37.87653379440307\n",
      "Epoch 47 \t Batch 1250 \t Validation Loss: 38.216039125823976\n",
      "Epoch 47 \t Batch 1300 \t Validation Loss: 38.44798460740309\n",
      "Epoch 47 \t Batch 1350 \t Validation Loss: 38.30544643402099\n",
      "Epoch 47 \t Batch 1400 \t Validation Loss: 38.17392078535897\n",
      "Epoch 47 \t Batch 1450 \t Validation Loss: 38.095853525359054\n",
      "Epoch 47 \t Batch 1500 \t Validation Loss: 38.31363222503662\n",
      "Epoch 47 \t Batch 1550 \t Validation Loss: 38.076552321526314\n",
      "Epoch 47 \t Batch 1600 \t Validation Loss: 37.915852976441386\n",
      "Epoch 47 \t Batch 1650 \t Validation Loss: 37.94330989317461\n",
      "Epoch 47 \t Batch 1700 \t Validation Loss: 37.879482684976914\n",
      "Epoch 47 \t Batch 1750 \t Validation Loss: 37.73709059524536\n",
      "Epoch 47 \t Batch 1800 \t Validation Loss: 37.67208463933733\n",
      "Epoch 47 \t Batch 1850 \t Validation Loss: 38.196899589847874\n",
      "Epoch 47 \t Batch 1900 \t Validation Loss: 38.48305353214866\n",
      "Epoch 47 \t Batch 1950 \t Validation Loss: 38.347370221064644\n",
      "Epoch 47 \t Batch 2000 \t Validation Loss: 38.248306206226346\n",
      "Epoch 47 \t Batch 2050 \t Validation Loss: 38.248769343771585\n",
      "Epoch 47 \t Batch 2100 \t Validation Loss: 38.14801484879993\n",
      "Epoch 47 \t Batch 2150 \t Validation Loss: 38.03898428362469\n",
      "Epoch 47 \t Batch 2200 \t Validation Loss: 37.92842411994934\n",
      "Epoch 47 \t Batch 2250 \t Validation Loss: 37.82656700176663\n",
      "Epoch 47 \t Batch 2300 \t Validation Loss: 37.62283512229505\n",
      "Epoch 47 \t Batch 2350 \t Validation Loss: 37.64075500396972\n",
      "Epoch 47 \t Batch 2400 \t Validation Loss: 37.805361793736616\n",
      "Epoch 47 \t Batch 2450 \t Validation Loss: 38.16877982616425\n",
      "Epoch 47 Training Loss: 21.668547265532233 Validation Loss: 38.35662150063685\n",
      "Epoch 47 completed\n",
      "Epoch 48 \t Batch 50 \t Training Loss: 20.889171619415283\n",
      "Epoch 48 \t Batch 100 \t Training Loss: 20.805648040771484\n",
      "Epoch 48 \t Batch 150 \t Training Loss: 20.583402818044025\n",
      "Epoch 48 \t Batch 200 \t Training Loss: 20.6322061920166\n",
      "Epoch 48 \t Batch 250 \t Training Loss: 20.64849262237549\n",
      "Epoch 48 \t Batch 300 \t Training Loss: 20.63111991246541\n",
      "Epoch 48 \t Batch 350 \t Training Loss: 20.63169640949794\n",
      "Epoch 48 \t Batch 400 \t Training Loss: 20.624035341739656\n",
      "Epoch 48 \t Batch 450 \t Training Loss: 20.678455831739637\n",
      "Epoch 48 \t Batch 500 \t Training Loss: 20.490115461349486\n",
      "Epoch 48 \t Batch 550 \t Training Loss: 20.564638983986594\n",
      "Epoch 48 \t Batch 600 \t Training Loss: 20.558633104960123\n",
      "Epoch 48 \t Batch 650 \t Training Loss: 20.567291256831243\n",
      "Epoch 48 \t Batch 700 \t Training Loss: 20.605853285108292\n",
      "Epoch 48 \t Batch 750 \t Training Loss: 20.62549126815796\n",
      "Epoch 48 \t Batch 800 \t Training Loss: 20.617088376283647\n",
      "Epoch 48 \t Batch 850 \t Training Loss: 20.61218464234296\n",
      "Epoch 48 \t Batch 900 \t Training Loss: 20.61872885915968\n",
      "Epoch 48 \t Batch 950 \t Training Loss: 20.6332139597441\n",
      "Epoch 48 \t Batch 1000 \t Training Loss: 20.622764673233032\n",
      "Epoch 48 \t Batch 1050 \t Training Loss: 20.634670956021264\n",
      "Epoch 48 \t Batch 1100 \t Training Loss: 20.619793426340276\n",
      "Epoch 48 \t Batch 1150 \t Training Loss: 20.594364257480787\n",
      "Epoch 48 \t Batch 1200 \t Training Loss: 20.597928409576415\n",
      "Epoch 48 \t Batch 1250 \t Training Loss: 20.63404878616333\n",
      "Epoch 48 \t Batch 1300 \t Training Loss: 20.64958758207468\n",
      "Epoch 48 \t Batch 1350 \t Training Loss: 20.58054520147818\n",
      "Epoch 48 \t Batch 1400 \t Training Loss: 20.5905562380382\n",
      "Epoch 48 \t Batch 1450 \t Training Loss: 20.60888026862309\n",
      "Epoch 48 \t Batch 1500 \t Training Loss: 20.605072326024374\n",
      "Epoch 48 \t Batch 1550 \t Training Loss: 20.63324085297123\n",
      "Epoch 48 \t Batch 1600 \t Training Loss: 20.603940559625627\n",
      "Epoch 48 \t Batch 1650 \t Training Loss: 20.627343129244718\n",
      "Epoch 48 \t Batch 1700 \t Training Loss: 20.605707767710967\n",
      "Epoch 48 \t Batch 1750 \t Training Loss: 20.59830257633754\n",
      "Epoch 48 \t Batch 1800 \t Training Loss: 20.605646293958028\n",
      "Epoch 48 \t Batch 1850 \t Training Loss: 20.603892804223136\n",
      "Epoch 48 \t Batch 1900 \t Training Loss: 20.622328131324366\n",
      "Epoch 48 \t Batch 1950 \t Training Loss: 20.660197310325426\n",
      "Epoch 48 \t Batch 2000 \t Training Loss: 20.674324961185455\n",
      "Epoch 48 \t Batch 2050 \t Training Loss: 20.686128288362085\n",
      "Epoch 48 \t Batch 2100 \t Training Loss: 20.714198932647705\n",
      "Epoch 48 \t Batch 2150 \t Training Loss: 20.754070062415543\n",
      "Epoch 48 \t Batch 2200 \t Training Loss: 20.73147015051408\n",
      "Epoch 48 \t Batch 2250 \t Training Loss: 20.722335116492378\n",
      "Epoch 48 \t Batch 2300 \t Training Loss: 20.72457545902418\n",
      "Epoch 48 \t Batch 2350 \t Training Loss: 20.731608426925984\n",
      "Epoch 48 \t Batch 2400 \t Training Loss: 20.733127173980076\n",
      "Epoch 48 \t Batch 2450 \t Training Loss: 20.73317658093511\n",
      "Epoch 48 \t Batch 2500 \t Training Loss: 20.753254244232178\n",
      "Epoch 48 \t Batch 2550 \t Training Loss: 20.75614564671236\n",
      "Epoch 48 \t Batch 2600 \t Training Loss: 20.7654642702983\n",
      "Epoch 48 \t Batch 2650 \t Training Loss: 20.74741560594091\n",
      "Epoch 48 \t Batch 2700 \t Training Loss: 20.765375618758025\n",
      "Epoch 48 \t Batch 2750 \t Training Loss: 20.774033683776857\n",
      "Epoch 48 \t Batch 2800 \t Training Loss: 20.805250436919078\n",
      "Epoch 48 \t Batch 2850 \t Training Loss: 20.821995860986544\n"
     ]
    }
   ],
   "source": [
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "# del model\n",
    "# del optimizer\n",
    "# del criterion\n",
    "# del trainloader\n",
    "# del validloader\n",
    "# torch.cuda.empty_cache()\n",
    "### Defining bigger FCN\n",
    "\n",
    "    \n",
    "model = BiggerFCN_pyramid()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data/data_no_outliers', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 128, shuffle = True, num_workers = 8)\n",
    "# trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data/data_no_outliers', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "# validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 128, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(100):  # 10 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%50==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%50==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'bigger_model_pyramid.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model = BiggerFCN()\n",
    "# model.load_state_dict(torch.load('bigger_model1.pth'))\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "\n",
    "# # Set mode to 'test'\n",
    "# mode = 'test'\n",
    "# ds_testing = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "# testloader = DataLoader(dataset = ds_testing, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "# # Testing loop\n",
    "# test_loss = 0.0\n",
    "# model.eval()\n",
    "# i = 0\n",
    "# for inputs, targets in testloader:\n",
    "#     i += 1\n",
    "#     if torch.cuda.is_available():\n",
    "#         inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "#     outputs = model(inputs)\n",
    "#     loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "#     test_loss += loss.item()\n",
    "#     if i % 20 == 0:\n",
    "#         print(f'Batch {i} \\t Testing Loss: {test_loss / i}')\n",
    "\n",
    "# print(f'Testing Loss bigger model: {test_loss / len(testloader)}')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = BiggerFCN_pyramid()\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('bigger_model_pyramid.pth'))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "mode = 'test'\n",
    "ds_testing = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data/data_no_outliers', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "testloader = DataLoader(dataset = ds_testing, batch_size = 256, shuffle = False, num_workers = 8)\n",
    "\n",
    "# Testing loop\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "i = 0\n",
    "for inputs, targets in testloader:\n",
    "    i += 1\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "    test_loss += loss.item()\n",
    "    if i % 20 == 0:\n",
    "        print(f'Batch {i} \\t Testing Loss: {test_loss / i}')\n",
    "\n",
    "print(f'Testing Loss bigger model pyramid: {test_loss / len(testloader)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
