{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S2_bands': {'B01': {'mean': 0.13021514, 'std': 0.017152175, 'min': 1e-04, 'max': 1.1213, 'p1': 0.1273, 'p99': 0.1074}, 'B02': {'mean': 0.1363337, 'std': 0.018509913, 'min': 1e-04, 'max': 1.8768, 'p1': 0.1366, 'p99': 0.1128}, 'B03': {'mean': 0.16427371, 'std': 0.02087248, 'min': 0.0411, 'max': 1.7888, 'p1': 0.1692, 'p99': 0.1364}, 'B04': {'mean': 0.13865142, 'std': 0.025569845, 'min': 0.0121, 'max': 1.7232, 'p1': 0.1445, 'p99': 0.1184}, 'B05': {'mean': 0.20296873, 'std': 0.028621713, 'min': 0.0672, 'max': 1.6344, 'p1': 0.2157, 'p99': 0.1591}, 'B06': {'mean': 0.38582557, 'std': 0.070499, 'min': 0.0758, 'max': 1.6699, 'p1': 0.3286, 'p99': 0.2766}, 'B07': {'mean': 0.4361872, 'std': 0.086211845, 'min': 0.0573, 'max': 1.6645, 'p1': 0.3621, 'p99': 0.2278}, 'B08': {'mean': 0.4448093, 'std': 0.08623231, 'min': 0.0737, 'max': 1.6976, 'p1': 0.3588, 'p99': 0.2122}, 'B8A': {'mean': 0.4580875, 'std': 0.08798952, 'min': 0.0772, 'max': 1.6709, 'p1': 0.3775, 'p99': 0.26}, 'B09': {'mean': 0.45806482, 'std': 0.08441155, 'min': 0.0066, 'max': 1.7178, 'p1': 0.3818, 'p99': 0.2655}, 'B11': {'mean': 0.2941879, 'std': 0.04741236, 'min': 0.0994, 'max': 1.5738, 'p1': 0.3279, 'p99': 0.5299}, 'B12': {'mean': 0.19771282, 'std': 0.0438055, 'min': 0.0974, 'max': 1.6086, 'p1': 0.22, 'p99': 0.1688}}, 'BM': {'bm': {'mean': 50.714764, 'std': 35.78637, 'min': 0.0, 'max': 312.03958, 'p1': 0.0, 'p99': 127.57817}, 'std': {'mean': 7.036375, 'std': 4.400113, 'min': 0.0, 'max': 145.97275, 'p1': 0.0, 'p99': 2.180725}}, 'Sentinel_metadata': {'S2_vegetation_score': {'mean': 96.60825, 'std': 11.659279, 'min': 20.0, 'max': 100.0, 'p1': 100.0, 'p99': 48.0}, 'S2_date': {'mean': 470.45947, 'std': 169.251, 'min': 1.0, 'max': 620.0, 'p1': 212.0, 'p99': 122.0}}, 'GEDI': {'agbd': {'mean': 77.14966, 'std': 79.39693, 'min': 0.66613775, 'max': 499.96765, 'p1': 1.0619253, 'p99': 75.21779}, 'agbd_se': {'mean': 7.2501493, 'std': 2.1052322, 'min': 2.981795, 'max': 12.131867, 'p1': 3.004118, 'p99': 11.066575}, 'rh98': {'mean': 12.344467, 'std': 9.239699, 'min': 0.030004844, 'max': 96.50001, 'p1': 2.9900095, 'p99': 33.88001}, 'date': {'mean': 394.3948, 'std': 149.13177, 'min': 46.0, 'max': 531.0, 'p1': 158.0, 'p99': 503.0}}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in binary mode for reading\n",
    "with open('data/normalization_values.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Now you can analyze the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training tiles:  45\n",
      "validation tiles:  10\n",
      "testing tiles:  15\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {'train': [], 'val': [], 'test': []} \n",
    "path_h5 = '/scratch2/biomass_estimation/code/ml/data'\n",
    "\n",
    "# Iterate over all the h5 files\n",
    "for fname in os.listdir(path_h5):\n",
    "    if fname.endswith('.h5'):\n",
    "        with h5py.File(os.path.join(path_h5, fname), 'r') as f:\n",
    "            # Get the list of all tiles in the file\n",
    "            all_tiles = list(f.keys())\n",
    "            \n",
    "            # Select one tile for validation, one for testing, and the rest for training\n",
    "            val_tile = all_tiles[0:2]\n",
    "            test_tile = all_tiles[2:5]\n",
    "            train_tiles = all_tiles[5:]\n",
    "            \n",
    "            # Add the selected tiles to the dictionary\n",
    "            data['val'].extend(val_tile)\n",
    "            data['test'].extend(test_tile)\n",
    "            data['train'].extend(train_tiles)\n",
    "\n",
    "print(\"training tiles: \", len(data['train']))\n",
    "print(\"validation tiles: \", len(data['val']))\n",
    "print(\"testing tiles: \", len(data['test']))\n",
    "# Pickle the DataFrame and save it to a file\n",
    "with open('/scratch2/biomass_estimation/code/ml/data/mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=18,\n",
    "                 channel_dims = (16, 32, 64, 128),\n",
    "                 num_outputs=1,\n",
    "                 kernel_size=3,\n",
    "                 stride=1):\n",
    "        \"\"\"\n",
    "        A simple fully convolutional neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        layers = list()\n",
    "        for i in range(len(channel_dims)):\n",
    "            in_channels = in_features if i == 0 else channel_dims[i-1]\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, \n",
    "                                    out_channels=channel_dims[i], \n",
    "                                    kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(num_features=channel_dims[i]))\n",
    "            layers.append(self.relu)\n",
    "        print(layers)\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(in_channels=channel_dims[-1], out_channels=num_outputs, kernel_size=1,\n",
    "                                     stride=1, padding=0, bias=True)\n",
    "        # self.fc = nn.Linear(15*15*num_outputs, 1)  # Fully connected layer to get a single output value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_output(x)\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        # predictions = self.fc(x)\n",
    "        # return predictions.squeeze()  # Remove the extra dimension\n",
    "        return x\n",
    "    \n",
    "model = SimpleFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "Epoch 1 \t Batch 20 \t Training Loss: 107.96627845764161\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 96.97118949890137\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 86.55185635884602\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 79.15793976783752\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 74.40045162200927\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 71.2119413693746\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 68.9102454321725\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 67.11922392845153\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 65.45766722361246\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 64.06149347305298\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 62.97395713112571\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 62.12037084897359\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 61.4299839753371\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 60.79450652258737\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 60.199456214904785\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 59.61034009456635\n",
      "Epoch 1 \t Batch 340 \t Training Loss: 59.111156070933625\n",
      "Epoch 1 \t Batch 360 \t Training Loss: 58.836543485853404\n",
      "Epoch 1 \t Batch 380 \t Training Loss: 58.45463349191766\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 58.125106496810915\n",
      "Epoch 1 \t Batch 420 \t Training Loss: 57.847829609825496\n",
      "Epoch 1 \t Batch 440 \t Training Loss: 57.568425846099856\n",
      "Epoch 1 \t Batch 460 \t Training Loss: 57.304801144807236\n",
      "Epoch 1 \t Batch 480 \t Training Loss: 57.12771626313528\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 56.874263473510744\n",
      "Epoch 1 \t Batch 520 \t Training Loss: 56.6769599547753\n",
      "Epoch 1 \t Batch 540 \t Training Loss: 56.511144878246164\n",
      "Epoch 1 \t Batch 560 \t Training Loss: 56.301423467908585\n",
      "Epoch 1 \t Batch 580 \t Training Loss: 56.15247921779238\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 55.98330140431722\n",
      "Epoch 1 \t Batch 620 \t Training Loss: 55.844592026741275\n",
      "Epoch 1 \t Batch 640 \t Training Loss: 55.685767298936845\n",
      "Epoch 1 \t Batch 660 \t Training Loss: 55.511227058641836\n",
      "Epoch 1 \t Batch 680 \t Training Loss: 55.37522890427533\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 55.24644851139614\n",
      "Epoch 1 \t Batch 720 \t Training Loss: 55.16043585671319\n",
      "Epoch 1 \t Batch 740 \t Training Loss: 55.08546589516305\n",
      "Epoch 1 \t Batch 760 \t Training Loss: 54.97427943380256\n",
      "Epoch 1 \t Batch 780 \t Training Loss: 54.85859143672845\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 54.72937567710876\n",
      "Epoch 1 \t Batch 820 \t Training Loss: 54.66192902820866\n",
      "Epoch 1 \t Batch 840 \t Training Loss: 54.578609952472505\n",
      "Epoch 1 \t Batch 860 \t Training Loss: 54.52444485065549\n",
      "Epoch 1 \t Batch 880 \t Training Loss: 54.44652063629844\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 54.35557930840386\n",
      "Epoch 1 \t Batch 20 \t Validation Loss: 13.67384114265442\n",
      "Epoch 1 \t Batch 40 \t Validation Loss: 18.21975984573364\n",
      "Epoch 1 \t Batch 60 \t Validation Loss: 17.847594245274863\n",
      "Epoch 1 \t Batch 80 \t Validation Loss: 18.577114868164063\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 20.485493335723877\n",
      "Epoch 1 \t Batch 120 \t Validation Loss: 22.29228823184967\n",
      "Epoch 1 \t Batch 140 \t Validation Loss: 23.268066508429392\n",
      "Epoch 1 \t Batch 160 \t Validation Loss: 26.153454130887987\n",
      "Epoch 1 \t Batch 180 \t Validation Loss: 30.933141321606108\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 33.16371443271637\n",
      "Epoch 1 \t Batch 220 \t Validation Loss: 35.32582260045138\n",
      "Epoch 1 \t Batch 240 \t Validation Loss: 36.23218187491099\n",
      "Epoch 1 \t Batch 260 \t Validation Loss: 38.893718220637396\n",
      "Epoch 1 \t Batch 280 \t Validation Loss: 40.3335202217102\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 41.84189879099528\n",
      "Epoch 1 \t Batch 320 \t Validation Loss: 42.5182168006897\n",
      "Epoch 1 \t Batch 340 \t Validation Loss: 42.44962785945219\n",
      "Epoch 1 \t Batch 360 \t Validation Loss: 42.44585256576538\n",
      "Epoch 1 \t Batch 380 \t Validation Loss: 42.795655215413944\n",
      "Epoch 1 \t Batch 400 \t Validation Loss: 42.26670094966889\n",
      "Epoch 1 \t Batch 420 \t Validation Loss: 42.21295507521857\n",
      "Epoch 1 \t Batch 440 \t Validation Loss: 41.82047927162864\n",
      "Epoch 1 \t Batch 460 \t Validation Loss: 41.941623882625414\n",
      "Epoch 1 \t Batch 480 \t Validation Loss: 42.37310252984365\n",
      "Epoch 1 \t Batch 500 \t Validation Loss: 42.05688134384155\n",
      "Epoch 1 \t Batch 520 \t Validation Loss: 41.841867245160614\n",
      "Epoch 1 \t Batch 540 \t Validation Loss: 41.48889763090346\n",
      "Epoch 1 \t Batch 560 \t Validation Loss: 41.21679513795035\n",
      "Epoch 1 \t Batch 580 \t Validation Loss: 41.00626513382484\n",
      "Epoch 1 \t Batch 600 \t Validation Loss: 41.08705578486125\n",
      "Epoch 1 Training Loss: 54.31591204910528 Validation Loss: 41.6560514035163\n",
      "Validation Loss Decreased(inf--->25660.12766456604) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 20 \t Training Loss: 49.50083675384521\n",
      "Epoch 2 \t Batch 40 \t Training Loss: 50.051589012145996\n",
      "Epoch 2 \t Batch 60 \t Training Loss: 50.271293512980144\n",
      "Epoch 2 \t Batch 80 \t Training Loss: 50.5182288646698\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 50.84779716491699\n",
      "Epoch 2 \t Batch 120 \t Training Loss: 50.97112429936727\n",
      "Epoch 2 \t Batch 140 \t Training Loss: 51.17836527143206\n",
      "Epoch 2 \t Batch 160 \t Training Loss: 50.98089225292206\n",
      "Epoch 2 \t Batch 180 \t Training Loss: 50.91295344034831\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 50.95534809112549\n",
      "Epoch 2 \t Batch 220 \t Training Loss: 50.94052881761031\n",
      "Epoch 2 \t Batch 240 \t Training Loss: 50.90286113421122\n",
      "Epoch 2 \t Batch 260 \t Training Loss: 51.058945949261\n",
      "Epoch 2 \t Batch 280 \t Training Loss: 51.04466255732945\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 50.9740988667806\n",
      "Epoch 2 \t Batch 320 \t Training Loss: 50.963631772994994\n",
      "Epoch 2 \t Batch 340 \t Training Loss: 50.93999553007238\n",
      "Epoch 2 \t Batch 360 \t Training Loss: 50.82650480270386\n",
      "Epoch 2 \t Batch 380 \t Training Loss: 50.874090445669076\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 50.887744102478024\n",
      "Epoch 2 \t Batch 420 \t Training Loss: 50.81253996349516\n",
      "Epoch 2 \t Batch 440 \t Training Loss: 50.765664369409734\n",
      "Epoch 2 \t Batch 460 \t Training Loss: 50.716868682529615\n",
      "Epoch 2 \t Batch 480 \t Training Loss: 50.6450093905131\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 50.644658020019534\n",
      "Epoch 2 \t Batch 520 \t Training Loss: 50.631315480745755\n",
      "Epoch 2 \t Batch 540 \t Training Loss: 50.62361474920202\n",
      "Epoch 2 \t Batch 560 \t Training Loss: 50.545272302627566\n",
      "Epoch 2 \t Batch 580 \t Training Loss: 50.555384372842724\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 50.49008708953858\n",
      "Epoch 2 \t Batch 620 \t Training Loss: 50.47055309664819\n",
      "Epoch 2 \t Batch 640 \t Training Loss: 50.46752934455871\n",
      "Epoch 2 \t Batch 660 \t Training Loss: 50.445594515944975\n",
      "Epoch 2 \t Batch 680 \t Training Loss: 50.45553724625531\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 50.428836784362794\n",
      "Epoch 2 \t Batch 720 \t Training Loss: 50.449919335047404\n",
      "Epoch 2 \t Batch 740 \t Training Loss: 50.39641195761191\n",
      "Epoch 2 \t Batch 760 \t Training Loss: 50.34799185803062\n",
      "Epoch 2 \t Batch 780 \t Training Loss: 50.35713048592592\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 50.3758505153656\n",
      "Epoch 2 \t Batch 820 \t Training Loss: 50.37648186567353\n",
      "Epoch 2 \t Batch 840 \t Training Loss: 50.371023091815765\n",
      "Epoch 2 \t Batch 860 \t Training Loss: 50.34420074196749\n",
      "Epoch 2 \t Batch 880 \t Training Loss: 50.343557409806685\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 50.36065194871691\n",
      "Epoch 2 \t Batch 20 \t Validation Loss: 17.450407028198242\n",
      "Epoch 2 \t Batch 40 \t Validation Loss: 22.34304780960083\n",
      "Epoch 2 \t Batch 60 \t Validation Loss: 21.62360207239787\n",
      "Epoch 2 \t Batch 80 \t Validation Loss: 22.687489330768585\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 24.271107835769655\n",
      "Epoch 2 \t Batch 120 \t Validation Loss: 25.43545951048533\n",
      "Epoch 2 \t Batch 140 \t Validation Loss: 25.969601706096103\n",
      "Epoch 2 \t Batch 160 \t Validation Loss: 28.05465449690819\n",
      "Epoch 2 \t Batch 180 \t Validation Loss: 31.116940397686427\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 32.51773898601532\n",
      "Epoch 2 \t Batch 220 \t Validation Loss: 33.698301623084326\n",
      "Epoch 2 \t Batch 240 \t Validation Loss: 34.01484646399816\n",
      "Epoch 2 \t Batch 260 \t Validation Loss: 36.003028279084425\n",
      "Epoch 2 \t Batch 280 \t Validation Loss: 37.06822636808668\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 37.97419791857401\n",
      "Epoch 2 \t Batch 320 \t Validation Loss: 38.361388459801674\n",
      "Epoch 2 \t Batch 340 \t Validation Loss: 38.33467204991509\n",
      "Epoch 2 \t Batch 360 \t Validation Loss: 38.354228160116406\n",
      "Epoch 2 \t Batch 380 \t Validation Loss: 38.66440827470077\n",
      "Epoch 2 \t Batch 400 \t Validation Loss: 38.42804433584213\n",
      "Epoch 2 \t Batch 420 \t Validation Loss: 38.51545938083104\n",
      "Epoch 2 \t Batch 440 \t Validation Loss: 38.37538074363362\n",
      "Epoch 2 \t Batch 460 \t Validation Loss: 38.67021463435629\n",
      "Epoch 2 \t Batch 480 \t Validation Loss: 39.20070892373721\n",
      "Epoch 2 \t Batch 500 \t Validation Loss: 38.932548952102664\n",
      "Epoch 2 \t Batch 520 \t Validation Loss: 38.90897189653837\n",
      "Epoch 2 \t Batch 540 \t Validation Loss: 38.69418922706887\n",
      "Epoch 2 \t Batch 560 \t Validation Loss: 38.51173256124769\n",
      "Epoch 2 \t Batch 580 \t Validation Loss: 38.43833198547363\n",
      "Epoch 2 \t Batch 600 \t Validation Loss: 38.637675536473594\n",
      "Epoch 2 Training Loss: 50.358432503536974 Validation Loss: 39.3780910659146\n",
      "Validation Loss Decreased(25660.12766456604--->24256.904096603394) Saving The Model\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 20 \t Training Loss: 49.876457595825194\n",
      "Epoch 3 \t Batch 40 \t Training Loss: 49.38908576965332\n",
      "Epoch 3 \t Batch 60 \t Training Loss: 50.16408920288086\n",
      "Epoch 3 \t Batch 80 \t Training Loss: 50.06775999069214\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 50.00422519683838\n",
      "Epoch 3 \t Batch 120 \t Training Loss: 50.00203253428141\n",
      "Epoch 3 \t Batch 140 \t Training Loss: 49.71882375989642\n",
      "Epoch 3 \t Batch 160 \t Training Loss: 49.64609625339508\n",
      "Epoch 3 \t Batch 180 \t Training Loss: 49.728306727939184\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 49.794318199157715\n",
      "Epoch 3 \t Batch 220 \t Training Loss: 49.98892225785689\n",
      "Epoch 3 \t Batch 240 \t Training Loss: 49.93708712259929\n",
      "Epoch 3 \t Batch 260 \t Training Loss: 49.95325757540189\n",
      "Epoch 3 \t Batch 280 \t Training Loss: 49.867359965188164\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 49.83148456573486\n",
      "Epoch 3 \t Batch 320 \t Training Loss: 49.83275009393692\n",
      "Epoch 3 \t Batch 340 \t Training Loss: 49.88571845783907\n",
      "Epoch 3 \t Batch 360 \t Training Loss: 49.851981820000546\n",
      "Epoch 3 \t Batch 380 \t Training Loss: 49.78423586393657\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 49.770522813796994\n",
      "Epoch 3 \t Batch 420 \t Training Loss: 49.783484540666855\n",
      "Epoch 3 \t Batch 440 \t Training Loss: 49.8193924557079\n",
      "Epoch 3 \t Batch 460 \t Training Loss: 49.797301342176354\n",
      "Epoch 3 \t Batch 480 \t Training Loss: 49.79129252433777\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 49.79793766784668\n",
      "Epoch 3 \t Batch 520 \t Training Loss: 49.78717386539166\n",
      "Epoch 3 \t Batch 540 \t Training Loss: 49.79999695530644\n",
      "Epoch 3 \t Batch 560 \t Training Loss: 49.797554690497265\n",
      "Epoch 3 \t Batch 580 \t Training Loss: 49.78858917499411\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 49.77412807464599\n",
      "Epoch 3 \t Batch 620 \t Training Loss: 49.81348178002142\n",
      "Epoch 3 \t Batch 640 \t Training Loss: 49.80230895876885\n",
      "Epoch 3 \t Batch 660 \t Training Loss: 49.754332369024105\n",
      "Epoch 3 \t Batch 680 \t Training Loss: 49.754572200775144\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 49.75062167576381\n",
      "Epoch 3 \t Batch 720 \t Training Loss: 49.75516809357537\n",
      "Epoch 3 \t Batch 740 \t Training Loss: 49.77926240353971\n",
      "Epoch 3 \t Batch 760 \t Training Loss: 49.83477744052285\n",
      "Epoch 3 \t Batch 780 \t Training Loss: 49.76892661559276\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 49.78395055770874\n",
      "Epoch 3 \t Batch 820 \t Training Loss: 49.71118955379579\n",
      "Epoch 3 \t Batch 840 \t Training Loss: 49.7014215923491\n",
      "Epoch 3 \t Batch 860 \t Training Loss: 49.7339527884195\n",
      "Epoch 3 \t Batch 880 \t Training Loss: 49.708206597241485\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 49.716499977111816\n",
      "Epoch 3 \t Batch 20 \t Validation Loss: 20.428366661071777\n",
      "Epoch 3 \t Batch 40 \t Validation Loss: 24.942191624641417\n",
      "Epoch 3 \t Batch 60 \t Validation Loss: 24.919420607884724\n",
      "Epoch 3 \t Batch 80 \t Validation Loss: 25.956437206268312\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 26.335119228363038\n",
      "Epoch 3 \t Batch 120 \t Validation Loss: 26.99418947696686\n",
      "Epoch 3 \t Batch 140 \t Validation Loss: 27.17073358808245\n",
      "Epoch 3 \t Batch 160 \t Validation Loss: 29.0702446103096\n",
      "Epoch 3 \t Batch 180 \t Validation Loss: 32.38197266260783\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 33.85109303474426\n",
      "Epoch 3 \t Batch 220 \t Validation Loss: 35.086767595464536\n",
      "Epoch 3 \t Batch 240 \t Validation Loss: 35.44910408655802\n",
      "Epoch 3 \t Batch 260 \t Validation Loss: 37.484877824783325\n",
      "Epoch 3 \t Batch 280 \t Validation Loss: 38.54686644077301\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 39.51473251024882\n",
      "Epoch 3 \t Batch 320 \t Validation Loss: 39.92202595770359\n",
      "Epoch 3 \t Batch 340 \t Validation Loss: 39.869563346750596\n",
      "Epoch 3 \t Batch 360 \t Validation Loss: 39.76776821613312\n",
      "Epoch 3 \t Batch 380 \t Validation Loss: 40.03176538316827\n",
      "Epoch 3 \t Batch 400 \t Validation Loss: 39.71959347486496\n",
      "Epoch 3 \t Batch 420 \t Validation Loss: 39.731839295795986\n",
      "Epoch 3 \t Batch 440 \t Validation Loss: 39.52486158501018\n",
      "Epoch 3 \t Batch 460 \t Validation Loss: 39.7320449186408\n",
      "Epoch 3 \t Batch 480 \t Validation Loss: 40.21010754307111\n",
      "Epoch 3 \t Batch 500 \t Validation Loss: 39.92371413612366\n",
      "Epoch 3 \t Batch 520 \t Validation Loss: 39.78456879579104\n",
      "Epoch 3 \t Batch 540 \t Validation Loss: 39.63661653553998\n",
      "Epoch 3 \t Batch 560 \t Validation Loss: 39.65284122228623\n",
      "Epoch 3 \t Batch 580 \t Validation Loss: 39.63166512292007\n",
      "Epoch 3 \t Batch 600 \t Validation Loss: 39.94085959593455\n",
      "Epoch 3 Training Loss: 49.71033350487962 Validation Loss: 40.622549625186174\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 20 \t Training Loss: 49.75792598724365\n",
      "Epoch 4 \t Batch 40 \t Training Loss: 48.65965595245361\n",
      "Epoch 4 \t Batch 60 \t Training Loss: 49.251777521769206\n",
      "Epoch 4 \t Batch 80 \t Training Loss: 49.27982316017151\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 49.2291845703125\n",
      "Epoch 4 \t Batch 120 \t Training Loss: 49.402844746907554\n",
      "Epoch 4 \t Batch 140 \t Training Loss: 49.42315352303641\n",
      "Epoch 4 \t Batch 160 \t Training Loss: 49.381020879745485\n",
      "Epoch 4 \t Batch 180 \t Training Loss: 49.45573071373833\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 49.33961219787598\n",
      "Epoch 4 \t Batch 220 \t Training Loss: 49.32567726482045\n",
      "Epoch 4 \t Batch 240 \t Training Loss: 49.2575181166331\n",
      "Epoch 4 \t Batch 260 \t Training Loss: 49.28689286158635\n",
      "Epoch 4 \t Batch 280 \t Training Loss: 49.32076984133039\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 49.189704182942705\n",
      "Epoch 4 \t Batch 320 \t Training Loss: 49.32221961021423\n",
      "Epoch 4 \t Batch 340 \t Training Loss: 49.345959338019874\n",
      "Epoch 4 \t Batch 360 \t Training Loss: 49.32418686548869\n",
      "Epoch 4 \t Batch 380 \t Training Loss: 49.36700535824424\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 49.38826961517334\n",
      "Epoch 4 \t Batch 420 \t Training Loss: 49.40887940724691\n",
      "Epoch 4 \t Batch 440 \t Training Loss: 49.40633228475397\n",
      "Epoch 4 \t Batch 460 \t Training Loss: 49.34104423522949\n",
      "Epoch 4 \t Batch 480 \t Training Loss: 49.3079548517863\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 49.303641151428224\n",
      "Epoch 4 \t Batch 520 \t Training Loss: 49.32983352220975\n",
      "Epoch 4 \t Batch 540 \t Training Loss: 49.34307180334021\n",
      "Epoch 4 \t Batch 560 \t Training Loss: 49.33788516862052\n",
      "Epoch 4 \t Batch 580 \t Training Loss: 49.361636273614295\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 49.3451380221049\n",
      "Epoch 4 \t Batch 620 \t Training Loss: 49.36302181366951\n",
      "Epoch 4 \t Batch 640 \t Training Loss: 49.29739612936974\n",
      "Epoch 4 \t Batch 660 \t Training Loss: 49.273623085021974\n",
      "Epoch 4 \t Batch 680 \t Training Loss: 49.30899621739107\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 49.31838174547468\n",
      "Epoch 4 \t Batch 720 \t Training Loss: 49.33444543414646\n",
      "Epoch 4 \t Batch 740 \t Training Loss: 49.36898712467503\n",
      "Epoch 4 \t Batch 760 \t Training Loss: 49.38046322872764\n",
      "Epoch 4 \t Batch 780 \t Training Loss: 49.35812706580529\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 49.37647311687469\n",
      "Epoch 4 \t Batch 820 \t Training Loss: 49.384093042699305\n",
      "Epoch 4 \t Batch 840 \t Training Loss: 49.38492079235259\n",
      "Epoch 4 \t Batch 860 \t Training Loss: 49.38099682386532\n",
      "Epoch 4 \t Batch 880 \t Training Loss: 49.37728618708524\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 49.3452337858412\n",
      "Epoch 4 \t Batch 20 \t Validation Loss: 20.79830856323242\n",
      "Epoch 4 \t Batch 40 \t Validation Loss: 24.635460948944093\n",
      "Epoch 4 \t Batch 60 \t Validation Loss: 24.315127658843995\n",
      "Epoch 4 \t Batch 80 \t Validation Loss: 25.010651206970216\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 25.520196533203126\n",
      "Epoch 4 \t Batch 120 \t Validation Loss: 26.5025728225708\n",
      "Epoch 4 \t Batch 140 \t Validation Loss: 26.79738793373108\n",
      "Epoch 4 \t Batch 160 \t Validation Loss: 29.05031574368477\n",
      "Epoch 4 \t Batch 180 \t Validation Loss: 33.05427129003737\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 34.862117114067075\n",
      "Epoch 4 \t Batch 220 \t Validation Loss: 36.43101164644415\n",
      "Epoch 4 \t Batch 240 \t Validation Loss: 36.986524283885956\n",
      "Epoch 4 \t Batch 260 \t Validation Loss: 39.26766975109394\n",
      "Epoch 4 \t Batch 280 \t Validation Loss: 40.4972387279783\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 41.715352805455524\n",
      "Epoch 4 \t Batch 320 \t Validation Loss: 42.24568109214306\n",
      "Epoch 4 \t Batch 340 \t Validation Loss: 42.181910310072055\n",
      "Epoch 4 \t Batch 360 \t Validation Loss: 42.05668283303579\n",
      "Epoch 4 \t Batch 380 \t Validation Loss: 42.326268374292475\n",
      "Epoch 4 \t Batch 400 \t Validation Loss: 41.89458295583725\n",
      "Epoch 4 \t Batch 420 \t Validation Loss: 41.96445045698257\n",
      "Epoch 4 \t Batch 440 \t Validation Loss: 41.654098937728186\n",
      "Epoch 4 \t Batch 460 \t Validation Loss: 41.819665073311846\n",
      "Epoch 4 \t Batch 480 \t Validation Loss: 42.285391042629875\n",
      "Epoch 4 \t Batch 500 \t Validation Loss: 42.010901556015014\n",
      "Epoch 4 \t Batch 520 \t Validation Loss: 41.689516117022585\n",
      "Epoch 4 \t Batch 540 \t Validation Loss: 41.24112256544608\n",
      "Epoch 4 \t Batch 560 \t Validation Loss: 40.873164403438565\n",
      "Epoch 4 \t Batch 580 \t Validation Loss: 40.51982434864702\n",
      "Epoch 4 \t Batch 600 \t Validation Loss: 40.565958116849266\n",
      "Epoch 4 Training Loss: 49.35096679987018 Validation Loss: 41.12567339779495\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 20 \t Training Loss: 48.71676902770996\n",
      "Epoch 5 \t Batch 40 \t Training Loss: 47.9249192237854\n",
      "Epoch 5 \t Batch 60 \t Training Loss: 48.40711657206217\n",
      "Epoch 5 \t Batch 80 \t Training Loss: 48.736485385894774\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 48.92202381134033\n",
      "Epoch 5 \t Batch 120 \t Training Loss: 49.15733807881673\n",
      "Epoch 5 \t Batch 140 \t Training Loss: 49.33723697662354\n",
      "Epoch 5 \t Batch 160 \t Training Loss: 49.24366137981415\n",
      "Epoch 5 \t Batch 180 \t Training Loss: 49.22533702850342\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 49.2779487991333\n",
      "Epoch 5 \t Batch 220 \t Training Loss: 49.196733596108174\n",
      "Epoch 5 \t Batch 240 \t Training Loss: 49.1778577486674\n",
      "Epoch 5 \t Batch 260 \t Training Loss: 49.030509567260744\n",
      "Epoch 5 \t Batch 280 \t Training Loss: 48.98856841496059\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 48.97125738779704\n",
      "Epoch 5 \t Batch 320 \t Training Loss: 48.99082034826279\n",
      "Epoch 5 \t Batch 340 \t Training Loss: 49.036443833743824\n",
      "Epoch 5 \t Batch 360 \t Training Loss: 49.01092032326592\n",
      "Epoch 5 \t Batch 380 \t Training Loss: 49.105298985933004\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 49.13973843574524\n",
      "Epoch 5 \t Batch 420 \t Training Loss: 49.16669415973482\n",
      "Epoch 5 \t Batch 440 \t Training Loss: 49.17626106955788\n",
      "Epoch 5 \t Batch 460 \t Training Loss: 49.123265606424084\n",
      "Epoch 5 \t Batch 480 \t Training Loss: 49.08281150658925\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 49.12496636199951\n",
      "Epoch 5 \t Batch 520 \t Training Loss: 49.118755626678464\n",
      "Epoch 5 \t Batch 540 \t Training Loss: 49.05601577758789\n",
      "Epoch 5 \t Batch 560 \t Training Loss: 49.08130524499076\n",
      "Epoch 5 \t Batch 580 \t Training Loss: 49.10490837097168\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 49.13824312845866\n",
      "Epoch 5 \t Batch 620 \t Training Loss: 49.11552237233808\n",
      "Epoch 5 \t Batch 640 \t Training Loss: 49.08819682598114\n",
      "Epoch 5 \t Batch 660 \t Training Loss: 49.11320505431204\n",
      "Epoch 5 \t Batch 680 \t Training Loss: 49.13512409434599\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 49.105561992100306\n",
      "Epoch 5 \t Batch 720 \t Training Loss: 49.09473147392273\n",
      "Epoch 5 \t Batch 740 \t Training Loss: 49.06498350710482\n",
      "Epoch 5 \t Batch 760 \t Training Loss: 49.108456907774276\n",
      "Epoch 5 \t Batch 780 \t Training Loss: 49.07152577913725\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 49.041935029029844\n",
      "Epoch 5 \t Batch 820 \t Training Loss: 49.05945005184267\n",
      "Epoch 5 \t Batch 840 \t Training Loss: 49.057045614151725\n",
      "Epoch 5 \t Batch 860 \t Training Loss: 49.0596786809522\n",
      "Epoch 5 \t Batch 880 \t Training Loss: 49.07062380530618\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 49.07321601019965\n",
      "Epoch 5 \t Batch 20 \t Validation Loss: 18.809834814071657\n",
      "Epoch 5 \t Batch 40 \t Validation Loss: 24.675281119346618\n",
      "Epoch 5 \t Batch 60 \t Validation Loss: 25.40485755602519\n",
      "Epoch 5 \t Batch 80 \t Validation Loss: 26.106478726863863\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 26.39964140892029\n",
      "Epoch 5 \t Batch 120 \t Validation Loss: 27.072758833567303\n",
      "Epoch 5 \t Batch 140 \t Validation Loss: 27.175414439610073\n",
      "Epoch 5 \t Batch 160 \t Validation Loss: 28.916803860664366\n",
      "Epoch 5 \t Batch 180 \t Validation Loss: 32.41480540699429\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 33.93945222377777\n",
      "Epoch 5 \t Batch 220 \t Validation Loss: 35.291107684915715\n",
      "Epoch 5 \t Batch 240 \t Validation Loss: 35.690091963609056\n",
      "Epoch 5 \t Batch 260 \t Validation Loss: 37.85565444506132\n",
      "Epoch 5 \t Batch 280 \t Validation Loss: 39.00349473612649\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 39.98521194140116\n",
      "Epoch 5 \t Batch 320 \t Validation Loss: 40.38468205034733\n",
      "Epoch 5 \t Batch 340 \t Validation Loss: 40.278905046687406\n",
      "Epoch 5 \t Batch 360 \t Validation Loss: 40.03676711453332\n",
      "Epoch 5 \t Batch 380 \t Validation Loss: 40.24903163658945\n",
      "Epoch 5 \t Batch 400 \t Validation Loss: 39.782860596179965\n",
      "Epoch 5 \t Batch 420 \t Validation Loss: 39.75034208978926\n",
      "Epoch 5 \t Batch 440 \t Validation Loss: 39.430625358494844\n",
      "Epoch 5 \t Batch 460 \t Validation Loss: 39.598171812555066\n",
      "Epoch 5 \t Batch 480 \t Validation Loss: 40.021953779459\n",
      "Epoch 5 \t Batch 500 \t Validation Loss: 39.703328699111935\n",
      "Epoch 5 \t Batch 520 \t Validation Loss: 39.40696121545938\n",
      "Epoch 5 \t Batch 540 \t Validation Loss: 39.079736644250374\n",
      "Epoch 5 \t Batch 560 \t Validation Loss: 38.81755941084453\n",
      "Epoch 5 \t Batch 580 \t Validation Loss: 38.540351895628305\n",
      "Epoch 5 \t Batch 600 \t Validation Loss: 38.69095665454864\n",
      "Epoch 5 Training Loss: 49.053401293905374 Validation Loss: 39.290109125050634\n",
      "Validation Loss Decreased(24256.904096603394--->24202.70722103119) Saving The Model\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 20 \t Training Loss: 49.53240070343018\n",
      "Epoch 6 \t Batch 40 \t Training Loss: 48.399935245513916\n",
      "Epoch 6 \t Batch 60 \t Training Loss: 48.63390343983968\n",
      "Epoch 6 \t Batch 80 \t Training Loss: 48.25970611572266\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 48.598562507629396\n",
      "Epoch 6 \t Batch 120 \t Training Loss: 48.53405952453613\n",
      "Epoch 6 \t Batch 140 \t Training Loss: 48.99622579302107\n",
      "Epoch 6 \t Batch 160 \t Training Loss: 49.14235036373138\n",
      "Epoch 6 \t Batch 180 \t Training Loss: 49.07998824649387\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 48.98470199584961\n",
      "Epoch 6 \t Batch 220 \t Training Loss: 48.904057242653586\n",
      "Epoch 6 \t Batch 240 \t Training Loss: 48.895848814646406\n",
      "Epoch 6 \t Batch 260 \t Training Loss: 48.803128462571365\n",
      "Epoch 6 \t Batch 280 \t Training Loss: 48.716946043287\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 48.626742401123046\n",
      "Epoch 6 \t Batch 320 \t Training Loss: 48.53282574415207\n",
      "Epoch 6 \t Batch 340 \t Training Loss: 48.53856058681713\n",
      "Epoch 6 \t Batch 360 \t Training Loss: 48.48850407070584\n",
      "Epoch 6 \t Batch 380 \t Training Loss: 48.480737816660024\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 48.466727619171145\n",
      "Epoch 6 \t Batch 420 \t Training Loss: 48.57291072663807\n",
      "Epoch 6 \t Batch 440 \t Training Loss: 48.582208173925224\n",
      "Epoch 6 \t Batch 460 \t Training Loss: 48.57413841745128\n",
      "Epoch 6 \t Batch 480 \t Training Loss: 48.61940478483836\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 48.68871656799316\n",
      "Epoch 6 \t Batch 520 \t Training Loss: 48.69319354570829\n",
      "Epoch 6 \t Batch 540 \t Training Loss: 48.63847149036549\n",
      "Epoch 6 \t Batch 560 \t Training Loss: 48.65218932288034\n",
      "Epoch 6 \t Batch 580 \t Training Loss: 48.684228752399314\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 48.70910250981649\n",
      "Epoch 6 \t Batch 620 \t Training Loss: 48.74348644133537\n",
      "Epoch 6 \t Batch 640 \t Training Loss: 48.737184554338455\n",
      "Epoch 6 \t Batch 660 \t Training Loss: 48.749966135892\n",
      "Epoch 6 \t Batch 680 \t Training Loss: 48.79621356515323\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 48.85164865221296\n",
      "Epoch 6 \t Batch 720 \t Training Loss: 48.81582785182529\n",
      "Epoch 6 \t Batch 740 \t Training Loss: 48.80225187250086\n",
      "Epoch 6 \t Batch 760 \t Training Loss: 48.77756374760678\n",
      "Epoch 6 \t Batch 780 \t Training Loss: 48.775122177906525\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 48.77588564872742\n",
      "Epoch 6 \t Batch 820 \t Training Loss: 48.770855149990176\n",
      "Epoch 6 \t Batch 840 \t Training Loss: 48.777904456002375\n",
      "Epoch 6 \t Batch 860 \t Training Loss: 48.80505589329919\n",
      "Epoch 6 \t Batch 880 \t Training Loss: 48.827181083505806\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 48.84054081810845\n",
      "Epoch 6 \t Batch 20 \t Validation Loss: 25.231075763702393\n",
      "Epoch 6 \t Batch 40 \t Validation Loss: 30.877985811233522\n",
      "Epoch 6 \t Batch 60 \t Validation Loss: 31.426241175333658\n",
      "Epoch 6 \t Batch 80 \t Validation Loss: 32.101484990119936\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 31.51496265411377\n",
      "Epoch 6 \t Batch 120 \t Validation Loss: 31.419674825668334\n",
      "Epoch 6 \t Batch 140 \t Validation Loss: 30.964650876181466\n",
      "Epoch 6 \t Batch 160 \t Validation Loss: 32.19617863893509\n",
      "Epoch 6 \t Batch 180 \t Validation Loss: 35.09354067908393\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 36.32191961288452\n",
      "Epoch 6 \t Batch 220 \t Validation Loss: 37.32842976830222\n",
      "Epoch 6 \t Batch 240 \t Validation Loss: 37.441204742590585\n",
      "Epoch 6 \t Batch 260 \t Validation Loss: 39.34850219579843\n",
      "Epoch 6 \t Batch 280 \t Validation Loss: 40.315239906311035\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 41.073932832082114\n",
      "Epoch 6 \t Batch 320 \t Validation Loss: 41.29688659310341\n",
      "Epoch 6 \t Batch 340 \t Validation Loss: 41.11200969359454\n",
      "Epoch 6 \t Batch 360 \t Validation Loss: 40.8195298300849\n",
      "Epoch 6 \t Batch 380 \t Validation Loss: 40.987062183179354\n",
      "Epoch 6 \t Batch 400 \t Validation Loss: 40.52688105583191\n",
      "Epoch 6 \t Batch 420 \t Validation Loss: 40.48511482874552\n",
      "Epoch 6 \t Batch 440 \t Validation Loss: 40.15698207941922\n",
      "Epoch 6 \t Batch 460 \t Validation Loss: 40.27064471037492\n",
      "Epoch 6 \t Batch 480 \t Validation Loss: 40.65793782869975\n",
      "Epoch 6 \t Batch 500 \t Validation Loss: 40.31766164779663\n",
      "Epoch 6 \t Batch 520 \t Validation Loss: 40.08777161378127\n",
      "Epoch 6 \t Batch 540 \t Validation Loss: 39.83192053194399\n",
      "Epoch 6 \t Batch 560 \t Validation Loss: 39.72879808630262\n",
      "Epoch 6 \t Batch 580 \t Validation Loss: 39.697352475133435\n",
      "Epoch 6 \t Batch 600 \t Validation Loss: 39.89441491127014\n",
      "Epoch 6 Training Loss: 48.865262242673914 Validation Loss: 40.600427029968856\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 20 \t Training Loss: 47.25439720153808\n",
      "Epoch 7 \t Batch 40 \t Training Loss: 47.94587278366089\n",
      "Epoch 7 \t Batch 60 \t Training Loss: 47.99195276896159\n",
      "Epoch 7 \t Batch 80 \t Training Loss: 48.259227991104126\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 48.554574470520016\n",
      "Epoch 7 \t Batch 120 \t Training Loss: 48.820244534810385\n",
      "Epoch 7 \t Batch 140 \t Training Loss: 49.10840748378209\n",
      "Epoch 7 \t Batch 160 \t Training Loss: 48.871484231948855\n",
      "Epoch 7 \t Batch 180 \t Training Loss: 48.7783802456326\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 48.734120769500734\n",
      "Epoch 7 \t Batch 220 \t Training Loss: 48.87585770000111\n",
      "Epoch 7 \t Batch 240 \t Training Loss: 48.79653021494548\n",
      "Epoch 7 \t Batch 260 \t Training Loss: 48.7230867092426\n",
      "Epoch 7 \t Batch 280 \t Training Loss: 48.76865852900914\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 48.77232711791992\n",
      "Epoch 7 \t Batch 320 \t Training Loss: 48.76784654855728\n",
      "Epoch 7 \t Batch 340 \t Training Loss: 48.615559869654035\n",
      "Epoch 7 \t Batch 360 \t Training Loss: 48.70542416042752\n",
      "Epoch 7 \t Batch 380 \t Training Loss: 48.75429200624165\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 48.66941533088684\n",
      "Epoch 7 \t Batch 420 \t Training Loss: 48.63145387740362\n",
      "Epoch 7 \t Batch 440 \t Training Loss: 48.63850956830112\n",
      "Epoch 7 \t Batch 460 \t Training Loss: 48.66466097624406\n",
      "Epoch 7 \t Batch 480 \t Training Loss: 48.62028467655182\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 48.608683647155765\n",
      "Epoch 7 \t Batch 520 \t Training Loss: 48.582772614405705\n",
      "Epoch 7 \t Batch 540 \t Training Loss: 48.606170936867045\n",
      "Epoch 7 \t Batch 560 \t Training Loss: 48.614248520987374\n",
      "Epoch 7 \t Batch 580 \t Training Loss: 48.59049516217462\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 48.617887261708574\n",
      "Epoch 7 \t Batch 620 \t Training Loss: 48.58241650981288\n",
      "Epoch 7 \t Batch 640 \t Training Loss: 48.62064260840416\n",
      "Epoch 7 \t Batch 660 \t Training Loss: 48.630084760261305\n",
      "Epoch 7 \t Batch 680 \t Training Loss: 48.701922714009\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 48.72375569479806\n",
      "Epoch 7 \t Batch 720 \t Training Loss: 48.6958315955268\n",
      "Epoch 7 \t Batch 740 \t Training Loss: 48.713953873917866\n",
      "Epoch 7 \t Batch 760 \t Training Loss: 48.70216252678319\n",
      "Epoch 7 \t Batch 780 \t Training Loss: 48.69484890179756\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 48.678980360031126\n",
      "Epoch 7 \t Batch 820 \t Training Loss: 48.695273678477214\n",
      "Epoch 7 \t Batch 840 \t Training Loss: 48.697716672079906\n",
      "Epoch 7 \t Batch 860 \t Training Loss: 48.728154231226725\n",
      "Epoch 7 \t Batch 880 \t Training Loss: 48.71879426782782\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 48.7170191277398\n",
      "Epoch 7 \t Batch 20 \t Validation Loss: 22.73989691734314\n",
      "Epoch 7 \t Batch 40 \t Validation Loss: 27.497934198379518\n",
      "Epoch 7 \t Batch 60 \t Validation Loss: 28.52307507197062\n",
      "Epoch 7 \t Batch 80 \t Validation Loss: 29.172783160209654\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 28.771305141448973\n",
      "Epoch 7 \t Batch 120 \t Validation Loss: 29.143275356292726\n",
      "Epoch 7 \t Batch 140 \t Validation Loss: 29.033915608269826\n",
      "Epoch 7 \t Batch 160 \t Validation Loss: 30.640750581026076\n",
      "Epoch 7 \t Batch 180 \t Validation Loss: 34.07635686132643\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 35.46796983242035\n",
      "Epoch 7 \t Batch 220 \t Validation Loss: 36.74653468132019\n",
      "Epoch 7 \t Batch 240 \t Validation Loss: 37.10860728820165\n",
      "Epoch 7 \t Batch 260 \t Validation Loss: 39.204405747927154\n",
      "Epoch 7 \t Batch 280 \t Validation Loss: 40.25421552317483\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 41.24054498990377\n",
      "Epoch 7 \t Batch 320 \t Validation Loss: 41.61209760606289\n",
      "Epoch 7 \t Batch 340 \t Validation Loss: 41.43043245708241\n",
      "Epoch 7 \t Batch 360 \t Validation Loss: 41.17321818934546\n",
      "Epoch 7 \t Batch 380 \t Validation Loss: 41.355400635066786\n",
      "Epoch 7 \t Batch 400 \t Validation Loss: 40.851614320278166\n",
      "Epoch 7 \t Batch 420 \t Validation Loss: 40.761636522838046\n",
      "Epoch 7 \t Batch 440 \t Validation Loss: 40.398144056580286\n",
      "Epoch 7 \t Batch 460 \t Validation Loss: 40.53335460372593\n",
      "Epoch 7 \t Batch 480 \t Validation Loss: 40.91969646414121\n",
      "Epoch 7 \t Batch 500 \t Validation Loss: 40.574656606674196\n",
      "Epoch 7 \t Batch 520 \t Validation Loss: 40.29067055445451\n",
      "Epoch 7 \t Batch 540 \t Validation Loss: 39.946580182181464\n",
      "Epoch 7 \t Batch 560 \t Validation Loss: 39.688273223808835\n",
      "Epoch 7 \t Batch 580 \t Validation Loss: 39.4481334604066\n",
      "Epoch 7 \t Batch 600 \t Validation Loss: 39.57050955295563\n",
      "Epoch 7 Training Loss: 48.688689257872404 Validation Loss: 40.190140923896394\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 20 \t Training Loss: 49.661896324157716\n",
      "Epoch 8 \t Batch 40 \t Training Loss: 48.587382316589355\n",
      "Epoch 8 \t Batch 60 \t Training Loss: 48.9064905166626\n",
      "Epoch 8 \t Batch 80 \t Training Loss: 48.74918417930603\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 48.66925579071045\n",
      "Epoch 8 \t Batch 120 \t Training Loss: 48.85734078089396\n",
      "Epoch 8 \t Batch 140 \t Training Loss: 48.71420219966343\n",
      "Epoch 8 \t Batch 160 \t Training Loss: 48.61856815814972\n",
      "Epoch 8 \t Batch 180 \t Training Loss: 48.54222708808051\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 48.67964319229126\n",
      "Epoch 8 \t Batch 220 \t Training Loss: 48.65502974770286\n",
      "Epoch 8 \t Batch 240 \t Training Loss: 48.67312164306641\n",
      "Epoch 8 \t Batch 260 \t Training Loss: 48.67255786015437\n",
      "Epoch 8 \t Batch 280 \t Training Loss: 48.64824717385428\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 48.611861890157066\n",
      "Epoch 8 \t Batch 320 \t Training Loss: 48.52247219085693\n",
      "Epoch 8 \t Batch 340 \t Training Loss: 48.599305164112764\n",
      "Epoch 8 \t Batch 360 \t Training Loss: 48.49056554370456\n",
      "Epoch 8 \t Batch 380 \t Training Loss: 48.44605984938772\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 48.4659827709198\n",
      "Epoch 8 \t Batch 420 \t Training Loss: 48.43624461946033\n",
      "Epoch 8 \t Batch 440 \t Training Loss: 48.49949259324507\n",
      "Epoch 8 \t Batch 460 \t Training Loss: 48.54152206752611\n",
      "Epoch 8 \t Batch 480 \t Training Loss: 48.54766955375671\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 48.59395096588135\n",
      "Epoch 8 \t Batch 520 \t Training Loss: 48.60523308974046\n",
      "Epoch 8 \t Batch 540 \t Training Loss: 48.56192564787688\n",
      "Epoch 8 \t Batch 560 \t Training Loss: 48.53907903943743\n",
      "Epoch 8 \t Batch 580 \t Training Loss: 48.50763797102303\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 48.48420110702514\n",
      "Epoch 8 \t Batch 620 \t Training Loss: 48.451578841670866\n",
      "Epoch 8 \t Batch 640 \t Training Loss: 48.43915987610817\n",
      "Epoch 8 \t Batch 660 \t Training Loss: 48.41205653566303\n",
      "Epoch 8 \t Batch 680 \t Training Loss: 48.42276461545159\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 48.43238122667585\n",
      "Epoch 8 \t Batch 720 \t Training Loss: 48.47265180481805\n",
      "Epoch 8 \t Batch 740 \t Training Loss: 48.47582233532055\n",
      "Epoch 8 \t Batch 760 \t Training Loss: 48.484772978330916\n",
      "Epoch 8 \t Batch 780 \t Training Loss: 48.47720711536896\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 48.475247411727906\n",
      "Epoch 8 \t Batch 820 \t Training Loss: 48.50448215298536\n",
      "Epoch 8 \t Batch 840 \t Training Loss: 48.520020916348415\n",
      "Epoch 8 \t Batch 860 \t Training Loss: 48.503930468891944\n",
      "Epoch 8 \t Batch 880 \t Training Loss: 48.51401382359592\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 48.53055534362793\n",
      "Epoch 8 \t Batch 20 \t Validation Loss: 16.89287896156311\n",
      "Epoch 8 \t Batch 40 \t Validation Loss: 23.551401448249816\n",
      "Epoch 8 \t Batch 60 \t Validation Loss: 24.168025779724122\n",
      "Epoch 8 \t Batch 80 \t Validation Loss: 25.26104850769043\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 25.50131513595581\n",
      "Epoch 8 \t Batch 120 \t Validation Loss: 26.206474844614664\n",
      "Epoch 8 \t Batch 140 \t Validation Loss: 26.420509229387555\n",
      "Epoch 8 \t Batch 160 \t Validation Loss: 28.27845059633255\n",
      "Epoch 8 \t Batch 180 \t Validation Loss: 31.69938564830356\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 33.222815470695494\n",
      "Epoch 8 \t Batch 220 \t Validation Loss: 34.54163110472939\n",
      "Epoch 8 \t Batch 240 \t Validation Loss: 34.95113650957743\n",
      "Epoch 8 \t Batch 260 \t Validation Loss: 37.10357285646292\n",
      "Epoch 8 \t Batch 280 \t Validation Loss: 38.21632122652871\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 39.186643584569296\n",
      "Epoch 8 \t Batch 320 \t Validation Loss: 39.60161100924015\n",
      "Epoch 8 \t Batch 340 \t Validation Loss: 39.5272774107316\n",
      "Epoch 8 \t Batch 360 \t Validation Loss: 39.339893118540445\n",
      "Epoch 8 \t Batch 380 \t Validation Loss: 39.62819073827643\n",
      "Epoch 8 \t Batch 400 \t Validation Loss: 39.25727973937988\n",
      "Epoch 8 \t Batch 420 \t Validation Loss: 39.27446771349226\n",
      "Epoch 8 \t Batch 440 \t Validation Loss: 39.05930236469616\n",
      "Epoch 8 \t Batch 460 \t Validation Loss: 39.3161784130594\n",
      "Epoch 8 \t Batch 480 \t Validation Loss: 39.75235052506129\n",
      "Epoch 8 \t Batch 500 \t Validation Loss: 39.44792782211304\n",
      "Epoch 8 \t Batch 520 \t Validation Loss: 39.263699768139766\n",
      "Epoch 8 \t Batch 540 \t Validation Loss: 38.969897462703564\n",
      "Epoch 8 \t Batch 560 \t Validation Loss: 38.71024339709963\n",
      "Epoch 8 \t Batch 580 \t Validation Loss: 38.46198228803174\n",
      "Epoch 8 \t Batch 600 \t Validation Loss: 38.62307660897573\n",
      "Epoch 8 Training Loss: 48.534192745662175 Validation Loss: 39.19608258736598\n",
      "Validation Loss Decreased(24202.70722103119--->24144.786873817444) Saving The Model\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 20 \t Training Loss: 47.72603549957275\n",
      "Epoch 9 \t Batch 40 \t Training Loss: 48.29287424087524\n",
      "Epoch 9 \t Batch 60 \t Training Loss: 48.43457616170247\n",
      "Epoch 9 \t Batch 80 \t Training Loss: 48.06402902603149\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 48.02042449951172\n",
      "Epoch 9 \t Batch 120 \t Training Loss: 47.77843608856201\n",
      "Epoch 9 \t Batch 140 \t Training Loss: 47.934785216195245\n",
      "Epoch 9 \t Batch 160 \t Training Loss: 48.106331896781924\n",
      "Epoch 9 \t Batch 180 \t Training Loss: 48.27829854753282\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 48.32476169586182\n",
      "Epoch 9 \t Batch 220 \t Training Loss: 48.46416306929155\n",
      "Epoch 9 \t Batch 240 \t Training Loss: 48.514874172210696\n",
      "Epoch 9 \t Batch 260 \t Training Loss: 48.52909234853891\n",
      "Epoch 9 \t Batch 280 \t Training Loss: 48.44474085399083\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 48.41279889424642\n",
      "Epoch 9 \t Batch 320 \t Training Loss: 48.4476593375206\n",
      "Epoch 9 \t Batch 340 \t Training Loss: 48.44833501927993\n",
      "Epoch 9 \t Batch 360 \t Training Loss: 48.4949479315016\n",
      "Epoch 9 \t Batch 380 \t Training Loss: 48.52270813992149\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 48.509947786331175\n",
      "Epoch 9 \t Batch 420 \t Training Loss: 48.49173898242769\n",
      "Epoch 9 \t Batch 440 \t Training Loss: 48.481503347917034\n",
      "Epoch 9 \t Batch 460 \t Training Loss: 48.49004872363547\n",
      "Epoch 9 \t Batch 480 \t Training Loss: 48.46032707691192\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 48.44599649810791\n",
      "Epoch 9 \t Batch 520 \t Training Loss: 48.43432148419894\n",
      "Epoch 9 \t Batch 540 \t Training Loss: 48.406481424967446\n",
      "Epoch 9 \t Batch 560 \t Training Loss: 48.344800097601755\n",
      "Epoch 9 \t Batch 580 \t Training Loss: 48.37632165448419\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 48.365846983591716\n",
      "Epoch 9 \t Batch 620 \t Training Loss: 48.364652430626656\n",
      "Epoch 9 \t Batch 640 \t Training Loss: 48.31665834784508\n",
      "Epoch 9 \t Batch 660 \t Training Loss: 48.32186919703628\n",
      "Epoch 9 \t Batch 680 \t Training Loss: 48.370049297108366\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 48.42855314527239\n",
      "Epoch 9 \t Batch 720 \t Training Loss: 48.42782680193583\n",
      "Epoch 9 \t Batch 740 \t Training Loss: 48.39720527545826\n",
      "Epoch 9 \t Batch 760 \t Training Loss: 48.35252176585951\n",
      "Epoch 9 \t Batch 780 \t Training Loss: 48.384559093377526\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 48.38036545276642\n",
      "Epoch 9 \t Batch 820 \t Training Loss: 48.40290432441525\n",
      "Epoch 9 \t Batch 840 \t Training Loss: 48.39856161390032\n",
      "Epoch 9 \t Batch 860 \t Training Loss: 48.42291740151339\n",
      "Epoch 9 \t Batch 880 \t Training Loss: 48.41226772395047\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 48.379062232971194\n",
      "Epoch 9 \t Batch 20 \t Validation Loss: 23.474731779098512\n",
      "Epoch 9 \t Batch 40 \t Validation Loss: 28.9434494972229\n",
      "Epoch 9 \t Batch 60 \t Validation Loss: 30.477350266774497\n",
      "Epoch 9 \t Batch 80 \t Validation Loss: 30.92670624256134\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 30.05399700164795\n",
      "Epoch 9 \t Batch 120 \t Validation Loss: 29.981397962570192\n",
      "Epoch 9 \t Batch 140 \t Validation Loss: 29.607382358823504\n",
      "Epoch 9 \t Batch 160 \t Validation Loss: 30.785700458288193\n",
      "Epoch 9 \t Batch 180 \t Validation Loss: 33.385925907558864\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 34.58857825279236\n",
      "Epoch 9 \t Batch 220 \t Validation Loss: 35.47008463252674\n",
      "Epoch 9 \t Batch 240 \t Validation Loss: 35.588714802265166\n",
      "Epoch 9 \t Batch 260 \t Validation Loss: 37.45881081361037\n",
      "Epoch 9 \t Batch 280 \t Validation Loss: 38.40763170719147\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 39.05319436073303\n",
      "Epoch 9 \t Batch 320 \t Validation Loss: 39.25738618671894\n",
      "Epoch 9 \t Batch 340 \t Validation Loss: 39.15073159442228\n",
      "Epoch 9 \t Batch 360 \t Validation Loss: 38.84355911943648\n",
      "Epoch 9 \t Batch 380 \t Validation Loss: 39.08228942720513\n",
      "Epoch 9 \t Batch 400 \t Validation Loss: 38.75395298242569\n",
      "Epoch 9 \t Batch 420 \t Validation Loss: 38.784622671490624\n",
      "Epoch 9 \t Batch 440 \t Validation Loss: 38.624209358475426\n",
      "Epoch 9 \t Batch 460 \t Validation Loss: 38.877367676859315\n",
      "Epoch 9 \t Batch 480 \t Validation Loss: 39.288881959517795\n",
      "Epoch 9 \t Batch 500 \t Validation Loss: 38.9806889629364\n",
      "Epoch 9 \t Batch 520 \t Validation Loss: 38.82384963952578\n",
      "Epoch 9 \t Batch 540 \t Validation Loss: 38.64920475041425\n",
      "Epoch 9 \t Batch 560 \t Validation Loss: 38.61538883107049\n",
      "Epoch 9 \t Batch 580 \t Validation Loss: 38.632311700952464\n",
      "Epoch 9 \t Batch 600 \t Validation Loss: 38.8962757730484\n",
      "Epoch 9 Training Loss: 48.4082921734293 Validation Loss: 39.56513067499384\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 20 \t Training Loss: 48.274664306640624\n",
      "Epoch 10 \t Batch 40 \t Training Loss: 48.92270746231079\n",
      "Epoch 10 \t Batch 60 \t Training Loss: 49.35150941212972\n",
      "Epoch 10 \t Batch 80 \t Training Loss: 48.813004064559934\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 48.46541072845459\n",
      "Epoch 10 \t Batch 120 \t Training Loss: 48.491154225667316\n",
      "Epoch 10 \t Batch 140 \t Training Loss: 48.378337832859586\n",
      "Epoch 10 \t Batch 160 \t Training Loss: 48.42646095752716\n",
      "Epoch 10 \t Batch 180 \t Training Loss: 48.49478668636746\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 48.534512252807616\n",
      "Epoch 10 \t Batch 220 \t Training Loss: 48.52722454071045\n",
      "Epoch 10 \t Batch 240 \t Training Loss: 48.43671325047811\n",
      "Epoch 10 \t Batch 260 \t Training Loss: 48.37472616342398\n",
      "Epoch 10 \t Batch 280 \t Training Loss: 48.36201171875\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 48.3416721089681\n",
      "Epoch 10 \t Batch 320 \t Training Loss: 48.32067798376083\n",
      "Epoch 10 \t Batch 340 \t Training Loss: 48.36728213815128\n",
      "Epoch 10 \t Batch 360 \t Training Loss: 48.33626972834269\n",
      "Epoch 10 \t Batch 380 \t Training Loss: 48.32864339728104\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 48.38108500480652\n",
      "Epoch 10 \t Batch 420 \t Training Loss: 48.354751704988026\n",
      "Epoch 10 \t Batch 440 \t Training Loss: 48.39860894463279\n",
      "Epoch 10 \t Batch 460 \t Training Loss: 48.38710500468378\n",
      "Epoch 10 \t Batch 480 \t Training Loss: 48.3808820327123\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 48.27854064941406\n",
      "Epoch 10 \t Batch 520 \t Training Loss: 48.28347192177406\n",
      "Epoch 10 \t Batch 540 \t Training Loss: 48.300640989232946\n",
      "Epoch 10 \t Batch 560 \t Training Loss: 48.30635529926845\n",
      "Epoch 10 \t Batch 580 \t Training Loss: 48.28673249606428\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 48.267962004343666\n",
      "Epoch 10 \t Batch 620 \t Training Loss: 48.227690235260994\n",
      "Epoch 10 \t Batch 640 \t Training Loss: 48.24845904707909\n",
      "Epoch 10 \t Batch 660 \t Training Loss: 48.24867235819499\n",
      "Epoch 10 \t Batch 680 \t Training Loss: 48.25608633826761\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 48.259103153773715\n",
      "Epoch 10 \t Batch 720 \t Training Loss: 48.25979999966091\n",
      "Epoch 10 \t Batch 740 \t Training Loss: 48.24265687787855\n",
      "Epoch 10 \t Batch 760 \t Training Loss: 48.20853596235576\n",
      "Epoch 10 \t Batch 780 \t Training Loss: 48.228569764357346\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 48.20386522769928\n",
      "Epoch 10 \t Batch 820 \t Training Loss: 48.23522424465273\n",
      "Epoch 10 \t Batch 840 \t Training Loss: 48.22428146543957\n",
      "Epoch 10 \t Batch 860 \t Training Loss: 48.22144455577052\n",
      "Epoch 10 \t Batch 880 \t Training Loss: 48.25865975293246\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 48.25831472184923\n",
      "Epoch 10 \t Batch 20 \t Validation Loss: 23.280185651779174\n",
      "Epoch 10 \t Batch 40 \t Validation Loss: 29.00565333366394\n",
      "Epoch 10 \t Batch 60 \t Validation Loss: 30.492235374450683\n",
      "Epoch 10 \t Batch 80 \t Validation Loss: 30.637390184402467\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 29.884058322906494\n",
      "Epoch 10 \t Batch 120 \t Validation Loss: 29.98594170411428\n",
      "Epoch 10 \t Batch 140 \t Validation Loss: 29.646087639672416\n",
      "Epoch 10 \t Batch 160 \t Validation Loss: 30.70789137482643\n",
      "Epoch 10 \t Batch 180 \t Validation Loss: 33.15013068517049\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 34.24112742424011\n",
      "Epoch 10 \t Batch 220 \t Validation Loss: 35.0404520294883\n",
      "Epoch 10 \t Batch 240 \t Validation Loss: 35.162703065077466\n",
      "Epoch 10 \t Batch 260 \t Validation Loss: 36.98127190516545\n",
      "Epoch 10 \t Batch 280 \t Validation Loss: 37.92662660394396\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 38.48198607444763\n",
      "Epoch 10 \t Batch 320 \t Validation Loss: 38.68596332371235\n",
      "Epoch 10 \t Batch 340 \t Validation Loss: 38.57345462406383\n",
      "Epoch 10 \t Batch 360 \t Validation Loss: 38.212962894969515\n",
      "Epoch 10 \t Batch 380 \t Validation Loss: 38.41240113910876\n",
      "Epoch 10 \t Batch 400 \t Validation Loss: 38.09450180053711\n",
      "Epoch 10 \t Batch 420 \t Validation Loss: 38.128949864705405\n",
      "Epoch 10 \t Batch 440 \t Validation Loss: 37.925251843712545\n",
      "Epoch 10 \t Batch 460 \t Validation Loss: 38.17302228264187\n",
      "Epoch 10 \t Batch 480 \t Validation Loss: 38.60500071048737\n",
      "Epoch 10 \t Batch 500 \t Validation Loss: 38.29423578643799\n",
      "Epoch 10 \t Batch 520 \t Validation Loss: 38.10890281383808\n",
      "Epoch 10 \t Batch 540 \t Validation Loss: 37.87363386154175\n",
      "Epoch 10 \t Batch 560 \t Validation Loss: 37.71891344274793\n",
      "Epoch 10 \t Batch 580 \t Validation Loss: 37.57113045659558\n",
      "Epoch 10 \t Batch 600 \t Validation Loss: 37.76755916595459\n",
      "Epoch 10 Training Loss: 48.3178871845341 Validation Loss: 38.407570142250556\n",
      "Validation Loss Decreased(24144.786873817444--->23659.063207626343) Saving The Model\n",
      "Epoch 10 completed\n",
      "Epoch 11 \t Batch 20 \t Training Loss: 49.58985004425049\n",
      "Epoch 11 \t Batch 40 \t Training Loss: 49.23137502670288\n",
      "Epoch 11 \t Batch 60 \t Training Loss: 48.945733133951826\n",
      "Epoch 11 \t Batch 80 \t Training Loss: 48.774589490890506\n",
      "Epoch 11 \t Batch 100 \t Training Loss: 48.45058746337891\n",
      "Epoch 11 \t Batch 120 \t Training Loss: 48.5719492594401\n",
      "Epoch 11 \t Batch 140 \t Training Loss: 48.31467522212437\n",
      "Epoch 11 \t Batch 160 \t Training Loss: 48.288495087623595\n",
      "Epoch 11 \t Batch 180 \t Training Loss: 48.208208868238664\n",
      "Epoch 11 \t Batch 200 \t Training Loss: 48.16882894515991\n",
      "Epoch 11 \t Batch 220 \t Training Loss: 48.181608113375574\n",
      "Epoch 11 \t Batch 240 \t Training Loss: 48.252117013931276\n",
      "Epoch 11 \t Batch 260 \t Training Loss: 48.23899327791654\n",
      "Epoch 11 \t Batch 280 \t Training Loss: 48.17761065619332\n",
      "Epoch 11 \t Batch 300 \t Training Loss: 48.15681299845377\n",
      "Epoch 11 \t Batch 320 \t Training Loss: 48.17239897251129\n",
      "Epoch 11 \t Batch 340 \t Training Loss: 48.152739502401914\n",
      "Epoch 11 \t Batch 360 \t Training Loss: 48.21309361987644\n",
      "Epoch 11 \t Batch 380 \t Training Loss: 48.22856797670063\n",
      "Epoch 11 \t Batch 400 \t Training Loss: 48.27225353240967\n",
      "Epoch 11 \t Batch 420 \t Training Loss: 48.23873475392659\n",
      "Epoch 11 \t Batch 440 \t Training Loss: 48.157453771071\n",
      "Epoch 11 \t Batch 460 \t Training Loss: 48.14539388573688\n",
      "Epoch 11 \t Batch 480 \t Training Loss: 48.19532053470611\n",
      "Epoch 11 \t Batch 500 \t Training Loss: 48.160772163391115\n",
      "Epoch 11 \t Batch 520 \t Training Loss: 48.22448009344248\n",
      "Epoch 11 \t Batch 540 \t Training Loss: 48.22611817253961\n",
      "Epoch 11 \t Batch 560 \t Training Loss: 48.23891113145011\n",
      "Epoch 11 \t Batch 580 \t Training Loss: 48.20469419545141\n",
      "Epoch 11 \t Batch 600 \t Training Loss: 48.190409138997396\n",
      "Epoch 11 \t Batch 620 \t Training Loss: 48.18236331324424\n",
      "Epoch 11 \t Batch 640 \t Training Loss: 48.18164903521538\n",
      "Epoch 11 \t Batch 660 \t Training Loss: 48.15278736461293\n",
      "Epoch 11 \t Batch 680 \t Training Loss: 48.1805484827827\n",
      "Epoch 11 \t Batch 700 \t Training Loss: 48.202821469988145\n",
      "Epoch 11 \t Batch 720 \t Training Loss: 48.20993045171102\n",
      "Epoch 11 \t Batch 740 \t Training Loss: 48.236049157219966\n",
      "Epoch 11 \t Batch 760 \t Training Loss: 48.20628514540823\n",
      "Epoch 11 \t Batch 780 \t Training Loss: 48.22268608777951\n",
      "Epoch 11 \t Batch 800 \t Training Loss: 48.25776739120484\n",
      "Epoch 11 \t Batch 820 \t Training Loss: 48.25300065947742\n",
      "Epoch 11 \t Batch 840 \t Training Loss: 48.255057684580485\n",
      "Epoch 11 \t Batch 860 \t Training Loss: 48.26330710566321\n",
      "Epoch 11 \t Batch 880 \t Training Loss: 48.26128437735817\n",
      "Epoch 11 \t Batch 900 \t Training Loss: 48.220555475023055\n",
      "Epoch 11 \t Batch 20 \t Validation Loss: 21.062566566467286\n",
      "Epoch 11 \t Batch 40 \t Validation Loss: 27.39957253932953\n",
      "Epoch 11 \t Batch 60 \t Validation Loss: 28.67760966618856\n",
      "Epoch 11 \t Batch 80 \t Validation Loss: 29.187844276428223\n",
      "Epoch 11 \t Batch 100 \t Validation Loss: 28.939740657806396\n",
      "Epoch 11 \t Batch 120 \t Validation Loss: 29.149759244918823\n",
      "Epoch 11 \t Batch 140 \t Validation Loss: 29.03794276373727\n",
      "Epoch 11 \t Batch 160 \t Validation Loss: 30.3639253616333\n",
      "Epoch 11 \t Batch 180 \t Validation Loss: 33.249207221137155\n",
      "Epoch 11 \t Batch 200 \t Validation Loss: 34.51726613044739\n",
      "Epoch 11 \t Batch 220 \t Validation Loss: 35.500347926399925\n",
      "Epoch 11 \t Batch 240 \t Validation Loss: 35.7234620253245\n",
      "Epoch 11 \t Batch 260 \t Validation Loss: 37.705553150177\n",
      "Epoch 11 \t Batch 280 \t Validation Loss: 38.76418792179653\n",
      "Epoch 11 \t Batch 300 \t Validation Loss: 39.42683382034302\n",
      "Epoch 11 \t Batch 320 \t Validation Loss: 39.66719186902046\n",
      "Epoch 11 \t Batch 340 \t Validation Loss: 39.54002527349135\n",
      "Epoch 11 \t Batch 360 \t Validation Loss: 39.24139394760132\n",
      "Epoch 11 \t Batch 380 \t Validation Loss: 39.443887283927516\n",
      "Epoch 11 \t Batch 400 \t Validation Loss: 39.064552426338196\n",
      "Epoch 11 \t Batch 420 \t Validation Loss: 39.04580559503464\n",
      "Epoch 11 \t Batch 440 \t Validation Loss: 38.7673293503848\n",
      "Epoch 11 \t Batch 460 \t Validation Loss: 38.973179796467655\n",
      "Epoch 11 \t Batch 480 \t Validation Loss: 39.38325056234996\n",
      "Epoch 11 \t Batch 500 \t Validation Loss: 39.02474635314941\n",
      "Epoch 11 \t Batch 520 \t Validation Loss: 38.81129887654231\n",
      "Epoch 11 \t Batch 540 \t Validation Loss: 38.59701796637641\n",
      "Epoch 11 \t Batch 560 \t Validation Loss: 38.37072636740548\n",
      "Epoch 11 \t Batch 580 \t Validation Loss: 38.15505878185404\n",
      "Epoch 11 \t Batch 600 \t Validation Loss: 38.36367597897848\n",
      "Epoch 11 Training Loss: 48.20438344658006 Validation Loss: 38.96957118480236\n",
      "Epoch 11 completed\n",
      "Epoch 12 \t Batch 20 \t Training Loss: 47.57520523071289\n",
      "Epoch 12 \t Batch 40 \t Training Loss: 47.96087331771851\n",
      "Epoch 12 \t Batch 60 \t Training Loss: 48.14052619934082\n",
      "Epoch 12 \t Batch 80 \t Training Loss: 48.04163227081299\n",
      "Epoch 12 \t Batch 100 \t Training Loss: 48.071900901794436\n",
      "Epoch 12 \t Batch 120 \t Training Loss: 47.86128533681234\n",
      "Epoch 12 \t Batch 140 \t Training Loss: 47.80903293064662\n",
      "Epoch 12 \t Batch 160 \t Training Loss: 47.79865911006927\n",
      "Epoch 12 \t Batch 180 \t Training Loss: 47.80315104590522\n",
      "Epoch 12 \t Batch 200 \t Training Loss: 47.80403202056885\n",
      "Epoch 12 \t Batch 220 \t Training Loss: 47.79482031735507\n",
      "Epoch 12 \t Batch 240 \t Training Loss: 47.75901568730672\n",
      "Epoch 12 \t Batch 260 \t Training Loss: 47.79176449408898\n",
      "Epoch 12 \t Batch 280 \t Training Loss: 47.79304914474487\n",
      "Epoch 12 \t Batch 300 \t Training Loss: 47.65584529876709\n",
      "Epoch 12 \t Batch 320 \t Training Loss: 47.67926353216171\n",
      "Epoch 12 \t Batch 340 \t Training Loss: 47.68859389810001\n",
      "Epoch 12 \t Batch 360 \t Training Loss: 47.77574055989583\n",
      "Epoch 12 \t Batch 380 \t Training Loss: 47.89081085606625\n",
      "Epoch 12 \t Batch 400 \t Training Loss: 47.981624851226805\n",
      "Epoch 12 \t Batch 420 \t Training Loss: 47.981577637082054\n",
      "Epoch 12 \t Batch 440 \t Training Loss: 48.051878044822\n",
      "Epoch 12 \t Batch 460 \t Training Loss: 48.11298234359078\n",
      "Epoch 12 \t Batch 480 \t Training Loss: 48.154314923286435\n",
      "Epoch 12 \t Batch 500 \t Training Loss: 48.09192583465576\n",
      "Epoch 12 \t Batch 520 \t Training Loss: 48.10814404120812\n",
      "Epoch 12 \t Batch 540 \t Training Loss: 48.106053790339715\n",
      "Epoch 12 \t Batch 560 \t Training Loss: 48.03854901450021\n",
      "Epoch 12 \t Batch 580 \t Training Loss: 48.09079593132282\n",
      "Epoch 12 \t Batch 600 \t Training Loss: 48.1431077448527\n",
      "Epoch 12 \t Batch 620 \t Training Loss: 48.13149292238297\n",
      "Epoch 12 \t Batch 640 \t Training Loss: 48.12851486206055\n",
      "Epoch 12 \t Batch 660 \t Training Loss: 48.168203897187205\n",
      "Epoch 12 \t Batch 680 \t Training Loss: 48.156343903261075\n",
      "Epoch 12 \t Batch 700 \t Training Loss: 48.11030766623361\n",
      "Epoch 12 \t Batch 720 \t Training Loss: 48.108594237433536\n",
      "Epoch 12 \t Batch 740 \t Training Loss: 48.13077728683884\n",
      "Epoch 12 \t Batch 760 \t Training Loss: 48.12203298367952\n",
      "Epoch 12 \t Batch 780 \t Training Loss: 48.13180318000989\n",
      "Epoch 12 \t Batch 800 \t Training Loss: 48.12538771629333\n",
      "Epoch 12 \t Batch 820 \t Training Loss: 48.140460991277926\n",
      "Epoch 12 \t Batch 840 \t Training Loss: 48.134611170632496\n",
      "Epoch 12 \t Batch 860 \t Training Loss: 48.1396562576294\n",
      "Epoch 12 \t Batch 880 \t Training Loss: 48.13021235899492\n",
      "Epoch 12 \t Batch 900 \t Training Loss: 48.131238526238334\n",
      "Epoch 12 \t Batch 20 \t Validation Loss: 24.176287984848024\n",
      "Epoch 12 \t Batch 40 \t Validation Loss: 28.78385488986969\n",
      "Epoch 12 \t Batch 60 \t Validation Loss: 30.33777912457784\n",
      "Epoch 12 \t Batch 80 \t Validation Loss: 30.389618957042693\n",
      "Epoch 12 \t Batch 100 \t Validation Loss: 29.80461260795593\n",
      "Epoch 12 \t Batch 120 \t Validation Loss: 29.915384380022683\n",
      "Epoch 12 \t Batch 140 \t Validation Loss: 29.563718884331838\n",
      "Epoch 12 \t Batch 160 \t Validation Loss: 30.736894172430038\n",
      "Epoch 12 \t Batch 180 \t Validation Loss: 33.43221244282193\n",
      "Epoch 12 \t Batch 200 \t Validation Loss: 34.75661971569061\n",
      "Epoch 12 \t Batch 220 \t Validation Loss: 35.58503773862665\n",
      "Epoch 12 \t Batch 240 \t Validation Loss: 35.757048936684924\n",
      "Epoch 12 \t Batch 260 \t Validation Loss: 37.67615551214952\n",
      "Epoch 12 \t Batch 280 \t Validation Loss: 38.66711794308254\n",
      "Epoch 12 \t Batch 300 \t Validation Loss: 39.30610052744547\n",
      "Epoch 12 \t Batch 320 \t Validation Loss: 39.548047012090684\n",
      "Epoch 12 \t Batch 340 \t Validation Loss: 39.48805860631606\n",
      "Epoch 12 \t Batch 360 \t Validation Loss: 39.129044961929324\n",
      "Epoch 12 \t Batch 380 \t Validation Loss: 39.32656381255702\n",
      "Epoch 12 \t Batch 400 \t Validation Loss: 39.012002012729646\n",
      "Epoch 12 \t Batch 420 \t Validation Loss: 39.080521072660176\n",
      "Epoch 12 \t Batch 440 \t Validation Loss: 38.88760991746729\n",
      "Epoch 12 \t Batch 460 \t Validation Loss: 39.153729942570564\n",
      "Epoch 12 \t Batch 480 \t Validation Loss: 39.607045044501625\n",
      "Epoch 12 \t Batch 500 \t Validation Loss: 39.329149084091185\n",
      "Epoch 12 \t Batch 520 \t Validation Loss: 39.12639416731321\n",
      "Epoch 12 \t Batch 540 \t Validation Loss: 38.807274184403596\n",
      "Epoch 12 \t Batch 560 \t Validation Loss: 38.54558962583542\n",
      "Epoch 12 \t Batch 580 \t Validation Loss: 38.19060106277466\n",
      "Epoch 12 \t Batch 600 \t Validation Loss: 38.3607003847758\n",
      "Epoch 12 Training Loss: 48.126793065793905 Validation Loss: 38.902264263722806\n",
      "Epoch 12 completed\n",
      "Epoch 13 \t Batch 20 \t Training Loss: 47.14347133636475\n",
      "Epoch 13 \t Batch 40 \t Training Loss: 47.37966909408569\n",
      "Epoch 13 \t Batch 60 \t Training Loss: 47.6624459584554\n",
      "Epoch 13 \t Batch 80 \t Training Loss: 48.118182516098024\n",
      "Epoch 13 \t Batch 100 \t Training Loss: 48.006609230041505\n",
      "Epoch 13 \t Batch 120 \t Training Loss: 47.90724643071493\n",
      "Epoch 13 \t Batch 140 \t Training Loss: 47.87418667929513\n",
      "Epoch 13 \t Batch 160 \t Training Loss: 47.927678322792055\n",
      "Epoch 13 \t Batch 180 \t Training Loss: 47.87082597944472\n",
      "Epoch 13 \t Batch 200 \t Training Loss: 48.012282333374024\n",
      "Epoch 13 \t Batch 220 \t Training Loss: 48.03319946635853\n",
      "Epoch 13 \t Batch 240 \t Training Loss: 47.99715280532837\n",
      "Epoch 13 \t Batch 260 \t Training Loss: 47.999694002591646\n",
      "Epoch 13 \t Batch 280 \t Training Loss: 48.05568911688668\n",
      "Epoch 13 \t Batch 300 \t Training Loss: 48.156832491556806\n",
      "Epoch 13 \t Batch 320 \t Training Loss: 48.1797590136528\n",
      "Epoch 13 \t Batch 340 \t Training Loss: 48.20650497885311\n",
      "Epoch 13 \t Batch 360 \t Training Loss: 48.154475688934326\n",
      "Epoch 13 \t Batch 380 \t Training Loss: 48.197588669626334\n",
      "Epoch 13 \t Batch 400 \t Training Loss: 48.280080766677855\n",
      "Epoch 13 \t Batch 420 \t Training Loss: 48.19246276673817\n",
      "Epoch 13 \t Batch 440 \t Training Loss: 48.168399151888764\n",
      "Epoch 13 \t Batch 460 \t Training Loss: 48.134171353215756\n",
      "Epoch 13 \t Batch 480 \t Training Loss: 48.142228078842166\n",
      "Epoch 13 \t Batch 500 \t Training Loss: 48.13952140808105\n",
      "Epoch 13 \t Batch 520 \t Training Loss: 48.155044570335974\n",
      "Epoch 13 \t Batch 540 \t Training Loss: 48.17701578493472\n",
      "Epoch 13 \t Batch 560 \t Training Loss: 48.12869097845895\n",
      "Epoch 13 \t Batch 580 \t Training Loss: 48.12162572268782\n",
      "Epoch 13 \t Batch 600 \t Training Loss: 48.116174373626706\n",
      "Epoch 13 \t Batch 620 \t Training Loss: 48.09025363306846\n",
      "Epoch 13 \t Batch 640 \t Training Loss: 48.11677635908127\n",
      "Epoch 13 \t Batch 660 \t Training Loss: 48.11087327436967\n",
      "Epoch 13 \t Batch 680 \t Training Loss: 48.10820105496575\n",
      "Epoch 13 \t Batch 700 \t Training Loss: 48.110602253505164\n",
      "Epoch 13 \t Batch 720 \t Training Loss: 48.10514652993944\n",
      "Epoch 13 \t Batch 740 \t Training Loss: 48.10457341477677\n",
      "Epoch 13 \t Batch 760 \t Training Loss: 48.06928172864412\n",
      "Epoch 13 \t Batch 780 \t Training Loss: 48.04319801330566\n",
      "Epoch 13 \t Batch 800 \t Training Loss: 48.011203179359434\n",
      "Epoch 13 \t Batch 820 \t Training Loss: 47.966509367779985\n",
      "Epoch 13 \t Batch 840 \t Training Loss: 47.96562053589594\n",
      "Epoch 13 \t Batch 860 \t Training Loss: 47.95246092330578\n",
      "Epoch 13 \t Batch 880 \t Training Loss: 47.97381649884311\n",
      "Epoch 13 \t Batch 900 \t Training Loss: 47.983378651936846\n",
      "Epoch 13 \t Batch 20 \t Validation Loss: 22.616998815536498\n",
      "Epoch 13 \t Batch 40 \t Validation Loss: 26.685620069503784\n",
      "Epoch 13 \t Batch 60 \t Validation Loss: 28.49366567929586\n",
      "Epoch 13 \t Batch 80 \t Validation Loss: 28.43416177034378\n",
      "Epoch 13 \t Batch 100 \t Validation Loss: 28.25674777030945\n",
      "Epoch 13 \t Batch 120 \t Validation Loss: 28.61747569243113\n",
      "Epoch 13 \t Batch 140 \t Validation Loss: 28.493599721363612\n",
      "Epoch 13 \t Batch 160 \t Validation Loss: 29.859942477941512\n",
      "Epoch 13 \t Batch 180 \t Validation Loss: 32.7371779706743\n",
      "Epoch 13 \t Batch 200 \t Validation Loss: 34.07116051197052\n",
      "Epoch 13 \t Batch 220 \t Validation Loss: 35.05536202083935\n",
      "Epoch 13 \t Batch 240 \t Validation Loss: 35.257904748121895\n",
      "Epoch 13 \t Batch 260 \t Validation Loss: 37.25902526928828\n",
      "Epoch 13 \t Batch 280 \t Validation Loss: 38.28956886019026\n",
      "Epoch 13 \t Batch 300 \t Validation Loss: 38.9903852335612\n",
      "Epoch 13 \t Batch 320 \t Validation Loss: 39.26934963166714\n",
      "Epoch 13 \t Batch 340 \t Validation Loss: 39.160116456536684\n",
      "Epoch 13 \t Batch 360 \t Validation Loss: 38.857908325725134\n",
      "Epoch 13 \t Batch 380 \t Validation Loss: 39.03973824099491\n",
      "Epoch 13 \t Batch 400 \t Validation Loss: 38.62368429899216\n",
      "Epoch 13 \t Batch 420 \t Validation Loss: 38.632733365467615\n",
      "Epoch 13 \t Batch 440 \t Validation Loss: 38.329359689625825\n",
      "Epoch 13 \t Batch 460 \t Validation Loss: 38.53475737986357\n",
      "Epoch 13 \t Batch 480 \t Validation Loss: 38.98786026835442\n",
      "Epoch 13 \t Batch 500 \t Validation Loss: 38.68041715812683\n",
      "Epoch 13 \t Batch 520 \t Validation Loss: 38.4268018575815\n",
      "Epoch 13 \t Batch 540 \t Validation Loss: 38.12058549457126\n",
      "Epoch 13 \t Batch 560 \t Validation Loss: 37.87043815680912\n",
      "Epoch 13 \t Batch 580 \t Validation Loss: 37.635068413306925\n",
      "Epoch 13 \t Batch 600 \t Validation Loss: 37.80729654630025\n",
      "Epoch 13 Training Loss: 48.0023217528831 Validation Loss: 38.40445552863084\n",
      "Validation Loss Decreased(23659.063207626343--->23657.144605636597) Saving The Model\n",
      "Epoch 13 completed\n",
      "Epoch 14 \t Batch 20 \t Training Loss: 49.21354598999024\n",
      "Epoch 14 \t Batch 40 \t Training Loss: 48.90146894454956\n",
      "Epoch 14 \t Batch 60 \t Training Loss: 48.18285134633382\n",
      "Epoch 14 \t Batch 80 \t Training Loss: 47.96837248802185\n",
      "Epoch 14 \t Batch 100 \t Training Loss: 47.951243743896484\n",
      "Epoch 14 \t Batch 120 \t Training Loss: 47.94645016988118\n",
      "Epoch 14 \t Batch 140 \t Training Loss: 47.93909571511405\n",
      "Epoch 14 \t Batch 160 \t Training Loss: 48.06312522888184\n",
      "Epoch 14 \t Batch 180 \t Training Loss: 48.20382599300808\n",
      "Epoch 14 \t Batch 200 \t Training Loss: 48.035538311004636\n",
      "Epoch 14 \t Batch 220 \t Training Loss: 48.03489313992587\n",
      "Epoch 14 \t Batch 240 \t Training Loss: 48.15602097511292\n",
      "Epoch 14 \t Batch 260 \t Training Loss: 47.945430872990535\n",
      "Epoch 14 \t Batch 280 \t Training Loss: 47.95543815067836\n",
      "Epoch 14 \t Batch 300 \t Training Loss: 48.013537343343096\n",
      "Epoch 14 \t Batch 320 \t Training Loss: 48.065027832984924\n",
      "Epoch 14 \t Batch 340 \t Training Loss: 47.94475703519933\n",
      "Epoch 14 \t Batch 360 \t Training Loss: 47.96564417945014\n",
      "Epoch 14 \t Batch 380 \t Training Loss: 47.97111140803287\n",
      "Epoch 14 \t Batch 400 \t Training Loss: 47.99633322715759\n",
      "Epoch 14 \t Batch 420 \t Training Loss: 47.991190256391256\n",
      "Epoch 14 \t Batch 440 \t Training Loss: 48.00494475798173\n",
      "Epoch 14 \t Batch 460 \t Training Loss: 48.04684407607369\n",
      "Epoch 14 \t Batch 480 \t Training Loss: 48.128370292981465\n",
      "Epoch 14 \t Batch 500 \t Training Loss: 48.05247052764893\n",
      "Epoch 14 \t Batch 520 \t Training Loss: 48.05099727924053\n",
      "Epoch 14 \t Batch 540 \t Training Loss: 48.02582514021132\n",
      "Epoch 14 \t Batch 560 \t Training Loss: 48.04203807285854\n",
      "Epoch 14 \t Batch 580 \t Training Loss: 48.059849331296725\n",
      "Epoch 14 \t Batch 600 \t Training Loss: 48.06560670852661\n",
      "Epoch 14 \t Batch 620 \t Training Loss: 48.067646808008995\n",
      "Epoch 14 \t Batch 640 \t Training Loss: 48.06569939255714\n",
      "Epoch 14 \t Batch 660 \t Training Loss: 48.01977997982141\n",
      "Epoch 14 \t Batch 680 \t Training Loss: 48.02423510831945\n",
      "Epoch 14 \t Batch 700 \t Training Loss: 48.02761026109968\n",
      "Epoch 14 \t Batch 720 \t Training Loss: 48.00149440235562\n",
      "Epoch 14 \t Batch 740 \t Training Loss: 48.01904142740611\n",
      "Epoch 14 \t Batch 760 \t Training Loss: 48.00593694385729\n",
      "Epoch 14 \t Batch 780 \t Training Loss: 47.966101064437474\n",
      "Epoch 14 \t Batch 800 \t Training Loss: 47.96759516239166\n",
      "Epoch 14 \t Batch 820 \t Training Loss: 47.99274601354831\n",
      "Epoch 14 \t Batch 840 \t Training Loss: 47.954857531048006\n",
      "Epoch 14 \t Batch 860 \t Training Loss: 47.951084416411646\n",
      "Epoch 14 \t Batch 880 \t Training Loss: 47.94563319032842\n",
      "Epoch 14 \t Batch 900 \t Training Loss: 47.97019492679172\n",
      "Epoch 14 \t Batch 20 \t Validation Loss: 27.40178999900818\n",
      "Epoch 14 \t Batch 40 \t Validation Loss: 30.56058328151703\n",
      "Epoch 14 \t Batch 60 \t Validation Loss: 32.60716853141785\n",
      "Epoch 14 \t Batch 80 \t Validation Loss: 32.00508989095688\n",
      "Epoch 14 \t Batch 100 \t Validation Loss: 31.111150579452513\n",
      "Epoch 14 \t Batch 120 \t Validation Loss: 31.112988972663878\n",
      "Epoch 14 \t Batch 140 \t Validation Loss: 30.64694459778922\n",
      "Epoch 14 \t Batch 160 \t Validation Loss: 31.74373523592949\n",
      "Epoch 14 \t Batch 180 \t Validation Loss: 34.493859651353624\n",
      "Epoch 14 \t Batch 200 \t Validation Loss: 35.8070956993103\n",
      "Epoch 14 \t Batch 220 \t Validation Loss: 36.72138214111328\n",
      "Epoch 14 \t Batch 240 \t Validation Loss: 36.83860877752304\n",
      "Epoch 14 \t Batch 260 \t Validation Loss: 38.851723722311164\n",
      "Epoch 14 \t Batch 280 \t Validation Loss: 39.88496553897858\n",
      "Epoch 14 \t Batch 300 \t Validation Loss: 40.49449717521667\n",
      "Epoch 14 \t Batch 320 \t Validation Loss: 40.66465224325657\n",
      "Epoch 14 \t Batch 340 \t Validation Loss: 40.483014681760004\n",
      "Epoch 14 \t Batch 360 \t Validation Loss: 40.08564451270633\n",
      "Epoch 14 \t Batch 380 \t Validation Loss: 40.30162260406896\n",
      "Epoch 14 \t Batch 400 \t Validation Loss: 39.915925104618076\n",
      "Epoch 14 \t Batch 420 \t Validation Loss: 39.8845007283347\n",
      "Epoch 14 \t Batch 440 \t Validation Loss: 39.64537824500691\n",
      "Epoch 14 \t Batch 460 \t Validation Loss: 39.86354356641355\n",
      "Epoch 14 \t Batch 480 \t Validation Loss: 40.23839063048363\n",
      "Epoch 14 \t Batch 500 \t Validation Loss: 39.881392808914185\n",
      "Epoch 14 \t Batch 520 \t Validation Loss: 39.70772424661196\n",
      "Epoch 14 \t Batch 540 \t Validation Loss: 39.394057501686945\n",
      "Epoch 14 \t Batch 560 \t Validation Loss: 39.16811190162386\n",
      "Epoch 14 \t Batch 580 \t Validation Loss: 38.96225117486099\n",
      "Epoch 14 \t Batch 600 \t Validation Loss: 39.112285618782046\n",
      "Epoch 14 Training Loss: 47.95247142629509 Validation Loss: 39.69554171778939\n",
      "Epoch 14 completed\n",
      "Epoch 15 \t Batch 20 \t Training Loss: 48.66715259552002\n",
      "Epoch 15 \t Batch 40 \t Training Loss: 48.07254123687744\n",
      "Epoch 15 \t Batch 60 \t Training Loss: 47.94162915547689\n",
      "Epoch 15 \t Batch 80 \t Training Loss: 47.45921506881714\n",
      "Epoch 15 \t Batch 100 \t Training Loss: 47.414043464660644\n",
      "Epoch 15 \t Batch 120 \t Training Loss: 47.24439884821574\n",
      "Epoch 15 \t Batch 140 \t Training Loss: 47.26988901410784\n",
      "Epoch 15 \t Batch 160 \t Training Loss: 47.268619012832644\n",
      "Epoch 15 \t Batch 180 \t Training Loss: 47.337766350640194\n",
      "Epoch 15 \t Batch 200 \t Training Loss: 47.323680000305174\n",
      "Epoch 15 \t Batch 220 \t Training Loss: 47.372539953751996\n",
      "Epoch 15 \t Batch 240 \t Training Loss: 47.514817762374875\n",
      "Epoch 15 \t Batch 260 \t Training Loss: 47.59452855036809\n",
      "Epoch 15 \t Batch 280 \t Training Loss: 47.60845593043736\n",
      "Epoch 15 \t Batch 300 \t Training Loss: 47.48563816070557\n",
      "Epoch 15 \t Batch 320 \t Training Loss: 47.57801244258881\n",
      "Epoch 15 \t Batch 340 \t Training Loss: 47.6838103687062\n",
      "Epoch 15 \t Batch 360 \t Training Loss: 47.74495906829834\n",
      "Epoch 15 \t Batch 380 \t Training Loss: 47.70428978769403\n",
      "Epoch 15 \t Batch 400 \t Training Loss: 47.76156928062439\n",
      "Epoch 15 \t Batch 420 \t Training Loss: 47.73543819245838\n",
      "Epoch 15 \t Batch 440 \t Training Loss: 47.772098272497004\n",
      "Epoch 15 \t Batch 460 \t Training Loss: 47.837461546193\n",
      "Epoch 15 \t Batch 480 \t Training Loss: 47.856293400128685\n",
      "Epoch 15 \t Batch 500 \t Training Loss: 47.86563772583008\n",
      "Epoch 15 \t Batch 520 \t Training Loss: 47.88214020362267\n",
      "Epoch 15 \t Batch 540 \t Training Loss: 47.8377726731477\n",
      "Epoch 15 \t Batch 560 \t Training Loss: 47.855492217200144\n",
      "Epoch 15 \t Batch 580 \t Training Loss: 47.78964256418163\n",
      "Epoch 15 \t Batch 600 \t Training Loss: 47.75921091079712\n",
      "Epoch 15 \t Batch 620 \t Training Loss: 47.81972088352327\n",
      "Epoch 15 \t Batch 640 \t Training Loss: 47.80430856347084\n",
      "Epoch 15 \t Batch 660 \t Training Loss: 47.83463912732673\n",
      "Epoch 15 \t Batch 680 \t Training Loss: 47.843505836935606\n",
      "Epoch 15 \t Batch 700 \t Training Loss: 47.84090644836426\n",
      "Epoch 15 \t Batch 720 \t Training Loss: 47.82784764501783\n",
      "Epoch 15 \t Batch 740 \t Training Loss: 47.87358962394096\n",
      "Epoch 15 \t Batch 760 \t Training Loss: 47.87667482275712\n",
      "Epoch 15 \t Batch 780 \t Training Loss: 47.860304495004506\n",
      "Epoch 15 \t Batch 800 \t Training Loss: 47.838213381767275\n",
      "Epoch 15 \t Batch 820 \t Training Loss: 47.83251353473198\n",
      "Epoch 15 \t Batch 840 \t Training Loss: 47.840011832827614\n",
      "Epoch 15 \t Batch 860 \t Training Loss: 47.842702359931415\n",
      "Epoch 15 \t Batch 880 \t Training Loss: 47.817324291576035\n",
      "Epoch 15 \t Batch 900 \t Training Loss: 47.85942059410943\n",
      "Epoch 15 \t Batch 20 \t Validation Loss: 34.66979169845581\n",
      "Epoch 15 \t Batch 40 \t Validation Loss: 35.94730758666992\n",
      "Epoch 15 \t Batch 60 \t Validation Loss: 38.73680632909139\n",
      "Epoch 15 \t Batch 80 \t Validation Loss: 37.62291307449341\n",
      "Epoch 15 \t Batch 100 \t Validation Loss: 35.78013719558716\n",
      "Epoch 15 \t Batch 120 \t Validation Loss: 35.0369297504425\n",
      "Epoch 15 \t Batch 140 \t Validation Loss: 34.03567879540579\n",
      "Epoch 15 \t Batch 160 \t Validation Loss: 34.68137708902359\n",
      "Epoch 15 \t Batch 180 \t Validation Loss: 36.91773857010735\n",
      "Epoch 15 \t Batch 200 \t Validation Loss: 37.81284230709076\n",
      "Epoch 15 \t Batch 220 \t Validation Loss: 38.41902907544916\n",
      "Epoch 15 \t Batch 240 \t Validation Loss: 38.3423401037852\n",
      "Epoch 15 \t Batch 260 \t Validation Loss: 40.138850927352905\n",
      "Epoch 15 \t Batch 280 \t Validation Loss: 40.99808803967067\n",
      "Epoch 15 \t Batch 300 \t Validation Loss: 41.395589192708336\n",
      "Epoch 15 \t Batch 320 \t Validation Loss: 41.4893020927906\n",
      "Epoch 15 \t Batch 340 \t Validation Loss: 41.251895220139446\n",
      "Epoch 15 \t Batch 360 \t Validation Loss: 40.8040637201733\n",
      "Epoch 15 \t Batch 380 \t Validation Loss: 40.988193090338456\n",
      "Epoch 15 \t Batch 400 \t Validation Loss: 40.60436773300171\n",
      "Epoch 15 \t Batch 420 \t Validation Loss: 40.58057718731108\n",
      "Epoch 15 \t Batch 440 \t Validation Loss: 40.43812093734741\n",
      "Epoch 15 \t Batch 460 \t Validation Loss: 40.7108370863873\n",
      "Epoch 15 \t Batch 480 \t Validation Loss: 41.07440532843272\n",
      "Epoch 15 \t Batch 500 \t Validation Loss: 40.69418144607544\n",
      "Epoch 15 \t Batch 520 \t Validation Loss: 40.6065141586157\n",
      "Epoch 15 \t Batch 540 \t Validation Loss: 40.306152529186676\n",
      "Epoch 15 \t Batch 560 \t Validation Loss: 40.026601946353914\n",
      "Epoch 15 \t Batch 580 \t Validation Loss: 39.738988174241165\n",
      "Epoch 15 \t Batch 600 \t Validation Loss: 39.88760054111481\n",
      "Epoch 15 Training Loss: 47.86442124960597 Validation Loss: 40.458055810494855\n",
      "Epoch 15 completed\n",
      "Epoch 16 \t Batch 20 \t Training Loss: 46.366431617736815\n",
      "Epoch 16 \t Batch 40 \t Training Loss: 46.435410785675046\n",
      "Epoch 16 \t Batch 60 \t Training Loss: 46.681648381551106\n",
      "Epoch 16 \t Batch 80 \t Training Loss: 47.320578050613406\n",
      "Epoch 16 \t Batch 100 \t Training Loss: 47.39545669555664\n",
      "Epoch 16 \t Batch 120 \t Training Loss: 47.71096550623576\n",
      "Epoch 16 \t Batch 140 \t Training Loss: 47.62750306810651\n",
      "Epoch 16 \t Batch 160 \t Training Loss: 47.82903604507446\n",
      "Epoch 16 \t Batch 180 \t Training Loss: 47.93496920267741\n",
      "Epoch 16 \t Batch 200 \t Training Loss: 47.83505237579346\n",
      "Epoch 16 \t Batch 220 \t Training Loss: 47.9485877643932\n",
      "Epoch 16 \t Batch 240 \t Training Loss: 47.94779866536458\n",
      "Epoch 16 \t Batch 260 \t Training Loss: 48.00500133220966\n",
      "Epoch 16 \t Batch 280 \t Training Loss: 47.89393211092268\n",
      "Epoch 16 \t Batch 300 \t Training Loss: 47.95559118906657\n",
      "Epoch 16 \t Batch 320 \t Training Loss: 47.95237309932709\n",
      "Epoch 16 \t Batch 340 \t Training Loss: 47.87442427242503\n",
      "Epoch 16 \t Batch 360 \t Training Loss: 47.86864114337497\n",
      "Epoch 16 \t Batch 380 \t Training Loss: 47.944594754670796\n",
      "Epoch 16 \t Batch 400 \t Training Loss: 47.84736755371094\n",
      "Epoch 16 \t Batch 420 \t Training Loss: 47.80338484446208\n",
      "Epoch 16 \t Batch 440 \t Training Loss: 47.779741165854716\n",
      "Epoch 16 \t Batch 460 \t Training Loss: 47.87408744977868\n",
      "Epoch 16 \t Batch 480 \t Training Loss: 47.90656866232554\n",
      "Epoch 16 \t Batch 500 \t Training Loss: 47.89697049713135\n",
      "Epoch 16 \t Batch 520 \t Training Loss: 47.85053911209106\n",
      "Epoch 16 \t Batch 540 \t Training Loss: 47.85915156470405\n",
      "Epoch 16 \t Batch 560 \t Training Loss: 47.91008472442627\n",
      "Epoch 16 \t Batch 580 \t Training Loss: 47.932988995519175\n",
      "Epoch 16 \t Batch 600 \t Training Loss: 47.96449403127035\n",
      "Epoch 16 \t Batch 620 \t Training Loss: 47.97115050285093\n",
      "Epoch 16 \t Batch 640 \t Training Loss: 47.9441858291626\n",
      "Epoch 16 \t Batch 660 \t Training Loss: 47.94582521265203\n",
      "Epoch 16 \t Batch 680 \t Training Loss: 47.938288957932414\n",
      "Epoch 16 \t Batch 700 \t Training Loss: 47.9647641318185\n",
      "Epoch 16 \t Batch 720 \t Training Loss: 47.93935156398349\n",
      "Epoch 16 \t Batch 740 \t Training Loss: 47.899596606074155\n",
      "Epoch 16 \t Batch 760 \t Training Loss: 47.892516070918035\n",
      "Epoch 16 \t Batch 780 \t Training Loss: 47.86734455304268\n",
      "Epoch 16 \t Batch 800 \t Training Loss: 47.8179936504364\n",
      "Epoch 16 \t Batch 820 \t Training Loss: 47.822630328666875\n",
      "Epoch 16 \t Batch 840 \t Training Loss: 47.81121839341663\n",
      "Epoch 16 \t Batch 860 \t Training Loss: 47.783334900612054\n",
      "Epoch 16 \t Batch 880 \t Training Loss: 47.79938765872608\n",
      "Epoch 16 \t Batch 900 \t Training Loss: 47.814815258449975\n",
      "Epoch 16 \t Batch 20 \t Validation Loss: 29.65874614715576\n",
      "Epoch 16 \t Batch 40 \t Validation Loss: 30.990929698944093\n",
      "Epoch 16 \t Batch 60 \t Validation Loss: 33.57172473271688\n",
      "Epoch 16 \t Batch 80 \t Validation Loss: 32.610757446289064\n",
      "Epoch 16 \t Batch 100 \t Validation Loss: 31.859252891540528\n",
      "Epoch 16 \t Batch 120 \t Validation Loss: 32.04880787531535\n",
      "Epoch 16 \t Batch 140 \t Validation Loss: 31.732583781651087\n",
      "Epoch 16 \t Batch 160 \t Validation Loss: 32.921752107143405\n",
      "Epoch 16 \t Batch 180 \t Validation Loss: 36.01702615420024\n",
      "Epoch 16 \t Batch 200 \t Validation Loss: 37.31210015773773\n",
      "Epoch 16 \t Batch 220 \t Validation Loss: 38.332427861473775\n",
      "Epoch 16 \t Batch 240 \t Validation Loss: 38.52868562936783\n",
      "Epoch 16 \t Batch 260 \t Validation Loss: 40.60930587695195\n",
      "Epoch 16 \t Batch 280 \t Validation Loss: 41.66483644076756\n",
      "Epoch 16 \t Batch 300 \t Validation Loss: 42.47687077204387\n",
      "Epoch 16 \t Batch 320 \t Validation Loss: 42.745894360542295\n",
      "Epoch 16 \t Batch 340 \t Validation Loss: 42.493644989238064\n",
      "Epoch 16 \t Batch 360 \t Validation Loss: 42.115995211071436\n",
      "Epoch 16 \t Batch 380 \t Validation Loss: 42.27690222137853\n",
      "Epoch 16 \t Batch 400 \t Validation Loss: 41.75028524637222\n",
      "Epoch 16 \t Batch 420 \t Validation Loss: 41.63644583792914\n",
      "Epoch 16 \t Batch 440 \t Validation Loss: 41.28924514813857\n",
      "Epoch 16 \t Batch 460 \t Validation Loss: 41.44895040885262\n",
      "Epoch 16 \t Batch 480 \t Validation Loss: 41.80018843213717\n",
      "Epoch 16 \t Batch 500 \t Validation Loss: 41.4097829875946\n",
      "Epoch 16 \t Batch 520 \t Validation Loss: 41.19476455358358\n",
      "Epoch 16 \t Batch 540 \t Validation Loss: 40.81175426730403\n",
      "Epoch 16 \t Batch 560 \t Validation Loss: 40.4784742644855\n",
      "Epoch 16 \t Batch 580 \t Validation Loss: 40.13995475275763\n",
      "Epoch 16 \t Batch 600 \t Validation Loss: 40.227232050895694\n",
      "Epoch 16 Training Loss: 47.813215149657935 Validation Loss: 40.7591066902334\n",
      "Epoch 16 completed\n",
      "Epoch 17 \t Batch 20 \t Training Loss: 47.64682502746582\n",
      "Epoch 17 \t Batch 40 \t Training Loss: 47.6288743019104\n",
      "Epoch 17 \t Batch 60 \t Training Loss: 47.42890631357829\n",
      "Epoch 17 \t Batch 80 \t Training Loss: 47.180884170532224\n",
      "Epoch 17 \t Batch 100 \t Training Loss: 47.064950447082516\n",
      "Epoch 17 \t Batch 120 \t Training Loss: 47.114736779530844\n",
      "Epoch 17 \t Batch 140 \t Training Loss: 47.35968181065151\n",
      "Epoch 17 \t Batch 160 \t Training Loss: 47.3118635892868\n",
      "Epoch 17 \t Batch 180 \t Training Loss: 47.3568939420912\n",
      "Epoch 17 \t Batch 200 \t Training Loss: 47.45424486160278\n",
      "Epoch 17 \t Batch 220 \t Training Loss: 47.55528536709872\n",
      "Epoch 17 \t Batch 240 \t Training Loss: 47.54143711725871\n",
      "Epoch 17 \t Batch 260 \t Training Loss: 47.499912643432616\n",
      "Epoch 17 \t Batch 280 \t Training Loss: 47.463422080448694\n",
      "Epoch 17 \t Batch 300 \t Training Loss: 47.48774035135905\n",
      "Epoch 17 \t Batch 320 \t Training Loss: 47.51823523044586\n",
      "Epoch 17 \t Batch 340 \t Training Loss: 47.52908008799833\n",
      "Epoch 17 \t Batch 360 \t Training Loss: 47.56692937215169\n",
      "Epoch 17 \t Batch 380 \t Training Loss: 47.60498688346461\n",
      "Epoch 17 \t Batch 400 \t Training Loss: 47.57781362533569\n",
      "Epoch 17 \t Batch 420 \t Training Loss: 47.5463830947876\n",
      "Epoch 17 \t Batch 440 \t Training Loss: 47.51771182146939\n",
      "Epoch 17 \t Batch 460 \t Training Loss: 47.578575830874236\n",
      "Epoch 17 \t Batch 480 \t Training Loss: 47.58018007278442\n",
      "Epoch 17 \t Batch 500 \t Training Loss: 47.63392111206055\n",
      "Epoch 17 \t Batch 520 \t Training Loss: 47.641822954324574\n",
      "Epoch 17 \t Batch 540 \t Training Loss: 47.674707582261824\n",
      "Epoch 17 \t Batch 560 \t Training Loss: 47.66551201002938\n",
      "Epoch 17 \t Batch 580 \t Training Loss: 47.72690509269977\n",
      "Epoch 17 \t Batch 600 \t Training Loss: 47.73786263783773\n",
      "Epoch 17 \t Batch 620 \t Training Loss: 47.757120569290656\n",
      "Epoch 17 \t Batch 640 \t Training Loss: 47.7634516954422\n",
      "Epoch 17 \t Batch 660 \t Training Loss: 47.79823886986935\n",
      "Epoch 17 \t Batch 680 \t Training Loss: 47.75571462967817\n",
      "Epoch 17 \t Batch 700 \t Training Loss: 47.76728187561035\n",
      "Epoch 17 \t Batch 720 \t Training Loss: 47.73189986546834\n",
      "Epoch 17 \t Batch 740 \t Training Loss: 47.734271173219426\n",
      "Epoch 17 \t Batch 760 \t Training Loss: 47.75460835507042\n",
      "Epoch 17 \t Batch 780 \t Training Loss: 47.68961871220515\n",
      "Epoch 17 \t Batch 800 \t Training Loss: 47.67762352943421\n",
      "Epoch 17 \t Batch 820 \t Training Loss: 47.65790014964778\n",
      "Epoch 17 \t Batch 840 \t Training Loss: 47.65441124779837\n",
      "Epoch 17 \t Batch 860 \t Training Loss: 47.6817169278167\n",
      "Epoch 17 \t Batch 880 \t Training Loss: 47.68701094714078\n",
      "Epoch 17 \t Batch 900 \t Training Loss: 47.72181770748562\n",
      "Epoch 17 \t Batch 20 \t Validation Loss: 32.65604295730591\n",
      "Epoch 17 \t Batch 40 \t Validation Loss: 34.231861448287965\n",
      "Epoch 17 \t Batch 60 \t Validation Loss: 36.66511030197144\n",
      "Epoch 17 \t Batch 80 \t Validation Loss: 35.677120327949524\n",
      "Epoch 17 \t Batch 100 \t Validation Loss: 34.41586938858032\n",
      "Epoch 17 \t Batch 120 \t Validation Loss: 34.063042132059735\n",
      "Epoch 17 \t Batch 140 \t Validation Loss: 33.34405336380005\n",
      "Epoch 17 \t Batch 160 \t Validation Loss: 34.25431752204895\n",
      "Epoch 17 \t Batch 180 \t Validation Loss: 36.858470339245265\n",
      "Epoch 17 \t Batch 200 \t Validation Loss: 37.90954061985016\n",
      "Epoch 17 \t Batch 220 \t Validation Loss: 38.65085703676397\n",
      "Epoch 17 \t Batch 240 \t Validation Loss: 38.71721787452698\n",
      "Epoch 17 \t Batch 260 \t Validation Loss: 40.60597643485436\n",
      "Epoch 17 \t Batch 280 \t Validation Loss: 41.525308915546965\n",
      "Epoch 17 \t Batch 300 \t Validation Loss: 42.14393881479899\n",
      "Epoch 17 \t Batch 320 \t Validation Loss: 42.32399228811264\n",
      "Epoch 17 \t Batch 340 \t Validation Loss: 42.084556725445914\n",
      "Epoch 17 \t Batch 360 \t Validation Loss: 41.684617805480954\n",
      "Epoch 17 \t Batch 380 \t Validation Loss: 41.81722372205634\n",
      "Epoch 17 \t Batch 400 \t Validation Loss: 41.34366267442703\n",
      "Epoch 17 \t Batch 420 \t Validation Loss: 41.24979794138954\n",
      "Epoch 17 \t Batch 440 \t Validation Loss: 40.9641736832532\n",
      "Epoch 17 \t Batch 460 \t Validation Loss: 41.162135221647176\n",
      "Epoch 17 \t Batch 480 \t Validation Loss: 41.52143941919009\n",
      "Epoch 17 \t Batch 500 \t Validation Loss: 41.16131212043762\n",
      "Epoch 17 \t Batch 520 \t Validation Loss: 40.98376575066493\n",
      "Epoch 17 \t Batch 540 \t Validation Loss: 40.62049732031645\n",
      "Epoch 17 \t Batch 560 \t Validation Loss: 40.353484908172064\n",
      "Epoch 17 \t Batch 580 \t Validation Loss: 40.114008155362356\n",
      "Epoch 17 \t Batch 600 \t Validation Loss: 40.20912758350372\n",
      "Epoch 17 Training Loss: 47.727655475246216 Validation Loss: 40.748361440448015\n",
      "Epoch 17 completed\n",
      "Epoch 18 \t Batch 20 \t Training Loss: 49.1651065826416\n",
      "Epoch 18 \t Batch 40 \t Training Loss: 48.438538551330566\n",
      "Epoch 18 \t Batch 60 \t Training Loss: 48.235211563110354\n",
      "Epoch 18 \t Batch 80 \t Training Loss: 47.82698941230774\n",
      "Epoch 18 \t Batch 100 \t Training Loss: 47.756941413879396\n",
      "Epoch 18 \t Batch 120 \t Training Loss: 47.91422363917033\n",
      "Epoch 18 \t Batch 140 \t Training Loss: 47.91965198516846\n",
      "Epoch 18 \t Batch 160 \t Training Loss: 48.056561303138736\n",
      "Epoch 18 \t Batch 180 \t Training Loss: 47.92215220133463\n",
      "Epoch 18 \t Batch 200 \t Training Loss: 48.00118944168091\n",
      "Epoch 18 \t Batch 220 \t Training Loss: 48.00826624090021\n",
      "Epoch 18 \t Batch 240 \t Training Loss: 48.02110033035278\n",
      "Epoch 18 \t Batch 260 \t Training Loss: 47.97964442326472\n",
      "Epoch 18 \t Batch 280 \t Training Loss: 47.99112757274083\n",
      "Epoch 18 \t Batch 300 \t Training Loss: 47.879148178100586\n",
      "Epoch 18 \t Batch 320 \t Training Loss: 47.894606506824495\n",
      "Epoch 18 \t Batch 340 \t Training Loss: 47.77848305421717\n",
      "Epoch 18 \t Batch 360 \t Training Loss: 47.7699241426256\n",
      "Epoch 18 \t Batch 380 \t Training Loss: 47.80365147841604\n",
      "Epoch 18 \t Batch 400 \t Training Loss: 47.75404582977295\n",
      "Epoch 18 \t Batch 420 \t Training Loss: 47.720392999194914\n",
      "Epoch 18 \t Batch 440 \t Training Loss: 47.695208358764646\n",
      "Epoch 18 \t Batch 460 \t Training Loss: 47.71937002099079\n",
      "Epoch 18 \t Batch 480 \t Training Loss: 47.68822061220805\n",
      "Epoch 18 \t Batch 500 \t Training Loss: 47.69405086517334\n",
      "Epoch 18 \t Batch 520 \t Training Loss: 47.66548356276292\n",
      "Epoch 18 \t Batch 540 \t Training Loss: 47.67205983621103\n",
      "Epoch 18 \t Batch 560 \t Training Loss: 47.705223771503995\n",
      "Epoch 18 \t Batch 580 \t Training Loss: 47.701513612681424\n",
      "Epoch 18 \t Batch 600 \t Training Loss: 47.67820117314657\n",
      "Epoch 18 \t Batch 620 \t Training Loss: 47.68266129647532\n",
      "Epoch 18 \t Batch 640 \t Training Loss: 47.67266196012497\n",
      "Epoch 18 \t Batch 660 \t Training Loss: 47.65215749451608\n",
      "Epoch 18 \t Batch 680 \t Training Loss: 47.642954702938304\n",
      "Epoch 18 \t Batch 700 \t Training Loss: 47.65256840297154\n",
      "Epoch 18 \t Batch 720 \t Training Loss: 47.66338591575622\n",
      "Epoch 18 \t Batch 740 \t Training Loss: 47.6352638038429\n",
      "Epoch 18 \t Batch 760 \t Training Loss: 47.6527135848999\n",
      "Epoch 18 \t Batch 780 \t Training Loss: 47.64712863335242\n",
      "Epoch 18 \t Batch 800 \t Training Loss: 47.63201626777649\n",
      "Epoch 18 \t Batch 820 \t Training Loss: 47.60675762455638\n",
      "Epoch 18 \t Batch 840 \t Training Loss: 47.63083470662435\n",
      "Epoch 18 \t Batch 860 \t Training Loss: 47.638806489456535\n",
      "Epoch 18 \t Batch 880 \t Training Loss: 47.619160400737414\n",
      "Epoch 18 \t Batch 900 \t Training Loss: 47.64435610877143\n",
      "Epoch 18 \t Batch 20 \t Validation Loss: 31.757208156585694\n",
      "Epoch 18 \t Batch 40 \t Validation Loss: 32.33008511066437\n",
      "Epoch 18 \t Batch 60 \t Validation Loss: 34.87071258227031\n",
      "Epoch 18 \t Batch 80 \t Validation Loss: 33.85570163726807\n",
      "Epoch 18 \t Batch 100 \t Validation Loss: 32.61459713935852\n",
      "Epoch 18 \t Batch 120 \t Validation Loss: 32.263786069552104\n",
      "Epoch 18 \t Batch 140 \t Validation Loss: 31.65560371535165\n",
      "Epoch 18 \t Batch 160 \t Validation Loss: 32.74515311121941\n",
      "Epoch 18 \t Batch 180 \t Validation Loss: 35.440422699186534\n",
      "Epoch 18 \t Batch 200 \t Validation Loss: 36.60385859966278\n",
      "Epoch 18 \t Batch 220 \t Validation Loss: 37.4555148341439\n",
      "Epoch 18 \t Batch 240 \t Validation Loss: 37.57295540173848\n",
      "Epoch 18 \t Batch 260 \t Validation Loss: 39.522416987785924\n",
      "Epoch 18 \t Batch 280 \t Validation Loss: 40.486368611880714\n",
      "Epoch 18 \t Batch 300 \t Validation Loss: 41.09699559847514\n",
      "Epoch 18 \t Batch 320 \t Validation Loss: 41.30622827112675\n",
      "Epoch 18 \t Batch 340 \t Validation Loss: 41.11065001487732\n",
      "Epoch 18 \t Batch 360 \t Validation Loss: 40.77918338775635\n",
      "Epoch 18 \t Batch 380 \t Validation Loss: 40.99232445516084\n",
      "Epoch 18 \t Batch 400 \t Validation Loss: 40.60119088888168\n",
      "Epoch 18 \t Batch 420 \t Validation Loss: 40.53349558058239\n",
      "Epoch 18 \t Batch 440 \t Validation Loss: 40.32504620118575\n",
      "Epoch 18 \t Batch 460 \t Validation Loss: 40.52303301355113\n",
      "Epoch 18 \t Batch 480 \t Validation Loss: 40.93507492542267\n",
      "Epoch 18 \t Batch 500 \t Validation Loss: 40.58108896636963\n",
      "Epoch 18 \t Batch 520 \t Validation Loss: 40.47734331534459\n",
      "Epoch 18 \t Batch 540 \t Validation Loss: 40.24863710580049\n",
      "Epoch 18 \t Batch 560 \t Validation Loss: 40.12981808015278\n",
      "Epoch 18 \t Batch 580 \t Validation Loss: 40.084521209782565\n",
      "Epoch 18 \t Batch 600 \t Validation Loss: 40.275745301246644\n",
      "Epoch 18 Training Loss: 47.66668617972371 Validation Loss: 40.93305395021067\n",
      "Epoch 18 completed\n",
      "Epoch 19 \t Batch 20 \t Training Loss: 48.06857147216797\n",
      "Epoch 19 \t Batch 40 \t Training Loss: 47.40303058624268\n",
      "Epoch 19 \t Batch 60 \t Training Loss: 47.66869322458903\n",
      "Epoch 19 \t Batch 80 \t Training Loss: 47.817072200775144\n",
      "Epoch 19 \t Batch 100 \t Training Loss: 47.762656173706056\n",
      "Epoch 19 \t Batch 120 \t Training Loss: 47.73309990564982\n",
      "Epoch 19 \t Batch 140 \t Training Loss: 47.53047684260777\n",
      "Epoch 19 \t Batch 160 \t Training Loss: 47.37819831371307\n",
      "Epoch 19 \t Batch 180 \t Training Loss: 47.39294675191243\n",
      "Epoch 19 \t Batch 200 \t Training Loss: 47.630641555786134\n",
      "Epoch 19 \t Batch 220 \t Training Loss: 47.511284706809306\n",
      "Epoch 19 \t Batch 240 \t Training Loss: 47.4370182355245\n",
      "Epoch 19 \t Batch 260 \t Training Loss: 47.414223465552695\n",
      "Epoch 19 \t Batch 280 \t Training Loss: 47.41083288192749\n",
      "Epoch 19 \t Batch 300 \t Training Loss: 47.557248776753745\n",
      "Epoch 19 \t Batch 320 \t Training Loss: 47.577290213108064\n",
      "Epoch 19 \t Batch 340 \t Training Loss: 47.61736639808206\n",
      "Epoch 19 \t Batch 360 \t Training Loss: 47.58004698223538\n",
      "Epoch 19 \t Batch 380 \t Training Loss: 47.61506394838032\n",
      "Epoch 19 \t Batch 400 \t Training Loss: 47.60880131721497\n",
      "Epoch 19 \t Batch 420 \t Training Loss: 47.564511735098705\n",
      "Epoch 19 \t Batch 440 \t Training Loss: 47.57754400426691\n",
      "Epoch 19 \t Batch 460 \t Training Loss: 47.56343433545983\n",
      "Epoch 19 \t Batch 480 \t Training Loss: 47.6408682346344\n",
      "Epoch 19 \t Batch 500 \t Training Loss: 47.64637815093994\n",
      "Epoch 19 \t Batch 520 \t Training Loss: 47.71937142885648\n",
      "Epoch 19 \t Batch 540 \t Training Loss: 47.75850796169705\n",
      "Epoch 19 \t Batch 560 \t Training Loss: 47.732476881572175\n",
      "Epoch 19 \t Batch 580 \t Training Loss: 47.71894798278809\n",
      "Epoch 19 \t Batch 600 \t Training Loss: 47.65746879577637\n",
      "Epoch 19 \t Batch 620 \t Training Loss: 47.622157662914645\n",
      "Epoch 19 \t Batch 640 \t Training Loss: 47.636121082305905\n",
      "Epoch 19 \t Batch 660 \t Training Loss: 47.60955227938565\n",
      "Epoch 19 \t Batch 680 \t Training Loss: 47.63316173553467\n",
      "Epoch 19 \t Batch 700 \t Training Loss: 47.639147578648156\n",
      "Epoch 19 \t Batch 720 \t Training Loss: 47.64310336642795\n",
      "Epoch 19 \t Batch 740 \t Training Loss: 47.64612010749611\n",
      "Epoch 19 \t Batch 760 \t Training Loss: 47.64176609139693\n",
      "Epoch 19 \t Batch 780 \t Training Loss: 47.6648319146572\n",
      "Epoch 19 \t Batch 800 \t Training Loss: 47.654641456604004\n",
      "Epoch 19 \t Batch 820 \t Training Loss: 47.672404093858674\n",
      "Epoch 19 \t Batch 840 \t Training Loss: 47.67534298215594\n",
      "Epoch 19 \t Batch 860 \t Training Loss: 47.63888168334961\n",
      "Epoch 19 \t Batch 880 \t Training Loss: 47.650848440690474\n",
      "Epoch 19 \t Batch 900 \t Training Loss: 47.625232183668345\n",
      "Epoch 19 \t Batch 20 \t Validation Loss: 28.23368878364563\n",
      "Epoch 19 \t Batch 40 \t Validation Loss: 32.18803241252899\n",
      "Epoch 19 \t Batch 60 \t Validation Loss: 34.54891514778137\n",
      "Epoch 19 \t Batch 80 \t Validation Loss: 34.3686225771904\n",
      "Epoch 19 \t Batch 100 \t Validation Loss: 33.73261397361755\n",
      "Epoch 19 \t Batch 120 \t Validation Loss: 33.38708647886912\n",
      "Epoch 19 \t Batch 140 \t Validation Loss: 32.726592901774815\n",
      "Epoch 19 \t Batch 160 \t Validation Loss: 33.803428345918654\n",
      "Epoch 19 \t Batch 180 \t Validation Loss: 36.358268891440495\n",
      "Epoch 19 \t Batch 200 \t Validation Loss: 37.51947018146515\n",
      "Epoch 19 \t Batch 220 \t Validation Loss: 38.30833226550709\n",
      "Epoch 19 \t Batch 240 \t Validation Loss: 38.33005535205205\n",
      "Epoch 19 \t Batch 260 \t Validation Loss: 40.23484515777001\n",
      "Epoch 19 \t Batch 280 \t Validation Loss: 41.171222404071266\n",
      "Epoch 19 \t Batch 300 \t Validation Loss: 41.77894551912944\n",
      "Epoch 19 \t Batch 320 \t Validation Loss: 41.97950134277344\n",
      "Epoch 19 \t Batch 340 \t Validation Loss: 41.77612855574664\n",
      "Epoch 19 \t Batch 360 \t Validation Loss: 41.454746092690364\n",
      "Epoch 19 \t Batch 380 \t Validation Loss: 41.631564943413984\n",
      "Epoch 19 \t Batch 400 \t Validation Loss: 41.226388454437256\n",
      "Epoch 19 \t Batch 420 \t Validation Loss: 41.19662583214896\n",
      "Epoch 19 \t Batch 440 \t Validation Loss: 41.03528239510276\n",
      "Epoch 19 \t Batch 460 \t Validation Loss: 41.27347630625186\n",
      "Epoch 19 \t Batch 480 \t Validation Loss: 41.68841848770777\n",
      "Epoch 19 \t Batch 500 \t Validation Loss: 41.35067883300781\n",
      "Epoch 19 \t Batch 520 \t Validation Loss: 41.225069286273076\n",
      "Epoch 19 \t Batch 540 \t Validation Loss: 40.87572015656365\n",
      "Epoch 19 \t Batch 560 \t Validation Loss: 40.59845277752195\n",
      "Epoch 19 \t Batch 580 \t Validation Loss: 40.32800650267765\n",
      "Epoch 19 \t Batch 600 \t Validation Loss: 40.43445914109548\n",
      "Epoch 19 Training Loss: 47.61097339975405 Validation Loss: 40.990452867049676\n",
      "Epoch 19 completed\n",
      "Epoch 20 \t Batch 20 \t Training Loss: 48.28846378326416\n",
      "Epoch 20 \t Batch 40 \t Training Loss: 47.32317724227905\n",
      "Epoch 20 \t Batch 60 \t Training Loss: 47.07407519022624\n",
      "Epoch 20 \t Batch 80 \t Training Loss: 47.599542665481565\n",
      "Epoch 20 \t Batch 100 \t Training Loss: 47.798210067749025\n",
      "Epoch 20 \t Batch 120 \t Training Loss: 47.957497437795006\n",
      "Epoch 20 \t Batch 140 \t Training Loss: 47.947725486755374\n",
      "Epoch 20 \t Batch 160 \t Training Loss: 47.85219247341156\n",
      "Epoch 20 \t Batch 180 \t Training Loss: 47.80658355289035\n",
      "Epoch 20 \t Batch 200 \t Training Loss: 47.829051647186276\n",
      "Epoch 20 \t Batch 220 \t Training Loss: 47.83455810546875\n",
      "Epoch 20 \t Batch 240 \t Training Loss: 47.876550372441606\n",
      "Epoch 20 \t Batch 260 \t Training Loss: 47.7734932239239\n",
      "Epoch 20 \t Batch 280 \t Training Loss: 47.70905452455793\n",
      "Epoch 20 \t Batch 300 \t Training Loss: 47.688698094685876\n",
      "Epoch 20 \t Batch 320 \t Training Loss: 47.74677270650864\n",
      "Epoch 20 \t Batch 340 \t Training Loss: 47.7732208925135\n",
      "Epoch 20 \t Batch 360 \t Training Loss: 47.727831829918756\n",
      "Epoch 20 \t Batch 380 \t Training Loss: 47.75320717661004\n",
      "Epoch 20 \t Batch 400 \t Training Loss: 47.68622447013855\n",
      "Epoch 20 \t Batch 420 \t Training Loss: 47.6800500143142\n",
      "Epoch 20 \t Batch 440 \t Training Loss: 47.674190096421675\n",
      "Epoch 20 \t Batch 460 \t Training Loss: 47.74719414089037\n",
      "Epoch 20 \t Batch 480 \t Training Loss: 47.77063992023468\n",
      "Epoch 20 \t Batch 500 \t Training Loss: 47.70644664001465\n",
      "Epoch 20 \t Batch 520 \t Training Loss: 47.67553692597609\n",
      "Epoch 20 \t Batch 540 \t Training Loss: 47.65990441640218\n",
      "Epoch 20 \t Batch 560 \t Training Loss: 47.68305655888149\n",
      "Epoch 20 \t Batch 580 \t Training Loss: 47.65385435696306\n",
      "Epoch 20 \t Batch 600 \t Training Loss: 47.620553194681804\n",
      "Epoch 20 \t Batch 620 \t Training Loss: 47.63744901226413\n",
      "Epoch 20 \t Batch 640 \t Training Loss: 47.62745274305344\n",
      "Epoch 20 \t Batch 660 \t Training Loss: 47.65267844922615\n",
      "Epoch 20 \t Batch 680 \t Training Loss: 47.64011227102841\n",
      "Epoch 20 \t Batch 700 \t Training Loss: 47.66161749703544\n",
      "Epoch 20 \t Batch 720 \t Training Loss: 47.686045466528995\n",
      "Epoch 20 \t Batch 740 \t Training Loss: 47.67043982325374\n",
      "Epoch 20 \t Batch 760 \t Training Loss: 47.66980399583515\n",
      "Epoch 20 \t Batch 780 \t Training Loss: 47.64741656963642\n",
      "Epoch 20 \t Batch 800 \t Training Loss: 47.636456089019774\n",
      "Epoch 20 \t Batch 820 \t Training Loss: 47.60490676600759\n",
      "Epoch 20 \t Batch 840 \t Training Loss: 47.619097341809955\n",
      "Epoch 20 \t Batch 860 \t Training Loss: 47.5958003776018\n",
      "Epoch 20 \t Batch 880 \t Training Loss: 47.55577693419023\n",
      "Epoch 20 \t Batch 900 \t Training Loss: 47.581685540941024\n",
      "Epoch 20 \t Batch 20 \t Validation Loss: 28.969710636138917\n",
      "Epoch 20 \t Batch 40 \t Validation Loss: 30.42611770629883\n",
      "Epoch 20 \t Batch 60 \t Validation Loss: 32.42002271016439\n",
      "Epoch 20 \t Batch 80 \t Validation Loss: 31.633057045936585\n",
      "Epoch 20 \t Batch 100 \t Validation Loss: 31.006206703186034\n",
      "Epoch 20 \t Batch 120 \t Validation Loss: 30.815162229537965\n",
      "Epoch 20 \t Batch 140 \t Validation Loss: 30.356991740635465\n",
      "Epoch 20 \t Batch 160 \t Validation Loss: 31.867576050758363\n",
      "Epoch 20 \t Batch 180 \t Validation Loss: 35.31119384235806\n",
      "Epoch 20 \t Batch 200 \t Validation Loss: 36.64797369480133\n",
      "Epoch 20 \t Batch 220 \t Validation Loss: 37.973688034577805\n",
      "Epoch 20 \t Batch 240 \t Validation Loss: 38.30538777510325\n",
      "Epoch 20 \t Batch 260 \t Validation Loss: 40.46984618627108\n",
      "Epoch 20 \t Batch 280 \t Validation Loss: 41.594289793287004\n",
      "Epoch 20 \t Batch 300 \t Validation Loss: 42.49747421900431\n",
      "Epoch 20 \t Batch 320 \t Validation Loss: 42.84524190425873\n",
      "Epoch 20 \t Batch 340 \t Validation Loss: 42.61361059862025\n",
      "Epoch 20 \t Batch 360 \t Validation Loss: 42.30594807200961\n",
      "Epoch 20 \t Batch 380 \t Validation Loss: 42.46866998421518\n",
      "Epoch 20 \t Batch 400 \t Validation Loss: 41.88836290121078\n",
      "Epoch 20 \t Batch 420 \t Validation Loss: 41.76323039872306\n",
      "Epoch 20 \t Batch 440 \t Validation Loss: 41.318814314495434\n",
      "Epoch 20 \t Batch 460 \t Validation Loss: 41.40493624106698\n",
      "Epoch 20 \t Batch 480 \t Validation Loss: 41.77400551040967\n",
      "Epoch 20 \t Batch 500 \t Validation Loss: 41.407550024032595\n",
      "Epoch 20 \t Batch 520 \t Validation Loss: 41.03672491770524\n",
      "Epoch 20 \t Batch 540 \t Validation Loss: 40.67197277457626\n",
      "Epoch 20 \t Batch 560 \t Validation Loss: 40.342724117210935\n",
      "Epoch 20 \t Batch 580 \t Validation Loss: 40.029382986857975\n",
      "Epoch 20 \t Batch 600 \t Validation Loss: 40.13303607781728\n",
      "Epoch 20 Training Loss: 47.57411791636093 Validation Loss: 40.70418542546111\n",
      "Epoch 20 completed\n",
      "Epoch 21 \t Batch 20 \t Training Loss: 47.1737548828125\n",
      "Epoch 21 \t Batch 40 \t Training Loss: 47.412782192230225\n",
      "Epoch 21 \t Batch 60 \t Training Loss: 47.9713160832723\n",
      "Epoch 21 \t Batch 80 \t Training Loss: 47.44410490989685\n",
      "Epoch 21 \t Batch 100 \t Training Loss: 47.256130142211916\n",
      "Epoch 21 \t Batch 120 \t Training Loss: 47.3106286684672\n",
      "Epoch 21 \t Batch 140 \t Training Loss: 47.23164871760777\n",
      "Epoch 21 \t Batch 160 \t Training Loss: 47.251117420196536\n",
      "Epoch 21 \t Batch 180 \t Training Loss: 47.1161131117079\n",
      "Epoch 21 \t Batch 200 \t Training Loss: 47.279152393341064\n",
      "Epoch 21 \t Batch 220 \t Training Loss: 47.30873806693337\n",
      "Epoch 21 \t Batch 240 \t Training Loss: 47.336097145080565\n",
      "Epoch 21 \t Batch 260 \t Training Loss: 47.30521235832801\n",
      "Epoch 21 \t Batch 280 \t Training Loss: 47.45270369393485\n",
      "Epoch 21 \t Batch 300 \t Training Loss: 47.44866964975993\n",
      "Epoch 21 \t Batch 320 \t Training Loss: 47.44955952167511\n",
      "Epoch 21 \t Batch 340 \t Training Loss: 47.49306723650764\n",
      "Epoch 21 \t Batch 360 \t Training Loss: 47.56726435555352\n",
      "Epoch 21 \t Batch 380 \t Training Loss: 47.57660890880384\n",
      "Epoch 21 \t Batch 400 \t Training Loss: 47.55630171775818\n",
      "Epoch 21 \t Batch 420 \t Training Loss: 47.60510946001325\n",
      "Epoch 21 \t Batch 440 \t Training Loss: 47.6187781680714\n",
      "Epoch 21 \t Batch 460 \t Training Loss: 47.6281807858011\n",
      "Epoch 21 \t Batch 480 \t Training Loss: 47.66177453200022\n",
      "Epoch 21 \t Batch 500 \t Training Loss: 47.69513359069824\n",
      "Epoch 21 \t Batch 520 \t Training Loss: 47.62472993410551\n",
      "Epoch 21 \t Batch 540 \t Training Loss: 47.648650628549085\n",
      "Epoch 21 \t Batch 560 \t Training Loss: 47.63027358736311\n",
      "Epoch 21 \t Batch 580 \t Training Loss: 47.64547565723288\n",
      "Epoch 21 \t Batch 600 \t Training Loss: 47.616115137736\n",
      "Epoch 21 \t Batch 620 \t Training Loss: 47.674907979657576\n",
      "Epoch 21 \t Batch 640 \t Training Loss: 47.681788223981854\n",
      "Epoch 21 \t Batch 660 \t Training Loss: 47.62621645493941\n",
      "Epoch 21 \t Batch 680 \t Training Loss: 47.61226471732645\n",
      "Epoch 21 \t Batch 700 \t Training Loss: 47.562437515258786\n",
      "Epoch 21 \t Batch 720 \t Training Loss: 47.50408583747016\n",
      "Epoch 21 \t Batch 740 \t Training Loss: 47.522499873187094\n",
      "Epoch 21 \t Batch 760 \t Training Loss: 47.51602184396041\n",
      "Epoch 21 \t Batch 780 \t Training Loss: 47.51694894448305\n",
      "Epoch 21 \t Batch 800 \t Training Loss: 47.49558032989502\n",
      "Epoch 21 \t Batch 820 \t Training Loss: 47.48277956799763\n",
      "Epoch 21 \t Batch 840 \t Training Loss: 47.45770572934832\n",
      "Epoch 21 \t Batch 860 \t Training Loss: 47.43843382901924\n",
      "Epoch 21 \t Batch 880 \t Training Loss: 47.48879256248474\n",
      "Epoch 21 \t Batch 900 \t Training Loss: 47.476681819491915\n",
      "Epoch 21 \t Batch 20 \t Validation Loss: 41.4246563911438\n",
      "Epoch 21 \t Batch 40 \t Validation Loss: 39.47666335105896\n",
      "Epoch 21 \t Batch 60 \t Validation Loss: 42.18896408081055\n",
      "Epoch 21 \t Batch 80 \t Validation Loss: 40.33595895767212\n",
      "Epoch 21 \t Batch 100 \t Validation Loss: 38.214397258758545\n",
      "Epoch 21 \t Batch 120 \t Validation Loss: 37.09857611656189\n",
      "Epoch 21 \t Batch 140 \t Validation Loss: 35.831753730773926\n",
      "Epoch 21 \t Batch 160 \t Validation Loss: 36.64004331827164\n",
      "Epoch 21 \t Batch 180 \t Validation Loss: 39.68537582291497\n",
      "Epoch 21 \t Batch 200 \t Validation Loss: 40.788073630332946\n",
      "Epoch 21 \t Batch 220 \t Validation Loss: 41.7724524194544\n",
      "Epoch 21 \t Batch 240 \t Validation Loss: 41.8732811888059\n",
      "Epoch 21 \t Batch 260 \t Validation Loss: 43.89496268125681\n",
      "Epoch 21 \t Batch 280 \t Validation Loss: 44.87272241796766\n",
      "Epoch 21 \t Batch 300 \t Validation Loss: 45.64134819348653\n",
      "Epoch 21 \t Batch 320 \t Validation Loss: 45.8431268364191\n",
      "Epoch 21 \t Batch 340 \t Validation Loss: 45.49576254171484\n",
      "Epoch 21 \t Batch 360 \t Validation Loss: 45.024121239450245\n",
      "Epoch 21 \t Batch 380 \t Validation Loss: 45.126016453692785\n",
      "Epoch 21 \t Batch 400 \t Validation Loss: 44.4984296965599\n",
      "Epoch 21 \t Batch 420 \t Validation Loss: 44.31854276430039\n",
      "Epoch 21 \t Batch 440 \t Validation Loss: 43.896941781044006\n",
      "Epoch 21 \t Batch 460 \t Validation Loss: 43.97041859212129\n",
      "Epoch 21 \t Batch 480 \t Validation Loss: 44.25548099478086\n",
      "Epoch 21 \t Batch 500 \t Validation Loss: 43.837179082870485\n",
      "Epoch 21 \t Batch 520 \t Validation Loss: 43.5209566281392\n",
      "Epoch 21 \t Batch 540 \t Validation Loss: 43.0344785672647\n",
      "Epoch 21 \t Batch 560 \t Validation Loss: 42.63038548231125\n",
      "Epoch 21 \t Batch 580 \t Validation Loss: 42.224183827433095\n",
      "Epoch 21 \t Batch 600 \t Validation Loss: 42.24075495243073\n",
      "Epoch 21 Training Loss: 47.4850249503665 Validation Loss: 42.71673215364481\n",
      "Epoch 21 completed\n",
      "Epoch 22 \t Batch 20 \t Training Loss: 46.36911449432373\n",
      "Epoch 22 \t Batch 40 \t Training Loss: 47.25339221954346\n",
      "Epoch 22 \t Batch 60 \t Training Loss: 46.93969930013021\n",
      "Epoch 22 \t Batch 80 \t Training Loss: 47.17956380844116\n",
      "Epoch 22 \t Batch 100 \t Training Loss: 47.12062698364258\n",
      "Epoch 22 \t Batch 120 \t Training Loss: 47.48896808624268\n",
      "Epoch 22 \t Batch 140 \t Training Loss: 47.51103831699916\n",
      "Epoch 22 \t Batch 160 \t Training Loss: 47.50786933898926\n",
      "Epoch 22 \t Batch 180 \t Training Loss: 47.393209309048125\n",
      "Epoch 22 \t Batch 200 \t Training Loss: 47.49706382751465\n",
      "Epoch 22 \t Batch 220 \t Training Loss: 47.43442173004151\n",
      "Epoch 22 \t Batch 240 \t Training Loss: 47.5010313351949\n",
      "Epoch 22 \t Batch 260 \t Training Loss: 47.46487488379845\n",
      "Epoch 22 \t Batch 280 \t Training Loss: 47.3800206048148\n",
      "Epoch 22 \t Batch 300 \t Training Loss: 47.465400530497234\n",
      "Epoch 22 \t Batch 320 \t Training Loss: 47.48278611898422\n",
      "Epoch 22 \t Batch 340 \t Training Loss: 47.428202202740835\n",
      "Epoch 22 \t Batch 360 \t Training Loss: 47.37350370619032\n",
      "Epoch 22 \t Batch 380 \t Training Loss: 47.294084629259615\n",
      "Epoch 22 \t Batch 400 \t Training Loss: 47.30729499816894\n",
      "Epoch 22 \t Batch 420 \t Training Loss: 47.321747080485025\n",
      "Epoch 22 \t Batch 440 \t Training Loss: 47.32668117176403\n",
      "Epoch 22 \t Batch 460 \t Training Loss: 47.32106197191322\n",
      "Epoch 22 \t Batch 480 \t Training Loss: 47.34684204260508\n",
      "Epoch 22 \t Batch 500 \t Training Loss: 47.30232988739014\n",
      "Epoch 22 \t Batch 520 \t Training Loss: 47.320006150465744\n",
      "Epoch 22 \t Batch 540 \t Training Loss: 47.419349380775735\n",
      "Epoch 22 \t Batch 560 \t Training Loss: 47.41037887845721\n",
      "Epoch 22 \t Batch 580 \t Training Loss: 47.36090728496683\n",
      "Epoch 22 \t Batch 600 \t Training Loss: 47.331140677134194\n",
      "Epoch 22 \t Batch 620 \t Training Loss: 47.34006277515042\n",
      "Epoch 22 \t Batch 640 \t Training Loss: 47.3394159913063\n",
      "Epoch 22 \t Batch 660 \t Training Loss: 47.32645559599905\n",
      "Epoch 22 \t Batch 680 \t Training Loss: 47.369055663838104\n",
      "Epoch 22 \t Batch 700 \t Training Loss: 47.34250404902867\n",
      "Epoch 22 \t Batch 720 \t Training Loss: 47.36178311771817\n",
      "Epoch 22 \t Batch 740 \t Training Loss: 47.390491037111026\n",
      "Epoch 22 \t Batch 760 \t Training Loss: 47.357180008135344\n",
      "Epoch 22 \t Batch 780 \t Training Loss: 47.390906104063376\n",
      "Epoch 22 \t Batch 800 \t Training Loss: 47.3999649810791\n",
      "Epoch 22 \t Batch 820 \t Training Loss: 47.41667780527254\n",
      "Epoch 22 \t Batch 840 \t Training Loss: 47.44038624990554\n",
      "Epoch 22 \t Batch 860 \t Training Loss: 47.4566653584325\n",
      "Epoch 22 \t Batch 880 \t Training Loss: 47.448262786865236\n",
      "Epoch 22 \t Batch 900 \t Training Loss: 47.444055031670466\n",
      "Epoch 22 \t Batch 20 \t Validation Loss: 30.934298515319824\n",
      "Epoch 22 \t Batch 40 \t Validation Loss: 31.54033350944519\n",
      "Epoch 22 \t Batch 60 \t Validation Loss: 33.95852966308594\n",
      "Epoch 22 \t Batch 80 \t Validation Loss: 32.916961061954495\n",
      "Epoch 22 \t Batch 100 \t Validation Loss: 32.11622134208679\n",
      "Epoch 22 \t Batch 120 \t Validation Loss: 32.006436196962994\n",
      "Epoch 22 \t Batch 140 \t Validation Loss: 31.458558470862254\n",
      "Epoch 22 \t Batch 160 \t Validation Loss: 32.69961745142937\n",
      "Epoch 22 \t Batch 180 \t Validation Loss: 35.59835862053765\n",
      "Epoch 22 \t Batch 200 \t Validation Loss: 36.80101260185242\n",
      "Epoch 22 \t Batch 220 \t Validation Loss: 37.73637451692061\n",
      "Epoch 22 \t Batch 240 \t Validation Loss: 37.8792453010877\n",
      "Epoch 22 \t Batch 260 \t Validation Loss: 39.946234163871175\n",
      "Epoch 22 \t Batch 280 \t Validation Loss: 40.99472656931196\n",
      "Epoch 22 \t Batch 300 \t Validation Loss: 41.686990642547606\n",
      "Epoch 22 \t Batch 320 \t Validation Loss: 41.93630300760269\n",
      "Epoch 22 \t Batch 340 \t Validation Loss: 41.7187435206245\n",
      "Epoch 22 \t Batch 360 \t Validation Loss: 41.394014207522076\n",
      "Epoch 22 \t Batch 380 \t Validation Loss: 41.5748773499539\n",
      "Epoch 22 \t Batch 400 \t Validation Loss: 41.096774470806125\n",
      "Epoch 22 \t Batch 420 \t Validation Loss: 41.00541933831715\n",
      "Epoch 22 \t Batch 440 \t Validation Loss: 40.70256567651575\n",
      "Epoch 22 \t Batch 460 \t Validation Loss: 40.901349596355274\n",
      "Epoch 22 \t Batch 480 \t Validation Loss: 41.29066859682401\n",
      "Epoch 22 \t Batch 500 \t Validation Loss: 40.943337732315065\n",
      "Epoch 22 \t Batch 520 \t Validation Loss: 40.76348714094895\n",
      "Epoch 22 \t Batch 540 \t Validation Loss: 40.4343885563038\n",
      "Epoch 22 \t Batch 560 \t Validation Loss: 40.17220647675651\n",
      "Epoch 22 \t Batch 580 \t Validation Loss: 39.95889494008031\n",
      "Epoch 22 \t Batch 600 \t Validation Loss: 40.08026294708252\n",
      "Epoch 22 Training Loss: 47.46216908562015 Validation Loss: 40.6413497739024\n",
      "Epoch 22 completed\n",
      "Epoch 23 \t Batch 20 \t Training Loss: 48.263928985595705\n",
      "Epoch 23 \t Batch 40 \t Training Loss: 47.98041477203369\n",
      "Epoch 23 \t Batch 60 \t Training Loss: 47.5894079208374\n",
      "Epoch 23 \t Batch 80 \t Training Loss: 47.448980522155765\n",
      "Epoch 23 \t Batch 100 \t Training Loss: 47.485297508239746\n",
      "Epoch 23 \t Batch 120 \t Training Loss: 47.5919589360555\n",
      "Epoch 23 \t Batch 140 \t Training Loss: 47.374807875497\n",
      "Epoch 23 \t Batch 160 \t Training Loss: 47.455129051208495\n",
      "Epoch 23 \t Batch 180 \t Training Loss: 47.647953287760416\n",
      "Epoch 23 \t Batch 200 \t Training Loss: 47.564172420501706\n",
      "Epoch 23 \t Batch 220 \t Training Loss: 47.555044208873404\n",
      "Epoch 23 \t Batch 240 \t Training Loss: 47.45307552019755\n",
      "Epoch 23 \t Batch 260 \t Training Loss: 47.317746866666354\n",
      "Epoch 23 \t Batch 280 \t Training Loss: 47.35170709065029\n",
      "Epoch 23 \t Batch 300 \t Training Loss: 47.35609504699707\n",
      "Epoch 23 \t Batch 320 \t Training Loss: 47.36050848960876\n",
      "Epoch 23 \t Batch 340 \t Training Loss: 47.41206132103415\n",
      "Epoch 23 \t Batch 360 \t Training Loss: 47.436905013190376\n",
      "Epoch 23 \t Batch 380 \t Training Loss: 47.37074434380782\n",
      "Epoch 23 \t Batch 400 \t Training Loss: 47.37271313667297\n",
      "Epoch 23 \t Batch 420 \t Training Loss: 47.39298295520601\n",
      "Epoch 23 \t Batch 440 \t Training Loss: 47.32735517675226\n",
      "Epoch 23 \t Batch 460 \t Training Loss: 47.38083138673202\n",
      "Epoch 23 \t Batch 480 \t Training Loss: 47.37497321764628\n",
      "Epoch 23 \t Batch 500 \t Training Loss: 47.390454040527345\n",
      "Epoch 23 \t Batch 520 \t Training Loss: 47.391916627150316\n",
      "Epoch 23 \t Batch 540 \t Training Loss: 47.46191767939815\n",
      "Epoch 23 \t Batch 560 \t Training Loss: 47.451772056307114\n",
      "Epoch 23 \t Batch 580 \t Training Loss: 47.42530019036655\n",
      "Epoch 23 \t Batch 600 \t Training Loss: 47.37886012395223\n",
      "Epoch 23 \t Batch 620 \t Training Loss: 47.4114075322305\n",
      "Epoch 23 \t Batch 640 \t Training Loss: 47.45427516102791\n",
      "Epoch 23 \t Batch 660 \t Training Loss: 47.455051786249335\n",
      "Epoch 23 \t Batch 680 \t Training Loss: 47.42912613924812\n",
      "Epoch 23 \t Batch 700 \t Training Loss: 47.39493452344622\n",
      "Epoch 23 \t Batch 720 \t Training Loss: 47.39610876507229\n",
      "Epoch 23 \t Batch 740 \t Training Loss: 47.3874215925062\n",
      "Epoch 23 \t Batch 760 \t Training Loss: 47.39473003086291\n",
      "Epoch 23 \t Batch 780 \t Training Loss: 47.40145319914207\n",
      "Epoch 23 \t Batch 800 \t Training Loss: 47.38682725906372\n",
      "Epoch 23 \t Batch 820 \t Training Loss: 47.40976696014404\n",
      "Epoch 23 \t Batch 840 \t Training Loss: 47.39662045524234\n",
      "Epoch 23 \t Batch 860 \t Training Loss: 47.396985985511954\n",
      "Epoch 23 \t Batch 880 \t Training Loss: 47.38029643839056\n",
      "Epoch 23 \t Batch 900 \t Training Loss: 47.384059037102595\n",
      "Epoch 23 \t Batch 20 \t Validation Loss: 34.79198923110962\n",
      "Epoch 23 \t Batch 40 \t Validation Loss: 34.229063749313354\n",
      "Epoch 23 \t Batch 60 \t Validation Loss: 37.415279706319176\n",
      "Epoch 23 \t Batch 80 \t Validation Loss: 35.91757606267929\n",
      "Epoch 23 \t Batch 100 \t Validation Loss: 34.8579639339447\n",
      "Epoch 23 \t Batch 120 \t Validation Loss: 34.468571146329246\n",
      "Epoch 23 \t Batch 140 \t Validation Loss: 33.71791480609349\n",
      "Epoch 23 \t Batch 160 \t Validation Loss: 34.67800322175026\n",
      "Epoch 23 \t Batch 180 \t Validation Loss: 37.40392780833774\n",
      "Epoch 23 \t Batch 200 \t Validation Loss: 38.51493755817413\n",
      "Epoch 23 \t Batch 220 \t Validation Loss: 39.34858571399342\n",
      "Epoch 23 \t Batch 240 \t Validation Loss: 39.43390792608261\n",
      "Epoch 23 \t Batch 260 \t Validation Loss: 41.40853833785424\n",
      "Epoch 23 \t Batch 280 \t Validation Loss: 42.37406716006143\n",
      "Epoch 23 \t Batch 300 \t Validation Loss: 42.99591082572937\n",
      "Epoch 23 \t Batch 320 \t Validation Loss: 43.19482074379921\n",
      "Epoch 23 \t Batch 340 \t Validation Loss: 42.94774537927964\n",
      "Epoch 23 \t Batch 360 \t Validation Loss: 42.53165970908271\n",
      "Epoch 23 \t Batch 380 \t Validation Loss: 42.64299380151849\n",
      "Epoch 23 \t Batch 400 \t Validation Loss: 42.09809907436371\n",
      "Epoch 23 \t Batch 420 \t Validation Loss: 41.986549345652264\n",
      "Epoch 23 \t Batch 440 \t Validation Loss: 41.635198007930406\n",
      "Epoch 23 \t Batch 460 \t Validation Loss: 41.808754518757695\n",
      "Epoch 23 \t Batch 480 \t Validation Loss: 42.148764208952585\n",
      "Epoch 23 \t Batch 500 \t Validation Loss: 41.79706629562378\n",
      "Epoch 23 \t Batch 520 \t Validation Loss: 41.55695489369906\n",
      "Epoch 23 \t Batch 540 \t Validation Loss: 41.209826140933565\n",
      "Epoch 23 \t Batch 560 \t Validation Loss: 40.9648053748267\n",
      "Epoch 23 \t Batch 580 \t Validation Loss: 40.73094502810774\n",
      "Epoch 23 \t Batch 600 \t Validation Loss: 40.84981264750163\n",
      "Epoch 23 Training Loss: 47.40769263780234 Validation Loss: 41.39504138835065\n",
      "Epoch 23 completed\n",
      "Epoch 24 \t Batch 20 \t Training Loss: 47.57355728149414\n",
      "Epoch 24 \t Batch 40 \t Training Loss: 46.690505790710446\n",
      "Epoch 24 \t Batch 60 \t Training Loss: 47.41014111836751\n",
      "Epoch 24 \t Batch 80 \t Training Loss: 47.50706515312195\n",
      "Epoch 24 \t Batch 100 \t Training Loss: 47.49604415893555\n",
      "Epoch 24 \t Batch 120 \t Training Loss: 47.6080953280131\n",
      "Epoch 24 \t Batch 140 \t Training Loss: 47.59671949659075\n",
      "Epoch 24 \t Batch 160 \t Training Loss: 47.56036005020142\n",
      "Epoch 24 \t Batch 180 \t Training Loss: 47.60622656080458\n",
      "Epoch 24 \t Batch 200 \t Training Loss: 47.61942211151123\n",
      "Epoch 24 \t Batch 220 \t Training Loss: 47.595302287015045\n",
      "Epoch 24 \t Batch 240 \t Training Loss: 47.63218183517456\n",
      "Epoch 24 \t Batch 260 \t Training Loss: 47.561515984168416\n",
      "Epoch 24 \t Batch 280 \t Training Loss: 47.695349379948205\n",
      "Epoch 24 \t Batch 300 \t Training Loss: 47.68303371429443\n",
      "Epoch 24 \t Batch 320 \t Training Loss: 47.608476316928865\n",
      "Epoch 24 \t Batch 340 \t Training Loss: 47.579609579198504\n",
      "Epoch 24 \t Batch 360 \t Training Loss: 47.571254857381184\n",
      "Epoch 24 \t Batch 380 \t Training Loss: 47.573489008451766\n",
      "Epoch 24 \t Batch 400 \t Training Loss: 47.571862449646\n",
      "Epoch 24 \t Batch 420 \t Training Loss: 47.55901941571917\n",
      "Epoch 24 \t Batch 440 \t Training Loss: 47.57897243499756\n",
      "Epoch 24 \t Batch 460 \t Training Loss: 47.55349354121996\n",
      "Epoch 24 \t Batch 480 \t Training Loss: 47.54553150335948\n",
      "Epoch 24 \t Batch 500 \t Training Loss: 47.55402541351318\n",
      "Epoch 24 \t Batch 520 \t Training Loss: 47.523206901550296\n",
      "Epoch 24 \t Batch 540 \t Training Loss: 47.467751065006965\n",
      "Epoch 24 \t Batch 560 \t Training Loss: 47.42620376178196\n",
      "Epoch 24 \t Batch 580 \t Training Loss: 47.36341802662817\n",
      "Epoch 24 \t Batch 600 \t Training Loss: 47.29304345448812\n",
      "Epoch 24 \t Batch 620 \t Training Loss: 47.30051596241613\n",
      "Epoch 24 \t Batch 640 \t Training Loss: 47.29772816896438\n",
      "Epoch 24 \t Batch 660 \t Training Loss: 47.31879669536244\n",
      "Epoch 24 \t Batch 680 \t Training Loss: 47.28708202698652\n",
      "Epoch 24 \t Batch 700 \t Training Loss: 47.31180236816406\n",
      "Epoch 24 \t Batch 720 \t Training Loss: 47.347993506325615\n",
      "Epoch 24 \t Batch 740 \t Training Loss: 47.357206385844464\n",
      "Epoch 24 \t Batch 760 \t Training Loss: 47.393575964475936\n",
      "Epoch 24 \t Batch 780 \t Training Loss: 47.40395888793163\n",
      "Epoch 24 \t Batch 800 \t Training Loss: 47.44198576927185\n",
      "Epoch 24 \t Batch 820 \t Training Loss: 47.41898996771836\n",
      "Epoch 24 \t Batch 840 \t Training Loss: 47.4248546009972\n",
      "Epoch 24 \t Batch 860 \t Training Loss: 47.39976458438607\n",
      "Epoch 24 \t Batch 880 \t Training Loss: 47.39066495028409\n",
      "Epoch 24 \t Batch 900 \t Training Loss: 47.37154062906901\n",
      "Epoch 24 \t Batch 20 \t Validation Loss: 23.482884645462036\n",
      "Epoch 24 \t Batch 40 \t Validation Loss: 25.6988454580307\n",
      "Epoch 24 \t Batch 60 \t Validation Loss: 27.701453002293906\n",
      "Epoch 24 \t Batch 80 \t Validation Loss: 27.10002804994583\n",
      "Epoch 24 \t Batch 100 \t Validation Loss: 27.137219705581664\n",
      "Epoch 24 \t Batch 120 \t Validation Loss: 27.59626224040985\n",
      "Epoch 24 \t Batch 140 \t Validation Loss: 27.496975571768626\n",
      "Epoch 24 \t Batch 160 \t Validation Loss: 29.160945558547972\n",
      "Epoch 24 \t Batch 180 \t Validation Loss: 32.349638827641805\n",
      "Epoch 24 \t Batch 200 \t Validation Loss: 33.887317872047426\n",
      "Epoch 24 \t Batch 220 \t Validation Loss: 35.052410407499835\n",
      "Epoch 24 \t Batch 240 \t Validation Loss: 35.44104641675949\n",
      "Epoch 24 \t Batch 260 \t Validation Loss: 37.65445452836843\n",
      "Epoch 24 \t Batch 280 \t Validation Loss: 38.84285511629922\n",
      "Epoch 24 \t Batch 300 \t Validation Loss: 39.626093537012736\n",
      "Epoch 24 \t Batch 320 \t Validation Loss: 39.99732553362846\n",
      "Epoch 24 \t Batch 340 \t Validation Loss: 39.94393078860114\n",
      "Epoch 24 \t Batch 360 \t Validation Loss: 39.66443625556098\n",
      "Epoch 24 \t Batch 380 \t Validation Loss: 39.915951758936835\n",
      "Epoch 24 \t Batch 400 \t Validation Loss: 39.57898926734924\n",
      "Epoch 24 \t Batch 420 \t Validation Loss: 39.60642744700114\n",
      "Epoch 24 \t Batch 440 \t Validation Loss: 39.440398220582445\n",
      "Epoch 24 \t Batch 460 \t Validation Loss: 39.66360520072605\n",
      "Epoch 24 \t Batch 480 \t Validation Loss: 40.103631631533304\n",
      "Epoch 24 \t Batch 500 \t Validation Loss: 39.826648643493655\n",
      "Epoch 24 \t Batch 520 \t Validation Loss: 39.63395270934472\n",
      "Epoch 24 \t Batch 540 \t Validation Loss: 39.312461065363\n",
      "Epoch 24 \t Batch 560 \t Validation Loss: 39.04039244992392\n",
      "Epoch 24 \t Batch 580 \t Validation Loss: 38.72755922120193\n",
      "Epoch 24 \t Batch 600 \t Validation Loss: 38.8773619556427\n",
      "Epoch 24 Training Loss: 47.37511565147076 Validation Loss: 39.4461994264033\n",
      "Epoch 24 completed\n",
      "Epoch 25 \t Batch 20 \t Training Loss: 46.311051559448245\n",
      "Epoch 25 \t Batch 40 \t Training Loss: 47.0463267326355\n",
      "Epoch 25 \t Batch 60 \t Training Loss: 47.166269302368164\n",
      "Epoch 25 \t Batch 80 \t Training Loss: 47.38173093795776\n",
      "Epoch 25 \t Batch 100 \t Training Loss: 47.21970500946045\n",
      "Epoch 25 \t Batch 120 \t Training Loss: 47.19587287902832\n",
      "Epoch 25 \t Batch 140 \t Training Loss: 47.23224473680769\n",
      "Epoch 25 \t Batch 160 \t Training Loss: 47.17594201564789\n",
      "Epoch 25 \t Batch 180 \t Training Loss: 47.246021334330244\n",
      "Epoch 25 \t Batch 200 \t Training Loss: 47.26890396118164\n",
      "Epoch 25 \t Batch 220 \t Training Loss: 47.17689188176935\n",
      "Epoch 25 \t Batch 240 \t Training Loss: 47.25510361989339\n",
      "Epoch 25 \t Batch 260 \t Training Loss: 47.265382590660685\n",
      "Epoch 25 \t Batch 280 \t Training Loss: 47.27236486162458\n",
      "Epoch 25 \t Batch 300 \t Training Loss: 47.27188495635986\n",
      "Epoch 25 \t Batch 320 \t Training Loss: 47.14108847379684\n",
      "Epoch 25 \t Batch 340 \t Training Loss: 47.229663590823904\n",
      "Epoch 25 \t Batch 360 \t Training Loss: 47.260086154937746\n",
      "Epoch 25 \t Batch 380 \t Training Loss: 47.31850752579538\n",
      "Epoch 25 \t Batch 400 \t Training Loss: 47.33088809967041\n",
      "Epoch 25 \t Batch 420 \t Training Loss: 47.29690747942243\n",
      "Epoch 25 \t Batch 440 \t Training Loss: 47.31336807771162\n",
      "Epoch 25 \t Batch 460 \t Training Loss: 47.369045498060146\n",
      "Epoch 25 \t Batch 480 \t Training Loss: 47.35720031261444\n",
      "Epoch 25 \t Batch 500 \t Training Loss: 47.32448316192627\n",
      "Epoch 25 \t Batch 520 \t Training Loss: 47.300182665311375\n",
      "Epoch 25 \t Batch 540 \t Training Loss: 47.26526513276277\n",
      "Epoch 25 \t Batch 560 \t Training Loss: 47.22084238869803\n",
      "Epoch 25 \t Batch 580 \t Training Loss: 47.277896137895254\n",
      "Epoch 25 \t Batch 600 \t Training Loss: 47.27940800348917\n",
      "Epoch 25 \t Batch 620 \t Training Loss: 47.31248289538968\n",
      "Epoch 25 \t Batch 640 \t Training Loss: 47.33462789654732\n",
      "Epoch 25 \t Batch 660 \t Training Loss: 47.31282420880867\n",
      "Epoch 25 \t Batch 680 \t Training Loss: 47.32819989709293\n",
      "Epoch 25 \t Batch 700 \t Training Loss: 47.323049240112304\n",
      "Epoch 25 \t Batch 720 \t Training Loss: 47.31731408437093\n",
      "Epoch 25 \t Batch 740 \t Training Loss: 47.32702062967661\n",
      "Epoch 25 \t Batch 760 \t Training Loss: 47.336876307035745\n",
      "Epoch 25 \t Batch 780 \t Training Loss: 47.35081290220603\n",
      "Epoch 25 \t Batch 800 \t Training Loss: 47.32836293697357\n",
      "Epoch 25 \t Batch 820 \t Training Loss: 47.313843103734456\n",
      "Epoch 25 \t Batch 840 \t Training Loss: 47.3053375516619\n",
      "Epoch 25 \t Batch 860 \t Training Loss: 47.292062009767044\n",
      "Epoch 25 \t Batch 880 \t Training Loss: 47.31942388794639\n",
      "Epoch 25 \t Batch 900 \t Training Loss: 47.31507994333903\n",
      "Epoch 25 \t Batch 20 \t Validation Loss: 30.85031270980835\n",
      "Epoch 25 \t Batch 40 \t Validation Loss: 30.890077710151672\n",
      "Epoch 25 \t Batch 60 \t Validation Loss: 33.181982056299844\n",
      "Epoch 25 \t Batch 80 \t Validation Loss: 32.203866159915925\n",
      "Epoch 25 \t Batch 100 \t Validation Loss: 31.229739179611204\n",
      "Epoch 25 \t Batch 120 \t Validation Loss: 30.900789515177408\n",
      "Epoch 25 \t Batch 140 \t Validation Loss: 30.315252017974853\n",
      "Epoch 25 \t Batch 160 \t Validation Loss: 31.736414897441865\n",
      "Epoch 25 \t Batch 180 \t Validation Loss: 34.74161019325256\n",
      "Epoch 25 \t Batch 200 \t Validation Loss: 36.22948899745941\n",
      "Epoch 25 \t Batch 220 \t Validation Loss: 37.22539839744568\n",
      "Epoch 25 \t Batch 240 \t Validation Loss: 37.47972300052643\n",
      "Epoch 25 \t Batch 260 \t Validation Loss: 39.59116933162396\n",
      "Epoch 25 \t Batch 280 \t Validation Loss: 40.67166549137661\n",
      "Epoch 25 \t Batch 300 \t Validation Loss: 41.400058294932045\n",
      "Epoch 25 \t Batch 320 \t Validation Loss: 41.711882987618445\n",
      "Epoch 25 \t Batch 340 \t Validation Loss: 41.55842114897335\n",
      "Epoch 25 \t Batch 360 \t Validation Loss: 41.22482277817196\n",
      "Epoch 25 \t Batch 380 \t Validation Loss: 41.54319763936495\n",
      "Epoch 25 \t Batch 400 \t Validation Loss: 41.29452995538711\n",
      "Epoch 25 \t Batch 420 \t Validation Loss: 41.33618274189177\n",
      "Epoch 25 \t Batch 440 \t Validation Loss: 41.39880914471366\n",
      "Epoch 25 \t Batch 460 \t Validation Loss: 41.755391670309976\n",
      "Epoch 25 \t Batch 480 \t Validation Loss: 42.17627687652906\n",
      "Epoch 25 \t Batch 500 \t Validation Loss: 41.85256649208069\n",
      "Epoch 25 \t Batch 520 \t Validation Loss: 41.886403107643126\n",
      "Epoch 25 \t Batch 540 \t Validation Loss: 41.457967872972844\n",
      "Epoch 25 \t Batch 560 \t Validation Loss: 41.08523425545011\n",
      "Epoch 25 \t Batch 580 \t Validation Loss: 40.68566605962556\n",
      "Epoch 25 \t Batch 600 \t Validation Loss: 40.753819483121234\n",
      "Epoch 25 Training Loss: 47.30984470030474 Validation Loss: 41.26728052442724\n",
      "Epoch 25 completed\n",
      "Epoch 26 \t Batch 20 \t Training Loss: 48.982899284362794\n",
      "Epoch 26 \t Batch 40 \t Training Loss: 47.358124828338624\n",
      "Epoch 26 \t Batch 60 \t Training Loss: 47.08473726908366\n",
      "Epoch 26 \t Batch 80 \t Training Loss: 46.955303287506105\n",
      "Epoch 26 \t Batch 100 \t Training Loss: 47.018727684020995\n",
      "Epoch 26 \t Batch 120 \t Training Loss: 47.18888594309489\n",
      "Epoch 26 \t Batch 140 \t Training Loss: 47.05118451799665\n",
      "Epoch 26 \t Batch 160 \t Training Loss: 47.122369456291196\n",
      "Epoch 26 \t Batch 180 \t Training Loss: 47.29885631137424\n",
      "Epoch 26 \t Batch 200 \t Training Loss: 47.432897415161136\n",
      "Epoch 26 \t Batch 220 \t Training Loss: 47.63916504599831\n",
      "Epoch 26 \t Batch 240 \t Training Loss: 47.51176738739014\n",
      "Epoch 26 \t Batch 260 \t Training Loss: 47.44037428635817\n",
      "Epoch 26 \t Batch 280 \t Training Loss: 47.61438996451242\n",
      "Epoch 26 \t Batch 300 \t Training Loss: 47.57605817159017\n",
      "Epoch 26 \t Batch 320 \t Training Loss: 47.50940235853195\n",
      "Epoch 26 \t Batch 340 \t Training Loss: 47.52342683006735\n",
      "Epoch 26 \t Batch 360 \t Training Loss: 47.49692859649658\n",
      "Epoch 26 \t Batch 380 \t Training Loss: 47.45506026619359\n",
      "Epoch 26 \t Batch 400 \t Training Loss: 47.52840809822082\n",
      "Epoch 26 \t Batch 420 \t Training Loss: 47.512697991870695\n",
      "Epoch 26 \t Batch 440 \t Training Loss: 47.49561667008833\n",
      "Epoch 26 \t Batch 460 \t Training Loss: 47.52131632099981\n",
      "Epoch 26 \t Batch 480 \t Training Loss: 47.50963472525279\n",
      "Epoch 26 \t Batch 500 \t Training Loss: 47.47561952972412\n",
      "Epoch 26 \t Batch 520 \t Training Loss: 47.47702837723952\n",
      "Epoch 26 \t Batch 540 \t Training Loss: 47.41105111440023\n",
      "Epoch 26 \t Batch 560 \t Training Loss: 47.38265256200518\n",
      "Epoch 26 \t Batch 580 \t Training Loss: 47.36645681446996\n",
      "Epoch 26 \t Batch 600 \t Training Loss: 47.41582487106323\n",
      "Epoch 26 \t Batch 620 \t Training Loss: 47.40832228506765\n",
      "Epoch 26 \t Batch 640 \t Training Loss: 47.35621038079262\n",
      "Epoch 26 \t Batch 660 \t Training Loss: 47.36376237580271\n",
      "Epoch 26 \t Batch 680 \t Training Loss: 47.39991030973547\n",
      "Epoch 26 \t Batch 700 \t Training Loss: 47.36445635659354\n",
      "Epoch 26 \t Batch 720 \t Training Loss: 47.372693877749974\n",
      "Epoch 26 \t Batch 740 \t Training Loss: 47.379148024481694\n",
      "Epoch 26 \t Batch 760 \t Training Loss: 47.359189108798375\n",
      "Epoch 26 \t Batch 780 \t Training Loss: 47.36069247906025\n",
      "Epoch 26 \t Batch 800 \t Training Loss: 47.32130115032196\n",
      "Epoch 26 \t Batch 820 \t Training Loss: 47.28511338582853\n",
      "Epoch 26 \t Batch 840 \t Training Loss: 47.31025489625477\n",
      "Epoch 26 \t Batch 860 \t Training Loss: 47.28049765742102\n",
      "Epoch 26 \t Batch 880 \t Training Loss: 47.26466944434426\n",
      "Epoch 26 \t Batch 900 \t Training Loss: 47.276768853929305\n",
      "Epoch 26 \t Batch 20 \t Validation Loss: 40.01976013183594\n",
      "Epoch 26 \t Batch 40 \t Validation Loss: 38.46290819644928\n",
      "Epoch 26 \t Batch 60 \t Validation Loss: 41.66760484377543\n",
      "Epoch 26 \t Batch 80 \t Validation Loss: 39.83193539381027\n",
      "Epoch 26 \t Batch 100 \t Validation Loss: 37.569536752700806\n",
      "Epoch 26 \t Batch 120 \t Validation Loss: 36.380950554211935\n",
      "Epoch 26 \t Batch 140 \t Validation Loss: 35.108415031433104\n",
      "Epoch 26 \t Batch 160 \t Validation Loss: 35.67494814395904\n",
      "Epoch 26 \t Batch 180 \t Validation Loss: 37.8431674586402\n",
      "Epoch 26 \t Batch 200 \t Validation Loss: 38.59507761478424\n",
      "Epoch 26 \t Batch 220 \t Validation Loss: 39.082208290967074\n",
      "Epoch 26 \t Batch 240 \t Validation Loss: 38.926342149575554\n",
      "Epoch 26 \t Batch 260 \t Validation Loss: 40.69678189204289\n",
      "Epoch 26 \t Batch 280 \t Validation Loss: 41.52897844995771\n",
      "Epoch 26 \t Batch 300 \t Validation Loss: 41.93429135004679\n",
      "Epoch 26 \t Batch 320 \t Validation Loss: 42.013473349809644\n",
      "Epoch 26 \t Batch 340 \t Validation Loss: 41.729428420347325\n",
      "Epoch 26 \t Batch 360 \t Validation Loss: 41.29629013538361\n",
      "Epoch 26 \t Batch 380 \t Validation Loss: 41.34313366287633\n",
      "Epoch 26 \t Batch 400 \t Validation Loss: 40.812578999996184\n",
      "Epoch 26 \t Batch 420 \t Validation Loss: 40.685852271034605\n",
      "Epoch 26 \t Batch 440 \t Validation Loss: 40.3028756163337\n",
      "Epoch 26 \t Batch 460 \t Validation Loss: 40.439122351356175\n",
      "Epoch 26 \t Batch 480 \t Validation Loss: 40.81931060751279\n",
      "Epoch 26 \t Batch 500 \t Validation Loss: 40.434625463485716\n",
      "Epoch 26 \t Batch 520 \t Validation Loss: 40.184830469351546\n",
      "Epoch 26 \t Batch 540 \t Validation Loss: 39.873675994519836\n",
      "Epoch 26 \t Batch 560 \t Validation Loss: 39.60992107221058\n",
      "Epoch 26 \t Batch 580 \t Validation Loss: 39.36063967244378\n",
      "Epoch 26 \t Batch 600 \t Validation Loss: 39.511672393480936\n",
      "Epoch 26 Training Loss: 47.27644943852981 Validation Loss: 40.08475610652527\n",
      "Epoch 26 completed\n",
      "Epoch 27 \t Batch 20 \t Training Loss: 46.314705276489256\n",
      "Epoch 27 \t Batch 40 \t Training Loss: 47.161016178131106\n",
      "Epoch 27 \t Batch 60 \t Training Loss: 46.39641443888346\n",
      "Epoch 27 \t Batch 80 \t Training Loss: 46.62488102912903\n",
      "Epoch 27 \t Batch 100 \t Training Loss: 46.67406391143799\n",
      "Epoch 27 \t Batch 120 \t Training Loss: 46.593904908498125\n",
      "Epoch 27 \t Batch 140 \t Training Loss: 46.77743538447789\n",
      "Epoch 27 \t Batch 160 \t Training Loss: 46.785837697982785\n",
      "Epoch 27 \t Batch 180 \t Training Loss: 46.85595103369819\n",
      "Epoch 27 \t Batch 200 \t Training Loss: 47.013533401489255\n",
      "Epoch 27 \t Batch 220 \t Training Loss: 47.09923960945823\n",
      "Epoch 27 \t Batch 240 \t Training Loss: 47.03243036270142\n",
      "Epoch 27 \t Batch 260 \t Training Loss: 47.126354202857385\n",
      "Epoch 27 \t Batch 280 \t Training Loss: 47.137847137451175\n",
      "Epoch 27 \t Batch 300 \t Training Loss: 47.176253166198734\n",
      "Epoch 27 \t Batch 320 \t Training Loss: 47.085518181324005\n",
      "Epoch 27 \t Batch 340 \t Training Loss: 47.007609434688796\n",
      "Epoch 27 \t Batch 360 \t Training Loss: 47.15034041934543\n",
      "Epoch 27 \t Batch 380 \t Training Loss: 47.18322791049355\n",
      "Epoch 27 \t Batch 400 \t Training Loss: 47.2425951385498\n",
      "Epoch 27 \t Batch 420 \t Training Loss: 47.21671017238072\n",
      "Epoch 27 \t Batch 440 \t Training Loss: 47.198276008259164\n",
      "Epoch 27 \t Batch 460 \t Training Loss: 47.16571807032046\n",
      "Epoch 27 \t Batch 480 \t Training Loss: 47.18697666327159\n",
      "Epoch 27 \t Batch 500 \t Training Loss: 47.217624946594235\n",
      "Epoch 27 \t Batch 520 \t Training Loss: 47.208689630948584\n",
      "Epoch 27 \t Batch 540 \t Training Loss: 47.21710696043792\n",
      "Epoch 27 \t Batch 560 \t Training Loss: 47.23048966952732\n",
      "Epoch 27 \t Batch 580 \t Training Loss: 47.22592864858693\n",
      "Epoch 27 \t Batch 600 \t Training Loss: 47.22250825246175\n",
      "Epoch 27 \t Batch 620 \t Training Loss: 47.21422996520996\n",
      "Epoch 27 \t Batch 640 \t Training Loss: 47.205562490224835\n",
      "Epoch 27 \t Batch 660 \t Training Loss: 47.190592395898065\n",
      "Epoch 27 \t Batch 680 \t Training Loss: 47.1787846957936\n",
      "Epoch 27 \t Batch 700 \t Training Loss: 47.16436455317906\n",
      "Epoch 27 \t Batch 720 \t Training Loss: 47.179023838043214\n",
      "Epoch 27 \t Batch 740 \t Training Loss: 47.166720225359946\n",
      "Epoch 27 \t Batch 760 \t Training Loss: 47.16923542524639\n",
      "Epoch 27 \t Batch 780 \t Training Loss: 47.165334148896044\n",
      "Epoch 27 \t Batch 800 \t Training Loss: 47.163956713676455\n",
      "Epoch 27 \t Batch 820 \t Training Loss: 47.18207979434874\n",
      "Epoch 27 \t Batch 840 \t Training Loss: 47.165856897263296\n",
      "Epoch 27 \t Batch 860 \t Training Loss: 47.20432525457338\n",
      "Epoch 27 \t Batch 880 \t Training Loss: 47.240200796994294\n",
      "Epoch 27 \t Batch 900 \t Training Loss: 47.258874855041505\n",
      "Epoch 27 \t Batch 20 \t Validation Loss: 49.02463312149048\n",
      "Epoch 27 \t Batch 40 \t Validation Loss: 45.50968360900879\n",
      "Epoch 27 \t Batch 60 \t Validation Loss: 48.71138095855713\n",
      "Epoch 27 \t Batch 80 \t Validation Loss: 46.41058526039124\n",
      "Epoch 27 \t Batch 100 \t Validation Loss: 43.48384300231933\n",
      "Epoch 27 \t Batch 120 \t Validation Loss: 41.728054173787434\n",
      "Epoch 27 \t Batch 140 \t Validation Loss: 39.897386360168454\n",
      "Epoch 27 \t Batch 160 \t Validation Loss: 40.21803160905838\n",
      "Epoch 27 \t Batch 180 \t Validation Loss: 42.453460762235856\n",
      "Epoch 27 \t Batch 200 \t Validation Loss: 43.176653513908384\n",
      "Epoch 27 \t Batch 220 \t Validation Loss: 43.63638683232394\n",
      "Epoch 27 \t Batch 240 \t Validation Loss: 43.4114794254303\n",
      "Epoch 27 \t Batch 260 \t Validation Loss: 45.158802960469174\n",
      "Epoch 27 \t Batch 280 \t Validation Loss: 45.9516982793808\n",
      "Epoch 27 \t Batch 300 \t Validation Loss: 46.44367970466614\n",
      "Epoch 27 \t Batch 320 \t Validation Loss: 46.49575538635254\n",
      "Epoch 27 \t Batch 340 \t Validation Loss: 46.11753945631139\n",
      "Epoch 27 \t Batch 360 \t Validation Loss: 45.598944139480594\n",
      "Epoch 27 \t Batch 380 \t Validation Loss: 45.63361729822661\n",
      "Epoch 27 \t Batch 400 \t Validation Loss: 45.07556184053421\n",
      "Epoch 27 \t Batch 420 \t Validation Loss: 44.908084353946506\n",
      "Epoch 27 \t Batch 440 \t Validation Loss: 44.60459520383315\n",
      "Epoch 27 \t Batch 460 \t Validation Loss: 44.6627319149349\n",
      "Epoch 27 \t Batch 480 \t Validation Loss: 44.95138898889224\n",
      "Epoch 27 \t Batch 500 \t Validation Loss: 44.5350343875885\n",
      "Epoch 27 \t Batch 520 \t Validation Loss: 44.237998637786276\n",
      "Epoch 27 \t Batch 540 \t Validation Loss: 43.737741238982586\n",
      "Epoch 27 \t Batch 560 \t Validation Loss: 43.3189890061106\n",
      "Epoch 27 \t Batch 580 \t Validation Loss: 42.884423191794035\n",
      "Epoch 27 \t Batch 600 \t Validation Loss: 42.88661040147146\n",
      "Epoch 27 Training Loss: 47.246666337307566 Validation Loss: 43.35343363223138\n",
      "Epoch 27 completed\n",
      "Epoch 28 \t Batch 20 \t Training Loss: 45.59789619445801\n",
      "Epoch 28 \t Batch 40 \t Training Loss: 46.406045246124265\n",
      "Epoch 28 \t Batch 60 \t Training Loss: 46.362159983317056\n",
      "Epoch 28 \t Batch 80 \t Training Loss: 46.37610206604004\n",
      "Epoch 28 \t Batch 100 \t Training Loss: 46.46977157592774\n",
      "Epoch 28 \t Batch 120 \t Training Loss: 46.57026770909627\n",
      "Epoch 28 \t Batch 140 \t Training Loss: 46.656133025033135\n",
      "Epoch 28 \t Batch 160 \t Training Loss: 46.81800599098206\n",
      "Epoch 28 \t Batch 180 \t Training Loss: 46.90446196662055\n",
      "Epoch 28 \t Batch 200 \t Training Loss: 46.93673110961914\n",
      "Epoch 28 \t Batch 220 \t Training Loss: 46.95194963975386\n",
      "Epoch 28 \t Batch 240 \t Training Loss: 46.88433238665263\n",
      "Epoch 28 \t Batch 260 \t Training Loss: 46.800412999666655\n",
      "Epoch 28 \t Batch 280 \t Training Loss: 46.7666337285723\n",
      "Epoch 28 \t Batch 300 \t Training Loss: 46.813741531372074\n",
      "Epoch 28 \t Batch 320 \t Training Loss: 46.85495913028717\n",
      "Epoch 28 \t Batch 340 \t Training Loss: 46.84574349347283\n",
      "Epoch 28 \t Batch 360 \t Training Loss: 46.81488223605686\n",
      "Epoch 28 \t Batch 380 \t Training Loss: 46.84357763591566\n",
      "Epoch 28 \t Batch 400 \t Training Loss: 46.87779578208924\n",
      "Epoch 28 \t Batch 420 \t Training Loss: 46.87669910249256\n",
      "Epoch 28 \t Batch 440 \t Training Loss: 46.94698153409091\n",
      "Epoch 28 \t Batch 460 \t Training Loss: 47.02351366955301\n",
      "Epoch 28 \t Batch 480 \t Training Loss: 46.97639168103536\n",
      "Epoch 28 \t Batch 500 \t Training Loss: 46.97798153686524\n",
      "Epoch 28 \t Batch 520 \t Training Loss: 46.98511537405161\n",
      "Epoch 28 \t Batch 540 \t Training Loss: 46.97323700940167\n",
      "Epoch 28 \t Batch 560 \t Training Loss: 46.98009898321969\n",
      "Epoch 28 \t Batch 580 \t Training Loss: 46.97945105454017\n",
      "Epoch 28 \t Batch 600 \t Training Loss: 47.009165573120114\n",
      "Epoch 28 \t Batch 620 \t Training Loss: 47.016980152745404\n",
      "Epoch 28 \t Batch 640 \t Training Loss: 47.07513006925583\n",
      "Epoch 28 \t Batch 660 \t Training Loss: 47.07217059279933\n",
      "Epoch 28 \t Batch 680 \t Training Loss: 47.090474807514866\n",
      "Epoch 28 \t Batch 700 \t Training Loss: 47.0856198992048\n",
      "Epoch 28 \t Batch 720 \t Training Loss: 47.094802448484636\n",
      "Epoch 28 \t Batch 740 \t Training Loss: 47.12383424398061\n",
      "Epoch 28 \t Batch 760 \t Training Loss: 47.15009050871196\n",
      "Epoch 28 \t Batch 780 \t Training Loss: 47.1350744051811\n",
      "Epoch 28 \t Batch 800 \t Training Loss: 47.17156430721283\n",
      "Epoch 28 \t Batch 820 \t Training Loss: 47.139725173391945\n",
      "Epoch 28 \t Batch 840 \t Training Loss: 47.16232191721598\n",
      "Epoch 28 \t Batch 860 \t Training Loss: 47.14559637114059\n",
      "Epoch 28 \t Batch 880 \t Training Loss: 47.1542723829096\n",
      "Epoch 28 \t Batch 900 \t Training Loss: 47.20896700117323\n",
      "Epoch 28 \t Batch 20 \t Validation Loss: 33.41344866752625\n",
      "Epoch 28 \t Batch 40 \t Validation Loss: 31.97809820175171\n",
      "Epoch 28 \t Batch 60 \t Validation Loss: 34.89323123296102\n",
      "Epoch 28 \t Batch 80 \t Validation Loss: 33.78372931480408\n",
      "Epoch 28 \t Batch 100 \t Validation Loss: 33.07830234527588\n",
      "Epoch 28 \t Batch 120 \t Validation Loss: 32.84286007881165\n",
      "Epoch 28 \t Batch 140 \t Validation Loss: 32.20684092385428\n",
      "Epoch 28 \t Batch 160 \t Validation Loss: 33.093696641921994\n",
      "Epoch 28 \t Batch 180 \t Validation Loss: 35.46027661429511\n",
      "Epoch 28 \t Batch 200 \t Validation Loss: 36.43597943305969\n",
      "Epoch 28 \t Batch 220 \t Validation Loss: 37.044859444011344\n",
      "Epoch 28 \t Batch 240 \t Validation Loss: 37.01992822090785\n",
      "Epoch 28 \t Batch 260 \t Validation Loss: 38.82905831703773\n",
      "Epoch 28 \t Batch 280 \t Validation Loss: 39.74888976642064\n",
      "Epoch 28 \t Batch 300 \t Validation Loss: 40.24144323348999\n",
      "Epoch 28 \t Batch 320 \t Validation Loss: 40.408178529143335\n",
      "Epoch 28 \t Batch 340 \t Validation Loss: 40.28760206278633\n",
      "Epoch 28 \t Batch 360 \t Validation Loss: 39.9054443915685\n",
      "Epoch 28 \t Batch 380 \t Validation Loss: 40.04799738934165\n",
      "Epoch 28 \t Batch 400 \t Validation Loss: 39.696615967750546\n",
      "Epoch 28 \t Batch 420 \t Validation Loss: 39.707784307570684\n",
      "Epoch 28 \t Batch 440 \t Validation Loss: 39.55148149837147\n",
      "Epoch 28 \t Batch 460 \t Validation Loss: 39.801278504081395\n",
      "Epoch 28 \t Batch 480 \t Validation Loss: 40.22531609137853\n",
      "Epoch 28 \t Batch 500 \t Validation Loss: 39.93838962936401\n",
      "Epoch 28 \t Batch 520 \t Validation Loss: 39.77064451804528\n",
      "Epoch 28 \t Batch 540 \t Validation Loss: 39.42531991534763\n",
      "Epoch 28 \t Batch 560 \t Validation Loss: 39.14935367447989\n",
      "Epoch 28 \t Batch 580 \t Validation Loss: 38.84389391603141\n",
      "Epoch 28 \t Batch 600 \t Validation Loss: 38.983361145655316\n",
      "Epoch 28 Training Loss: 47.20492073649798 Validation Loss: 39.53164382414384\n",
      "Epoch 28 completed\n",
      "Epoch 29 \t Batch 20 \t Training Loss: 47.695195007324216\n",
      "Epoch 29 \t Batch 40 \t Training Loss: 47.70065851211548\n",
      "Epoch 29 \t Batch 60 \t Training Loss: 47.23459084828695\n",
      "Epoch 29 \t Batch 80 \t Training Loss: 47.265841245651245\n",
      "Epoch 29 \t Batch 100 \t Training Loss: 46.86169956207275\n",
      "Epoch 29 \t Batch 120 \t Training Loss: 47.177362537384035\n",
      "Epoch 29 \t Batch 140 \t Training Loss: 46.94102107456752\n",
      "Epoch 29 \t Batch 160 \t Training Loss: 47.082805156707764\n",
      "Epoch 29 \t Batch 180 \t Training Loss: 47.23201821645101\n",
      "Epoch 29 \t Batch 200 \t Training Loss: 47.187019691467285\n",
      "Epoch 29 \t Batch 220 \t Training Loss: 47.212074453180485\n",
      "Epoch 29 \t Batch 240 \t Training Loss: 47.19408710797628\n",
      "Epoch 29 \t Batch 260 \t Training Loss: 47.253493074270395\n",
      "Epoch 29 \t Batch 280 \t Training Loss: 47.20861542565482\n",
      "Epoch 29 \t Batch 300 \t Training Loss: 47.14451152801514\n",
      "Epoch 29 \t Batch 320 \t Training Loss: 47.279504692554475\n",
      "Epoch 29 \t Batch 340 \t Training Loss: 47.28154612148509\n",
      "Epoch 29 \t Batch 360 \t Training Loss: 47.21488805347019\n",
      "Epoch 29 \t Batch 380 \t Training Loss: 47.244013093647204\n",
      "Epoch 29 \t Batch 400 \t Training Loss: 47.220606546401974\n",
      "Epoch 29 \t Batch 420 \t Training Loss: 47.1511052994501\n",
      "Epoch 29 \t Batch 440 \t Training Loss: 47.18685471794822\n",
      "Epoch 29 \t Batch 460 \t Training Loss: 47.22256108159604\n",
      "Epoch 29 \t Batch 480 \t Training Loss: 47.24803829987844\n",
      "Epoch 29 \t Batch 500 \t Training Loss: 47.23527370452881\n",
      "Epoch 29 \t Batch 520 \t Training Loss: 47.24536895751953\n",
      "Epoch 29 \t Batch 540 \t Training Loss: 47.257486223291465\n",
      "Epoch 29 \t Batch 560 \t Training Loss: 47.25220036506653\n",
      "Epoch 29 \t Batch 580 \t Training Loss: 47.27241241191996\n",
      "Epoch 29 \t Batch 600 \t Training Loss: 47.23346133550008\n",
      "Epoch 29 \t Batch 620 \t Training Loss: 47.28464990431262\n",
      "Epoch 29 \t Batch 640 \t Training Loss: 47.280308508872984\n",
      "Epoch 29 \t Batch 660 \t Training Loss: 47.22466096589059\n",
      "Epoch 29 \t Batch 680 \t Training Loss: 47.23447942733765\n",
      "Epoch 29 \t Batch 700 \t Training Loss: 47.24257900782994\n",
      "Epoch 29 \t Batch 720 \t Training Loss: 47.223576635784575\n",
      "Epoch 29 \t Batch 740 \t Training Loss: 47.22567696442475\n",
      "Epoch 29 \t Batch 760 \t Training Loss: 47.20568226764077\n",
      "Epoch 29 \t Batch 780 \t Training Loss: 47.15634731390537\n",
      "Epoch 29 \t Batch 800 \t Training Loss: 47.15354159355164\n",
      "Epoch 29 \t Batch 820 \t Training Loss: 47.178676377273185\n",
      "Epoch 29 \t Batch 840 \t Training Loss: 47.154548009236656\n",
      "Epoch 29 \t Batch 860 \t Training Loss: 47.14530497262644\n",
      "Epoch 29 \t Batch 880 \t Training Loss: 47.171439890428026\n",
      "Epoch 29 \t Batch 900 \t Training Loss: 47.18629233042399\n",
      "Epoch 29 \t Batch 20 \t Validation Loss: 55.01486301422119\n",
      "Epoch 29 \t Batch 40 \t Validation Loss: 50.88685584068298\n",
      "Epoch 29 \t Batch 60 \t Validation Loss: 54.87259181340536\n",
      "Epoch 29 \t Batch 80 \t Validation Loss: 51.66134245395661\n",
      "Epoch 29 \t Batch 100 \t Validation Loss: 47.54713380813599\n",
      "Epoch 29 \t Batch 120 \t Validation Loss: 45.24009079933167\n",
      "Epoch 29 \t Batch 140 \t Validation Loss: 42.92896160398211\n",
      "Epoch 29 \t Batch 160 \t Validation Loss: 42.28842191696167\n",
      "Epoch 29 \t Batch 180 \t Validation Loss: 42.96127959357368\n",
      "Epoch 29 \t Batch 200 \t Validation Loss: 42.942511777877804\n",
      "Epoch 29 \t Batch 220 \t Validation Loss: 42.73023289767178\n",
      "Epoch 29 \t Batch 240 \t Validation Loss: 42.00460913181305\n",
      "Epoch 29 \t Batch 260 \t Validation Loss: 43.14078698158264\n",
      "Epoch 29 \t Batch 280 \t Validation Loss: 43.51503493785858\n",
      "Epoch 29 \t Batch 300 \t Validation Loss: 43.36749048550924\n",
      "Epoch 29 \t Batch 320 \t Validation Loss: 43.10445653498173\n",
      "Epoch 29 \t Batch 340 \t Validation Loss: 42.68326872376834\n",
      "Epoch 29 \t Batch 360 \t Validation Loss: 42.09188749525282\n",
      "Epoch 29 \t Batch 380 \t Validation Loss: 42.06855187416077\n",
      "Epoch 29 \t Batch 400 \t Validation Loss: 41.58968571901321\n",
      "Epoch 29 \t Batch 420 \t Validation Loss: 41.43717810994103\n",
      "Epoch 29 \t Batch 440 \t Validation Loss: 41.16097556677732\n",
      "Epoch 29 \t Batch 460 \t Validation Loss: 41.30172214300736\n",
      "Epoch 29 \t Batch 480 \t Validation Loss: 41.58628632823626\n",
      "Epoch 29 \t Batch 500 \t Validation Loss: 41.115346364974975\n",
      "Epoch 29 \t Batch 520 \t Validation Loss: 40.952676762067355\n",
      "Epoch 29 \t Batch 540 \t Validation Loss: 40.601156146438036\n",
      "Epoch 29 \t Batch 560 \t Validation Loss: 40.298571249416895\n",
      "Epoch 29 \t Batch 580 \t Validation Loss: 40.04007848213459\n",
      "Epoch 29 \t Batch 600 \t Validation Loss: 40.14189647038778\n",
      "Epoch 29 Training Loss: 47.15786619196801 Validation Loss: 40.70919542808037\n",
      "Epoch 29 completed\n",
      "Epoch 30 \t Batch 20 \t Training Loss: 48.60017700195313\n",
      "Epoch 30 \t Batch 40 \t Training Loss: 47.66123962402344\n",
      "Epoch 30 \t Batch 60 \t Training Loss: 47.547343190511064\n",
      "Epoch 30 \t Batch 80 \t Training Loss: 47.68409280776977\n",
      "Epoch 30 \t Batch 100 \t Training Loss: 47.26779430389404\n",
      "Epoch 30 \t Batch 120 \t Training Loss: 47.395030657450356\n",
      "Epoch 30 \t Batch 140 \t Training Loss: 47.22334763663156\n",
      "Epoch 30 \t Batch 160 \t Training Loss: 47.10767753124237\n",
      "Epoch 30 \t Batch 180 \t Training Loss: 47.16740267011854\n",
      "Epoch 30 \t Batch 200 \t Training Loss: 47.21070859909057\n",
      "Epoch 30 \t Batch 220 \t Training Loss: 47.11337197043679\n",
      "Epoch 30 \t Batch 240 \t Training Loss: 47.21146434148152\n",
      "Epoch 30 \t Batch 260 \t Training Loss: 47.311725777846114\n",
      "Epoch 30 \t Batch 280 \t Training Loss: 47.33814528329032\n",
      "Epoch 30 \t Batch 300 \t Training Loss: 47.336595166524255\n",
      "Epoch 30 \t Batch 320 \t Training Loss: 47.32197977304459\n",
      "Epoch 30 \t Batch 340 \t Training Loss: 47.38123410168816\n",
      "Epoch 30 \t Batch 360 \t Training Loss: 47.37467784881592\n",
      "Epoch 30 \t Batch 380 \t Training Loss: 47.396149896320544\n",
      "Epoch 30 \t Batch 400 \t Training Loss: 47.33847170829773\n",
      "Epoch 30 \t Batch 420 \t Training Loss: 47.42662909371512\n",
      "Epoch 30 \t Batch 440 \t Training Loss: 47.491593993793835\n",
      "Epoch 30 \t Batch 460 \t Training Loss: 47.4591378585152\n",
      "Epoch 30 \t Batch 480 \t Training Loss: 47.36817624568939\n",
      "Epoch 30 \t Batch 500 \t Training Loss: 47.32863500976563\n",
      "Epoch 30 \t Batch 520 \t Training Loss: 47.353339635408844\n",
      "Epoch 30 \t Batch 540 \t Training Loss: 47.31249552832709\n",
      "Epoch 30 \t Batch 560 \t Training Loss: 47.273246901375906\n",
      "Epoch 30 \t Batch 580 \t Training Loss: 47.27470401895457\n",
      "Epoch 30 \t Batch 600 \t Training Loss: 47.26116533279419\n",
      "Epoch 30 \t Batch 620 \t Training Loss: 47.31177783473846\n",
      "Epoch 30 \t Batch 640 \t Training Loss: 47.277234971523285\n",
      "Epoch 30 \t Batch 660 \t Training Loss: 47.28160602107192\n",
      "Epoch 30 \t Batch 680 \t Training Loss: 47.251761167189656\n",
      "Epoch 30 \t Batch 700 \t Training Loss: 47.249659396580284\n",
      "Epoch 30 \t Batch 720 \t Training Loss: 47.2362382305993\n",
      "Epoch 30 \t Batch 740 \t Training Loss: 47.220989515974715\n",
      "Epoch 30 \t Batch 760 \t Training Loss: 47.22428324348048\n",
      "Epoch 30 \t Batch 780 \t Training Loss: 47.16938688816168\n",
      "Epoch 30 \t Batch 800 \t Training Loss: 47.12802917480469\n",
      "Epoch 30 \t Batch 820 \t Training Loss: 47.15752018719185\n",
      "Epoch 30 \t Batch 840 \t Training Loss: 47.14359223047892\n",
      "Epoch 30 \t Batch 860 \t Training Loss: 47.140731505460515\n",
      "Epoch 30 \t Batch 880 \t Training Loss: 47.13029142726551\n",
      "Epoch 30 \t Batch 900 \t Training Loss: 47.0972179751926\n",
      "Epoch 30 \t Batch 20 \t Validation Loss: 48.23275194168091\n",
      "Epoch 30 \t Batch 40 \t Validation Loss: 44.63026165962219\n",
      "Epoch 30 \t Batch 60 \t Validation Loss: 48.11352663040161\n",
      "Epoch 30 \t Batch 80 \t Validation Loss: 45.83884055614472\n",
      "Epoch 30 \t Batch 100 \t Validation Loss: 42.741097908020016\n",
      "Epoch 30 \t Batch 120 \t Validation Loss: 40.97488668759664\n",
      "Epoch 30 \t Batch 140 \t Validation Loss: 39.170316205705916\n",
      "Epoch 30 \t Batch 160 \t Validation Loss: 39.461958539485934\n",
      "Epoch 30 \t Batch 180 \t Validation Loss: 41.5159641901652\n",
      "Epoch 30 \t Batch 200 \t Validation Loss: 42.15680919647217\n",
      "Epoch 30 \t Batch 220 \t Validation Loss: 42.515033167058775\n",
      "Epoch 30 \t Batch 240 \t Validation Loss: 42.20413842201233\n",
      "Epoch 30 \t Batch 260 \t Validation Loss: 43.85048503508935\n",
      "Epoch 30 \t Batch 280 \t Validation Loss: 44.56241198948452\n",
      "Epoch 30 \t Batch 300 \t Validation Loss: 45.002223822275795\n",
      "Epoch 30 \t Batch 320 \t Validation Loss: 44.99800544679165\n",
      "Epoch 30 \t Batch 340 \t Validation Loss: 44.60617126857533\n",
      "Epoch 30 \t Batch 360 \t Validation Loss: 44.15047324233585\n",
      "Epoch 30 \t Batch 380 \t Validation Loss: 44.19416096084996\n",
      "Epoch 30 \t Batch 400 \t Validation Loss: 43.70301587343216\n",
      "Epoch 30 \t Batch 420 \t Validation Loss: 43.532302365984236\n",
      "Epoch 30 \t Batch 440 \t Validation Loss: 43.278905140269885\n",
      "Epoch 30 \t Batch 460 \t Validation Loss: 43.3679983097574\n",
      "Epoch 30 \t Batch 480 \t Validation Loss: 43.7023499528567\n",
      "Epoch 30 \t Batch 500 \t Validation Loss: 43.27245527648926\n",
      "Epoch 30 \t Batch 520 \t Validation Loss: 43.089805856117835\n",
      "Epoch 30 \t Batch 540 \t Validation Loss: 42.714217277809425\n",
      "Epoch 30 \t Batch 560 \t Validation Loss: 42.451640779631475\n",
      "Epoch 30 \t Batch 580 \t Validation Loss: 42.23439996324736\n",
      "Epoch 30 \t Batch 600 \t Validation Loss: 42.32449735959371\n",
      "Epoch 30 Training Loss: 47.10938931447469 Validation Loss: 42.86146154961029\n",
      "Epoch 30 completed\n",
      "Epoch 31 \t Batch 20 \t Training Loss: 47.02602348327637\n",
      "Epoch 31 \t Batch 40 \t Training Loss: 47.19210405349732\n",
      "Epoch 31 \t Batch 60 \t Training Loss: 47.23043670654297\n",
      "Epoch 31 \t Batch 80 \t Training Loss: 46.98690152168274\n",
      "Epoch 31 \t Batch 100 \t Training Loss: 47.201079788208006\n",
      "Epoch 31 \t Batch 120 \t Training Loss: 47.22903219858805\n",
      "Epoch 31 \t Batch 140 \t Training Loss: 47.15409254346575\n",
      "Epoch 31 \t Batch 160 \t Training Loss: 47.132566928863525\n",
      "Epoch 31 \t Batch 180 \t Training Loss: 47.134958839416505\n",
      "Epoch 31 \t Batch 200 \t Training Loss: 46.879388637542725\n",
      "Epoch 31 \t Batch 220 \t Training Loss: 46.843266261707655\n",
      "Epoch 31 \t Batch 240 \t Training Loss: 46.86180170377096\n",
      "Epoch 31 \t Batch 260 \t Training Loss: 46.75309691795936\n",
      "Epoch 31 \t Batch 280 \t Training Loss: 46.7079103742327\n",
      "Epoch 31 \t Batch 300 \t Training Loss: 46.59838535308838\n",
      "Epoch 31 \t Batch 320 \t Training Loss: 46.603419518470766\n",
      "Epoch 31 \t Batch 340 \t Training Loss: 46.67595323674819\n",
      "Epoch 31 \t Batch 360 \t Training Loss: 46.68287602530585\n",
      "Epoch 31 \t Batch 380 \t Training Loss: 46.64011701282702\n",
      "Epoch 31 \t Batch 400 \t Training Loss: 46.752359790802004\n",
      "Epoch 31 \t Batch 420 \t Training Loss: 46.795592925662085\n",
      "Epoch 31 \t Batch 440 \t Training Loss: 46.83536789634011\n",
      "Epoch 31 \t Batch 460 \t Training Loss: 46.849546847136125\n",
      "Epoch 31 \t Batch 480 \t Training Loss: 46.855958207448325\n",
      "Epoch 31 \t Batch 500 \t Training Loss: 46.902709312438965\n",
      "Epoch 31 \t Batch 520 \t Training Loss: 46.903937911987306\n",
      "Epoch 31 \t Batch 540 \t Training Loss: 46.945563598915385\n",
      "Epoch 31 \t Batch 560 \t Training Loss: 46.95289865221296\n",
      "Epoch 31 \t Batch 580 \t Training Loss: 46.970352389894686\n",
      "Epoch 31 \t Batch 600 \t Training Loss: 47.020191809336346\n",
      "Epoch 31 \t Batch 620 \t Training Loss: 47.02700426040157\n",
      "Epoch 31 \t Batch 640 \t Training Loss: 47.05342018008232\n",
      "Epoch 31 \t Batch 660 \t Training Loss: 47.06438382466634\n",
      "Epoch 31 \t Batch 680 \t Training Loss: 47.062430993248434\n",
      "Epoch 31 \t Batch 700 \t Training Loss: 47.08389463152204\n",
      "Epoch 31 \t Batch 720 \t Training Loss: 47.11582770347595\n",
      "Epoch 31 \t Batch 740 \t Training Loss: 47.12307618630899\n",
      "Epoch 31 \t Batch 760 \t Training Loss: 47.11597654443038\n",
      "Epoch 31 \t Batch 780 \t Training Loss: 47.101497395833334\n",
      "Epoch 31 \t Batch 800 \t Training Loss: 47.09180396080017\n",
      "Epoch 31 \t Batch 820 \t Training Loss: 47.096179329476705\n",
      "Epoch 31 \t Batch 840 \t Training Loss: 47.099347922915506\n",
      "Epoch 31 \t Batch 860 \t Training Loss: 47.1228742067204\n",
      "Epoch 31 \t Batch 880 \t Training Loss: 47.13034162087874\n",
      "Epoch 31 \t Batch 900 \t Training Loss: 47.111964518229165\n",
      "Epoch 31 \t Batch 20 \t Validation Loss: 55.962749004364014\n",
      "Epoch 31 \t Batch 40 \t Validation Loss: 50.59801125526428\n",
      "Epoch 31 \t Batch 60 \t Validation Loss: 54.51596113840739\n",
      "Epoch 31 \t Batch 80 \t Validation Loss: 51.61244502067566\n",
      "Epoch 31 \t Batch 100 \t Validation Loss: 47.13175210952759\n",
      "Epoch 31 \t Batch 120 \t Validation Loss: 44.5331392288208\n",
      "Epoch 31 \t Batch 140 \t Validation Loss: 42.1607597896031\n",
      "Epoch 31 \t Batch 160 \t Validation Loss: 42.16108298301697\n",
      "Epoch 31 \t Batch 180 \t Validation Loss: 44.1537441306644\n",
      "Epoch 31 \t Batch 200 \t Validation Loss: 44.67427720546723\n",
      "Epoch 31 \t Batch 220 \t Validation Loss: 44.94769220785661\n",
      "Epoch 31 \t Batch 240 \t Validation Loss: 44.55299393335978\n",
      "Epoch 31 \t Batch 260 \t Validation Loss: 46.19858357722943\n",
      "Epoch 31 \t Batch 280 \t Validation Loss: 46.87473075389862\n",
      "Epoch 31 \t Batch 300 \t Validation Loss: 47.231975342432655\n",
      "Epoch 31 \t Batch 320 \t Validation Loss: 47.19988243877888\n",
      "Epoch 31 \t Batch 340 \t Validation Loss: 46.72630303046282\n",
      "Epoch 31 \t Batch 360 \t Validation Loss: 46.180381411976285\n",
      "Epoch 31 \t Batch 380 \t Validation Loss: 46.207850217819214\n",
      "Epoch 31 \t Batch 400 \t Validation Loss: 45.65330761194229\n",
      "Epoch 31 \t Batch 420 \t Validation Loss: 45.44865479469299\n",
      "Epoch 31 \t Batch 440 \t Validation Loss: 45.16661824096333\n",
      "Epoch 31 \t Batch 460 \t Validation Loss: 45.259469324609505\n",
      "Epoch 31 \t Batch 480 \t Validation Loss: 45.5460700849692\n",
      "Epoch 31 \t Batch 500 \t Validation Loss: 45.07899894142151\n",
      "Epoch 31 \t Batch 520 \t Validation Loss: 44.89900538921356\n",
      "Epoch 31 \t Batch 540 \t Validation Loss: 44.41196015852469\n",
      "Epoch 31 \t Batch 560 \t Validation Loss: 44.009026965073176\n",
      "Epoch 31 \t Batch 580 \t Validation Loss: 43.62556937151942\n",
      "Epoch 31 \t Batch 600 \t Validation Loss: 43.628313450813295\n",
      "Epoch 31 Training Loss: 47.093440514484435 Validation Loss: 44.11368862839488\n",
      "Epoch 31 completed\n",
      "Epoch 32 \t Batch 20 \t Training Loss: 46.870281028747556\n",
      "Epoch 32 \t Batch 40 \t Training Loss: 46.62526693344116\n",
      "Epoch 32 \t Batch 60 \t Training Loss: 47.153778902689616\n",
      "Epoch 32 \t Batch 80 \t Training Loss: 47.18480143547058\n",
      "Epoch 32 \t Batch 100 \t Training Loss: 47.24255477905273\n",
      "Epoch 32 \t Batch 120 \t Training Loss: 47.14534209569295\n",
      "Epoch 32 \t Batch 140 \t Training Loss: 47.3078366960798\n",
      "Epoch 32 \t Batch 160 \t Training Loss: 47.266094374656674\n",
      "Epoch 32 \t Batch 180 \t Training Loss: 47.37119924757216\n",
      "Epoch 32 \t Batch 200 \t Training Loss: 47.3855004119873\n",
      "Epoch 32 \t Batch 220 \t Training Loss: 47.36601321480491\n",
      "Epoch 32 \t Batch 240 \t Training Loss: 47.326499144236244\n",
      "Epoch 32 \t Batch 260 \t Training Loss: 47.357257490891676\n",
      "Epoch 32 \t Batch 280 \t Training Loss: 47.31212323052542\n",
      "Epoch 32 \t Batch 300 \t Training Loss: 47.3495184580485\n",
      "Epoch 32 \t Batch 320 \t Training Loss: 47.280934834480284\n",
      "Epoch 32 \t Batch 340 \t Training Loss: 47.262134394926186\n",
      "Epoch 32 \t Batch 360 \t Training Loss: 47.199994532267254\n",
      "Epoch 32 \t Batch 380 \t Training Loss: 47.177140898453565\n",
      "Epoch 32 \t Batch 400 \t Training Loss: 47.06909942626953\n",
      "Epoch 32 \t Batch 420 \t Training Loss: 47.03378503890264\n",
      "Epoch 32 \t Batch 440 \t Training Loss: 47.04677075472745\n",
      "Epoch 32 \t Batch 460 \t Training Loss: 46.97098733653193\n",
      "Epoch 32 \t Batch 480 \t Training Loss: 46.98183134396871\n",
      "Epoch 32 \t Batch 500 \t Training Loss: 46.98685904693603\n",
      "Epoch 32 \t Batch 520 \t Training Loss: 46.93875530683077\n",
      "Epoch 32 \t Batch 540 \t Training Loss: 46.995277320014104\n",
      "Epoch 32 \t Batch 560 \t Training Loss: 46.98325195993696\n",
      "Epoch 32 \t Batch 580 \t Training Loss: 47.01879117899927\n",
      "Epoch 32 \t Batch 600 \t Training Loss: 47.056867535909014\n",
      "Epoch 32 \t Batch 620 \t Training Loss: 47.08078092144382\n",
      "Epoch 32 \t Batch 640 \t Training Loss: 47.075380223989484\n",
      "Epoch 32 \t Batch 660 \t Training Loss: 47.04292062701601\n",
      "Epoch 32 \t Batch 680 \t Training Loss: 47.05871362125173\n",
      "Epoch 32 \t Batch 700 \t Training Loss: 47.076064213344026\n",
      "Epoch 32 \t Batch 720 \t Training Loss: 47.07292017406888\n",
      "Epoch 32 \t Batch 740 \t Training Loss: 47.08177384556951\n",
      "Epoch 32 \t Batch 760 \t Training Loss: 47.04213741704037\n",
      "Epoch 32 \t Batch 780 \t Training Loss: 47.081302036383214\n",
      "Epoch 32 \t Batch 800 \t Training Loss: 47.054629101753235\n",
      "Epoch 32 \t Batch 820 \t Training Loss: 47.079417624124666\n",
      "Epoch 32 \t Batch 840 \t Training Loss: 47.0785405476888\n",
      "Epoch 32 \t Batch 860 \t Training Loss: 47.078523671349814\n",
      "Epoch 32 \t Batch 880 \t Training Loss: 47.061774865063754\n",
      "Epoch 32 \t Batch 900 \t Training Loss: 47.04457689497206\n",
      "Epoch 32 \t Batch 20 \t Validation Loss: 45.15663528442383\n",
      "Epoch 32 \t Batch 40 \t Validation Loss: 41.88057284355163\n",
      "Epoch 32 \t Batch 60 \t Validation Loss: 44.98107229868571\n",
      "Epoch 32 \t Batch 80 \t Validation Loss: 43.03034355640411\n",
      "Epoch 32 \t Batch 100 \t Validation Loss: 40.215564517974855\n",
      "Epoch 32 \t Batch 120 \t Validation Loss: 38.64001256624858\n",
      "Epoch 32 \t Batch 140 \t Validation Loss: 37.075755085263935\n",
      "Epoch 32 \t Batch 160 \t Validation Loss: 37.67072897553444\n",
      "Epoch 32 \t Batch 180 \t Validation Loss: 40.14062614970737\n",
      "Epoch 32 \t Batch 200 \t Validation Loss: 41.00162284374237\n",
      "Epoch 32 \t Batch 220 \t Validation Loss: 41.63037998459556\n",
      "Epoch 32 \t Batch 240 \t Validation Loss: 41.504097588857015\n",
      "Epoch 32 \t Batch 260 \t Validation Loss: 43.32259853803195\n",
      "Epoch 32 \t Batch 280 \t Validation Loss: 44.16215047836304\n",
      "Epoch 32 \t Batch 300 \t Validation Loss: 44.71717575709025\n",
      "Epoch 32 \t Batch 320 \t Validation Loss: 44.81443962454796\n",
      "Epoch 32 \t Batch 340 \t Validation Loss: 44.47369102590224\n",
      "Epoch 32 \t Batch 360 \t Validation Loss: 44.018209875954526\n",
      "Epoch 32 \t Batch 380 \t Validation Loss: 44.07249191685727\n",
      "Epoch 32 \t Batch 400 \t Validation Loss: 43.48292486667633\n",
      "Epoch 32 \t Batch 420 \t Validation Loss: 43.32255682264056\n",
      "Epoch 32 \t Batch 440 \t Validation Loss: 42.939723487333815\n",
      "Epoch 32 \t Batch 460 \t Validation Loss: 43.01272207757701\n",
      "Epoch 32 \t Batch 480 \t Validation Loss: 43.313254006703694\n",
      "Epoch 32 \t Batch 500 \t Validation Loss: 42.91450437164307\n",
      "Epoch 32 \t Batch 520 \t Validation Loss: 42.63355606152461\n",
      "Epoch 32 \t Batch 540 \t Validation Loss: 42.20154143439399\n",
      "Epoch 32 \t Batch 560 \t Validation Loss: 41.86600491659982\n",
      "Epoch 32 \t Batch 580 \t Validation Loss: 41.51522591031831\n",
      "Epoch 32 \t Batch 600 \t Validation Loss: 41.5732772954305\n",
      "Epoch 32 Training Loss: 47.070027866176325 Validation Loss: 42.07938437028365\n",
      "Epoch 32 completed\n",
      "Epoch 33 \t Batch 20 \t Training Loss: 46.106227493286134\n",
      "Epoch 33 \t Batch 40 \t Training Loss: 46.64093208312988\n",
      "Epoch 33 \t Batch 60 \t Training Loss: 46.669233830769855\n",
      "Epoch 33 \t Batch 80 \t Training Loss: 46.587579870224\n",
      "Epoch 33 \t Batch 100 \t Training Loss: 46.67518394470215\n",
      "Epoch 33 \t Batch 120 \t Training Loss: 46.93683942159017\n",
      "Epoch 33 \t Batch 140 \t Training Loss: 47.06465219770159\n",
      "Epoch 33 \t Batch 160 \t Training Loss: 46.950962805747984\n",
      "Epoch 33 \t Batch 180 \t Training Loss: 46.946754370795354\n",
      "Epoch 33 \t Batch 200 \t Training Loss: 46.94250255584717\n",
      "Epoch 33 \t Batch 220 \t Training Loss: 47.06032022996382\n",
      "Epoch 33 \t Batch 240 \t Training Loss: 46.974134302139284\n",
      "Epoch 33 \t Batch 260 \t Training Loss: 46.88922617985652\n",
      "Epoch 33 \t Batch 280 \t Training Loss: 46.826992225646975\n",
      "Epoch 33 \t Batch 300 \t Training Loss: 46.7943998336792\n",
      "Epoch 33 \t Batch 320 \t Training Loss: 46.765025842189786\n",
      "Epoch 33 \t Batch 340 \t Training Loss: 46.80371849957634\n",
      "Epoch 33 \t Batch 360 \t Training Loss: 46.83021363152398\n",
      "Epoch 33 \t Batch 380 \t Training Loss: 46.76878216392115\n",
      "Epoch 33 \t Batch 400 \t Training Loss: 46.78688714981079\n",
      "Epoch 33 \t Batch 420 \t Training Loss: 46.804895546322776\n",
      "Epoch 33 \t Batch 440 \t Training Loss: 46.81273138739846\n",
      "Epoch 33 \t Batch 460 \t Training Loss: 46.793603275133215\n",
      "Epoch 33 \t Batch 480 \t Training Loss: 46.78434686660766\n",
      "Epoch 33 \t Batch 500 \t Training Loss: 46.80227906036377\n",
      "Epoch 33 \t Batch 520 \t Training Loss: 46.83294663796058\n",
      "Epoch 33 \t Batch 540 \t Training Loss: 46.851478265832974\n",
      "Epoch 33 \t Batch 560 \t Training Loss: 46.86523534911019\n",
      "Epoch 33 \t Batch 580 \t Training Loss: 46.9099166343952\n",
      "Epoch 33 \t Batch 600 \t Training Loss: 46.90204464594523\n",
      "Epoch 33 \t Batch 620 \t Training Loss: 46.910135533732756\n",
      "Epoch 33 \t Batch 640 \t Training Loss: 46.89791299104691\n",
      "Epoch 33 \t Batch 660 \t Training Loss: 46.8930769255667\n",
      "Epoch 33 \t Batch 680 \t Training Loss: 46.92085033865536\n",
      "Epoch 33 \t Batch 700 \t Training Loss: 46.91992504119873\n",
      "Epoch 33 \t Batch 720 \t Training Loss: 46.93761290444268\n",
      "Epoch 33 \t Batch 740 \t Training Loss: 46.93695224555763\n",
      "Epoch 33 \t Batch 760 \t Training Loss: 46.93870640302959\n",
      "Epoch 33 \t Batch 780 \t Training Loss: 46.976996896205804\n",
      "Epoch 33 \t Batch 800 \t Training Loss: 47.02414868354797\n",
      "Epoch 33 \t Batch 820 \t Training Loss: 47.05850313698373\n",
      "Epoch 33 \t Batch 840 \t Training Loss: 47.056189973013744\n",
      "Epoch 33 \t Batch 860 \t Training Loss: 47.03883650580118\n",
      "Epoch 33 \t Batch 880 \t Training Loss: 47.01880915381692\n",
      "Epoch 33 \t Batch 900 \t Training Loss: 47.02946237776015\n",
      "Epoch 33 \t Batch 20 \t Validation Loss: 33.97816834449768\n",
      "Epoch 33 \t Batch 40 \t Validation Loss: 33.930167055130006\n",
      "Epoch 33 \t Batch 60 \t Validation Loss: 36.50856059392293\n",
      "Epoch 33 \t Batch 80 \t Validation Loss: 35.32877069711685\n",
      "Epoch 33 \t Batch 100 \t Validation Loss: 33.85699856758118\n",
      "Epoch 33 \t Batch 120 \t Validation Loss: 33.13857823212941\n",
      "Epoch 33 \t Batch 140 \t Validation Loss: 32.29128256525312\n",
      "Epoch 33 \t Batch 160 \t Validation Loss: 33.57750626206398\n",
      "Epoch 33 \t Batch 180 \t Validation Loss: 36.74860481686062\n",
      "Epoch 33 \t Batch 200 \t Validation Loss: 38.02488867759705\n",
      "Epoch 33 \t Batch 220 \t Validation Loss: 39.057827464017\n",
      "Epoch 33 \t Batch 240 \t Validation Loss: 39.248026021321614\n",
      "Epoch 33 \t Batch 260 \t Validation Loss: 41.33619486001822\n",
      "Epoch 33 \t Batch 280 \t Validation Loss: 42.37621918065207\n",
      "Epoch 33 \t Batch 300 \t Validation Loss: 43.206647087732954\n",
      "Epoch 33 \t Batch 320 \t Validation Loss: 43.48328070938587\n",
      "Epoch 33 \t Batch 340 \t Validation Loss: 43.221478852103736\n",
      "Epoch 33 \t Batch 360 \t Validation Loss: 42.8905173169242\n",
      "Epoch 33 \t Batch 380 \t Validation Loss: 43.03539283401088\n",
      "Epoch 33 \t Batch 400 \t Validation Loss: 42.50763603925705\n",
      "Epoch 33 \t Batch 420 \t Validation Loss: 42.38042415210179\n",
      "Epoch 33 \t Batch 440 \t Validation Loss: 42.078121824698016\n",
      "Epoch 33 \t Batch 460 \t Validation Loss: 42.22437936326732\n",
      "Epoch 33 \t Batch 480 \t Validation Loss: 42.574613549311955\n",
      "Epoch 33 \t Batch 500 \t Validation Loss: 42.19303503608704\n",
      "Epoch 33 \t Batch 520 \t Validation Loss: 41.99669959728534\n",
      "Epoch 33 \t Batch 540 \t Validation Loss: 41.56780766027945\n",
      "Epoch 33 \t Batch 560 \t Validation Loss: 41.226699577059065\n",
      "Epoch 33 \t Batch 580 \t Validation Loss: 40.87799988121822\n",
      "Epoch 33 \t Batch 600 \t Validation Loss: 40.93808837254842\n",
      "Epoch 33 Training Loss: 47.019823088900736 Validation Loss: 41.46432474681309\n",
      "Epoch 33 completed\n",
      "Epoch 34 \t Batch 20 \t Training Loss: 46.94726486206055\n",
      "Epoch 34 \t Batch 40 \t Training Loss: 47.14902486801147\n",
      "Epoch 34 \t Batch 60 \t Training Loss: 46.495591417948404\n",
      "Epoch 34 \t Batch 80 \t Training Loss: 46.49537262916565\n",
      "Epoch 34 \t Batch 100 \t Training Loss: 46.444337158203126\n",
      "Epoch 34 \t Batch 120 \t Training Loss: 46.520653533935544\n",
      "Epoch 34 \t Batch 140 \t Training Loss: 46.53154070717948\n",
      "Epoch 34 \t Batch 160 \t Training Loss: 46.6057199716568\n",
      "Epoch 34 \t Batch 180 \t Training Loss: 46.58266569773356\n",
      "Epoch 34 \t Batch 200 \t Training Loss: 46.703598232269286\n",
      "Epoch 34 \t Batch 220 \t Training Loss: 46.737072268399324\n",
      "Epoch 34 \t Batch 240 \t Training Loss: 46.75676709810893\n",
      "Epoch 34 \t Batch 260 \t Training Loss: 46.909925196721005\n",
      "Epoch 34 \t Batch 280 \t Training Loss: 46.92225439889091\n",
      "Epoch 34 \t Batch 300 \t Training Loss: 46.86230161031087\n",
      "Epoch 34 \t Batch 320 \t Training Loss: 46.934640884399414\n",
      "Epoch 34 \t Batch 340 \t Training Loss: 46.98289842044606\n",
      "Epoch 34 \t Batch 360 \t Training Loss: 47.000290870666504\n",
      "Epoch 34 \t Batch 380 \t Training Loss: 47.039279184843366\n",
      "Epoch 34 \t Batch 400 \t Training Loss: 47.077771739959715\n",
      "Epoch 34 \t Batch 420 \t Training Loss: 47.04629847208659\n",
      "Epoch 34 \t Batch 440 \t Training Loss: 47.066297487779096\n",
      "Epoch 34 \t Batch 460 \t Training Loss: 47.08067563927692\n",
      "Epoch 34 \t Batch 480 \t Training Loss: 47.06249604225159\n",
      "Epoch 34 \t Batch 500 \t Training Loss: 47.02161572265625\n",
      "Epoch 34 \t Batch 520 \t Training Loss: 46.98132586112389\n",
      "Epoch 34 \t Batch 540 \t Training Loss: 46.980888500920045\n",
      "Epoch 34 \t Batch 560 \t Training Loss: 47.031466776984075\n",
      "Epoch 34 \t Batch 580 \t Training Loss: 47.029933284891065\n",
      "Epoch 34 \t Batch 600 \t Training Loss: 47.011222248077395\n",
      "Epoch 34 \t Batch 620 \t Training Loss: 47.063724862375565\n",
      "Epoch 34 \t Batch 640 \t Training Loss: 47.11206952929497\n",
      "Epoch 34 \t Batch 660 \t Training Loss: 47.07243432709665\n",
      "Epoch 34 \t Batch 680 \t Training Loss: 47.04941142587101\n",
      "Epoch 34 \t Batch 700 \t Training Loss: 47.06343653542655\n",
      "Epoch 34 \t Batch 720 \t Training Loss: 47.036730480194095\n",
      "Epoch 34 \t Batch 740 \t Training Loss: 46.98533856159932\n",
      "Epoch 34 \t Batch 760 \t Training Loss: 46.97952989277087\n",
      "Epoch 34 \t Batch 780 \t Training Loss: 47.00909258035513\n",
      "Epoch 34 \t Batch 800 \t Training Loss: 47.055382289886474\n",
      "Epoch 34 \t Batch 820 \t Training Loss: 47.065916898773935\n",
      "Epoch 34 \t Batch 840 \t Training Loss: 47.03619726725987\n",
      "Epoch 34 \t Batch 860 \t Training Loss: 47.00263809825099\n",
      "Epoch 34 \t Batch 880 \t Training Loss: 46.970772210034454\n",
      "Epoch 34 \t Batch 900 \t Training Loss: 46.96906898498535\n",
      "Epoch 34 \t Batch 20 \t Validation Loss: 45.99869337081909\n",
      "Epoch 34 \t Batch 40 \t Validation Loss: 42.10763099193573\n",
      "Epoch 34 \t Batch 60 \t Validation Loss: 45.66377879778544\n",
      "Epoch 34 \t Batch 80 \t Validation Loss: 43.28333874940872\n",
      "Epoch 34 \t Batch 100 \t Validation Loss: 40.17063654899597\n",
      "Epoch 34 \t Batch 120 \t Validation Loss: 38.47052795886994\n",
      "Epoch 34 \t Batch 140 \t Validation Loss: 36.857125002997265\n",
      "Epoch 34 \t Batch 160 \t Validation Loss: 37.45081734061241\n",
      "Epoch 34 \t Batch 180 \t Validation Loss: 39.87720802095201\n",
      "Epoch 34 \t Batch 200 \t Validation Loss: 40.81219114303589\n",
      "Epoch 34 \t Batch 220 \t Validation Loss: 41.4135251825506\n",
      "Epoch 34 \t Batch 240 \t Validation Loss: 41.308860250314076\n",
      "Epoch 34 \t Batch 260 \t Validation Loss: 43.125440098689154\n",
      "Epoch 34 \t Batch 280 \t Validation Loss: 43.9894163608551\n",
      "Epoch 34 \t Batch 300 \t Validation Loss: 44.49381813685099\n",
      "Epoch 34 \t Batch 320 \t Validation Loss: 44.573456186056134\n",
      "Epoch 34 \t Batch 340 \t Validation Loss: 44.256509068432976\n",
      "Epoch 34 \t Batch 360 \t Validation Loss: 43.782623608907066\n",
      "Epoch 34 \t Batch 380 \t Validation Loss: 43.89678003411544\n",
      "Epoch 34 \t Batch 400 \t Validation Loss: 43.400787086486815\n",
      "Epoch 34 \t Batch 420 \t Validation Loss: 43.323402395702544\n",
      "Epoch 34 \t Batch 440 \t Validation Loss: 43.05790278261358\n",
      "Epoch 34 \t Batch 460 \t Validation Loss: 43.21695741570514\n",
      "Epoch 34 \t Batch 480 \t Validation Loss: 43.53165092865626\n",
      "Epoch 34 \t Batch 500 \t Validation Loss: 43.16290190887451\n",
      "Epoch 34 \t Batch 520 \t Validation Loss: 42.97285710481497\n",
      "Epoch 34 \t Batch 540 \t Validation Loss: 42.48025630668358\n",
      "Epoch 34 \t Batch 560 \t Validation Loss: 42.07858565194266\n",
      "Epoch 34 \t Batch 580 \t Validation Loss: 41.67816489647175\n",
      "Epoch 34 \t Batch 600 \t Validation Loss: 41.68928820292155\n",
      "Epoch 34 Training Loss: 46.983536074439726 Validation Loss: 42.19141107720214\n",
      "Epoch 34 completed\n",
      "Epoch 35 \t Batch 20 \t Training Loss: 46.257754516601565\n",
      "Epoch 35 \t Batch 40 \t Training Loss: 46.402391242980954\n",
      "Epoch 35 \t Batch 60 \t Training Loss: 46.560163434346514\n",
      "Epoch 35 \t Batch 80 \t Training Loss: 47.06379375457764\n",
      "Epoch 35 \t Batch 100 \t Training Loss: 46.86015041351318\n",
      "Epoch 35 \t Batch 120 \t Training Loss: 46.777732340494794\n",
      "Epoch 35 \t Batch 140 \t Training Loss: 46.79085993085589\n",
      "Epoch 35 \t Batch 160 \t Training Loss: 46.63506422042847\n",
      "Epoch 35 \t Batch 180 \t Training Loss: 46.64563352796767\n",
      "Epoch 35 \t Batch 200 \t Training Loss: 46.83011581420899\n",
      "Epoch 35 \t Batch 220 \t Training Loss: 46.84505483453924\n",
      "Epoch 35 \t Batch 240 \t Training Loss: 46.86059858004252\n",
      "Epoch 35 \t Batch 260 \t Training Loss: 46.91172417860765\n",
      "Epoch 35 \t Batch 280 \t Training Loss: 46.95550922666277\n",
      "Epoch 35 \t Batch 300 \t Training Loss: 47.00490357716878\n",
      "Epoch 35 \t Batch 320 \t Training Loss: 47.0537228345871\n",
      "Epoch 35 \t Batch 340 \t Training Loss: 46.9807165033677\n",
      "Epoch 35 \t Batch 360 \t Training Loss: 46.92307712766859\n",
      "Epoch 35 \t Batch 380 \t Training Loss: 46.958007290488794\n",
      "Epoch 35 \t Batch 400 \t Training Loss: 47.00905435562134\n",
      "Epoch 35 \t Batch 420 \t Training Loss: 46.96296730949765\n",
      "Epoch 35 \t Batch 440 \t Training Loss: 46.89782274419611\n",
      "Epoch 35 \t Batch 460 \t Training Loss: 46.93191208217455\n",
      "Epoch 35 \t Batch 480 \t Training Loss: 46.933009465535484\n",
      "Epoch 35 \t Batch 500 \t Training Loss: 46.91683405303955\n",
      "Epoch 35 \t Batch 520 \t Training Loss: 46.90958416278546\n",
      "Epoch 35 \t Batch 540 \t Training Loss: 46.93869928430628\n",
      "Epoch 35 \t Batch 560 \t Training Loss: 46.940626457759315\n",
      "Epoch 35 \t Batch 580 \t Training Loss: 46.95795774788692\n",
      "Epoch 35 \t Batch 600 \t Training Loss: 46.91639410654704\n",
      "Epoch 35 \t Batch 620 \t Training Loss: 46.966689195940575\n",
      "Epoch 35 \t Batch 640 \t Training Loss: 46.96728063225746\n",
      "Epoch 35 \t Batch 660 \t Training Loss: 46.949510834433816\n",
      "Epoch 35 \t Batch 680 \t Training Loss: 46.961110350664924\n",
      "Epoch 35 \t Batch 700 \t Training Loss: 46.941127962384904\n",
      "Epoch 35 \t Batch 720 \t Training Loss: 46.95998185475667\n",
      "Epoch 35 \t Batch 740 \t Training Loss: 46.92938997938826\n",
      "Epoch 35 \t Batch 760 \t Training Loss: 46.92724683159276\n",
      "Epoch 35 \t Batch 780 \t Training Loss: 46.92585615011362\n",
      "Epoch 35 \t Batch 800 \t Training Loss: 46.91284738063812\n",
      "Epoch 35 \t Batch 820 \t Training Loss: 46.889299341527426\n",
      "Epoch 35 \t Batch 840 \t Training Loss: 46.90052259990147\n",
      "Epoch 35 \t Batch 860 \t Training Loss: 46.935464144861974\n",
      "Epoch 35 \t Batch 880 \t Training Loss: 46.96212840080261\n",
      "Epoch 35 \t Batch 900 \t Training Loss: 46.935499860975476\n",
      "Epoch 35 \t Batch 20 \t Validation Loss: 37.93861093521118\n",
      "Epoch 35 \t Batch 40 \t Validation Loss: 37.558085918426514\n",
      "Epoch 35 \t Batch 60 \t Validation Loss: 41.03301321665446\n",
      "Epoch 35 \t Batch 80 \t Validation Loss: 39.23357787132263\n",
      "Epoch 35 \t Batch 100 \t Validation Loss: 37.11194244384765\n",
      "Epoch 35 \t Batch 120 \t Validation Loss: 36.023948685328165\n",
      "Epoch 35 \t Batch 140 \t Validation Loss: 34.81277077538626\n",
      "Epoch 35 \t Batch 160 \t Validation Loss: 35.59592661857605\n",
      "Epoch 35 \t Batch 180 \t Validation Loss: 38.15470573107402\n",
      "Epoch 35 \t Batch 200 \t Validation Loss: 39.10051374912262\n",
      "Epoch 35 \t Batch 220 \t Validation Loss: 39.78899293812839\n",
      "Epoch 35 \t Batch 240 \t Validation Loss: 39.74983514944712\n",
      "Epoch 35 \t Batch 260 \t Validation Loss: 41.5957452077132\n",
      "Epoch 35 \t Batch 280 \t Validation Loss: 42.46121698447636\n",
      "Epoch 35 \t Batch 300 \t Validation Loss: 43.03653889020284\n",
      "Epoch 35 \t Batch 320 \t Validation Loss: 43.1897446423769\n",
      "Epoch 35 \t Batch 340 \t Validation Loss: 42.90303773599513\n",
      "Epoch 35 \t Batch 360 \t Validation Loss: 42.496158724360996\n",
      "Epoch 35 \t Batch 380 \t Validation Loss: 42.610857223209585\n",
      "Epoch 35 \t Batch 400 \t Validation Loss: 42.16002253293991\n",
      "Epoch 35 \t Batch 420 \t Validation Loss: 42.05801763761611\n",
      "Epoch 35 \t Batch 440 \t Validation Loss: 41.8248021104119\n",
      "Epoch 35 \t Batch 460 \t Validation Loss: 41.97226922201074\n",
      "Epoch 35 \t Batch 480 \t Validation Loss: 42.318089963992435\n",
      "Epoch 35 \t Batch 500 \t Validation Loss: 41.92965245628357\n",
      "Epoch 35 \t Batch 520 \t Validation Loss: 41.742154104893025\n",
      "Epoch 35 \t Batch 540 \t Validation Loss: 41.336896159913806\n",
      "Epoch 35 \t Batch 560 \t Validation Loss: 41.02374706438609\n",
      "Epoch 35 \t Batch 580 \t Validation Loss: 40.71276676079322\n",
      "Epoch 35 \t Batch 600 \t Validation Loss: 40.792900907198586\n",
      "Epoch 35 Training Loss: 46.934407894587956 Validation Loss: 41.332881371696274\n",
      "Epoch 35 completed\n",
      "Epoch 36 \t Batch 20 \t Training Loss: 46.1652271270752\n",
      "Epoch 36 \t Batch 40 \t Training Loss: 46.046711444854736\n",
      "Epoch 36 \t Batch 60 \t Training Loss: 46.44192676544189\n",
      "Epoch 36 \t Batch 80 \t Training Loss: 46.477301359176636\n",
      "Epoch 36 \t Batch 100 \t Training Loss: 46.45865295410156\n",
      "Epoch 36 \t Batch 120 \t Training Loss: 46.31853322982788\n",
      "Epoch 36 \t Batch 140 \t Training Loss: 46.506326348440986\n",
      "Epoch 36 \t Batch 160 \t Training Loss: 46.60225312709808\n",
      "Epoch 36 \t Batch 180 \t Training Loss: 46.72377870347765\n",
      "Epoch 36 \t Batch 200 \t Training Loss: 46.77175308227539\n",
      "Epoch 36 \t Batch 220 \t Training Loss: 46.79436721801758\n",
      "Epoch 36 \t Batch 240 \t Training Loss: 46.71322153409322\n",
      "Epoch 36 \t Batch 260 \t Training Loss: 46.81920014894926\n",
      "Epoch 36 \t Batch 280 \t Training Loss: 46.794722488948274\n",
      "Epoch 36 \t Batch 300 \t Training Loss: 46.8436501566569\n",
      "Epoch 36 \t Batch 320 \t Training Loss: 46.77277438640594\n",
      "Epoch 36 \t Batch 340 \t Training Loss: 46.78054360782399\n",
      "Epoch 36 \t Batch 360 \t Training Loss: 46.79965581893921\n",
      "Epoch 36 \t Batch 380 \t Training Loss: 46.790872955322264\n",
      "Epoch 36 \t Batch 400 \t Training Loss: 46.75991536140442\n",
      "Epoch 36 \t Batch 420 \t Training Loss: 46.75443044389997\n",
      "Epoch 36 \t Batch 440 \t Training Loss: 46.74931054548784\n",
      "Epoch 36 \t Batch 460 \t Training Loss: 46.73697444252346\n",
      "Epoch 36 \t Batch 480 \t Training Loss: 46.74322442213694\n",
      "Epoch 36 \t Batch 500 \t Training Loss: 46.772477447509765\n",
      "Epoch 36 \t Batch 520 \t Training Loss: 46.799915746542126\n",
      "Epoch 36 \t Batch 540 \t Training Loss: 46.82573934484411\n",
      "Epoch 36 \t Batch 560 \t Training Loss: 46.839710337775095\n",
      "Epoch 36 \t Batch 580 \t Training Loss: 46.82367113705339\n",
      "Epoch 36 \t Batch 600 \t Training Loss: 46.89620300928752\n",
      "Epoch 36 \t Batch 620 \t Training Loss: 46.917733820023074\n",
      "Epoch 36 \t Batch 640 \t Training Loss: 46.92217511534691\n",
      "Epoch 36 \t Batch 660 \t Training Loss: 46.91089962468003\n",
      "Epoch 36 \t Batch 680 \t Training Loss: 46.90522730771233\n",
      "Epoch 36 \t Batch 700 \t Training Loss: 46.92475113459996\n",
      "Epoch 36 \t Batch 720 \t Training Loss: 46.881277640660606\n",
      "Epoch 36 \t Batch 740 \t Training Loss: 46.88350637796763\n",
      "Epoch 36 \t Batch 760 \t Training Loss: 46.92440849103426\n",
      "Epoch 36 \t Batch 780 \t Training Loss: 46.93646757663824\n",
      "Epoch 36 \t Batch 800 \t Training Loss: 46.979369411468504\n",
      "Epoch 36 \t Batch 820 \t Training Loss: 46.995069624737994\n",
      "Epoch 36 \t Batch 840 \t Training Loss: 46.996912819998606\n",
      "Epoch 36 \t Batch 860 \t Training Loss: 46.94853148793065\n",
      "Epoch 36 \t Batch 880 \t Training Loss: 46.95003972053528\n",
      "Epoch 36 \t Batch 900 \t Training Loss: 46.940555034213595\n",
      "Epoch 36 \t Batch 20 \t Validation Loss: 31.376371097564697\n",
      "Epoch 36 \t Batch 40 \t Validation Loss: 31.820868134498596\n",
      "Epoch 36 \t Batch 60 \t Validation Loss: 33.778528451919556\n",
      "Epoch 36 \t Batch 80 \t Validation Loss: 32.99821821451187\n",
      "Epoch 36 \t Batch 100 \t Validation Loss: 31.92682005882263\n",
      "Epoch 36 \t Batch 120 \t Validation Loss: 31.53998509248098\n",
      "Epoch 36 \t Batch 140 \t Validation Loss: 30.952636970792497\n",
      "Epoch 36 \t Batch 160 \t Validation Loss: 32.32103138566017\n",
      "Epoch 36 \t Batch 180 \t Validation Loss: 35.215666977564496\n",
      "Epoch 36 \t Batch 200 \t Validation Loss: 36.459133458137515\n",
      "Epoch 36 \t Batch 220 \t Validation Loss: 37.37630556713451\n",
      "Epoch 36 \t Batch 240 \t Validation Loss: 37.556995741526286\n",
      "Epoch 36 \t Batch 260 \t Validation Loss: 39.594069447884195\n",
      "Epoch 36 \t Batch 280 \t Validation Loss: 40.612592342921666\n",
      "Epoch 36 \t Batch 300 \t Validation Loss: 41.34846750259399\n",
      "Epoch 36 \t Batch 320 \t Validation Loss: 41.62895641922951\n",
      "Epoch 36 \t Batch 340 \t Validation Loss: 41.46133627611048\n",
      "Epoch 36 \t Batch 360 \t Validation Loss: 41.24799756473965\n",
      "Epoch 36 \t Batch 380 \t Validation Loss: 41.48908473065025\n",
      "Epoch 36 \t Batch 400 \t Validation Loss: 41.1855193901062\n",
      "Epoch 36 \t Batch 420 \t Validation Loss: 41.1488054457165\n",
      "Epoch 36 \t Batch 440 \t Validation Loss: 41.08295011520386\n",
      "Epoch 36 \t Batch 460 \t Validation Loss: 41.303769294075344\n",
      "Epoch 36 \t Batch 480 \t Validation Loss: 41.74321287870407\n",
      "Epoch 36 \t Batch 500 \t Validation Loss: 41.39726332092285\n",
      "Epoch 36 \t Batch 520 \t Validation Loss: 41.34854065821721\n",
      "Epoch 36 \t Batch 540 \t Validation Loss: 41.06388909022014\n",
      "Epoch 36 \t Batch 560 \t Validation Loss: 40.87248019150325\n",
      "Epoch 36 \t Batch 580 \t Validation Loss: 40.69374094995959\n",
      "Epoch 36 \t Batch 600 \t Validation Loss: 40.85103824297587\n",
      "Epoch 36 Training Loss: 46.93322203957528 Validation Loss: 41.458406603181515\n",
      "Epoch 36 completed\n",
      "Epoch 37 \t Batch 20 \t Training Loss: 46.12914485931397\n",
      "Epoch 37 \t Batch 40 \t Training Loss: 46.203754901885986\n",
      "Epoch 37 \t Batch 60 \t Training Loss: 46.044418525695804\n",
      "Epoch 37 \t Batch 80 \t Training Loss: 46.17483248710632\n",
      "Epoch 37 \t Batch 100 \t Training Loss: 46.098390655517576\n",
      "Epoch 37 \t Batch 120 \t Training Loss: 46.36612453460693\n",
      "Epoch 37 \t Batch 140 \t Training Loss: 46.3571331841605\n",
      "Epoch 37 \t Batch 160 \t Training Loss: 46.418360638618466\n",
      "Epoch 37 \t Batch 180 \t Training Loss: 46.45056224399143\n",
      "Epoch 37 \t Batch 200 \t Training Loss: 46.52720285415649\n",
      "Epoch 37 \t Batch 220 \t Training Loss: 46.58196456215598\n",
      "Epoch 37 \t Batch 240 \t Training Loss: 46.58764654795329\n",
      "Epoch 37 \t Batch 260 \t Training Loss: 46.470832105783316\n",
      "Epoch 37 \t Batch 280 \t Training Loss: 46.55335775102888\n",
      "Epoch 37 \t Batch 300 \t Training Loss: 46.60036479949951\n",
      "Epoch 37 \t Batch 320 \t Training Loss: 46.60391038656235\n",
      "Epoch 37 \t Batch 340 \t Training Loss: 46.586412160536824\n",
      "Epoch 37 \t Batch 360 \t Training Loss: 46.604484515719946\n",
      "Epoch 37 \t Batch 380 \t Training Loss: 46.650666166606705\n",
      "Epoch 37 \t Batch 400 \t Training Loss: 46.74036787033081\n",
      "Epoch 37 \t Batch 420 \t Training Loss: 46.72014587039039\n",
      "Epoch 37 \t Batch 440 \t Training Loss: 46.73161916732788\n",
      "Epoch 37 \t Batch 460 \t Training Loss: 46.70184264805006\n",
      "Epoch 37 \t Batch 480 \t Training Loss: 46.698754541079204\n",
      "Epoch 37 \t Batch 500 \t Training Loss: 46.75882775878906\n",
      "Epoch 37 \t Batch 520 \t Training Loss: 46.71871284338144\n",
      "Epoch 37 \t Batch 540 \t Training Loss: 46.75724846875226\n",
      "Epoch 37 \t Batch 560 \t Training Loss: 46.725317927769254\n",
      "Epoch 37 \t Batch 580 \t Training Loss: 46.741259923474544\n",
      "Epoch 37 \t Batch 600 \t Training Loss: 46.72281348546346\n",
      "Epoch 37 \t Batch 620 \t Training Loss: 46.746589131509104\n",
      "Epoch 37 \t Batch 640 \t Training Loss: 46.74279634356499\n",
      "Epoch 37 \t Batch 660 \t Training Loss: 46.75242866747307\n",
      "Epoch 37 \t Batch 680 \t Training Loss: 46.73813584271599\n",
      "Epoch 37 \t Batch 700 \t Training Loss: 46.78334855760847\n",
      "Epoch 37 \t Batch 720 \t Training Loss: 46.767216486401026\n",
      "Epoch 37 \t Batch 740 \t Training Loss: 46.79773063659668\n",
      "Epoch 37 \t Batch 760 \t Training Loss: 46.81745813771298\n",
      "Epoch 37 \t Batch 780 \t Training Loss: 46.85169888031788\n",
      "Epoch 37 \t Batch 800 \t Training Loss: 46.832926502227785\n",
      "Epoch 37 \t Batch 820 \t Training Loss: 46.8028305193273\n",
      "Epoch 37 \t Batch 840 \t Training Loss: 46.834999738420755\n",
      "Epoch 37 \t Batch 860 \t Training Loss: 46.83888572870299\n",
      "Epoch 37 \t Batch 880 \t Training Loss: 46.86604333357378\n",
      "Epoch 37 \t Batch 900 \t Training Loss: 46.87933235168457\n",
      "Epoch 37 \t Batch 20 \t Validation Loss: 32.37086181640625\n",
      "Epoch 37 \t Batch 40 \t Validation Loss: 32.13129351139069\n",
      "Epoch 37 \t Batch 60 \t Validation Loss: 34.59908231099447\n",
      "Epoch 37 \t Batch 80 \t Validation Loss: 33.581153786182405\n",
      "Epoch 37 \t Batch 100 \t Validation Loss: 32.67384943962097\n",
      "Epoch 37 \t Batch 120 \t Validation Loss: 32.41672947406769\n",
      "Epoch 37 \t Batch 140 \t Validation Loss: 31.715802907943726\n",
      "Epoch 37 \t Batch 160 \t Validation Loss: 32.88707063794136\n",
      "Epoch 37 \t Batch 180 \t Validation Loss: 35.63450468381246\n",
      "Epoch 37 \t Batch 200 \t Validation Loss: 36.71070993900299\n",
      "Epoch 37 \t Batch 220 \t Validation Loss: 37.5954072822224\n",
      "Epoch 37 \t Batch 240 \t Validation Loss: 37.68051322301229\n",
      "Epoch 37 \t Batch 260 \t Validation Loss: 39.62766043956463\n",
      "Epoch 37 \t Batch 280 \t Validation Loss: 40.60789317744119\n",
      "Epoch 37 \t Batch 300 \t Validation Loss: 41.257834157943726\n",
      "Epoch 37 \t Batch 320 \t Validation Loss: 41.51673072874546\n",
      "Epoch 37 \t Batch 340 \t Validation Loss: 41.32001298175138\n",
      "Epoch 37 \t Batch 360 \t Validation Loss: 41.02736276785533\n",
      "Epoch 37 \t Batch 380 \t Validation Loss: 41.17228147105167\n",
      "Epoch 37 \t Batch 400 \t Validation Loss: 40.698180799484255\n",
      "Epoch 37 \t Batch 420 \t Validation Loss: 40.62597850390843\n",
      "Epoch 37 \t Batch 440 \t Validation Loss: 40.333488232439215\n",
      "Epoch 37 \t Batch 460 \t Validation Loss: 40.490023299922115\n",
      "Epoch 37 \t Batch 480 \t Validation Loss: 40.89356666604678\n",
      "Epoch 37 \t Batch 500 \t Validation Loss: 40.55962781715393\n",
      "Epoch 37 \t Batch 520 \t Validation Loss: 40.353649972035335\n",
      "Epoch 37 \t Batch 540 \t Validation Loss: 39.996218557711\n",
      "Epoch 37 \t Batch 560 \t Validation Loss: 39.6969653572355\n",
      "Epoch 37 \t Batch 580 \t Validation Loss: 39.3686343883646\n",
      "Epoch 37 \t Batch 600 \t Validation Loss: 39.500986868540444\n",
      "Epoch 37 Training Loss: 46.881789166898734 Validation Loss: 40.04504036593747\n",
      "Epoch 37 completed\n",
      "Epoch 38 \t Batch 20 \t Training Loss: 45.9835693359375\n",
      "Epoch 38 \t Batch 40 \t Training Loss: 46.48417196273804\n",
      "Epoch 38 \t Batch 60 \t Training Loss: 47.02189947764079\n",
      "Epoch 38 \t Batch 80 \t Training Loss: 47.10478892326355\n",
      "Epoch 38 \t Batch 100 \t Training Loss: 46.95460742950439\n",
      "Epoch 38 \t Batch 120 \t Training Loss: 46.86943273544311\n",
      "Epoch 38 \t Batch 140 \t Training Loss: 46.975731304713655\n",
      "Epoch 38 \t Batch 160 \t Training Loss: 46.73104114532471\n",
      "Epoch 38 \t Batch 180 \t Training Loss: 46.72237508561876\n",
      "Epoch 38 \t Batch 200 \t Training Loss: 46.63651174545288\n",
      "Epoch 38 \t Batch 220 \t Training Loss: 46.568833871321246\n",
      "Epoch 38 \t Batch 240 \t Training Loss: 46.620969724655154\n",
      "Epoch 38 \t Batch 260 \t Training Loss: 46.538766098022464\n",
      "Epoch 38 \t Batch 280 \t Training Loss: 46.533430753435404\n",
      "Epoch 38 \t Batch 300 \t Training Loss: 46.536526362101235\n",
      "Epoch 38 \t Batch 320 \t Training Loss: 46.515273201465604\n",
      "Epoch 38 \t Batch 340 \t Training Loss: 46.54499002344468\n",
      "Epoch 38 \t Batch 360 \t Training Loss: 46.6513452106052\n",
      "Epoch 38 \t Batch 380 \t Training Loss: 46.66161699796978\n",
      "Epoch 38 \t Batch 400 \t Training Loss: 46.67506464958191\n",
      "Epoch 38 \t Batch 420 \t Training Loss: 46.67210751488095\n",
      "Epoch 38 \t Batch 440 \t Training Loss: 46.668018237027255\n",
      "Epoch 38 \t Batch 460 \t Training Loss: 46.6927769370701\n",
      "Epoch 38 \t Batch 480 \t Training Loss: 46.70744873682658\n",
      "Epoch 38 \t Batch 500 \t Training Loss: 46.68648876953125\n",
      "Epoch 38 \t Batch 520 \t Training Loss: 46.73268941732553\n",
      "Epoch 38 \t Batch 540 \t Training Loss: 46.73049907684326\n",
      "Epoch 38 \t Batch 560 \t Training Loss: 46.807450553349085\n",
      "Epoch 38 \t Batch 580 \t Training Loss: 46.80328751268058\n",
      "Epoch 38 \t Batch 600 \t Training Loss: 46.79161242802938\n",
      "Epoch 38 \t Batch 620 \t Training Loss: 46.79471493997882\n",
      "Epoch 38 \t Batch 640 \t Training Loss: 46.80755162239075\n",
      "Epoch 38 \t Batch 660 \t Training Loss: 46.86120556340073\n",
      "Epoch 38 \t Batch 680 \t Training Loss: 46.83633633781882\n",
      "Epoch 38 \t Batch 700 \t Training Loss: 46.84463729313442\n",
      "Epoch 38 \t Batch 720 \t Training Loss: 46.868335798051625\n",
      "Epoch 38 \t Batch 740 \t Training Loss: 46.85824033376333\n",
      "Epoch 38 \t Batch 760 \t Training Loss: 46.86935220015676\n",
      "Epoch 38 \t Batch 780 \t Training Loss: 46.88640320117657\n",
      "Epoch 38 \t Batch 800 \t Training Loss: 46.88196575641632\n",
      "Epoch 38 \t Batch 820 \t Training Loss: 46.86405631739919\n",
      "Epoch 38 \t Batch 840 \t Training Loss: 46.8687407993135\n",
      "Epoch 38 \t Batch 860 \t Training Loss: 46.84546167684156\n",
      "Epoch 38 \t Batch 880 \t Training Loss: 46.869973161003806\n",
      "Epoch 38 \t Batch 900 \t Training Loss: 46.86287274254693\n",
      "Epoch 38 \t Batch 20 \t Validation Loss: 45.75811758041382\n",
      "Epoch 38 \t Batch 40 \t Validation Loss: 41.71067440509796\n",
      "Epoch 38 \t Batch 60 \t Validation Loss: 44.95875226656596\n",
      "Epoch 38 \t Batch 80 \t Validation Loss: 42.50414180755615\n",
      "Epoch 38 \t Batch 100 \t Validation Loss: 39.903176364898684\n",
      "Epoch 38 \t Batch 120 \t Validation Loss: 38.490451431274415\n",
      "Epoch 38 \t Batch 140 \t Validation Loss: 37.03647116252354\n",
      "Epoch 38 \t Batch 160 \t Validation Loss: 37.482597732543944\n",
      "Epoch 38 \t Batch 180 \t Validation Loss: 39.698466306262546\n",
      "Epoch 38 \t Batch 200 \t Validation Loss: 40.39958048343659\n",
      "Epoch 38 \t Batch 220 \t Validation Loss: 40.896848526867956\n",
      "Epoch 38 \t Batch 240 \t Validation Loss: 40.740751310189566\n",
      "Epoch 38 \t Batch 260 \t Validation Loss: 42.43163645817683\n",
      "Epoch 38 \t Batch 280 \t Validation Loss: 43.18207996232169\n",
      "Epoch 38 \t Batch 300 \t Validation Loss: 43.65914139429728\n",
      "Epoch 38 \t Batch 320 \t Validation Loss: 43.7485125541687\n",
      "Epoch 38 \t Batch 340 \t Validation Loss: 43.40573588539572\n",
      "Epoch 38 \t Batch 360 \t Validation Loss: 42.93342627419366\n",
      "Epoch 38 \t Batch 380 \t Validation Loss: 43.015267203983505\n",
      "Epoch 38 \t Batch 400 \t Validation Loss: 42.50702984571457\n",
      "Epoch 38 \t Batch 420 \t Validation Loss: 42.352769209089736\n",
      "Epoch 38 \t Batch 440 \t Validation Loss: 42.06419068900021\n",
      "Epoch 38 \t Batch 460 \t Validation Loss: 42.24851241526397\n",
      "Epoch 38 \t Batch 480 \t Validation Loss: 42.56743364532789\n",
      "Epoch 38 \t Batch 500 \t Validation Loss: 42.157910619735716\n",
      "Epoch 38 \t Batch 520 \t Validation Loss: 41.97779458119319\n",
      "Epoch 38 \t Batch 540 \t Validation Loss: 41.623662832048204\n",
      "Epoch 38 \t Batch 560 \t Validation Loss: 41.34031458922795\n",
      "Epoch 38 \t Batch 580 \t Validation Loss: 41.083179033213646\n",
      "Epoch 38 \t Batch 600 \t Validation Loss: 41.184589042663575\n",
      "Epoch 38 Training Loss: 46.86365548552968 Validation Loss: 41.73300790167474\n",
      "Epoch 38 completed\n",
      "Epoch 39 \t Batch 20 \t Training Loss: 46.94009838104248\n",
      "Epoch 39 \t Batch 40 \t Training Loss: 46.36224098205567\n",
      "Epoch 39 \t Batch 60 \t Training Loss: 46.410879071553545\n",
      "Epoch 39 \t Batch 80 \t Training Loss: 46.68841953277588\n",
      "Epoch 39 \t Batch 100 \t Training Loss: 46.50261379241943\n",
      "Epoch 39 \t Batch 120 \t Training Loss: 46.302054182688394\n",
      "Epoch 39 \t Batch 140 \t Training Loss: 46.456611606052945\n",
      "Epoch 39 \t Batch 160 \t Training Loss: 46.51422312259674\n",
      "Epoch 39 \t Batch 180 \t Training Loss: 46.688927035861546\n",
      "Epoch 39 \t Batch 200 \t Training Loss: 46.80715045928955\n",
      "Epoch 39 \t Batch 220 \t Training Loss: 46.90645008087158\n",
      "Epoch 39 \t Batch 240 \t Training Loss: 46.94259794553121\n",
      "Epoch 39 \t Batch 260 \t Training Loss: 46.88409070235032\n",
      "Epoch 39 \t Batch 280 \t Training Loss: 46.79287302834647\n",
      "Epoch 39 \t Batch 300 \t Training Loss: 46.7461450958252\n",
      "Epoch 39 \t Batch 320 \t Training Loss: 46.80636936426163\n",
      "Epoch 39 \t Batch 340 \t Training Loss: 46.792041374655334\n",
      "Epoch 39 \t Batch 360 \t Training Loss: 46.90694108539157\n",
      "Epoch 39 \t Batch 380 \t Training Loss: 46.86973631005538\n",
      "Epoch 39 \t Batch 400 \t Training Loss: 46.83253943443298\n",
      "Epoch 39 \t Batch 420 \t Training Loss: 46.843324207124255\n",
      "Epoch 39 \t Batch 440 \t Training Loss: 46.838364297693424\n",
      "Epoch 39 \t Batch 460 \t Training Loss: 46.88965600884479\n",
      "Epoch 39 \t Batch 480 \t Training Loss: 46.870319390296935\n",
      "Epoch 39 \t Batch 500 \t Training Loss: 46.935657333374024\n",
      "Epoch 39 \t Batch 520 \t Training Loss: 46.91400775909424\n",
      "Epoch 39 \t Batch 540 \t Training Loss: 46.92722114986844\n",
      "Epoch 39 \t Batch 560 \t Training Loss: 46.88590798377991\n",
      "Epoch 39 \t Batch 580 \t Training Loss: 46.878743842552446\n",
      "Epoch 39 \t Batch 600 \t Training Loss: 46.916372515360514\n",
      "Epoch 39 \t Batch 620 \t Training Loss: 46.92759646754111\n",
      "Epoch 39 \t Batch 640 \t Training Loss: 46.97874973416329\n",
      "Epoch 39 \t Batch 660 \t Training Loss: 46.93795863642837\n",
      "Epoch 39 \t Batch 680 \t Training Loss: 46.87291496501249\n",
      "Epoch 39 \t Batch 700 \t Training Loss: 46.844400912693565\n",
      "Epoch 39 \t Batch 720 \t Training Loss: 46.858625348409014\n",
      "Epoch 39 \t Batch 740 \t Training Loss: 46.83537796639107\n",
      "Epoch 39 \t Batch 760 \t Training Loss: 46.789529910840486\n",
      "Epoch 39 \t Batch 780 \t Training Loss: 46.760994764474724\n",
      "Epoch 39 \t Batch 800 \t Training Loss: 46.777199048995975\n",
      "Epoch 39 \t Batch 820 \t Training Loss: 46.76519624198355\n",
      "Epoch 39 \t Batch 840 \t Training Loss: 46.77148627780733\n",
      "Epoch 39 \t Batch 860 \t Training Loss: 46.75281141857768\n",
      "Epoch 39 \t Batch 880 \t Training Loss: 46.78251038464633\n",
      "Epoch 39 \t Batch 900 \t Training Loss: 46.81304830763075\n",
      "Epoch 39 \t Batch 20 \t Validation Loss: 48.53948678970337\n",
      "Epoch 39 \t Batch 40 \t Validation Loss: 44.3740330696106\n",
      "Epoch 39 \t Batch 60 \t Validation Loss: 47.62260967890422\n",
      "Epoch 39 \t Batch 80 \t Validation Loss: 45.38159070014954\n",
      "Epoch 39 \t Batch 100 \t Validation Loss: 42.316222763061525\n",
      "Epoch 39 \t Batch 120 \t Validation Loss: 40.51417597134908\n",
      "Epoch 39 \t Batch 140 \t Validation Loss: 38.7413923399789\n",
      "Epoch 39 \t Batch 160 \t Validation Loss: 39.1359160065651\n",
      "Epoch 39 \t Batch 180 \t Validation Loss: 41.618229643503824\n",
      "Epoch 39 \t Batch 200 \t Validation Loss: 42.276771516799926\n",
      "Epoch 39 \t Batch 220 \t Validation Loss: 42.879792074723674\n",
      "Epoch 39 \t Batch 240 \t Validation Loss: 42.70277339220047\n",
      "Epoch 39 \t Batch 260 \t Validation Loss: 44.50463926241948\n",
      "Epoch 39 \t Batch 280 \t Validation Loss: 45.308036007199966\n",
      "Epoch 39 \t Batch 300 \t Validation Loss: 45.86891622543335\n",
      "Epoch 39 \t Batch 320 \t Validation Loss: 45.957031750679015\n",
      "Epoch 39 \t Batch 340 \t Validation Loss: 45.5316941429587\n",
      "Epoch 39 \t Batch 360 \t Validation Loss: 45.04124462339613\n",
      "Epoch 39 \t Batch 380 \t Validation Loss: 44.99627647148935\n",
      "Epoch 39 \t Batch 400 \t Validation Loss: 44.27060196638107\n",
      "Epoch 39 \t Batch 420 \t Validation Loss: 43.99498561223348\n",
      "Epoch 39 \t Batch 440 \t Validation Loss: 43.431403732299806\n",
      "Epoch 39 \t Batch 460 \t Validation Loss: 43.43208017349243\n",
      "Epoch 39 \t Batch 480 \t Validation Loss: 43.69994360605876\n",
      "Epoch 39 \t Batch 500 \t Validation Loss: 43.230706954956055\n",
      "Epoch 39 \t Batch 520 \t Validation Loss: 42.844981641035815\n",
      "Epoch 39 \t Batch 540 \t Validation Loss: 42.425717463316744\n",
      "Epoch 39 \t Batch 560 \t Validation Loss: 42.04347862175533\n",
      "Epoch 39 \t Batch 580 \t Validation Loss: 41.652012154151656\n",
      "Epoch 39 \t Batch 600 \t Validation Loss: 41.72823355356852\n",
      "Epoch 39 Training Loss: 46.81170463354013 Validation Loss: 42.236038870625684\n",
      "Epoch 39 completed\n",
      "Epoch 40 \t Batch 20 \t Training Loss: 46.3030481338501\n",
      "Epoch 40 \t Batch 40 \t Training Loss: 45.998556232452394\n",
      "Epoch 40 \t Batch 60 \t Training Loss: 46.686745198567706\n",
      "Epoch 40 \t Batch 80 \t Training Loss: 46.283349943161014\n",
      "Epoch 40 \t Batch 100 \t Training Loss: 46.51927978515625\n",
      "Epoch 40 \t Batch 120 \t Training Loss: 46.47043323516846\n",
      "Epoch 40 \t Batch 140 \t Training Loss: 46.36534851619175\n",
      "Epoch 40 \t Batch 160 \t Training Loss: 46.44572513103485\n",
      "Epoch 40 \t Batch 180 \t Training Loss: 46.404183302985295\n",
      "Epoch 40 \t Batch 200 \t Training Loss: 46.50596353530884\n",
      "Epoch 40 \t Batch 220 \t Training Loss: 46.559126541831276\n",
      "Epoch 40 \t Batch 240 \t Training Loss: 46.63041919072469\n",
      "Epoch 40 \t Batch 260 \t Training Loss: 46.67897441570575\n",
      "Epoch 40 \t Batch 280 \t Training Loss: 46.7127559525626\n",
      "Epoch 40 \t Batch 300 \t Training Loss: 46.69750914255778\n",
      "Epoch 40 \t Batch 320 \t Training Loss: 46.740207493305206\n",
      "Epoch 40 \t Batch 340 \t Training Loss: 46.70814674601835\n",
      "Epoch 40 \t Batch 360 \t Training Loss: 46.6827287250095\n",
      "Epoch 40 \t Batch 380 \t Training Loss: 46.711311561182924\n",
      "Epoch 40 \t Batch 400 \t Training Loss: 46.746782598495486\n",
      "Epoch 40 \t Batch 420 \t Training Loss: 46.814108267284574\n",
      "Epoch 40 \t Batch 440 \t Training Loss: 46.8175482229753\n",
      "Epoch 40 \t Batch 460 \t Training Loss: 46.78248643460481\n",
      "Epoch 40 \t Batch 480 \t Training Loss: 46.81458341280619\n",
      "Epoch 40 \t Batch 500 \t Training Loss: 46.851932891845706\n",
      "Epoch 40 \t Batch 520 \t Training Loss: 46.8641495191134\n",
      "Epoch 40 \t Batch 540 \t Training Loss: 46.847734211109305\n",
      "Epoch 40 \t Batch 560 \t Training Loss: 46.86508133752005\n",
      "Epoch 40 \t Batch 580 \t Training Loss: 46.865021830591665\n",
      "Epoch 40 \t Batch 600 \t Training Loss: 46.86184865315755\n",
      "Epoch 40 \t Batch 620 \t Training Loss: 46.855891929134245\n",
      "Epoch 40 \t Batch 640 \t Training Loss: 46.82165407538414\n",
      "Epoch 40 \t Batch 660 \t Training Loss: 46.808900792670975\n",
      "Epoch 40 \t Batch 680 \t Training Loss: 46.785958189122816\n",
      "Epoch 40 \t Batch 700 \t Training Loss: 46.82815321786063\n",
      "Epoch 40 \t Batch 720 \t Training Loss: 46.83128464486864\n",
      "Epoch 40 \t Batch 740 \t Training Loss: 46.82955760440311\n",
      "Epoch 40 \t Batch 760 \t Training Loss: 46.82893496061626\n",
      "Epoch 40 \t Batch 780 \t Training Loss: 46.830799792363095\n",
      "Epoch 40 \t Batch 800 \t Training Loss: 46.84290180206299\n",
      "Epoch 40 \t Batch 820 \t Training Loss: 46.867075454898\n",
      "Epoch 40 \t Batch 840 \t Training Loss: 46.835506593613395\n",
      "Epoch 40 \t Batch 860 \t Training Loss: 46.80139259959376\n",
      "Epoch 40 \t Batch 880 \t Training Loss: 46.80096942728216\n",
      "Epoch 40 \t Batch 900 \t Training Loss: 46.810423639085556\n",
      "Epoch 40 \t Batch 20 \t Validation Loss: 50.67551794052124\n",
      "Epoch 40 \t Batch 40 \t Validation Loss: 45.21673402786255\n",
      "Epoch 40 \t Batch 60 \t Validation Loss: 48.41637945175171\n",
      "Epoch 40 \t Batch 80 \t Validation Loss: 45.906120252609256\n",
      "Epoch 40 \t Batch 100 \t Validation Loss: 42.56041435241699\n",
      "Epoch 40 \t Batch 120 \t Validation Loss: 40.81210899353027\n",
      "Epoch 40 \t Batch 140 \t Validation Loss: 39.02593626976013\n",
      "Epoch 40 \t Batch 160 \t Validation Loss: 39.22357220053673\n",
      "Epoch 40 \t Batch 180 \t Validation Loss: 41.10652230580648\n",
      "Epoch 40 \t Batch 200 \t Validation Loss: 41.6444487953186\n",
      "Epoch 40 \t Batch 220 \t Validation Loss: 41.951472759246826\n",
      "Epoch 40 \t Batch 240 \t Validation Loss: 41.644752184549965\n",
      "Epoch 40 \t Batch 260 \t Validation Loss: 43.23723613298856\n",
      "Epoch 40 \t Batch 280 \t Validation Loss: 43.93913698877607\n",
      "Epoch 40 \t Batch 300 \t Validation Loss: 44.28008879979451\n",
      "Epoch 40 \t Batch 320 \t Validation Loss: 44.30355747938156\n",
      "Epoch 40 \t Batch 340 \t Validation Loss: 43.93414935504689\n",
      "Epoch 40 \t Batch 360 \t Validation Loss: 43.4700049718221\n",
      "Epoch 40 \t Batch 380 \t Validation Loss: 43.540771692677545\n",
      "Epoch 40 \t Batch 400 \t Validation Loss: 43.09418409109116\n",
      "Epoch 40 \t Batch 420 \t Validation Loss: 42.976138189860755\n",
      "Epoch 40 \t Batch 440 \t Validation Loss: 42.740257412737066\n",
      "Epoch 40 \t Batch 460 \t Validation Loss: 42.91662761232127\n",
      "Epoch 40 \t Batch 480 \t Validation Loss: 43.23242314855258\n",
      "Epoch 40 \t Batch 500 \t Validation Loss: 42.81214147377014\n",
      "Epoch 40 \t Batch 520 \t Validation Loss: 42.687453689942\n",
      "Epoch 40 \t Batch 540 \t Validation Loss: 42.322428726266935\n",
      "Epoch 40 \t Batch 560 \t Validation Loss: 42.04333628416062\n",
      "Epoch 40 \t Batch 580 \t Validation Loss: 41.78446345986991\n",
      "Epoch 40 \t Batch 600 \t Validation Loss: 41.87309882005056\n",
      "Epoch 40 Training Loss: 46.79980494957844 Validation Loss: 42.397588499180685\n",
      "Epoch 40 completed\n",
      "Epoch 41 \t Batch 20 \t Training Loss: 45.09314937591553\n",
      "Epoch 41 \t Batch 40 \t Training Loss: 45.535965538024904\n",
      "Epoch 41 \t Batch 60 \t Training Loss: 45.44212601979574\n",
      "Epoch 41 \t Batch 80 \t Training Loss: 45.572458267211914\n",
      "Epoch 41 \t Batch 100 \t Training Loss: 45.927858390808105\n",
      "Epoch 41 \t Batch 120 \t Training Loss: 46.136629072825116\n",
      "Epoch 41 \t Batch 140 \t Training Loss: 46.1637905393328\n",
      "Epoch 41 \t Batch 160 \t Training Loss: 46.23415668010712\n",
      "Epoch 41 \t Batch 180 \t Training Loss: 46.29263899061415\n",
      "Epoch 41 \t Batch 200 \t Training Loss: 46.31813409805298\n",
      "Epoch 41 \t Batch 220 \t Training Loss: 46.45722335468639\n",
      "Epoch 41 \t Batch 240 \t Training Loss: 46.416708676020306\n",
      "Epoch 41 \t Batch 260 \t Training Loss: 46.49481447660006\n",
      "Epoch 41 \t Batch 280 \t Training Loss: 46.5956074987139\n",
      "Epoch 41 \t Batch 300 \t Training Loss: 46.64899368286133\n",
      "Epoch 41 \t Batch 320 \t Training Loss: 46.61810214519501\n",
      "Epoch 41 \t Batch 340 \t Training Loss: 46.55188288969152\n",
      "Epoch 41 \t Batch 360 \t Training Loss: 46.586674001481796\n",
      "Epoch 41 \t Batch 380 \t Training Loss: 46.66967154051128\n",
      "Epoch 41 \t Batch 400 \t Training Loss: 46.70269385337829\n",
      "Epoch 41 \t Batch 420 \t Training Loss: 46.625792158217656\n",
      "Epoch 41 \t Batch 440 \t Training Loss: 46.72380158684471\n",
      "Epoch 41 \t Batch 460 \t Training Loss: 46.737061492256494\n",
      "Epoch 41 \t Batch 480 \t Training Loss: 46.7223117351532\n",
      "Epoch 41 \t Batch 500 \t Training Loss: 46.72051080322266\n",
      "Epoch 41 \t Batch 520 \t Training Loss: 46.69673192684467\n",
      "Epoch 41 \t Batch 540 \t Training Loss: 46.7064076812179\n",
      "Epoch 41 \t Batch 560 \t Training Loss: 46.718197338921684\n",
      "Epoch 41 \t Batch 580 \t Training Loss: 46.739501729504816\n",
      "Epoch 41 \t Batch 600 \t Training Loss: 46.79469733556112\n",
      "Epoch 41 \t Batch 620 \t Training Loss: 46.75435126519972\n",
      "Epoch 41 \t Batch 640 \t Training Loss: 46.799529963731764\n",
      "Epoch 41 \t Batch 660 \t Training Loss: 46.79289018457586\n",
      "Epoch 41 \t Batch 680 \t Training Loss: 46.80099693186143\n",
      "Epoch 41 \t Batch 700 \t Training Loss: 46.78649637494768\n",
      "Epoch 41 \t Batch 720 \t Training Loss: 46.801933723025854\n",
      "Epoch 41 \t Batch 740 \t Training Loss: 46.79349695669638\n",
      "Epoch 41 \t Batch 760 \t Training Loss: 46.81764983126992\n",
      "Epoch 41 \t Batch 780 \t Training Loss: 46.79792261368189\n",
      "Epoch 41 \t Batch 800 \t Training Loss: 46.80971400737762\n",
      "Epoch 41 \t Batch 820 \t Training Loss: 46.810280362571156\n",
      "Epoch 41 \t Batch 840 \t Training Loss: 46.809237543741865\n",
      "Epoch 41 \t Batch 860 \t Training Loss: 46.81157800097798\n",
      "Epoch 41 \t Batch 880 \t Training Loss: 46.809232863512904\n",
      "Epoch 41 \t Batch 900 \t Training Loss: 46.778217764960395\n",
      "Epoch 41 \t Batch 20 \t Validation Loss: 35.241200590133666\n",
      "Epoch 41 \t Batch 40 \t Validation Loss: 33.38729784488678\n",
      "Epoch 41 \t Batch 60 \t Validation Loss: 36.6001216729482\n",
      "Epoch 41 \t Batch 80 \t Validation Loss: 35.038779497146606\n",
      "Epoch 41 \t Batch 100 \t Validation Loss: 33.69459171295166\n",
      "Epoch 41 \t Batch 120 \t Validation Loss: 33.031670316060385\n",
      "Epoch 41 \t Batch 140 \t Validation Loss: 32.10575033596584\n",
      "Epoch 41 \t Batch 160 \t Validation Loss: 32.914862734079364\n",
      "Epoch 41 \t Batch 180 \t Validation Loss: 34.86071245935228\n",
      "Epoch 41 \t Batch 200 \t Validation Loss: 35.73588163852692\n",
      "Epoch 41 \t Batch 220 \t Validation Loss: 36.26710277470676\n",
      "Epoch 41 \t Batch 240 \t Validation Loss: 36.168587827682494\n",
      "Epoch 41 \t Batch 260 \t Validation Loss: 37.876627551592314\n",
      "Epoch 41 \t Batch 280 \t Validation Loss: 38.73772301333291\n",
      "Epoch 41 \t Batch 300 \t Validation Loss: 39.05306241671244\n",
      "Epoch 41 \t Batch 320 \t Validation Loss: 39.17350019514561\n",
      "Epoch 41 \t Batch 340 \t Validation Loss: 39.029393288668466\n",
      "Epoch 41 \t Batch 360 \t Validation Loss: 38.69712795416514\n",
      "Epoch 41 \t Batch 380 \t Validation Loss: 38.910458597384\n",
      "Epoch 41 \t Batch 400 \t Validation Loss: 38.65566085100174\n",
      "Epoch 41 \t Batch 420 \t Validation Loss: 38.706434601829166\n",
      "Epoch 41 \t Batch 440 \t Validation Loss: 38.66577287803997\n",
      "Epoch 41 \t Batch 460 \t Validation Loss: 38.961310761907825\n",
      "Epoch 41 \t Batch 480 \t Validation Loss: 39.398248849312466\n",
      "Epoch 41 \t Batch 500 \t Validation Loss: 39.07950890541077\n",
      "Epoch 41 \t Batch 520 \t Validation Loss: 39.0495322062419\n",
      "Epoch 41 \t Batch 540 \t Validation Loss: 38.76749902831183\n",
      "Epoch 41 \t Batch 560 \t Validation Loss: 38.548773266587936\n",
      "Epoch 41 \t Batch 580 \t Validation Loss: 38.32822098403141\n",
      "Epoch 41 \t Batch 600 \t Validation Loss: 38.49652215798696\n",
      "Epoch 41 Training Loss: 46.78896245259625 Validation Loss: 39.078981887210496\n",
      "Epoch 41 completed\n",
      "Epoch 42 \t Batch 20 \t Training Loss: 45.90180931091309\n",
      "Epoch 42 \t Batch 40 \t Training Loss: 45.36592721939087\n",
      "Epoch 42 \t Batch 60 \t Training Loss: 45.76917260487874\n",
      "Epoch 42 \t Batch 80 \t Training Loss: 45.8528350353241\n",
      "Epoch 42 \t Batch 100 \t Training Loss: 46.14980407714844\n",
      "Epoch 42 \t Batch 120 \t Training Loss: 46.20229040781657\n",
      "Epoch 42 \t Batch 140 \t Training Loss: 46.149279022216795\n",
      "Epoch 42 \t Batch 160 \t Training Loss: 46.207027554512024\n",
      "Epoch 42 \t Batch 180 \t Training Loss: 46.12950676812066\n",
      "Epoch 42 \t Batch 200 \t Training Loss: 46.25363046646118\n",
      "Epoch 42 \t Batch 220 \t Training Loss: 46.27412716258656\n",
      "Epoch 42 \t Batch 240 \t Training Loss: 46.358895953496294\n",
      "Epoch 42 \t Batch 260 \t Training Loss: 46.43142631237323\n",
      "Epoch 42 \t Batch 280 \t Training Loss: 46.50703872953142\n",
      "Epoch 42 \t Batch 300 \t Training Loss: 46.59350658416748\n",
      "Epoch 42 \t Batch 320 \t Training Loss: 46.59684380292892\n",
      "Epoch 42 \t Batch 340 \t Training Loss: 46.59306391547708\n",
      "Epoch 42 \t Batch 360 \t Training Loss: 46.54222265879313\n",
      "Epoch 42 \t Batch 380 \t Training Loss: 46.557035235354775\n",
      "Epoch 42 \t Batch 400 \t Training Loss: 46.56755836486816\n",
      "Epoch 42 \t Batch 420 \t Training Loss: 46.59551635015578\n",
      "Epoch 42 \t Batch 440 \t Training Loss: 46.58412557948719\n",
      "Epoch 42 \t Batch 460 \t Training Loss: 46.580609562086025\n",
      "Epoch 42 \t Batch 480 \t Training Loss: 46.57054623762767\n",
      "Epoch 42 \t Batch 500 \t Training Loss: 46.5381588973999\n",
      "Epoch 42 \t Batch 520 \t Training Loss: 46.600099952404314\n",
      "Epoch 42 \t Batch 540 \t Training Loss: 46.58911912706163\n",
      "Epoch 42 \t Batch 560 \t Training Loss: 46.626102563313076\n",
      "Epoch 42 \t Batch 580 \t Training Loss: 46.61339367175924\n",
      "Epoch 42 \t Batch 600 \t Training Loss: 46.58439490000407\n",
      "Epoch 42 \t Batch 620 \t Training Loss: 46.592587015705725\n",
      "Epoch 42 \t Batch 640 \t Training Loss: 46.60317366719246\n",
      "Epoch 42 \t Batch 660 \t Training Loss: 46.59251715920188\n",
      "Epoch 42 \t Batch 680 \t Training Loss: 46.61210160535924\n",
      "Epoch 42 \t Batch 700 \t Training Loss: 46.620788083757674\n",
      "Epoch 42 \t Batch 720 \t Training Loss: 46.65215437677171\n",
      "Epoch 42 \t Batch 740 \t Training Loss: 46.65478067655821\n",
      "Epoch 42 \t Batch 760 \t Training Loss: 46.65497404901605\n",
      "Epoch 42 \t Batch 780 \t Training Loss: 46.67996281843919\n",
      "Epoch 42 \t Batch 800 \t Training Loss: 46.667548661232\n",
      "Epoch 42 \t Batch 820 \t Training Loss: 46.69582372525843\n",
      "Epoch 42 \t Batch 840 \t Training Loss: 46.68727596827916\n",
      "Epoch 42 \t Batch 860 \t Training Loss: 46.7152160067891\n",
      "Epoch 42 \t Batch 880 \t Training Loss: 46.71872981244867\n",
      "Epoch 42 \t Batch 900 \t Training Loss: 46.75494566175673\n",
      "Epoch 42 \t Batch 20 \t Validation Loss: 57.650870132446286\n",
      "Epoch 42 \t Batch 40 \t Validation Loss: 51.772275352478026\n",
      "Epoch 42 \t Batch 60 \t Validation Loss: 54.929347356160484\n",
      "Epoch 42 \t Batch 80 \t Validation Loss: 52.27031059265137\n",
      "Epoch 42 \t Batch 100 \t Validation Loss: 47.51690065383911\n",
      "Epoch 42 \t Batch 120 \t Validation Loss: 44.594372018178305\n",
      "Epoch 42 \t Batch 140 \t Validation Loss: 42.11283025741577\n",
      "Epoch 42 \t Batch 160 \t Validation Loss: 42.095961403846744\n",
      "Epoch 42 \t Batch 180 \t Validation Loss: 44.03301452000936\n",
      "Epoch 42 \t Batch 200 \t Validation Loss: 44.42794268608093\n",
      "Epoch 42 \t Batch 220 \t Validation Loss: 44.660419940948486\n",
      "Epoch 42 \t Batch 240 \t Validation Loss: 44.229074577490486\n",
      "Epoch 42 \t Batch 260 \t Validation Loss: 45.77331272638761\n",
      "Epoch 42 \t Batch 280 \t Validation Loss: 46.35667334965297\n",
      "Epoch 42 \t Batch 300 \t Validation Loss: 46.785813852945964\n",
      "Epoch 42 \t Batch 320 \t Validation Loss: 46.736898329854014\n",
      "Epoch 42 \t Batch 340 \t Validation Loss: 46.230456450406244\n",
      "Epoch 42 \t Batch 360 \t Validation Loss: 45.707304787635806\n",
      "Epoch 42 \t Batch 380 \t Validation Loss: 45.69596900187041\n",
      "Epoch 42 \t Batch 400 \t Validation Loss: 45.07457317590713\n",
      "Epoch 42 \t Batch 420 \t Validation Loss: 44.81236495744614\n",
      "Epoch 42 \t Batch 440 \t Validation Loss: 44.40916563164104\n",
      "Epoch 42 \t Batch 460 \t Validation Loss: 44.50619159574094\n",
      "Epoch 42 \t Batch 480 \t Validation Loss: 44.765621783336\n",
      "Epoch 42 \t Batch 500 \t Validation Loss: 44.29486594581604\n",
      "Epoch 42 \t Batch 520 \t Validation Loss: 44.12107103971334\n",
      "Epoch 42 \t Batch 540 \t Validation Loss: 43.655868751031385\n",
      "Epoch 42 \t Batch 560 \t Validation Loss: 43.285701378754204\n",
      "Epoch 42 \t Batch 580 \t Validation Loss: 42.93637493232201\n",
      "Epoch 42 \t Batch 600 \t Validation Loss: 42.96087349096934\n",
      "Epoch 42 Training Loss: 46.75210701912284 Validation Loss: 43.46592138494764\n",
      "Epoch 42 completed\n",
      "Epoch 43 \t Batch 20 \t Training Loss: 46.69338321685791\n",
      "Epoch 43 \t Batch 40 \t Training Loss: 46.747237014770505\n",
      "Epoch 43 \t Batch 60 \t Training Loss: 46.72314535776774\n",
      "Epoch 43 \t Batch 80 \t Training Loss: 46.27014183998108\n",
      "Epoch 43 \t Batch 100 \t Training Loss: 46.5349361038208\n",
      "Epoch 43 \t Batch 120 \t Training Loss: 46.58738012313843\n",
      "Epoch 43 \t Batch 140 \t Training Loss: 46.5658232825143\n",
      "Epoch 43 \t Batch 160 \t Training Loss: 46.66715414524079\n",
      "Epoch 43 \t Batch 180 \t Training Loss: 46.694304868910045\n",
      "Epoch 43 \t Batch 200 \t Training Loss: 46.82797248840332\n",
      "Epoch 43 \t Batch 220 \t Training Loss: 47.03248805999756\n",
      "Epoch 43 \t Batch 240 \t Training Loss: 47.04585477511088\n",
      "Epoch 43 \t Batch 260 \t Training Loss: 47.128505442692685\n",
      "Epoch 43 \t Batch 280 \t Training Loss: 47.19519410814558\n",
      "Epoch 43 \t Batch 300 \t Training Loss: 47.13652400970459\n",
      "Epoch 43 \t Batch 320 \t Training Loss: 47.09318228960037\n",
      "Epoch 43 \t Batch 340 \t Training Loss: 46.97352398143095\n",
      "Epoch 43 \t Batch 360 \t Training Loss: 46.95856235292223\n",
      "Epoch 43 \t Batch 380 \t Training Loss: 46.9080892663253\n",
      "Epoch 43 \t Batch 400 \t Training Loss: 46.884909620285036\n",
      "Epoch 43 \t Batch 420 \t Training Loss: 46.900555392674036\n",
      "Epoch 43 \t Batch 440 \t Training Loss: 46.89745766032826\n",
      "Epoch 43 \t Batch 460 \t Training Loss: 46.88119148586107\n",
      "Epoch 43 \t Batch 480 \t Training Loss: 46.883373721440634\n",
      "Epoch 43 \t Batch 500 \t Training Loss: 46.85303849792481\n",
      "Epoch 43 \t Batch 520 \t Training Loss: 46.853150081634524\n",
      "Epoch 43 \t Batch 540 \t Training Loss: 46.83511807477033\n",
      "Epoch 43 \t Batch 560 \t Training Loss: 46.779550198146275\n",
      "Epoch 43 \t Batch 580 \t Training Loss: 46.737315026645\n",
      "Epoch 43 \t Batch 600 \t Training Loss: 46.76849412282308\n",
      "Epoch 43 \t Batch 620 \t Training Loss: 46.80477988335394\n",
      "Epoch 43 \t Batch 640 \t Training Loss: 46.807490676641464\n",
      "Epoch 43 \t Batch 660 \t Training Loss: 46.759565474770284\n",
      "Epoch 43 \t Batch 680 \t Training Loss: 46.72463396857766\n",
      "Epoch 43 \t Batch 700 \t Training Loss: 46.753830184936525\n",
      "Epoch 43 \t Batch 720 \t Training Loss: 46.766763597064546\n",
      "Epoch 43 \t Batch 740 \t Training Loss: 46.77777126415356\n",
      "Epoch 43 \t Batch 760 \t Training Loss: 46.77172096152054\n",
      "Epoch 43 \t Batch 780 \t Training Loss: 46.756279881795244\n",
      "Epoch 43 \t Batch 800 \t Training Loss: 46.72508688926697\n",
      "Epoch 43 \t Batch 820 \t Training Loss: 46.72398991468476\n",
      "Epoch 43 \t Batch 840 \t Training Loss: 46.74733811787197\n",
      "Epoch 43 \t Batch 860 \t Training Loss: 46.743941941372185\n",
      "Epoch 43 \t Batch 880 \t Training Loss: 46.717874704707754\n",
      "Epoch 43 \t Batch 900 \t Training Loss: 46.739307958814834\n",
      "Epoch 43 \t Batch 20 \t Validation Loss: 52.59092617034912\n",
      "Epoch 43 \t Batch 40 \t Validation Loss: 46.862452030181885\n",
      "Epoch 43 \t Batch 60 \t Validation Loss: 50.577120049794516\n",
      "Epoch 43 \t Batch 80 \t Validation Loss: 47.82596516609192\n",
      "Epoch 43 \t Batch 100 \t Validation Loss: 43.881138763427735\n",
      "Epoch 43 \t Batch 120 \t Validation Loss: 41.58785072962443\n",
      "Epoch 43 \t Batch 140 \t Validation Loss: 39.52140934807914\n",
      "Epoch 43 \t Batch 160 \t Validation Loss: 39.738037300109866\n",
      "Epoch 43 \t Batch 180 \t Validation Loss: 41.668624591827395\n",
      "Epoch 43 \t Batch 200 \t Validation Loss: 42.210570068359374\n",
      "Epoch 43 \t Batch 220 \t Validation Loss: 42.54040651321411\n",
      "Epoch 43 \t Batch 240 \t Validation Loss: 42.23819004694621\n",
      "Epoch 43 \t Batch 260 \t Validation Loss: 43.86988044518691\n",
      "Epoch 43 \t Batch 280 \t Validation Loss: 44.59114471503666\n",
      "Epoch 43 \t Batch 300 \t Validation Loss: 44.963748699824016\n",
      "Epoch 43 \t Batch 320 \t Validation Loss: 44.965187433362004\n",
      "Epoch 43 \t Batch 340 \t Validation Loss: 44.58791347110973\n",
      "Epoch 43 \t Batch 360 \t Validation Loss: 44.08121482796139\n",
      "Epoch 43 \t Batch 380 \t Validation Loss: 44.13577254947863\n",
      "Epoch 43 \t Batch 400 \t Validation Loss: 43.625370798110964\n",
      "Epoch 43 \t Batch 420 \t Validation Loss: 43.467931879134404\n",
      "Epoch 43 \t Batch 440 \t Validation Loss: 43.207251574776386\n",
      "Epoch 43 \t Batch 460 \t Validation Loss: 43.35261451472407\n",
      "Epoch 43 \t Batch 480 \t Validation Loss: 43.64952574968338\n",
      "Epoch 43 \t Batch 500 \t Validation Loss: 43.24321257019043\n",
      "Epoch 43 \t Batch 520 \t Validation Loss: 43.08803122960604\n",
      "Epoch 43 \t Batch 540 \t Validation Loss: 42.6562208281623\n",
      "Epoch 43 \t Batch 560 \t Validation Loss: 42.32550710950579\n",
      "Epoch 43 \t Batch 580 \t Validation Loss: 41.963700327379954\n",
      "Epoch 43 \t Batch 600 \t Validation Loss: 42.03387951533\n",
      "Epoch 43 Training Loss: 46.72474534378967 Validation Loss: 42.53117230341032\n",
      "Epoch 43 completed\n",
      "Epoch 44 \t Batch 20 \t Training Loss: 46.5916332244873\n",
      "Epoch 44 \t Batch 40 \t Training Loss: 46.5528359413147\n",
      "Epoch 44 \t Batch 60 \t Training Loss: 46.59447663625081\n",
      "Epoch 44 \t Batch 80 \t Training Loss: 46.59577841758728\n",
      "Epoch 44 \t Batch 100 \t Training Loss: 46.68335792541504\n",
      "Epoch 44 \t Batch 120 \t Training Loss: 46.635561307271324\n",
      "Epoch 44 \t Batch 140 \t Training Loss: 46.68000164031982\n",
      "Epoch 44 \t Batch 160 \t Training Loss: 46.49187505245209\n",
      "Epoch 44 \t Batch 180 \t Training Loss: 46.48436453077528\n",
      "Epoch 44 \t Batch 200 \t Training Loss: 46.44783466339111\n",
      "Epoch 44 \t Batch 220 \t Training Loss: 46.411616516113284\n",
      "Epoch 44 \t Batch 240 \t Training Loss: 46.484570344289146\n",
      "Epoch 44 \t Batch 260 \t Training Loss: 46.60373216775747\n",
      "Epoch 44 \t Batch 280 \t Training Loss: 46.57147182737078\n",
      "Epoch 44 \t Batch 300 \t Training Loss: 46.568645286560056\n",
      "Epoch 44 \t Batch 320 \t Training Loss: 46.582041668891904\n",
      "Epoch 44 \t Batch 340 \t Training Loss: 46.70664640314439\n",
      "Epoch 44 \t Batch 360 \t Training Loss: 46.673214234246146\n",
      "Epoch 44 \t Batch 380 \t Training Loss: 46.673853462620784\n",
      "Epoch 44 \t Batch 400 \t Training Loss: 46.69209206581116\n",
      "Epoch 44 \t Batch 420 \t Training Loss: 46.69196608407157\n",
      "Epoch 44 \t Batch 440 \t Training Loss: 46.68580519069325\n",
      "Epoch 44 \t Batch 460 \t Training Loss: 46.73649371603261\n",
      "Epoch 44 \t Batch 480 \t Training Loss: 46.61766510009765\n",
      "Epoch 44 \t Batch 500 \t Training Loss: 46.59717770385742\n",
      "Epoch 44 \t Batch 520 \t Training Loss: 46.5959437590379\n",
      "Epoch 44 \t Batch 540 \t Training Loss: 46.629155469823765\n",
      "Epoch 44 \t Batch 560 \t Training Loss: 46.654031017848425\n",
      "Epoch 44 \t Batch 580 \t Training Loss: 46.68233176921976\n",
      "Epoch 44 \t Batch 600 \t Training Loss: 46.67464324315389\n",
      "Epoch 44 \t Batch 620 \t Training Loss: 46.712872265231226\n",
      "Epoch 44 \t Batch 640 \t Training Loss: 46.67940063476563\n",
      "Epoch 44 \t Batch 660 \t Training Loss: 46.70679139223966\n",
      "Epoch 44 \t Batch 680 \t Training Loss: 46.707437346963324\n",
      "Epoch 44 \t Batch 700 \t Training Loss: 46.69420908791678\n",
      "Epoch 44 \t Batch 720 \t Training Loss: 46.679470252990725\n",
      "Epoch 44 \t Batch 740 \t Training Loss: 46.704592725392935\n",
      "Epoch 44 \t Batch 760 \t Training Loss: 46.70618663085134\n",
      "Epoch 44 \t Batch 780 \t Training Loss: 46.70981800861848\n",
      "Epoch 44 \t Batch 800 \t Training Loss: 46.69361551761627\n",
      "Epoch 44 \t Batch 820 \t Training Loss: 46.693268352601585\n",
      "Epoch 44 \t Batch 840 \t Training Loss: 46.699724851335795\n",
      "Epoch 44 \t Batch 860 \t Training Loss: 46.7069990202438\n",
      "Epoch 44 \t Batch 880 \t Training Loss: 46.70122186053883\n",
      "Epoch 44 \t Batch 900 \t Training Loss: 46.69300200992161\n",
      "Epoch 44 \t Batch 20 \t Validation Loss: 44.14252099990845\n",
      "Epoch 44 \t Batch 40 \t Validation Loss: 40.88337669372559\n",
      "Epoch 44 \t Batch 60 \t Validation Loss: 44.243639596303304\n",
      "Epoch 44 \t Batch 80 \t Validation Loss: 42.312439668178556\n",
      "Epoch 44 \t Batch 100 \t Validation Loss: 39.792913866043094\n",
      "Epoch 44 \t Batch 120 \t Validation Loss: 38.08867834409078\n",
      "Epoch 44 \t Batch 140 \t Validation Loss: 36.42851949419294\n",
      "Epoch 44 \t Batch 160 \t Validation Loss: 36.62725967168808\n",
      "Epoch 44 \t Batch 180 \t Validation Loss: 38.41345027287801\n",
      "Epoch 44 \t Batch 200 \t Validation Loss: 38.931752939224246\n",
      "Epoch 44 \t Batch 220 \t Validation Loss: 39.27323880629106\n",
      "Epoch 44 \t Batch 240 \t Validation Loss: 39.004131790002184\n",
      "Epoch 44 \t Batch 260 \t Validation Loss: 40.62603477331308\n",
      "Epoch 44 \t Batch 280 \t Validation Loss: 41.37613191604614\n",
      "Epoch 44 \t Batch 300 \t Validation Loss: 41.5965119934082\n",
      "Epoch 44 \t Batch 320 \t Validation Loss: 41.58937053382397\n",
      "Epoch 44 \t Batch 340 \t Validation Loss: 41.29941529386184\n",
      "Epoch 44 \t Batch 360 \t Validation Loss: 40.77603019873301\n",
      "Epoch 44 \t Batch 380 \t Validation Loss: 40.83936260373969\n",
      "Epoch 44 \t Batch 400 \t Validation Loss: 40.344858372211455\n",
      "Epoch 44 \t Batch 420 \t Validation Loss: 40.23740084284828\n",
      "Epoch 44 \t Batch 440 \t Validation Loss: 39.9241345687346\n",
      "Epoch 44 \t Batch 460 \t Validation Loss: 40.08053372217261\n",
      "Epoch 44 \t Batch 480 \t Validation Loss: 40.43785251577695\n",
      "Epoch 44 \t Batch 500 \t Validation Loss: 40.048563535690306\n",
      "Epoch 44 \t Batch 520 \t Validation Loss: 39.81478846990145\n",
      "Epoch 44 \t Batch 540 \t Validation Loss: 39.489775816599526\n",
      "Epoch 44 \t Batch 560 \t Validation Loss: 39.25137869630541\n",
      "Epoch 44 \t Batch 580 \t Validation Loss: 39.04710988011853\n",
      "Epoch 44 \t Batch 600 \t Validation Loss: 39.20320727666219\n",
      "Epoch 44 Training Loss: 46.68668240679259 Validation Loss: 39.79235768318176\n",
      "Epoch 44 completed\n",
      "Epoch 45 \t Batch 20 \t Training Loss: 44.83065662384033\n",
      "Epoch 45 \t Batch 40 \t Training Loss: 45.378843212127684\n",
      "Epoch 45 \t Batch 60 \t Training Loss: 45.87772477467855\n",
      "Epoch 45 \t Batch 80 \t Training Loss: 45.78261289596558\n",
      "Epoch 45 \t Batch 100 \t Training Loss: 46.02500991821289\n",
      "Epoch 45 \t Batch 120 \t Training Loss: 45.99773693084717\n",
      "Epoch 45 \t Batch 140 \t Training Loss: 46.19496108463832\n",
      "Epoch 45 \t Batch 160 \t Training Loss: 46.29539856910706\n",
      "Epoch 45 \t Batch 180 \t Training Loss: 46.11486477322049\n",
      "Epoch 45 \t Batch 200 \t Training Loss: 46.15593811035156\n",
      "Epoch 45 \t Batch 220 \t Training Loss: 46.3257249658758\n",
      "Epoch 45 \t Batch 240 \t Training Loss: 46.31836225191752\n",
      "Epoch 45 \t Batch 260 \t Training Loss: 46.28687547536997\n",
      "Epoch 45 \t Batch 280 \t Training Loss: 46.439012799944194\n",
      "Epoch 45 \t Batch 300 \t Training Loss: 46.40005917867025\n",
      "Epoch 45 \t Batch 320 \t Training Loss: 46.47545959949493\n",
      "Epoch 45 \t Batch 340 \t Training Loss: 46.41609991858987\n",
      "Epoch 45 \t Batch 360 \t Training Loss: 46.44713396496243\n",
      "Epoch 45 \t Batch 380 \t Training Loss: 46.50393103549355\n",
      "Epoch 45 \t Batch 400 \t Training Loss: 46.580324001312256\n",
      "Epoch 45 \t Batch 420 \t Training Loss: 46.582036200023836\n",
      "Epoch 45 \t Batch 440 \t Training Loss: 46.62018412676724\n",
      "Epoch 45 \t Batch 460 \t Training Loss: 46.637523327703065\n",
      "Epoch 45 \t Batch 480 \t Training Loss: 46.699925780296326\n",
      "Epoch 45 \t Batch 500 \t Training Loss: 46.6840980758667\n",
      "Epoch 45 \t Batch 520 \t Training Loss: 46.6616067812993\n",
      "Epoch 45 \t Batch 540 \t Training Loss: 46.628768009609644\n",
      "Epoch 45 \t Batch 560 \t Training Loss: 46.6507315499442\n",
      "Epoch 45 \t Batch 580 \t Training Loss: 46.70134437166411\n",
      "Epoch 45 \t Batch 600 \t Training Loss: 46.70446224848429\n",
      "Epoch 45 \t Batch 620 \t Training Loss: 46.7066267382714\n",
      "Epoch 45 \t Batch 640 \t Training Loss: 46.72028522491455\n",
      "Epoch 45 \t Batch 660 \t Training Loss: 46.73443806966146\n",
      "Epoch 45 \t Batch 680 \t Training Loss: 46.72470116895788\n",
      "Epoch 45 \t Batch 700 \t Training Loss: 46.70449113573347\n",
      "Epoch 45 \t Batch 720 \t Training Loss: 46.677791606055365\n",
      "Epoch 45 \t Batch 740 \t Training Loss: 46.66582830274427\n",
      "Epoch 45 \t Batch 760 \t Training Loss: 46.646623445812025\n",
      "Epoch 45 \t Batch 780 \t Training Loss: 46.6547856648763\n",
      "Epoch 45 \t Batch 800 \t Training Loss: 46.65830328941345\n",
      "Epoch 45 \t Batch 820 \t Training Loss: 46.640046668634184\n",
      "Epoch 45 \t Batch 840 \t Training Loss: 46.63558682487125\n",
      "Epoch 45 \t Batch 860 \t Training Loss: 46.67636775526889\n",
      "Epoch 45 \t Batch 880 \t Training Loss: 46.6430459542708\n",
      "Epoch 45 \t Batch 900 \t Training Loss: 46.651536827087405\n",
      "Epoch 45 \t Batch 20 \t Validation Loss: 36.67989978790283\n",
      "Epoch 45 \t Batch 40 \t Validation Loss: 36.15374066829681\n",
      "Epoch 45 \t Batch 60 \t Validation Loss: 38.67346552213033\n",
      "Epoch 45 \t Batch 80 \t Validation Loss: 37.1187026143074\n",
      "Epoch 45 \t Batch 100 \t Validation Loss: 35.44736256599426\n",
      "Epoch 45 \t Batch 120 \t Validation Loss: 34.90958197116852\n",
      "Epoch 45 \t Batch 140 \t Validation Loss: 33.90901995386396\n",
      "Epoch 45 \t Batch 160 \t Validation Loss: 34.7961459338665\n",
      "Epoch 45 \t Batch 180 \t Validation Loss: 37.33234837849935\n",
      "Epoch 45 \t Batch 200 \t Validation Loss: 38.2892782497406\n",
      "Epoch 45 \t Batch 220 \t Validation Loss: 38.98935109918768\n",
      "Epoch 45 \t Batch 240 \t Validation Loss: 38.93919665416082\n",
      "Epoch 45 \t Batch 260 \t Validation Loss: 40.81828923592201\n",
      "Epoch 45 \t Batch 280 \t Validation Loss: 41.7350928408759\n",
      "Epoch 45 \t Batch 300 \t Validation Loss: 42.27239303906759\n",
      "Epoch 45 \t Batch 320 \t Validation Loss: 42.43663301765919\n",
      "Epoch 45 \t Batch 340 \t Validation Loss: 42.16308513809653\n",
      "Epoch 45 \t Batch 360 \t Validation Loss: 41.79707402388255\n",
      "Epoch 45 \t Batch 380 \t Validation Loss: 41.985297885694\n",
      "Epoch 45 \t Batch 400 \t Validation Loss: 41.612798523902896\n",
      "Epoch 45 \t Batch 420 \t Validation Loss: 41.59171833083743\n",
      "Epoch 45 \t Batch 440 \t Validation Loss: 41.462197453325444\n",
      "Epoch 45 \t Batch 460 \t Validation Loss: 41.709882559983626\n",
      "Epoch 45 \t Batch 480 \t Validation Loss: 42.082792804638544\n",
      "Epoch 45 \t Batch 500 \t Validation Loss: 41.72517576026917\n",
      "Epoch 45 \t Batch 520 \t Validation Loss: 41.672785256459164\n",
      "Epoch 45 \t Batch 540 \t Validation Loss: 41.306586816575795\n",
      "Epoch 45 \t Batch 560 \t Validation Loss: 41.02746739387512\n",
      "Epoch 45 \t Batch 580 \t Validation Loss: 40.80338687239022\n",
      "Epoch 45 \t Batch 600 \t Validation Loss: 40.89532581965128\n",
      "Epoch 45 Training Loss: 46.671963773870935 Validation Loss: 41.45520391711941\n",
      "Epoch 45 completed\n",
      "Epoch 46 \t Batch 20 \t Training Loss: 46.27583961486816\n",
      "Epoch 46 \t Batch 40 \t Training Loss: 46.82330112457275\n",
      "Epoch 46 \t Batch 60 \t Training Loss: 46.36012051900228\n",
      "Epoch 46 \t Batch 80 \t Training Loss: 46.30398678779602\n",
      "Epoch 46 \t Batch 100 \t Training Loss: 46.55211284637451\n",
      "Epoch 46 \t Batch 120 \t Training Loss: 46.67144727706909\n",
      "Epoch 46 \t Batch 140 \t Training Loss: 46.690155683244974\n",
      "Epoch 46 \t Batch 160 \t Training Loss: 46.65442972183227\n",
      "Epoch 46 \t Batch 180 \t Training Loss: 46.69769997066922\n",
      "Epoch 46 \t Batch 200 \t Training Loss: 46.6745591545105\n",
      "Epoch 46 \t Batch 220 \t Training Loss: 46.75593050176447\n",
      "Epoch 46 \t Batch 240 \t Training Loss: 46.79147318204244\n",
      "Epoch 46 \t Batch 260 \t Training Loss: 46.84104934105506\n",
      "Epoch 46 \t Batch 280 \t Training Loss: 46.822695868355886\n",
      "Epoch 46 \t Batch 300 \t Training Loss: 46.8142985534668\n",
      "Epoch 46 \t Batch 320 \t Training Loss: 46.79183452129364\n",
      "Epoch 46 \t Batch 340 \t Training Loss: 46.83254878100227\n",
      "Epoch 46 \t Batch 360 \t Training Loss: 46.84828734927707\n",
      "Epoch 46 \t Batch 380 \t Training Loss: 46.859443413583854\n",
      "Epoch 46 \t Batch 400 \t Training Loss: 46.813610315322876\n",
      "Epoch 46 \t Batch 420 \t Training Loss: 46.815231314159576\n",
      "Epoch 46 \t Batch 440 \t Training Loss: 46.75138714530251\n",
      "Epoch 46 \t Batch 460 \t Training Loss: 46.74769695116126\n",
      "Epoch 46 \t Batch 480 \t Training Loss: 46.72999648253123\n",
      "Epoch 46 \t Batch 500 \t Training Loss: 46.70591623687744\n",
      "Epoch 46 \t Batch 520 \t Training Loss: 46.72658115533682\n",
      "Epoch 46 \t Batch 540 \t Training Loss: 46.728959987781664\n",
      "Epoch 46 \t Batch 560 \t Training Loss: 46.72344968659537\n",
      "Epoch 46 \t Batch 580 \t Training Loss: 46.73321024138352\n",
      "Epoch 46 \t Batch 600 \t Training Loss: 46.73942895253499\n",
      "Epoch 46 \t Batch 620 \t Training Loss: 46.74571039753575\n",
      "Epoch 46 \t Batch 640 \t Training Loss: 46.745345717668535\n",
      "Epoch 46 \t Batch 660 \t Training Loss: 46.71575173464689\n",
      "Epoch 46 \t Batch 680 \t Training Loss: 46.725156469906075\n",
      "Epoch 46 \t Batch 700 \t Training Loss: 46.723157871791294\n",
      "Epoch 46 \t Batch 720 \t Training Loss: 46.72434870402018\n",
      "Epoch 46 \t Batch 740 \t Training Loss: 46.735188494501884\n",
      "Epoch 46 \t Batch 760 \t Training Loss: 46.7339448928833\n",
      "Epoch 46 \t Batch 780 \t Training Loss: 46.72568426376734\n",
      "Epoch 46 \t Batch 800 \t Training Loss: 46.73093247413635\n",
      "Epoch 46 \t Batch 820 \t Training Loss: 46.744286062659285\n",
      "Epoch 46 \t Batch 840 \t Training Loss: 46.73553681146531\n",
      "Epoch 46 \t Batch 860 \t Training Loss: 46.706669190872546\n",
      "Epoch 46 \t Batch 880 \t Training Loss: 46.68616355982694\n",
      "Epoch 46 \t Batch 900 \t Training Loss: 46.650045772128635\n",
      "Epoch 46 \t Batch 20 \t Validation Loss: 45.815389442443845\n",
      "Epoch 46 \t Batch 40 \t Validation Loss: 42.72483811378479\n",
      "Epoch 46 \t Batch 60 \t Validation Loss: 45.92522013982137\n",
      "Epoch 46 \t Batch 80 \t Validation Loss: 43.574535489082336\n",
      "Epoch 46 \t Batch 100 \t Validation Loss: 40.686810932159425\n",
      "Epoch 46 \t Batch 120 \t Validation Loss: 39.07373743057251\n",
      "Epoch 46 \t Batch 140 \t Validation Loss: 37.412465831211634\n",
      "Epoch 46 \t Batch 160 \t Validation Loss: 37.73016288280487\n",
      "Epoch 46 \t Batch 180 \t Validation Loss: 39.782777378294206\n",
      "Epoch 46 \t Batch 200 \t Validation Loss: 40.34495939731598\n",
      "Epoch 46 \t Batch 220 \t Validation Loss: 40.78639624769038\n",
      "Epoch 46 \t Batch 240 \t Validation Loss: 40.57460316816966\n",
      "Epoch 46 \t Batch 260 \t Validation Loss: 42.25816102394691\n",
      "Epoch 46 \t Batch 280 \t Validation Loss: 43.02925980772291\n",
      "Epoch 46 \t Batch 300 \t Validation Loss: 43.39301504453023\n",
      "Epoch 46 \t Batch 320 \t Validation Loss: 43.439594647288324\n",
      "Epoch 46 \t Batch 340 \t Validation Loss: 43.08053131944993\n",
      "Epoch 46 \t Batch 360 \t Validation Loss: 42.58589602576362\n",
      "Epoch 46 \t Batch 380 \t Validation Loss: 42.63655375681425\n",
      "Epoch 46 \t Batch 400 \t Validation Loss: 42.08680812120438\n",
      "Epoch 46 \t Batch 420 \t Validation Loss: 41.94991782733372\n",
      "Epoch 46 \t Batch 440 \t Validation Loss: 41.57592148997567\n",
      "Epoch 46 \t Batch 460 \t Validation Loss: 41.70389078181723\n",
      "Epoch 46 \t Batch 480 \t Validation Loss: 42.0177685201168\n",
      "Epoch 46 \t Batch 500 \t Validation Loss: 41.60466452217102\n",
      "Epoch 46 \t Batch 520 \t Validation Loss: 41.39405484566322\n",
      "Epoch 46 \t Batch 540 \t Validation Loss: 40.99824754926893\n",
      "Epoch 46 \t Batch 560 \t Validation Loss: 40.66974098341806\n",
      "Epoch 46 \t Batch 580 \t Validation Loss: 40.370367198154845\n",
      "Epoch 46 \t Batch 600 \t Validation Loss: 40.45823854446411\n",
      "Epoch 46 Training Loss: 46.643005637332166 Validation Loss: 40.982882214831065\n",
      "Epoch 46 completed\n",
      "Epoch 47 \t Batch 20 \t Training Loss: 46.80296459197998\n",
      "Epoch 47 \t Batch 40 \t Training Loss: 46.606595516204834\n",
      "Epoch 47 \t Batch 60 \t Training Loss: 46.75277729034424\n",
      "Epoch 47 \t Batch 80 \t Training Loss: 46.79107975959778\n",
      "Epoch 47 \t Batch 100 \t Training Loss: 46.5825617980957\n",
      "Epoch 47 \t Batch 120 \t Training Loss: 46.64309240976969\n",
      "Epoch 47 \t Batch 140 \t Training Loss: 46.40009182521275\n",
      "Epoch 47 \t Batch 160 \t Training Loss: 46.4184805393219\n",
      "Epoch 47 \t Batch 180 \t Training Loss: 46.55808465745714\n",
      "Epoch 47 \t Batch 200 \t Training Loss: 46.5322699546814\n",
      "Epoch 47 \t Batch 220 \t Training Loss: 46.5675611669367\n",
      "Epoch 47 \t Batch 240 \t Training Loss: 46.61589552561442\n",
      "Epoch 47 \t Batch 260 \t Training Loss: 46.63448682931753\n",
      "Epoch 47 \t Batch 280 \t Training Loss: 46.73285658700126\n",
      "Epoch 47 \t Batch 300 \t Training Loss: 46.675029335021975\n",
      "Epoch 47 \t Batch 320 \t Training Loss: 46.6026936173439\n",
      "Epoch 47 \t Batch 340 \t Training Loss: 46.65288112864775\n",
      "Epoch 47 \t Batch 360 \t Training Loss: 46.6039285129971\n",
      "Epoch 47 \t Batch 380 \t Training Loss: 46.539189539457624\n",
      "Epoch 47 \t Batch 400 \t Training Loss: 46.4475859451294\n",
      "Epoch 47 \t Batch 420 \t Training Loss: 46.4646332967849\n",
      "Epoch 47 \t Batch 440 \t Training Loss: 46.47605517994274\n",
      "Epoch 47 \t Batch 460 \t Training Loss: 46.47774438443391\n",
      "Epoch 47 \t Batch 480 \t Training Loss: 46.453186988830566\n",
      "Epoch 47 \t Batch 500 \t Training Loss: 46.415258422851565\n",
      "Epoch 47 \t Batch 520 \t Training Loss: 46.41874487950252\n",
      "Epoch 47 \t Batch 540 \t Training Loss: 46.400534276609065\n",
      "Epoch 47 \t Batch 560 \t Training Loss: 46.3578845500946\n",
      "Epoch 47 \t Batch 580 \t Training Loss: 46.32212167279474\n",
      "Epoch 47 \t Batch 600 \t Training Loss: 46.34140656789144\n",
      "Epoch 47 \t Batch 620 \t Training Loss: 46.38434171984272\n",
      "Epoch 47 \t Batch 640 \t Training Loss: 46.34961983561516\n",
      "Epoch 47 \t Batch 660 \t Training Loss: 46.34811499046557\n",
      "Epoch 47 \t Batch 680 \t Training Loss: 46.38211328282076\n",
      "Epoch 47 \t Batch 700 \t Training Loss: 46.435754628862654\n",
      "Epoch 47 \t Batch 720 \t Training Loss: 46.42159109645419\n",
      "Epoch 47 \t Batch 740 \t Training Loss: 46.4206527658411\n",
      "Epoch 47 \t Batch 760 \t Training Loss: 46.42717819213867\n",
      "Epoch 47 \t Batch 780 \t Training Loss: 46.47333583831787\n",
      "Epoch 47 \t Batch 800 \t Training Loss: 46.470738224983215\n",
      "Epoch 47 \t Batch 820 \t Training Loss: 46.47302199573052\n",
      "Epoch 47 \t Batch 840 \t Training Loss: 46.474473748888286\n",
      "Epoch 47 \t Batch 860 \t Training Loss: 46.51761925719505\n",
      "Epoch 47 \t Batch 880 \t Training Loss: 46.56645860671997\n",
      "Epoch 47 \t Batch 900 \t Training Loss: 46.59427445729573\n",
      "Epoch 47 \t Batch 20 \t Validation Loss: 43.44737043380737\n",
      "Epoch 47 \t Batch 40 \t Validation Loss: 39.4297189950943\n",
      "Epoch 47 \t Batch 60 \t Validation Loss: 42.56196618080139\n",
      "Epoch 47 \t Batch 80 \t Validation Loss: 40.83967533111572\n",
      "Epoch 47 \t Batch 100 \t Validation Loss: 38.38444570541382\n",
      "Epoch 47 \t Batch 120 \t Validation Loss: 36.879530843098955\n",
      "Epoch 47 \t Batch 140 \t Validation Loss: 35.4211758681706\n",
      "Epoch 47 \t Batch 160 \t Validation Loss: 36.13211984038353\n",
      "Epoch 47 \t Batch 180 \t Validation Loss: 38.759328842163086\n",
      "Epoch 47 \t Batch 200 \t Validation Loss: 39.666154594421386\n",
      "Epoch 47 \t Batch 220 \t Validation Loss: 40.39254189404574\n",
      "Epoch 47 \t Batch 240 \t Validation Loss: 40.378490988413496\n",
      "Epoch 47 \t Batch 260 \t Validation Loss: 42.25316395025987\n",
      "Epoch 47 \t Batch 280 \t Validation Loss: 43.187023980276926\n",
      "Epoch 47 \t Batch 300 \t Validation Loss: 43.75296237945557\n",
      "Epoch 47 \t Batch 320 \t Validation Loss: 43.92776355147362\n",
      "Epoch 47 \t Batch 340 \t Validation Loss: 43.62386063407449\n",
      "Epoch 47 \t Batch 360 \t Validation Loss: 43.17396435207791\n",
      "Epoch 47 \t Batch 380 \t Validation Loss: 43.256862630342184\n",
      "Epoch 47 \t Batch 400 \t Validation Loss: 42.69984341621399\n",
      "Epoch 47 \t Batch 420 \t Validation Loss: 42.58865040824527\n",
      "Epoch 47 \t Batch 440 \t Validation Loss: 42.23663624416698\n",
      "Epoch 47 \t Batch 460 \t Validation Loss: 42.35447305596393\n",
      "Epoch 47 \t Batch 480 \t Validation Loss: 42.67732302347819\n",
      "Epoch 47 \t Batch 500 \t Validation Loss: 42.30086499404907\n",
      "Epoch 47 \t Batch 520 \t Validation Loss: 42.04617476830116\n",
      "Epoch 47 \t Batch 540 \t Validation Loss: 41.61827372091788\n",
      "Epoch 47 \t Batch 560 \t Validation Loss: 41.28188652311053\n",
      "Epoch 47 \t Batch 580 \t Validation Loss: 40.9160004747325\n",
      "Epoch 47 \t Batch 600 \t Validation Loss: 41.00087054570516\n",
      "Epoch 47 Training Loss: 46.59752073048765 Validation Loss: 41.522101792422205\n",
      "Epoch 47 completed\n",
      "Epoch 48 \t Batch 20 \t Training Loss: 45.4809648513794\n",
      "Epoch 48 \t Batch 40 \t Training Loss: 46.143798446655275\n",
      "Epoch 48 \t Batch 60 \t Training Loss: 46.81064701080322\n",
      "Epoch 48 \t Batch 80 \t Training Loss: 47.0104739189148\n",
      "Epoch 48 \t Batch 100 \t Training Loss: 46.70640594482422\n",
      "Epoch 48 \t Batch 120 \t Training Loss: 46.54020643234253\n",
      "Epoch 48 \t Batch 140 \t Training Loss: 46.50289505549839\n",
      "Epoch 48 \t Batch 160 \t Training Loss: 46.65455420017243\n",
      "Epoch 48 \t Batch 180 \t Training Loss: 46.59184345669217\n",
      "Epoch 48 \t Batch 200 \t Training Loss: 46.68099382400513\n",
      "Epoch 48 \t Batch 220 \t Training Loss: 46.51352871981534\n",
      "Epoch 48 \t Batch 240 \t Training Loss: 46.45608016649882\n",
      "Epoch 48 \t Batch 260 \t Training Loss: 46.53466055943416\n",
      "Epoch 48 \t Batch 280 \t Training Loss: 46.62992553710937\n",
      "Epoch 48 \t Batch 300 \t Training Loss: 46.681075223286946\n",
      "Epoch 48 \t Batch 320 \t Training Loss: 46.66807007789612\n",
      "Epoch 48 \t Batch 340 \t Training Loss: 46.601007551305436\n",
      "Epoch 48 \t Batch 360 \t Training Loss: 46.59360819922553\n",
      "Epoch 48 \t Batch 380 \t Training Loss: 46.60129397542853\n",
      "Epoch 48 \t Batch 400 \t Training Loss: 46.59561824798584\n",
      "Epoch 48 \t Batch 420 \t Training Loss: 46.54120889391218\n",
      "Epoch 48 \t Batch 440 \t Training Loss: 46.557178748737684\n",
      "Epoch 48 \t Batch 460 \t Training Loss: 46.496103220400606\n",
      "Epoch 48 \t Batch 480 \t Training Loss: 46.55760669708252\n",
      "Epoch 48 \t Batch 500 \t Training Loss: 46.543280700683596\n",
      "Epoch 48 \t Batch 520 \t Training Loss: 46.55198733990009\n",
      "Epoch 48 \t Batch 540 \t Training Loss: 46.578929837544756\n",
      "Epoch 48 \t Batch 560 \t Training Loss: 46.5327214717865\n",
      "Epoch 48 \t Batch 580 \t Training Loss: 46.54811125130489\n",
      "Epoch 48 \t Batch 600 \t Training Loss: 46.55250863393148\n",
      "Epoch 48 \t Batch 620 \t Training Loss: 46.51575406597507\n",
      "Epoch 48 \t Batch 640 \t Training Loss: 46.51335650086403\n",
      "Epoch 48 \t Batch 660 \t Training Loss: 46.49211642525413\n",
      "Epoch 48 \t Batch 680 \t Training Loss: 46.5007446457358\n",
      "Epoch 48 \t Batch 700 \t Training Loss: 46.51999156951904\n",
      "Epoch 48 \t Batch 720 \t Training Loss: 46.48460918532477\n",
      "Epoch 48 \t Batch 740 \t Training Loss: 46.492179035496065\n",
      "Epoch 48 \t Batch 760 \t Training Loss: 46.46565727936594\n",
      "Epoch 48 \t Batch 780 \t Training Loss: 46.46362387339274\n",
      "Epoch 48 \t Batch 800 \t Training Loss: 46.508702917099\n",
      "Epoch 48 \t Batch 820 \t Training Loss: 46.550568482933976\n",
      "Epoch 48 \t Batch 840 \t Training Loss: 46.56646879741124\n",
      "Epoch 48 \t Batch 860 \t Training Loss: 46.56569335848786\n",
      "Epoch 48 \t Batch 880 \t Training Loss: 46.561275378140536\n",
      "Epoch 48 \t Batch 900 \t Training Loss: 46.58584632025825\n",
      "Epoch 48 \t Batch 20 \t Validation Loss: 34.276483821868894\n",
      "Epoch 48 \t Batch 40 \t Validation Loss: 33.68772881031036\n",
      "Epoch 48 \t Batch 60 \t Validation Loss: 36.09701542854309\n",
      "Epoch 48 \t Batch 80 \t Validation Loss: 34.540096926689145\n",
      "Epoch 48 \t Batch 100 \t Validation Loss: 33.26797737121582\n",
      "Epoch 48 \t Batch 120 \t Validation Loss: 32.683795340855916\n",
      "Epoch 48 \t Batch 140 \t Validation Loss: 31.86136019570487\n",
      "Epoch 48 \t Batch 160 \t Validation Loss: 32.838455653190614\n",
      "Epoch 48 \t Batch 180 \t Validation Loss: 35.423063357671104\n",
      "Epoch 48 \t Batch 200 \t Validation Loss: 36.455181097984315\n",
      "Epoch 48 \t Batch 220 \t Validation Loss: 37.22753099528226\n",
      "Epoch 48 \t Batch 240 \t Validation Loss: 37.30147569974263\n",
      "Epoch 48 \t Batch 260 \t Validation Loss: 39.23887552848229\n",
      "Epoch 48 \t Batch 280 \t Validation Loss: 40.245638731547764\n",
      "Epoch 48 \t Batch 300 \t Validation Loss: 40.769485759735105\n",
      "Epoch 48 \t Batch 320 \t Validation Loss: 40.967956840991974\n",
      "Epoch 48 \t Batch 340 \t Validation Loss: 40.7563408795525\n",
      "Epoch 48 \t Batch 360 \t Validation Loss: 40.365081543392606\n",
      "Epoch 48 \t Batch 380 \t Validation Loss: 40.60100279105337\n",
      "Epoch 48 \t Batch 400 \t Validation Loss: 40.2692733502388\n",
      "Epoch 48 \t Batch 420 \t Validation Loss: 40.27497159639994\n",
      "Epoch 48 \t Batch 440 \t Validation Loss: 40.13842277310111\n",
      "Epoch 48 \t Batch 460 \t Validation Loss: 40.43918330773063\n",
      "Epoch 48 \t Batch 480 \t Validation Loss: 40.82541672984759\n",
      "Epoch 48 \t Batch 500 \t Validation Loss: 40.47347767066955\n",
      "Epoch 48 \t Batch 520 \t Validation Loss: 40.437316256303056\n",
      "Epoch 48 \t Batch 540 \t Validation Loss: 40.111473408451786\n",
      "Epoch 48 \t Batch 560 \t Validation Loss: 39.837567939077104\n",
      "Epoch 48 \t Batch 580 \t Validation Loss: 39.62002649307251\n",
      "Epoch 48 \t Batch 600 \t Validation Loss: 39.747541926701864\n",
      "Epoch 48 Training Loss: 46.57392701509865 Validation Loss: 40.31533246845394\n",
      "Epoch 48 completed\n",
      "Epoch 49 \t Batch 20 \t Training Loss: 45.98337993621826\n",
      "Epoch 49 \t Batch 40 \t Training Loss: 46.221151447296144\n",
      "Epoch 49 \t Batch 60 \t Training Loss: 46.450151379903154\n",
      "Epoch 49 \t Batch 80 \t Training Loss: 46.247733688354494\n",
      "Epoch 49 \t Batch 100 \t Training Loss: 46.25680046081543\n",
      "Epoch 49 \t Batch 120 \t Training Loss: 46.17364104588827\n",
      "Epoch 49 \t Batch 140 \t Training Loss: 45.89990599496024\n",
      "Epoch 49 \t Batch 160 \t Training Loss: 46.0681312084198\n",
      "Epoch 49 \t Batch 180 \t Training Loss: 46.17732516394721\n",
      "Epoch 49 \t Batch 200 \t Training Loss: 46.215525741577146\n",
      "Epoch 49 \t Batch 220 \t Training Loss: 46.158106647838245\n",
      "Epoch 49 \t Batch 240 \t Training Loss: 46.08303526242574\n",
      "Epoch 49 \t Batch 260 \t Training Loss: 46.09063188112699\n",
      "Epoch 49 \t Batch 280 \t Training Loss: 46.16920204162598\n",
      "Epoch 49 \t Batch 300 \t Training Loss: 46.26593012491862\n",
      "Epoch 49 \t Batch 320 \t Training Loss: 46.35702469348907\n",
      "Epoch 49 \t Batch 340 \t Training Loss: 46.47431447646197\n",
      "Epoch 49 \t Batch 360 \t Training Loss: 46.43208491007487\n",
      "Epoch 49 \t Batch 380 \t Training Loss: 46.42598433243601\n",
      "Epoch 49 \t Batch 400 \t Training Loss: 46.4247499370575\n",
      "Epoch 49 \t Batch 420 \t Training Loss: 46.44572267078218\n",
      "Epoch 49 \t Batch 440 \t Training Loss: 46.44261331558228\n",
      "Epoch 49 \t Batch 460 \t Training Loss: 46.458593393408734\n",
      "Epoch 49 \t Batch 480 \t Training Loss: 46.476575605074565\n",
      "Epoch 49 \t Batch 500 \t Training Loss: 46.4904169845581\n",
      "Epoch 49 \t Batch 520 \t Training Loss: 46.5004159120413\n",
      "Epoch 49 \t Batch 540 \t Training Loss: 46.521422923052754\n",
      "Epoch 49 \t Batch 560 \t Training Loss: 46.53461528505598\n",
      "Epoch 49 \t Batch 580 \t Training Loss: 46.54393916952199\n",
      "Epoch 49 \t Batch 600 \t Training Loss: 46.60335838317871\n",
      "Epoch 49 \t Batch 620 \t Training Loss: 46.603721612499605\n",
      "Epoch 49 \t Batch 640 \t Training Loss: 46.57053081989288\n",
      "Epoch 49 \t Batch 660 \t Training Loss: 46.57940279064756\n",
      "Epoch 49 \t Batch 680 \t Training Loss: 46.56451593286851\n",
      "Epoch 49 \t Batch 700 \t Training Loss: 46.557863736833845\n",
      "Epoch 49 \t Batch 720 \t Training Loss: 46.57625669373407\n",
      "Epoch 49 \t Batch 740 \t Training Loss: 46.54838004241119\n",
      "Epoch 49 \t Batch 760 \t Training Loss: 46.5358950464349\n",
      "Epoch 49 \t Batch 780 \t Training Loss: 46.506796196179515\n",
      "Epoch 49 \t Batch 800 \t Training Loss: 46.51614822864533\n",
      "Epoch 49 \t Batch 820 \t Training Loss: 46.549420198580115\n",
      "Epoch 49 \t Batch 840 \t Training Loss: 46.58616216750372\n",
      "Epoch 49 \t Batch 860 \t Training Loss: 46.580950967655625\n",
      "Epoch 49 \t Batch 880 \t Training Loss: 46.57714273279363\n",
      "Epoch 49 \t Batch 900 \t Training Loss: 46.56773773617215\n",
      "Epoch 49 \t Batch 20 \t Validation Loss: 41.95456748008728\n",
      "Epoch 49 \t Batch 40 \t Validation Loss: 38.919375443458556\n",
      "Epoch 49 \t Batch 60 \t Validation Loss: 42.444397656122845\n",
      "Epoch 49 \t Batch 80 \t Validation Loss: 40.646281683444975\n",
      "Epoch 49 \t Batch 100 \t Validation Loss: 38.164518461227416\n",
      "Epoch 49 \t Batch 120 \t Validation Loss: 36.74178111553192\n",
      "Epoch 49 \t Batch 140 \t Validation Loss: 35.32196692058018\n",
      "Epoch 49 \t Batch 160 \t Validation Loss: 35.972902464866635\n",
      "Epoch 49 \t Batch 180 \t Validation Loss: 38.383220492468936\n",
      "Epoch 49 \t Batch 200 \t Validation Loss: 39.189720478057865\n",
      "Epoch 49 \t Batch 220 \t Validation Loss: 39.789380680431016\n",
      "Epoch 49 \t Batch 240 \t Validation Loss: 39.70792758464813\n",
      "Epoch 49 \t Batch 260 \t Validation Loss: 41.46008189274715\n",
      "Epoch 49 \t Batch 280 \t Validation Loss: 42.23098277705056\n",
      "Epoch 49 \t Batch 300 \t Validation Loss: 42.75451404253642\n",
      "Epoch 49 \t Batch 320 \t Validation Loss: 42.882688066363336\n",
      "Epoch 49 \t Batch 340 \t Validation Loss: 42.5760143700768\n",
      "Epoch 49 \t Batch 360 \t Validation Loss: 42.150106369124515\n",
      "Epoch 49 \t Batch 380 \t Validation Loss: 42.198740550091394\n",
      "Epoch 49 \t Batch 400 \t Validation Loss: 41.64679854631424\n",
      "Epoch 49 \t Batch 420 \t Validation Loss: 41.54314638546535\n",
      "Epoch 49 \t Batch 440 \t Validation Loss: 41.16243928345767\n",
      "Epoch 49 \t Batch 460 \t Validation Loss: 41.27768666433251\n",
      "Epoch 49 \t Batch 480 \t Validation Loss: 41.6262248535951\n",
      "Epoch 49 \t Batch 500 \t Validation Loss: 41.25102861213684\n",
      "Epoch 49 \t Batch 520 \t Validation Loss: 40.989267281385565\n",
      "Epoch 49 \t Batch 540 \t Validation Loss: 40.6175171940415\n",
      "Epoch 49 \t Batch 560 \t Validation Loss: 40.34774629899434\n",
      "Epoch 49 \t Batch 580 \t Validation Loss: 40.05380270892176\n",
      "Epoch 49 \t Batch 600 \t Validation Loss: 40.164109886487324\n",
      "Epoch 49 Training Loss: 46.56475660767134 Validation Loss: 40.70934541349287\n",
      "Epoch 49 completed\n",
      "Epoch 50 \t Batch 20 \t Training Loss: 45.63858871459961\n",
      "Epoch 50 \t Batch 40 \t Training Loss: 46.76529588699341\n",
      "Epoch 50 \t Batch 60 \t Training Loss: 45.97087230682373\n",
      "Epoch 50 \t Batch 80 \t Training Loss: 46.07854404449463\n",
      "Epoch 50 \t Batch 100 \t Training Loss: 46.01099006652832\n",
      "Epoch 50 \t Batch 120 \t Training Loss: 46.214995511372884\n",
      "Epoch 50 \t Batch 140 \t Training Loss: 46.03916405269078\n",
      "Epoch 50 \t Batch 160 \t Training Loss: 46.123828649520874\n",
      "Epoch 50 \t Batch 180 \t Training Loss: 46.0979104148017\n",
      "Epoch 50 \t Batch 200 \t Training Loss: 46.18215070724487\n",
      "Epoch 50 \t Batch 220 \t Training Loss: 46.304370776089755\n",
      "Epoch 50 \t Batch 240 \t Training Loss: 46.400647179285684\n",
      "Epoch 50 \t Batch 260 \t Training Loss: 46.385228450481705\n",
      "Epoch 50 \t Batch 280 \t Training Loss: 46.511220291682655\n",
      "Epoch 50 \t Batch 300 \t Training Loss: 46.447010091145835\n",
      "Epoch 50 \t Batch 320 \t Training Loss: 46.51788576841354\n",
      "Epoch 50 \t Batch 340 \t Training Loss: 46.61809513989617\n",
      "Epoch 50 \t Batch 360 \t Training Loss: 46.560982121361626\n",
      "Epoch 50 \t Batch 380 \t Training Loss: 46.59352613750257\n",
      "Epoch 50 \t Batch 400 \t Training Loss: 46.64254884719848\n",
      "Epoch 50 \t Batch 420 \t Training Loss: 46.595428112574986\n",
      "Epoch 50 \t Batch 440 \t Training Loss: 46.59217832738703\n",
      "Epoch 50 \t Batch 460 \t Training Loss: 46.58306132606838\n",
      "Epoch 50 \t Batch 480 \t Training Loss: 46.587234965960185\n",
      "Epoch 50 \t Batch 500 \t Training Loss: 46.64331914520264\n",
      "Epoch 50 \t Batch 520 \t Training Loss: 46.608612735454855\n",
      "Epoch 50 \t Batch 540 \t Training Loss: 46.57207957373725\n",
      "Epoch 50 \t Batch 560 \t Training Loss: 46.54827100890024\n",
      "Epoch 50 \t Batch 580 \t Training Loss: 46.53730431260734\n",
      "Epoch 50 \t Batch 600 \t Training Loss: 46.53903279622396\n",
      "Epoch 50 \t Batch 620 \t Training Loss: 46.56164053024784\n",
      "Epoch 50 \t Batch 640 \t Training Loss: 46.55217274427414\n",
      "Epoch 50 \t Batch 660 \t Training Loss: 46.54484708959406\n",
      "Epoch 50 \t Batch 680 \t Training Loss: 46.54581166996675\n",
      "Epoch 50 \t Batch 700 \t Training Loss: 46.52652756827218\n",
      "Epoch 50 \t Batch 720 \t Training Loss: 46.5649662176768\n",
      "Epoch 50 \t Batch 740 \t Training Loss: 46.54601236291834\n",
      "Epoch 50 \t Batch 760 \t Training Loss: 46.57309759039628\n",
      "Epoch 50 \t Batch 780 \t Training Loss: 46.61992963155111\n",
      "Epoch 50 \t Batch 800 \t Training Loss: 46.60522578716278\n",
      "Epoch 50 \t Batch 820 \t Training Loss: 46.56239726834181\n",
      "Epoch 50 \t Batch 840 \t Training Loss: 46.57624169758388\n",
      "Epoch 50 \t Batch 860 \t Training Loss: 46.530652112739034\n",
      "Epoch 50 \t Batch 880 \t Training Loss: 46.52695814479481\n",
      "Epoch 50 \t Batch 900 \t Training Loss: 46.51641805436876\n",
      "Epoch 50 \t Batch 20 \t Validation Loss: 50.523679161071776\n",
      "Epoch 50 \t Batch 40 \t Validation Loss: 46.28873920440674\n",
      "Epoch 50 \t Batch 60 \t Validation Loss: 50.13619378407796\n",
      "Epoch 50 \t Batch 80 \t Validation Loss: 47.46869821548462\n",
      "Epoch 50 \t Batch 100 \t Validation Loss: 43.9079365158081\n",
      "Epoch 50 \t Batch 120 \t Validation Loss: 41.646727021535234\n",
      "Epoch 50 \t Batch 140 \t Validation Loss: 39.628408323015485\n",
      "Epoch 50 \t Batch 160 \t Validation Loss: 39.81929203271866\n",
      "Epoch 50 \t Batch 180 \t Validation Loss: 42.15343638526069\n",
      "Epoch 50 \t Batch 200 \t Validation Loss: 42.732522850036624\n",
      "Epoch 50 \t Batch 220 \t Validation Loss: 43.26264069297097\n",
      "Epoch 50 \t Batch 240 \t Validation Loss: 43.07266923586528\n",
      "Epoch 50 \t Batch 260 \t Validation Loss: 44.81686921853286\n",
      "Epoch 50 \t Batch 280 \t Validation Loss: 45.58238872119359\n",
      "Epoch 50 \t Batch 300 \t Validation Loss: 46.074427134195965\n",
      "Epoch 50 \t Batch 320 \t Validation Loss: 46.100052654743195\n",
      "Epoch 50 \t Batch 340 \t Validation Loss: 45.650041327756995\n",
      "Epoch 50 \t Batch 360 \t Validation Loss: 45.10032865206401\n",
      "Epoch 50 \t Batch 380 \t Validation Loss: 45.080579541858874\n",
      "Epoch 50 \t Batch 400 \t Validation Loss: 44.36268662929535\n",
      "Epoch 50 \t Batch 420 \t Validation Loss: 44.087275323413664\n",
      "Epoch 50 \t Batch 440 \t Validation Loss: 43.54619569344954\n",
      "Epoch 50 \t Batch 460 \t Validation Loss: 43.581883094621745\n",
      "Epoch 50 \t Batch 480 \t Validation Loss: 43.82111245393753\n",
      "Epoch 50 \t Batch 500 \t Validation Loss: 43.36611106109619\n",
      "Epoch 50 \t Batch 520 \t Validation Loss: 43.02492728233337\n",
      "Epoch 50 \t Batch 540 \t Validation Loss: 42.59518808435511\n",
      "Epoch 50 \t Batch 560 \t Validation Loss: 42.24727329186031\n",
      "Epoch 50 \t Batch 580 \t Validation Loss: 41.84433298439815\n",
      "Epoch 50 \t Batch 600 \t Validation Loss: 41.933272930781044\n",
      "Epoch 50 Training Loss: 46.53109387197048 Validation Loss: 42.434217874105876\n",
      "Epoch 50 completed\n",
      "Epoch 51 \t Batch 20 \t Training Loss: 47.69585266113281\n",
      "Epoch 51 \t Batch 40 \t Training Loss: 46.77193517684937\n",
      "Epoch 51 \t Batch 60 \t Training Loss: 46.50999590555827\n",
      "Epoch 51 \t Batch 80 \t Training Loss: 46.35644521713257\n",
      "Epoch 51 \t Batch 100 \t Training Loss: 46.39868427276611\n",
      "Epoch 51 \t Batch 120 \t Training Loss: 46.33583529790243\n",
      "Epoch 51 \t Batch 140 \t Training Loss: 46.34531247275216\n",
      "Epoch 51 \t Batch 160 \t Training Loss: 46.384553503990176\n",
      "Epoch 51 \t Batch 180 \t Training Loss: 46.473712793986\n",
      "Epoch 51 \t Batch 200 \t Training Loss: 46.440354328155514\n",
      "Epoch 51 \t Batch 220 \t Training Loss: 46.42987532182173\n",
      "Epoch 51 \t Batch 240 \t Training Loss: 46.391283178329466\n",
      "Epoch 51 \t Batch 260 \t Training Loss: 46.363714716984674\n",
      "Epoch 51 \t Batch 280 \t Training Loss: 46.36823835372925\n",
      "Epoch 51 \t Batch 300 \t Training Loss: 46.37549279530843\n",
      "Epoch 51 \t Batch 320 \t Training Loss: 46.33039456605911\n",
      "Epoch 51 \t Batch 340 \t Training Loss: 46.39801911746754\n",
      "Epoch 51 \t Batch 360 \t Training Loss: 46.45158072577583\n",
      "Epoch 51 \t Batch 380 \t Training Loss: 46.48720934014571\n",
      "Epoch 51 \t Batch 400 \t Training Loss: 46.4900271987915\n",
      "Epoch 51 \t Batch 420 \t Training Loss: 46.4973388671875\n",
      "Epoch 51 \t Batch 440 \t Training Loss: 46.483443763039325\n",
      "Epoch 51 \t Batch 460 \t Training Loss: 46.44982532832933\n",
      "Epoch 51 \t Batch 480 \t Training Loss: 46.479701884587605\n",
      "Epoch 51 \t Batch 500 \t Training Loss: 46.426515594482424\n",
      "Epoch 51 \t Batch 520 \t Training Loss: 46.43202514648438\n",
      "Epoch 51 \t Batch 540 \t Training Loss: 46.419781338727034\n",
      "Epoch 51 \t Batch 560 \t Training Loss: 46.383862365995135\n",
      "Epoch 51 \t Batch 580 \t Training Loss: 46.391572715496196\n",
      "Epoch 51 \t Batch 600 \t Training Loss: 46.360215771993005\n",
      "Epoch 51 \t Batch 620 \t Training Loss: 46.36726202810964\n",
      "Epoch 51 \t Batch 640 \t Training Loss: 46.37661915421486\n",
      "Epoch 51 \t Batch 660 \t Training Loss: 46.383625984191895\n",
      "Epoch 51 \t Batch 680 \t Training Loss: 46.43358759599573\n",
      "Epoch 51 \t Batch 700 \t Training Loss: 46.480416968209404\n",
      "Epoch 51 \t Batch 720 \t Training Loss: 46.48101570341322\n",
      "Epoch 51 \t Batch 740 \t Training Loss: 46.47217078337798\n",
      "Epoch 51 \t Batch 760 \t Training Loss: 46.452349948883054\n",
      "Epoch 51 \t Batch 780 \t Training Loss: 46.48674623538286\n",
      "Epoch 51 \t Batch 800 \t Training Loss: 46.47508955001831\n",
      "Epoch 51 \t Batch 820 \t Training Loss: 46.45682459575374\n",
      "Epoch 51 \t Batch 840 \t Training Loss: 46.451759143102734\n",
      "Epoch 51 \t Batch 860 \t Training Loss: 46.47613616322362\n",
      "Epoch 51 \t Batch 880 \t Training Loss: 46.51180058392612\n",
      "Epoch 51 \t Batch 900 \t Training Loss: 46.50851686689589\n",
      "Epoch 51 \t Batch 20 \t Validation Loss: 40.0049768447876\n",
      "Epoch 51 \t Batch 40 \t Validation Loss: 36.335770225524904\n",
      "Epoch 51 \t Batch 60 \t Validation Loss: 39.486365683873494\n",
      "Epoch 51 \t Batch 80 \t Validation Loss: 37.34261162281037\n",
      "Epoch 51 \t Batch 100 \t Validation Loss: 35.3694722366333\n",
      "Epoch 51 \t Batch 120 \t Validation Loss: 34.56459927558899\n",
      "Epoch 51 \t Batch 140 \t Validation Loss: 33.51391614505223\n",
      "Epoch 51 \t Batch 160 \t Validation Loss: 34.45544266104698\n",
      "Epoch 51 \t Batch 180 \t Validation Loss: 37.41291930940416\n",
      "Epoch 51 \t Batch 200 \t Validation Loss: 38.5136519241333\n",
      "Epoch 51 \t Batch 220 \t Validation Loss: 39.396430639787155\n",
      "Epoch 51 \t Batch 240 \t Validation Loss: 39.50827122529348\n",
      "Epoch 51 \t Batch 260 \t Validation Loss: 41.504589524635904\n",
      "Epoch 51 \t Batch 280 \t Validation Loss: 42.498174922806875\n",
      "Epoch 51 \t Batch 300 \t Validation Loss: 43.24515560468038\n",
      "Epoch 51 \t Batch 320 \t Validation Loss: 43.46266455352306\n",
      "Epoch 51 \t Batch 340 \t Validation Loss: 43.183468170727004\n",
      "Epoch 51 \t Batch 360 \t Validation Loss: 42.74214436478085\n",
      "Epoch 51 \t Batch 380 \t Validation Loss: 42.884311896876284\n",
      "Epoch 51 \t Batch 400 \t Validation Loss: 42.335857973098754\n",
      "Epoch 51 \t Batch 420 \t Validation Loss: 42.203548093069166\n",
      "Epoch 51 \t Batch 440 \t Validation Loss: 41.82523108612407\n",
      "Epoch 51 \t Batch 460 \t Validation Loss: 41.97147003878718\n",
      "Epoch 51 \t Batch 480 \t Validation Loss: 42.29385148485502\n",
      "Epoch 51 \t Batch 500 \t Validation Loss: 41.910566030502316\n",
      "Epoch 51 \t Batch 520 \t Validation Loss: 41.67653459218832\n",
      "Epoch 51 \t Batch 540 \t Validation Loss: 41.24855403370327\n",
      "Epoch 51 \t Batch 560 \t Validation Loss: 40.9159836104938\n",
      "Epoch 51 \t Batch 580 \t Validation Loss: 40.542357962706994\n",
      "Epoch 51 \t Batch 600 \t Validation Loss: 40.63947152296702\n",
      "Epoch 51 Training Loss: 46.50690732683454 Validation Loss: 41.16062325161773\n",
      "Epoch 51 completed\n",
      "Epoch 52 \t Batch 20 \t Training Loss: 46.614216804504395\n",
      "Epoch 52 \t Batch 40 \t Training Loss: 46.940930652618405\n",
      "Epoch 52 \t Batch 60 \t Training Loss: 46.745321146647136\n",
      "Epoch 52 \t Batch 80 \t Training Loss: 46.67024393081665\n",
      "Epoch 52 \t Batch 100 \t Training Loss: 46.41166790008545\n",
      "Epoch 52 \t Batch 120 \t Training Loss: 46.567818355560306\n",
      "Epoch 52 \t Batch 140 \t Training Loss: 46.87733064379011\n",
      "Epoch 52 \t Batch 160 \t Training Loss: 47.03797302246094\n",
      "Epoch 52 \t Batch 180 \t Training Loss: 46.98541539510091\n",
      "Epoch 52 \t Batch 200 \t Training Loss: 46.97891920089722\n",
      "Epoch 52 \t Batch 220 \t Training Loss: 47.113956486095084\n",
      "Epoch 52 \t Batch 240 \t Training Loss: 47.013931369781496\n",
      "Epoch 52 \t Batch 260 \t Training Loss: 46.981713779156024\n",
      "Epoch 52 \t Batch 280 \t Training Loss: 46.93482924870082\n",
      "Epoch 52 \t Batch 300 \t Training Loss: 46.92800237019857\n",
      "Epoch 52 \t Batch 320 \t Training Loss: 46.84269086122513\n",
      "Epoch 52 \t Batch 340 \t Training Loss: 46.837856932247384\n",
      "Epoch 52 \t Batch 360 \t Training Loss: 46.79505552715725\n",
      "Epoch 52 \t Batch 380 \t Training Loss: 46.75987575932553\n",
      "Epoch 52 \t Batch 400 \t Training Loss: 46.73558780670166\n",
      "Epoch 52 \t Batch 420 \t Training Loss: 46.729930805024644\n",
      "Epoch 52 \t Batch 440 \t Training Loss: 46.751286619359796\n",
      "Epoch 52 \t Batch 460 \t Training Loss: 46.7835331046063\n",
      "Epoch 52 \t Batch 480 \t Training Loss: 46.71886883576711\n",
      "Epoch 52 \t Batch 500 \t Training Loss: 46.69999965667725\n",
      "Epoch 52 \t Batch 520 \t Training Loss: 46.67508817085853\n",
      "Epoch 52 \t Batch 540 \t Training Loss: 46.64069519042969\n",
      "Epoch 52 \t Batch 560 \t Training Loss: 46.6358162675585\n",
      "Epoch 52 \t Batch 580 \t Training Loss: 46.62862292322619\n",
      "Epoch 52 \t Batch 600 \t Training Loss: 46.606925093332926\n",
      "Epoch 52 \t Batch 620 \t Training Loss: 46.62387398750551\n",
      "Epoch 52 \t Batch 640 \t Training Loss: 46.623408424854276\n",
      "Epoch 52 \t Batch 660 \t Training Loss: 46.62645333030007\n",
      "Epoch 52 \t Batch 680 \t Training Loss: 46.65419152203728\n",
      "Epoch 52 \t Batch 700 \t Training Loss: 46.60032108851841\n",
      "Epoch 52 \t Batch 720 \t Training Loss: 46.60160108672248\n",
      "Epoch 52 \t Batch 740 \t Training Loss: 46.556481356234166\n",
      "Epoch 52 \t Batch 760 \t Training Loss: 46.5869918271115\n",
      "Epoch 52 \t Batch 780 \t Training Loss: 46.57487803728153\n",
      "Epoch 52 \t Batch 800 \t Training Loss: 46.58562298297882\n",
      "Epoch 52 \t Batch 820 \t Training Loss: 46.55570807573272\n",
      "Epoch 52 \t Batch 840 \t Training Loss: 46.55258635112217\n",
      "Epoch 52 \t Batch 860 \t Training Loss: 46.53419491080351\n",
      "Epoch 52 \t Batch 880 \t Training Loss: 46.532656491886485\n",
      "Epoch 52 \t Batch 900 \t Training Loss: 46.51508772532145\n",
      "Epoch 52 \t Batch 20 \t Validation Loss: 47.12063112258911\n",
      "Epoch 52 \t Batch 40 \t Validation Loss: 42.32226650714874\n",
      "Epoch 52 \t Batch 60 \t Validation Loss: 46.17556885083516\n",
      "Epoch 52 \t Batch 80 \t Validation Loss: 44.200748813152316\n",
      "Epoch 52 \t Batch 100 \t Validation Loss: 40.848208074569705\n",
      "Epoch 52 \t Batch 120 \t Validation Loss: 38.96509042580922\n",
      "Epoch 52 \t Batch 140 \t Validation Loss: 37.14690345355442\n",
      "Epoch 52 \t Batch 160 \t Validation Loss: 37.44429385066032\n",
      "Epoch 52 \t Batch 180 \t Validation Loss: 39.50513527658251\n",
      "Epoch 52 \t Batch 200 \t Validation Loss: 40.11655773639679\n",
      "Epoch 52 \t Batch 220 \t Validation Loss: 40.54264038692821\n",
      "Epoch 52 \t Batch 240 \t Validation Loss: 40.34902374347051\n",
      "Epoch 52 \t Batch 260 \t Validation Loss: 41.99298696884742\n",
      "Epoch 52 \t Batch 280 \t Validation Loss: 42.7351236309324\n",
      "Epoch 52 \t Batch 300 \t Validation Loss: 43.153453779220584\n",
      "Epoch 52 \t Batch 320 \t Validation Loss: 43.22080257833004\n",
      "Epoch 52 \t Batch 340 \t Validation Loss: 42.89310704119065\n",
      "Epoch 52 \t Batch 360 \t Validation Loss: 42.385737495952185\n",
      "Epoch 52 \t Batch 380 \t Validation Loss: 42.467072250968535\n",
      "Epoch 52 \t Batch 400 \t Validation Loss: 41.94388636112213\n",
      "Epoch 52 \t Batch 420 \t Validation Loss: 41.84191128412883\n",
      "Epoch 52 \t Batch 440 \t Validation Loss: 41.491656216708094\n",
      "Epoch 52 \t Batch 460 \t Validation Loss: 41.67967571175617\n",
      "Epoch 52 \t Batch 480 \t Validation Loss: 42.00528046687444\n",
      "Epoch 52 \t Batch 500 \t Validation Loss: 41.61557913589478\n",
      "Epoch 52 \t Batch 520 \t Validation Loss: 41.41495278798617\n",
      "Epoch 52 \t Batch 540 \t Validation Loss: 40.99718949353253\n",
      "Epoch 52 \t Batch 560 \t Validation Loss: 40.64140362909862\n",
      "Epoch 52 \t Batch 580 \t Validation Loss: 40.231259002356694\n",
      "Epoch 52 \t Batch 600 \t Validation Loss: 40.3506892379125\n",
      "Epoch 52 Training Loss: 46.48550166272545 Validation Loss: 40.89847351823534\n",
      "Epoch 52 completed\n",
      "Epoch 53 \t Batch 20 \t Training Loss: 47.252037620544435\n",
      "Epoch 53 \t Batch 40 \t Training Loss: 46.97809457778931\n",
      "Epoch 53 \t Batch 60 \t Training Loss: 46.62656021118164\n",
      "Epoch 53 \t Batch 80 \t Training Loss: 46.54984483718872\n",
      "Epoch 53 \t Batch 100 \t Training Loss: 46.47024150848389\n",
      "Epoch 53 \t Batch 120 \t Training Loss: 46.584694894154865\n",
      "Epoch 53 \t Batch 140 \t Training Loss: 46.57877992902483\n",
      "Epoch 53 \t Batch 160 \t Training Loss: 46.4117125749588\n",
      "Epoch 53 \t Batch 180 \t Training Loss: 46.42055954403347\n",
      "Epoch 53 \t Batch 200 \t Training Loss: 46.39019802093506\n",
      "Epoch 53 \t Batch 220 \t Training Loss: 46.53758914253928\n",
      "Epoch 53 \t Batch 240 \t Training Loss: 46.4379282951355\n",
      "Epoch 53 \t Batch 260 \t Training Loss: 46.351457375746506\n",
      "Epoch 53 \t Batch 280 \t Training Loss: 46.37662382125855\n",
      "Epoch 53 \t Batch 300 \t Training Loss: 46.42090447743734\n",
      "Epoch 53 \t Batch 320 \t Training Loss: 46.41114172935486\n",
      "Epoch 53 \t Batch 340 \t Training Loss: 46.47603895523969\n",
      "Epoch 53 \t Batch 360 \t Training Loss: 46.46677513122559\n",
      "Epoch 53 \t Batch 380 \t Training Loss: 46.58759657207288\n",
      "Epoch 53 \t Batch 400 \t Training Loss: 46.59611680030823\n",
      "Epoch 53 \t Batch 420 \t Training Loss: 46.61052837371826\n",
      "Epoch 53 \t Batch 440 \t Training Loss: 46.607957207072864\n",
      "Epoch 53 \t Batch 460 \t Training Loss: 46.58234408005424\n",
      "Epoch 53 \t Batch 480 \t Training Loss: 46.578529222806296\n",
      "Epoch 53 \t Batch 500 \t Training Loss: 46.61397672271728\n",
      "Epoch 53 \t Batch 520 \t Training Loss: 46.61951153094952\n",
      "Epoch 53 \t Batch 540 \t Training Loss: 46.65043200740108\n",
      "Epoch 53 \t Batch 560 \t Training Loss: 46.63396907533918\n",
      "Epoch 53 \t Batch 580 \t Training Loss: 46.581709421092064\n",
      "Epoch 53 \t Batch 600 \t Training Loss: 46.56322226206461\n",
      "Epoch 53 \t Batch 620 \t Training Loss: 46.475619223810014\n",
      "Epoch 53 \t Batch 640 \t Training Loss: 46.50630057454109\n",
      "Epoch 53 \t Batch 660 \t Training Loss: 46.5043201677727\n",
      "Epoch 53 \t Batch 680 \t Training Loss: 46.47061451743631\n",
      "Epoch 53 \t Batch 700 \t Training Loss: 46.46636365073068\n",
      "Epoch 53 \t Batch 720 \t Training Loss: 46.456006564034354\n",
      "Epoch 53 \t Batch 740 \t Training Loss: 46.43558386209849\n",
      "Epoch 53 \t Batch 760 \t Training Loss: 46.44310906058863\n",
      "Epoch 53 \t Batch 780 \t Training Loss: 46.46747031089587\n",
      "Epoch 53 \t Batch 800 \t Training Loss: 46.4906463432312\n",
      "Epoch 53 \t Batch 820 \t Training Loss: 46.4516951351631\n",
      "Epoch 53 \t Batch 840 \t Training Loss: 46.470408612205865\n",
      "Epoch 53 \t Batch 860 \t Training Loss: 46.44737223691718\n",
      "Epoch 53 \t Batch 880 \t Training Loss: 46.44082356799733\n",
      "Epoch 53 \t Batch 900 \t Training Loss: 46.420002496507436\n",
      "Epoch 53 \t Batch 20 \t Validation Loss: 35.11463556289673\n",
      "Epoch 53 \t Batch 40 \t Validation Loss: 33.369442629814145\n",
      "Epoch 53 \t Batch 60 \t Validation Loss: 35.9339676062266\n",
      "Epoch 53 \t Batch 80 \t Validation Loss: 34.25019295215607\n",
      "Epoch 53 \t Batch 100 \t Validation Loss: 32.99985643386841\n",
      "Epoch 53 \t Batch 120 \t Validation Loss: 32.421908680597944\n",
      "Epoch 53 \t Batch 140 \t Validation Loss: 31.655723292487007\n",
      "Epoch 53 \t Batch 160 \t Validation Loss: 32.923626857995984\n",
      "Epoch 53 \t Batch 180 \t Validation Loss: 36.10132575035095\n",
      "Epoch 53 \t Batch 200 \t Validation Loss: 37.31414542675018\n",
      "Epoch 53 \t Batch 220 \t Validation Loss: 38.36425628228621\n",
      "Epoch 53 \t Batch 240 \t Validation Loss: 38.56979245742162\n",
      "Epoch 53 \t Batch 260 \t Validation Loss: 40.66093776409443\n",
      "Epoch 53 \t Batch 280 \t Validation Loss: 41.717816298348566\n",
      "Epoch 53 \t Batch 300 \t Validation Loss: 42.52400400797526\n",
      "Epoch 53 \t Batch 320 \t Validation Loss: 42.80989645123482\n",
      "Epoch 53 \t Batch 340 \t Validation Loss: 42.58094111049876\n",
      "Epoch 53 \t Batch 360 \t Validation Loss: 42.25771522521973\n",
      "Epoch 53 \t Batch 380 \t Validation Loss: 42.42376506956\n",
      "Epoch 53 \t Batch 400 \t Validation Loss: 41.920174152851104\n",
      "Epoch 53 \t Batch 420 \t Validation Loss: 41.81809842472985\n",
      "Epoch 53 \t Batch 440 \t Validation Loss: 41.533792129429905\n",
      "Epoch 53 \t Batch 460 \t Validation Loss: 41.65968115226082\n",
      "Epoch 53 \t Batch 480 \t Validation Loss: 42.020867111285526\n",
      "Epoch 53 \t Batch 500 \t Validation Loss: 41.64665677070618\n",
      "Epoch 53 \t Batch 520 \t Validation Loss: 41.432173010019156\n",
      "Epoch 53 \t Batch 540 \t Validation Loss: 41.02168369999638\n",
      "Epoch 53 \t Batch 560 \t Validation Loss: 40.70521215370723\n",
      "Epoch 53 \t Batch 580 \t Validation Loss: 40.353965788874135\n",
      "Epoch 53 \t Batch 600 \t Validation Loss: 40.436354875564575\n",
      "Epoch 53 Training Loss: 46.42218642822399 Validation Loss: 40.96804922896546\n",
      "Epoch 53 completed\n",
      "Epoch 54 \t Batch 20 \t Training Loss: 46.189627456665036\n",
      "Epoch 54 \t Batch 40 \t Training Loss: 46.80341510772705\n",
      "Epoch 54 \t Batch 60 \t Training Loss: 46.276023928324385\n",
      "Epoch 54 \t Batch 80 \t Training Loss: 45.99467992782593\n",
      "Epoch 54 \t Batch 100 \t Training Loss: 46.19597522735596\n",
      "Epoch 54 \t Batch 120 \t Training Loss: 46.251086393992104\n",
      "Epoch 54 \t Batch 140 \t Training Loss: 46.213511031014576\n",
      "Epoch 54 \t Batch 160 \t Training Loss: 46.100755381584165\n",
      "Epoch 54 \t Batch 180 \t Training Loss: 46.14420632256402\n",
      "Epoch 54 \t Batch 200 \t Training Loss: 46.32916975021362\n",
      "Epoch 54 \t Batch 220 \t Training Loss: 46.331246324019\n",
      "Epoch 54 \t Batch 240 \t Training Loss: 46.2547078927358\n",
      "Epoch 54 \t Batch 260 \t Training Loss: 46.28208481715276\n",
      "Epoch 54 \t Batch 280 \t Training Loss: 46.30793217250279\n",
      "Epoch 54 \t Batch 300 \t Training Loss: 46.26089619954427\n",
      "Epoch 54 \t Batch 320 \t Training Loss: 46.2491512298584\n",
      "Epoch 54 \t Batch 340 \t Training Loss: 46.1764684901518\n",
      "Epoch 54 \t Batch 360 \t Training Loss: 46.11540104548136\n",
      "Epoch 54 \t Batch 380 \t Training Loss: 46.13088184155916\n",
      "Epoch 54 \t Batch 400 \t Training Loss: 46.07594904899597\n",
      "Epoch 54 \t Batch 420 \t Training Loss: 46.13488507952009\n",
      "Epoch 54 \t Batch 440 \t Training Loss: 46.20865240964022\n",
      "Epoch 54 \t Batch 460 \t Training Loss: 46.21810265416684\n",
      "Epoch 54 \t Batch 480 \t Training Loss: 46.237147903442384\n",
      "Epoch 54 \t Batch 500 \t Training Loss: 46.271136009216306\n",
      "Epoch 54 \t Batch 520 \t Training Loss: 46.23403418614314\n",
      "Epoch 54 \t Batch 540 \t Training Loss: 46.301024705392344\n",
      "Epoch 54 \t Batch 560 \t Training Loss: 46.3149245262146\n",
      "Epoch 54 \t Batch 580 \t Training Loss: 46.32264509529903\n",
      "Epoch 54 \t Batch 600 \t Training Loss: 46.31804373423258\n",
      "Epoch 54 \t Batch 620 \t Training Loss: 46.3495474292386\n",
      "Epoch 54 \t Batch 640 \t Training Loss: 46.36335580945015\n",
      "Epoch 54 \t Batch 660 \t Training Loss: 46.380510584513345\n",
      "Epoch 54 \t Batch 680 \t Training Loss: 46.35062102710499\n",
      "Epoch 54 \t Batch 700 \t Training Loss: 46.34002570561\n",
      "Epoch 54 \t Batch 720 \t Training Loss: 46.37388455073039\n",
      "Epoch 54 \t Batch 740 \t Training Loss: 46.37297401943722\n",
      "Epoch 54 \t Batch 760 \t Training Loss: 46.35166193309583\n",
      "Epoch 54 \t Batch 780 \t Training Loss: 46.342909661317485\n",
      "Epoch 54 \t Batch 800 \t Training Loss: 46.356998386383054\n",
      "Epoch 54 \t Batch 820 \t Training Loss: 46.34964553553883\n",
      "Epoch 54 \t Batch 840 \t Training Loss: 46.385878608340306\n",
      "Epoch 54 \t Batch 860 \t Training Loss: 46.3984996263371\n",
      "Epoch 54 \t Batch 880 \t Training Loss: 46.413550641319965\n",
      "Epoch 54 \t Batch 900 \t Training Loss: 46.410427267286515\n",
      "Epoch 54 \t Batch 20 \t Validation Loss: 52.55897493362427\n",
      "Epoch 54 \t Batch 40 \t Validation Loss: 46.897021770477295\n",
      "Epoch 54 \t Batch 60 \t Validation Loss: 50.50733680725098\n",
      "Epoch 54 \t Batch 80 \t Validation Loss: 47.64002465009689\n",
      "Epoch 54 \t Batch 100 \t Validation Loss: 43.680531892776486\n",
      "Epoch 54 \t Batch 120 \t Validation Loss: 41.31522494951884\n",
      "Epoch 54 \t Batch 140 \t Validation Loss: 39.266986056736535\n",
      "Epoch 54 \t Batch 160 \t Validation Loss: 39.345700240135194\n",
      "Epoch 54 \t Batch 180 \t Validation Loss: 41.26778192520142\n",
      "Epoch 54 \t Batch 200 \t Validation Loss: 41.81634706497192\n",
      "Epoch 54 \t Batch 220 \t Validation Loss: 42.08317378651012\n",
      "Epoch 54 \t Batch 240 \t Validation Loss: 41.7470583597819\n",
      "Epoch 54 \t Batch 260 \t Validation Loss: 43.366349546725935\n",
      "Epoch 54 \t Batch 280 \t Validation Loss: 44.04344059058598\n",
      "Epoch 54 \t Batch 300 \t Validation Loss: 44.35937940915426\n",
      "Epoch 54 \t Batch 320 \t Validation Loss: 44.357285597920416\n",
      "Epoch 54 \t Batch 340 \t Validation Loss: 43.979286791296566\n",
      "Epoch 54 \t Batch 360 \t Validation Loss: 43.43313567638397\n",
      "Epoch 54 \t Batch 380 \t Validation Loss: 43.48009172991702\n",
      "Epoch 54 \t Batch 400 \t Validation Loss: 42.965855486392975\n",
      "Epoch 54 \t Batch 420 \t Validation Loss: 42.82758936882019\n",
      "Epoch 54 \t Batch 440 \t Validation Loss: 42.534895699674436\n",
      "Epoch 54 \t Batch 460 \t Validation Loss: 42.68966821380283\n",
      "Epoch 54 \t Batch 480 \t Validation Loss: 42.98195852637291\n",
      "Epoch 54 \t Batch 500 \t Validation Loss: 42.558704381942746\n",
      "Epoch 54 \t Batch 520 \t Validation Loss: 42.41572068104377\n",
      "Epoch 54 \t Batch 540 \t Validation Loss: 41.99388644783585\n",
      "Epoch 54 \t Batch 560 \t Validation Loss: 41.66429232358932\n",
      "Epoch 54 \t Batch 580 \t Validation Loss: 41.33110652956469\n",
      "Epoch 54 \t Batch 600 \t Validation Loss: 41.40863990624746\n",
      "Epoch 54 Training Loss: 46.41328176833291 Validation Loss: 41.94933931084422\n",
      "Epoch 54 completed\n",
      "Epoch 55 \t Batch 20 \t Training Loss: 48.01914939880371\n",
      "Epoch 55 \t Batch 40 \t Training Loss: 47.483332824707034\n",
      "Epoch 55 \t Batch 60 \t Training Loss: 46.47536563873291\n",
      "Epoch 55 \t Batch 80 \t Training Loss: 46.6191237449646\n",
      "Epoch 55 \t Batch 100 \t Training Loss: 46.65403930664063\n",
      "Epoch 55 \t Batch 120 \t Training Loss: 46.50583511988322\n",
      "Epoch 55 \t Batch 140 \t Training Loss: 46.430936622619626\n",
      "Epoch 55 \t Batch 160 \t Training Loss: 46.33424994945526\n",
      "Epoch 55 \t Batch 180 \t Training Loss: 46.212515852186414\n",
      "Epoch 55 \t Batch 200 \t Training Loss: 46.21935417175293\n",
      "Epoch 55 \t Batch 220 \t Training Loss: 46.17932401136918\n",
      "Epoch 55 \t Batch 240 \t Training Loss: 46.27765156428019\n",
      "Epoch 55 \t Batch 260 \t Training Loss: 46.362156281104454\n",
      "Epoch 55 \t Batch 280 \t Training Loss: 46.36152155739921\n",
      "Epoch 55 \t Batch 300 \t Training Loss: 46.37069746653239\n",
      "Epoch 55 \t Batch 320 \t Training Loss: 46.42007069587707\n",
      "Epoch 55 \t Batch 340 \t Training Loss: 46.46304018357221\n",
      "Epoch 55 \t Batch 360 \t Training Loss: 46.486145941416424\n",
      "Epoch 55 \t Batch 380 \t Training Loss: 46.54744955364026\n",
      "Epoch 55 \t Batch 400 \t Training Loss: 46.51711229324341\n",
      "Epoch 55 \t Batch 420 \t Training Loss: 46.502418154761905\n",
      "Epoch 55 \t Batch 440 \t Training Loss: 46.54172049435702\n",
      "Epoch 55 \t Batch 460 \t Training Loss: 46.50395173611848\n",
      "Epoch 55 \t Batch 480 \t Training Loss: 46.56970421473185\n",
      "Epoch 55 \t Batch 500 \t Training Loss: 46.53584039306641\n",
      "Epoch 55 \t Batch 520 \t Training Loss: 46.51284277989314\n",
      "Epoch 55 \t Batch 540 \t Training Loss: 46.47498519332321\n",
      "Epoch 55 \t Batch 560 \t Training Loss: 46.467068733487814\n",
      "Epoch 55 \t Batch 580 \t Training Loss: 46.42942206941802\n",
      "Epoch 55 \t Batch 600 \t Training Loss: 46.49595020294189\n",
      "Epoch 55 \t Batch 620 \t Training Loss: 46.498487084911716\n",
      "Epoch 55 \t Batch 640 \t Training Loss: 46.44565263986588\n",
      "Epoch 55 \t Batch 660 \t Training Loss: 46.461778195699054\n",
      "Epoch 55 \t Batch 680 \t Training Loss: 46.49077196682201\n",
      "Epoch 55 \t Batch 700 \t Training Loss: 46.46937847137451\n",
      "Epoch 55 \t Batch 720 \t Training Loss: 46.44773428175184\n",
      "Epoch 55 \t Batch 740 \t Training Loss: 46.462251348753234\n",
      "Epoch 55 \t Batch 760 \t Training Loss: 46.48196530593069\n",
      "Epoch 55 \t Batch 780 \t Training Loss: 46.49108319893861\n",
      "Epoch 55 \t Batch 800 \t Training Loss: 46.45967139720917\n",
      "Epoch 55 \t Batch 820 \t Training Loss: 46.438832315584506\n",
      "Epoch 55 \t Batch 840 \t Training Loss: 46.43352433613369\n",
      "Epoch 55 \t Batch 860 \t Training Loss: 46.410603425669116\n",
      "Epoch 55 \t Batch 880 \t Training Loss: 46.41731553944675\n",
      "Epoch 55 \t Batch 900 \t Training Loss: 46.393095889621314\n",
      "Epoch 55 \t Batch 20 \t Validation Loss: 46.199959659576415\n",
      "Epoch 55 \t Batch 40 \t Validation Loss: 42.8336124420166\n",
      "Epoch 55 \t Batch 60 \t Validation Loss: 45.93705991109212\n",
      "Epoch 55 \t Batch 80 \t Validation Loss: 43.97881171703339\n",
      "Epoch 55 \t Batch 100 \t Validation Loss: 41.714235076904295\n",
      "Epoch 55 \t Batch 120 \t Validation Loss: 40.40847911834717\n",
      "Epoch 55 \t Batch 140 \t Validation Loss: 38.70444070271083\n",
      "Epoch 55 \t Batch 160 \t Validation Loss: 38.4690686583519\n",
      "Epoch 55 \t Batch 180 \t Validation Loss: 39.33404550552368\n",
      "Epoch 55 \t Batch 200 \t Validation Loss: 39.475707893371585\n",
      "Epoch 55 \t Batch 220 \t Validation Loss: 39.427238368988036\n",
      "Epoch 55 \t Batch 240 \t Validation Loss: 38.830930801232654\n",
      "Epoch 55 \t Batch 260 \t Validation Loss: 40.06955708357004\n",
      "Epoch 55 \t Batch 280 \t Validation Loss: 40.57303575107029\n",
      "Epoch 55 \t Batch 300 \t Validation Loss: 40.52410710016886\n",
      "Epoch 55 \t Batch 320 \t Validation Loss: 40.3908906430006\n",
      "Epoch 55 \t Batch 340 \t Validation Loss: 40.0869186148924\n",
      "Epoch 55 \t Batch 360 \t Validation Loss: 39.58752970430586\n",
      "Epoch 55 \t Batch 380 \t Validation Loss: 39.699241128720736\n",
      "Epoch 55 \t Batch 400 \t Validation Loss: 39.35771589040756\n",
      "Epoch 55 \t Batch 420 \t Validation Loss: 39.394052005949476\n",
      "Epoch 55 \t Batch 440 \t Validation Loss: 39.25699386163191\n",
      "Epoch 55 \t Batch 460 \t Validation Loss: 39.55972841096961\n",
      "Epoch 55 \t Batch 480 \t Validation Loss: 39.92605359951655\n",
      "Epoch 55 \t Batch 500 \t Validation Loss: 39.54856327438355\n",
      "Epoch 55 \t Batch 520 \t Validation Loss: 39.48823178731478\n",
      "Epoch 55 \t Batch 540 \t Validation Loss: 39.16182172916554\n",
      "Epoch 55 \t Batch 560 \t Validation Loss: 38.9069287947246\n",
      "Epoch 55 \t Batch 580 \t Validation Loss: 38.636909116547685\n",
      "Epoch 55 \t Batch 600 \t Validation Loss: 38.78224732716878\n",
      "Epoch 55 Training Loss: 46.4034843320025 Validation Loss: 39.3470385043652\n",
      "Epoch 55 completed\n",
      "Epoch 56 \t Batch 20 \t Training Loss: 46.74334659576416\n",
      "Epoch 56 \t Batch 40 \t Training Loss: 46.77989702224731\n",
      "Epoch 56 \t Batch 60 \t Training Loss: 46.323170852661136\n",
      "Epoch 56 \t Batch 80 \t Training Loss: 46.297262620925906\n",
      "Epoch 56 \t Batch 100 \t Training Loss: 46.15777400970459\n",
      "Epoch 56 \t Batch 120 \t Training Loss: 46.09675448735555\n",
      "Epoch 56 \t Batch 140 \t Training Loss: 46.22163579123361\n",
      "Epoch 56 \t Batch 160 \t Training Loss: 46.354669427871706\n",
      "Epoch 56 \t Batch 180 \t Training Loss: 46.2819753434923\n",
      "Epoch 56 \t Batch 200 \t Training Loss: 46.2696187210083\n",
      "Epoch 56 \t Batch 220 \t Training Loss: 46.1672231500799\n",
      "Epoch 56 \t Batch 240 \t Training Loss: 46.03104960123698\n",
      "Epoch 56 \t Batch 260 \t Training Loss: 45.968567378704364\n",
      "Epoch 56 \t Batch 280 \t Training Loss: 45.84954776763916\n",
      "Epoch 56 \t Batch 300 \t Training Loss: 45.793921915690106\n",
      "Epoch 56 \t Batch 320 \t Training Loss: 45.91274091005325\n",
      "Epoch 56 \t Batch 340 \t Training Loss: 45.9673703698551\n",
      "Epoch 56 \t Batch 360 \t Training Loss: 46.06994809044732\n",
      "Epoch 56 \t Batch 380 \t Training Loss: 46.04023332093892\n",
      "Epoch 56 \t Batch 400 \t Training Loss: 46.084703054428104\n",
      "Epoch 56 \t Batch 420 \t Training Loss: 46.03931120009649\n",
      "Epoch 56 \t Batch 440 \t Training Loss: 46.13613631508567\n",
      "Epoch 56 \t Batch 460 \t Training Loss: 46.12921456046726\n",
      "Epoch 56 \t Batch 480 \t Training Loss: 46.13004879156748\n",
      "Epoch 56 \t Batch 500 \t Training Loss: 46.11297789764404\n",
      "Epoch 56 \t Batch 520 \t Training Loss: 46.078774532904994\n",
      "Epoch 56 \t Batch 540 \t Training Loss: 46.1600061416626\n",
      "Epoch 56 \t Batch 560 \t Training Loss: 46.186849805286954\n",
      "Epoch 56 \t Batch 580 \t Training Loss: 46.196265674459525\n",
      "Epoch 56 \t Batch 600 \t Training Loss: 46.165967750549314\n",
      "Epoch 56 \t Batch 620 \t Training Loss: 46.1405423133604\n",
      "Epoch 56 \t Batch 640 \t Training Loss: 46.181260138750076\n",
      "Epoch 56 \t Batch 660 \t Training Loss: 46.18121837269176\n",
      "Epoch 56 \t Batch 680 \t Training Loss: 46.208951579823214\n",
      "Epoch 56 \t Batch 700 \t Training Loss: 46.233378868103024\n",
      "Epoch 56 \t Batch 720 \t Training Loss: 46.24808335834079\n",
      "Epoch 56 \t Batch 740 \t Training Loss: 46.259912645494616\n",
      "Epoch 56 \t Batch 760 \t Training Loss: 46.27073607193796\n",
      "Epoch 56 \t Batch 780 \t Training Loss: 46.29305421389066\n",
      "Epoch 56 \t Batch 800 \t Training Loss: 46.31463315963745\n",
      "Epoch 56 \t Batch 820 \t Training Loss: 46.2908809243179\n",
      "Epoch 56 \t Batch 840 \t Training Loss: 46.334471666245236\n",
      "Epoch 56 \t Batch 860 \t Training Loss: 46.33345433168633\n",
      "Epoch 56 \t Batch 880 \t Training Loss: 46.34637967456471\n",
      "Epoch 56 \t Batch 900 \t Training Loss: 46.341801456875274\n",
      "Epoch 56 \t Batch 20 \t Validation Loss: 46.18962917327881\n",
      "Epoch 56 \t Batch 40 \t Validation Loss: 41.97586121559143\n",
      "Epoch 56 \t Batch 60 \t Validation Loss: 44.8121018409729\n",
      "Epoch 56 \t Batch 80 \t Validation Loss: 42.67100667953491\n",
      "Epoch 56 \t Batch 100 \t Validation Loss: 39.71527675628662\n",
      "Epoch 56 \t Batch 120 \t Validation Loss: 38.24352973302205\n",
      "Epoch 56 \t Batch 140 \t Validation Loss: 36.746585382734025\n",
      "Epoch 56 \t Batch 160 \t Validation Loss: 37.59100339412689\n",
      "Epoch 56 \t Batch 180 \t Validation Loss: 40.20725621647305\n",
      "Epoch 56 \t Batch 200 \t Validation Loss: 41.15492796421051\n",
      "Epoch 56 \t Batch 220 \t Validation Loss: 41.836455540223554\n",
      "Epoch 56 \t Batch 240 \t Validation Loss: 41.74959989786148\n",
      "Epoch 56 \t Batch 260 \t Validation Loss: 43.60154623618493\n",
      "Epoch 56 \t Batch 280 \t Validation Loss: 44.44749850205013\n",
      "Epoch 56 \t Batch 300 \t Validation Loss: 45.107656958897905\n",
      "Epoch 56 \t Batch 320 \t Validation Loss: 45.299705755710605\n",
      "Epoch 56 \t Batch 340 \t Validation Loss: 44.97791295332067\n",
      "Epoch 56 \t Batch 360 \t Validation Loss: 44.643293698628746\n",
      "Epoch 56 \t Batch 380 \t Validation Loss: 44.808951034043965\n",
      "Epoch 56 \t Batch 400 \t Validation Loss: 44.41928099870682\n",
      "Epoch 56 \t Batch 420 \t Validation Loss: 44.332735731488185\n",
      "Epoch 56 \t Batch 440 \t Validation Loss: 44.22769391536713\n",
      "Epoch 56 \t Batch 460 \t Validation Loss: 44.40693619976873\n",
      "Epoch 56 \t Batch 480 \t Validation Loss: 44.748212426900864\n",
      "Epoch 56 \t Batch 500 \t Validation Loss: 44.35380380821228\n",
      "Epoch 56 \t Batch 520 \t Validation Loss: 44.323164593256436\n",
      "Epoch 56 \t Batch 540 \t Validation Loss: 43.85419819266708\n",
      "Epoch 56 \t Batch 560 \t Validation Loss: 43.49395875419889\n",
      "Epoch 56 \t Batch 580 \t Validation Loss: 43.15297223617291\n",
      "Epoch 56 \t Batch 600 \t Validation Loss: 43.17101403395335\n",
      "Epoch 56 Training Loss: 46.344064685484575 Validation Loss: 43.67678298733451\n",
      "Epoch 56 completed\n",
      "Epoch 57 \t Batch 20 \t Training Loss: 45.47026023864746\n",
      "Epoch 57 \t Batch 40 \t Training Loss: 46.1008638381958\n",
      "Epoch 57 \t Batch 60 \t Training Loss: 45.98223082224528\n",
      "Epoch 57 \t Batch 80 \t Training Loss: 45.84049687385559\n",
      "Epoch 57 \t Batch 100 \t Training Loss: 45.612005920410155\n",
      "Epoch 57 \t Batch 120 \t Training Loss: 45.58524700800578\n",
      "Epoch 57 \t Batch 140 \t Training Loss: 45.85131435394287\n",
      "Epoch 57 \t Batch 160 \t Training Loss: 45.81427817344665\n",
      "Epoch 57 \t Batch 180 \t Training Loss: 45.66022353702121\n",
      "Epoch 57 \t Batch 200 \t Training Loss: 45.721487808227536\n",
      "Epoch 57 \t Batch 220 \t Training Loss: 45.89903534975919\n",
      "Epoch 57 \t Batch 240 \t Training Loss: 45.914148791631064\n",
      "Epoch 57 \t Batch 260 \t Training Loss: 45.98249262296236\n",
      "Epoch 57 \t Batch 280 \t Training Loss: 45.961070905412946\n",
      "Epoch 57 \t Batch 300 \t Training Loss: 46.0450275293986\n",
      "Epoch 57 \t Batch 320 \t Training Loss: 46.1906179189682\n",
      "Epoch 57 \t Batch 340 \t Training Loss: 46.10890360439525\n",
      "Epoch 57 \t Batch 360 \t Training Loss: 46.13099552790324\n",
      "Epoch 57 \t Batch 380 \t Training Loss: 46.203352546691896\n",
      "Epoch 57 \t Batch 400 \t Training Loss: 46.282954902648925\n",
      "Epoch 57 \t Batch 420 \t Training Loss: 46.238102204459054\n",
      "Epoch 57 \t Batch 440 \t Training Loss: 46.25362919894132\n",
      "Epoch 57 \t Batch 460 \t Training Loss: 46.230275958517325\n",
      "Epoch 57 \t Batch 480 \t Training Loss: 46.21751322746277\n",
      "Epoch 57 \t Batch 500 \t Training Loss: 46.19677082061767\n",
      "Epoch 57 \t Batch 520 \t Training Loss: 46.220683992826025\n",
      "Epoch 57 \t Batch 540 \t Training Loss: 46.216147670039426\n",
      "Epoch 57 \t Batch 560 \t Training Loss: 46.20742107118879\n",
      "Epoch 57 \t Batch 580 \t Training Loss: 46.23127068486707\n",
      "Epoch 57 \t Batch 600 \t Training Loss: 46.18222689946492\n",
      "Epoch 57 \t Batch 620 \t Training Loss: 46.25666397463891\n",
      "Epoch 57 \t Batch 640 \t Training Loss: 46.26735312342644\n",
      "Epoch 57 \t Batch 660 \t Training Loss: 46.248027847752425\n",
      "Epoch 57 \t Batch 680 \t Training Loss: 46.26540764079375\n",
      "Epoch 57 \t Batch 700 \t Training Loss: 46.25642109462193\n",
      "Epoch 57 \t Batch 720 \t Training Loss: 46.27302782270643\n",
      "Epoch 57 \t Batch 740 \t Training Loss: 46.297439992750014\n",
      "Epoch 57 \t Batch 760 \t Training Loss: 46.27350758502358\n",
      "Epoch 57 \t Batch 780 \t Training Loss: 46.25850447630271\n",
      "Epoch 57 \t Batch 800 \t Training Loss: 46.27824695587158\n",
      "Epoch 57 \t Batch 820 \t Training Loss: 46.301572627556034\n",
      "Epoch 57 \t Batch 840 \t Training Loss: 46.313969244275775\n",
      "Epoch 57 \t Batch 860 \t Training Loss: 46.28427304556203\n",
      "Epoch 57 \t Batch 880 \t Training Loss: 46.287088420174335\n",
      "Epoch 57 \t Batch 900 \t Training Loss: 46.31384957631429\n",
      "Epoch 57 \t Batch 20 \t Validation Loss: 51.081833076477054\n",
      "Epoch 57 \t Batch 40 \t Validation Loss: 45.35218999385834\n",
      "Epoch 57 \t Batch 60 \t Validation Loss: 48.88743275006612\n",
      "Epoch 57 \t Batch 80 \t Validation Loss: 46.80108126401901\n",
      "Epoch 57 \t Batch 100 \t Validation Loss: 43.58849242210388\n",
      "Epoch 57 \t Batch 120 \t Validation Loss: 41.36649275620778\n",
      "Epoch 57 \t Batch 140 \t Validation Loss: 39.326065506253926\n",
      "Epoch 57 \t Batch 160 \t Validation Loss: 39.360506516695025\n",
      "Epoch 57 \t Batch 180 \t Validation Loss: 41.247198306189645\n",
      "Epoch 57 \t Batch 200 \t Validation Loss: 41.67182253837586\n",
      "Epoch 57 \t Batch 220 \t Validation Loss: 41.944548182054\n",
      "Epoch 57 \t Batch 240 \t Validation Loss: 41.6073141058286\n",
      "Epoch 57 \t Batch 260 \t Validation Loss: 43.198279373462384\n",
      "Epoch 57 \t Batch 280 \t Validation Loss: 43.87150973592486\n",
      "Epoch 57 \t Batch 300 \t Validation Loss: 44.19982201894124\n",
      "Epoch 57 \t Batch 320 \t Validation Loss: 44.18500607013702\n",
      "Epoch 57 \t Batch 340 \t Validation Loss: 43.81953444761388\n",
      "Epoch 57 \t Batch 360 \t Validation Loss: 43.27697390980191\n",
      "Epoch 57 \t Batch 380 \t Validation Loss: 43.24513044608267\n",
      "Epoch 57 \t Batch 400 \t Validation Loss: 42.62478708982468\n",
      "Epoch 57 \t Batch 420 \t Validation Loss: 42.45085581597828\n",
      "Epoch 57 \t Batch 440 \t Validation Loss: 41.99074264222925\n",
      "Epoch 57 \t Batch 460 \t Validation Loss: 42.05473048790641\n",
      "Epoch 57 \t Batch 480 \t Validation Loss: 42.361535944541295\n",
      "Epoch 57 \t Batch 500 \t Validation Loss: 41.94867189216614\n",
      "Epoch 57 \t Batch 520 \t Validation Loss: 41.60599860961621\n",
      "Epoch 57 \t Batch 540 \t Validation Loss: 41.22077152993944\n",
      "Epoch 57 \t Batch 560 \t Validation Loss: 40.92086262192045\n",
      "Epoch 57 \t Batch 580 \t Validation Loss: 40.578067054419684\n",
      "Epoch 57 \t Batch 600 \t Validation Loss: 40.708546655972796\n",
      "Epoch 57 Training Loss: 46.33245242668992 Validation Loss: 41.23731143908067\n",
      "Epoch 57 completed\n",
      "Epoch 58 \t Batch 20 \t Training Loss: 47.29563236236572\n",
      "Epoch 58 \t Batch 40 \t Training Loss: 46.21522960662842\n",
      "Epoch 58 \t Batch 60 \t Training Loss: 46.16162624359131\n",
      "Epoch 58 \t Batch 80 \t Training Loss: 45.654793691635135\n",
      "Epoch 58 \t Batch 100 \t Training Loss: 45.93671810150146\n",
      "Epoch 58 \t Batch 120 \t Training Loss: 45.984975051879886\n",
      "Epoch 58 \t Batch 140 \t Training Loss: 45.90826211656843\n",
      "Epoch 58 \t Batch 160 \t Training Loss: 45.880047988891604\n",
      "Epoch 58 \t Batch 180 \t Training Loss: 45.91046911875407\n",
      "Epoch 58 \t Batch 200 \t Training Loss: 45.94412721633911\n",
      "Epoch 58 \t Batch 220 \t Training Loss: 45.917224901372734\n",
      "Epoch 58 \t Batch 240 \t Training Loss: 45.90580515861511\n",
      "Epoch 58 \t Batch 260 \t Training Loss: 45.98286482004019\n",
      "Epoch 58 \t Batch 280 \t Training Loss: 46.00526520865304\n",
      "Epoch 58 \t Batch 300 \t Training Loss: 46.13382442474365\n",
      "Epoch 58 \t Batch 320 \t Training Loss: 46.15998133420944\n",
      "Epoch 58 \t Batch 340 \t Training Loss: 46.08820314968334\n",
      "Epoch 58 \t Batch 360 \t Training Loss: 46.11642713546753\n",
      "Epoch 58 \t Batch 380 \t Training Loss: 46.04707726930317\n",
      "Epoch 58 \t Batch 400 \t Training Loss: 46.04985418319702\n",
      "Epoch 58 \t Batch 420 \t Training Loss: 46.06631527855283\n",
      "Epoch 58 \t Batch 440 \t Training Loss: 46.142321491241454\n",
      "Epoch 58 \t Batch 460 \t Training Loss: 46.10078111731488\n",
      "Epoch 58 \t Batch 480 \t Training Loss: 46.063758579889935\n",
      "Epoch 58 \t Batch 500 \t Training Loss: 46.10219009399414\n",
      "Epoch 58 \t Batch 520 \t Training Loss: 46.15375738877516\n",
      "Epoch 58 \t Batch 540 \t Training Loss: 46.17874972731979\n",
      "Epoch 58 \t Batch 560 \t Training Loss: 46.1598532812936\n",
      "Epoch 58 \t Batch 580 \t Training Loss: 46.20288593029154\n",
      "Epoch 58 \t Batch 600 \t Training Loss: 46.26860808054606\n",
      "Epoch 58 \t Batch 620 \t Training Loss: 46.29226343708654\n",
      "Epoch 58 \t Batch 640 \t Training Loss: 46.27196272611618\n",
      "Epoch 58 \t Batch 660 \t Training Loss: 46.29916945948745\n",
      "Epoch 58 \t Batch 680 \t Training Loss: 46.295089800217575\n",
      "Epoch 58 \t Batch 700 \t Training Loss: 46.28012394496373\n",
      "Epoch 58 \t Batch 720 \t Training Loss: 46.303776772816974\n",
      "Epoch 58 \t Batch 740 \t Training Loss: 46.32513998907966\n",
      "Epoch 58 \t Batch 760 \t Training Loss: 46.29151546076724\n",
      "Epoch 58 \t Batch 780 \t Training Loss: 46.29721817603478\n",
      "Epoch 58 \t Batch 800 \t Training Loss: 46.285537104606625\n",
      "Epoch 58 \t Batch 820 \t Training Loss: 46.281494042931534\n",
      "Epoch 58 \t Batch 840 \t Training Loss: 46.26279613858178\n",
      "Epoch 58 \t Batch 860 \t Training Loss: 46.26434462347696\n",
      "Epoch 58 \t Batch 880 \t Training Loss: 46.26898456920277\n",
      "Epoch 58 \t Batch 900 \t Training Loss: 46.26680279201931\n",
      "Epoch 58 \t Batch 20 \t Validation Loss: 53.158564472198485\n",
      "Epoch 58 \t Batch 40 \t Validation Loss: 46.499070453643796\n",
      "Epoch 58 \t Batch 60 \t Validation Loss: 51.18833875656128\n",
      "Epoch 58 \t Batch 80 \t Validation Loss: 48.344465661048886\n",
      "Epoch 58 \t Batch 100 \t Validation Loss: 44.6326563835144\n",
      "Epoch 58 \t Batch 120 \t Validation Loss: 42.34001951217651\n",
      "Epoch 58 \t Batch 140 \t Validation Loss: 40.28641857419695\n",
      "Epoch 58 \t Batch 160 \t Validation Loss: 40.91111917495728\n",
      "Epoch 58 \t Batch 180 \t Validation Loss: 42.87794404029846\n",
      "Epoch 58 \t Batch 200 \t Validation Loss: 43.60559390544891\n",
      "Epoch 58 \t Batch 220 \t Validation Loss: 43.89527936848727\n",
      "Epoch 58 \t Batch 240 \t Validation Loss: 43.48728711605072\n",
      "Epoch 58 \t Batch 260 \t Validation Loss: 45.131684772784894\n",
      "Epoch 58 \t Batch 280 \t Validation Loss: 45.83261966705322\n",
      "Epoch 58 \t Batch 300 \t Validation Loss: 46.301280403137206\n",
      "Epoch 58 \t Batch 320 \t Validation Loss: 46.37155303955078\n",
      "Epoch 58 \t Batch 340 \t Validation Loss: 45.95913121840533\n",
      "Epoch 58 \t Batch 360 \t Validation Loss: 45.64882283740573\n",
      "Epoch 58 \t Batch 380 \t Validation Loss: 45.92678509260479\n",
      "Epoch 58 \t Batch 400 \t Validation Loss: 45.66528408050537\n",
      "Epoch 58 \t Batch 420 \t Validation Loss: 45.6490649109795\n",
      "Epoch 58 \t Batch 440 \t Validation Loss: 45.8322215015238\n",
      "Epoch 58 \t Batch 460 \t Validation Loss: 46.28152850814487\n",
      "Epoch 58 \t Batch 480 \t Validation Loss: 46.62909128467242\n",
      "Epoch 58 \t Batch 500 \t Validation Loss: 46.23617700004578\n",
      "Epoch 58 \t Batch 520 \t Validation Loss: 46.50326996399806\n",
      "Epoch 58 \t Batch 540 \t Validation Loss: 46.022009494569566\n",
      "Epoch 58 \t Batch 560 \t Validation Loss: 45.62809504611152\n",
      "Epoch 58 \t Batch 580 \t Validation Loss: 45.300947384998715\n",
      "Epoch 58 \t Batch 600 \t Validation Loss: 45.27777707576752\n",
      "Epoch 58 Training Loss: 46.30585082042698 Validation Loss: 45.74289526722648\n",
      "Epoch 58 completed\n",
      "Epoch 59 \t Batch 20 \t Training Loss: 46.34436225891113\n",
      "Epoch 59 \t Batch 40 \t Training Loss: 46.125489234924316\n",
      "Epoch 59 \t Batch 60 \t Training Loss: 46.19488995869954\n",
      "Epoch 59 \t Batch 80 \t Training Loss: 46.08988494873047\n",
      "Epoch 59 \t Batch 100 \t Training Loss: 46.32055973052979\n",
      "Epoch 59 \t Batch 120 \t Training Loss: 46.374758052825925\n",
      "Epoch 59 \t Batch 140 \t Training Loss: 46.19103783198765\n",
      "Epoch 59 \t Batch 160 \t Training Loss: 46.2800897359848\n",
      "Epoch 59 \t Batch 180 \t Training Loss: 46.36610647837321\n",
      "Epoch 59 \t Batch 200 \t Training Loss: 46.30496202468872\n",
      "Epoch 59 \t Batch 220 \t Training Loss: 46.19794451973655\n",
      "Epoch 59 \t Batch 240 \t Training Loss: 46.20412378311157\n",
      "Epoch 59 \t Batch 260 \t Training Loss: 46.19250561640813\n",
      "Epoch 59 \t Batch 280 \t Training Loss: 46.17083821977888\n",
      "Epoch 59 \t Batch 300 \t Training Loss: 46.174935760498045\n",
      "Epoch 59 \t Batch 320 \t Training Loss: 46.29514993429184\n",
      "Epoch 59 \t Batch 340 \t Training Loss: 46.31151377734016\n",
      "Epoch 59 \t Batch 360 \t Training Loss: 46.31422811084323\n",
      "Epoch 59 \t Batch 380 \t Training Loss: 46.364592190792685\n",
      "Epoch 59 \t Batch 400 \t Training Loss: 46.38261574745178\n",
      "Epoch 59 \t Batch 420 \t Training Loss: 46.4095305397397\n",
      "Epoch 59 \t Batch 440 \t Training Loss: 46.44515211798928\n",
      "Epoch 59 \t Batch 460 \t Training Loss: 46.46864229285199\n",
      "Epoch 59 \t Batch 480 \t Training Loss: 46.46601157983144\n",
      "Epoch 59 \t Batch 500 \t Training Loss: 46.395965637207034\n",
      "Epoch 59 \t Batch 520 \t Training Loss: 46.40144034165603\n",
      "Epoch 59 \t Batch 540 \t Training Loss: 46.41081422170003\n",
      "Epoch 59 \t Batch 560 \t Training Loss: 46.430865267344885\n",
      "Epoch 59 \t Batch 580 \t Training Loss: 46.449620338966106\n",
      "Epoch 59 \t Batch 600 \t Training Loss: 46.44749059041341\n",
      "Epoch 59 \t Batch 620 \t Training Loss: 46.47759775961599\n",
      "Epoch 59 \t Batch 640 \t Training Loss: 46.397605150938034\n",
      "Epoch 59 \t Batch 660 \t Training Loss: 46.33695982152766\n",
      "Epoch 59 \t Batch 680 \t Training Loss: 46.36039830937105\n",
      "Epoch 59 \t Batch 700 \t Training Loss: 46.34755604880197\n",
      "Epoch 59 \t Batch 720 \t Training Loss: 46.34098012182448\n",
      "Epoch 59 \t Batch 740 \t Training Loss: 46.33479052363215\n",
      "Epoch 59 \t Batch 760 \t Training Loss: 46.350283075633804\n",
      "Epoch 59 \t Batch 780 \t Training Loss: 46.310450940254405\n",
      "Epoch 59 \t Batch 800 \t Training Loss: 46.32329914569855\n",
      "Epoch 59 \t Batch 820 \t Training Loss: 46.30129020039628\n",
      "Epoch 59 \t Batch 840 \t Training Loss: 46.324899637131466\n",
      "Epoch 59 \t Batch 860 \t Training Loss: 46.33272252193717\n",
      "Epoch 59 \t Batch 880 \t Training Loss: 46.3054847760634\n",
      "Epoch 59 \t Batch 900 \t Training Loss: 46.31697054545084\n",
      "Epoch 59 \t Batch 20 \t Validation Loss: 48.182603645324704\n",
      "Epoch 59 \t Batch 40 \t Validation Loss: 43.347976493835446\n",
      "Epoch 59 \t Batch 60 \t Validation Loss: 46.84030284881592\n",
      "Epoch 59 \t Batch 80 \t Validation Loss: 44.837668204307555\n",
      "Epoch 59 \t Batch 100 \t Validation Loss: 41.90821315765381\n",
      "Epoch 59 \t Batch 120 \t Validation Loss: 40.1346156279246\n",
      "Epoch 59 \t Batch 140 \t Validation Loss: 38.35006508146014\n",
      "Epoch 59 \t Batch 160 \t Validation Loss: 38.543835079669954\n",
      "Epoch 59 \t Batch 180 \t Validation Loss: 40.60002368291219\n",
      "Epoch 59 \t Batch 200 \t Validation Loss: 41.13350593566894\n",
      "Epoch 59 \t Batch 220 \t Validation Loss: 41.547129995172675\n",
      "Epoch 59 \t Batch 240 \t Validation Loss: 41.29397337039312\n",
      "Epoch 59 \t Batch 260 \t Validation Loss: 42.95446511782133\n",
      "Epoch 59 \t Batch 280 \t Validation Loss: 43.66011840956552\n",
      "Epoch 59 \t Batch 300 \t Validation Loss: 44.05301366170247\n",
      "Epoch 59 \t Batch 320 \t Validation Loss: 44.06666169166565\n",
      "Epoch 59 \t Batch 340 \t Validation Loss: 43.69501741072711\n",
      "Epoch 59 \t Batch 360 \t Validation Loss: 43.16588092645009\n",
      "Epoch 59 \t Batch 380 \t Validation Loss: 43.15875583949842\n",
      "Epoch 59 \t Batch 400 \t Validation Loss: 42.54045576572418\n",
      "Epoch 59 \t Batch 420 \t Validation Loss: 42.3565971942175\n",
      "Epoch 59 \t Batch 440 \t Validation Loss: 41.90676236369393\n",
      "Epoch 59 \t Batch 460 \t Validation Loss: 41.96748346038487\n",
      "Epoch 59 \t Batch 480 \t Validation Loss: 42.269006325801215\n",
      "Epoch 59 \t Batch 500 \t Validation Loss: 41.837750719070435\n",
      "Epoch 59 \t Batch 520 \t Validation Loss: 41.52234619030585\n",
      "Epoch 59 \t Batch 540 \t Validation Loss: 41.1056869842388\n",
      "Epoch 59 \t Batch 560 \t Validation Loss: 40.77969399690628\n",
      "Epoch 59 \t Batch 580 \t Validation Loss: 40.44572206530078\n",
      "Epoch 59 \t Batch 600 \t Validation Loss: 40.53771394252777\n",
      "Epoch 59 Training Loss: 46.29645709305059 Validation Loss: 41.05930758915938\n",
      "Epoch 59 completed\n",
      "Epoch 60 \t Batch 20 \t Training Loss: 46.34245567321777\n",
      "Epoch 60 \t Batch 40 \t Training Loss: 46.2769814491272\n",
      "Epoch 60 \t Batch 60 \t Training Loss: 46.1740774790446\n",
      "Epoch 60 \t Batch 80 \t Training Loss: 46.280871295928954\n",
      "Epoch 60 \t Batch 100 \t Training Loss: 46.391233367919924\n",
      "Epoch 60 \t Batch 120 \t Training Loss: 46.60721753438314\n",
      "Epoch 60 \t Batch 140 \t Training Loss: 46.737336594717846\n",
      "Epoch 60 \t Batch 160 \t Training Loss: 46.6020690202713\n",
      "Epoch 60 \t Batch 180 \t Training Loss: 46.576796764797635\n",
      "Epoch 60 \t Batch 200 \t Training Loss: 46.46340433120727\n",
      "Epoch 60 \t Batch 220 \t Training Loss: 46.47287165901878\n",
      "Epoch 60 \t Batch 240 \t Training Loss: 46.519221703211464\n",
      "Epoch 60 \t Batch 260 \t Training Loss: 46.352141761779784\n",
      "Epoch 60 \t Batch 280 \t Training Loss: 46.33909878049578\n",
      "Epoch 60 \t Batch 300 \t Training Loss: 46.34724580128988\n",
      "Epoch 60 \t Batch 320 \t Training Loss: 46.311903584003446\n",
      "Epoch 60 \t Batch 340 \t Training Loss: 46.30764291426715\n",
      "Epoch 60 \t Batch 360 \t Training Loss: 46.29969668918186\n",
      "Epoch 60 \t Batch 380 \t Training Loss: 46.31895065307617\n",
      "Epoch 60 \t Batch 400 \t Training Loss: 46.34820706367493\n",
      "Epoch 60 \t Batch 420 \t Training Loss: 46.44082908630371\n",
      "Epoch 60 \t Batch 440 \t Training Loss: 46.470685958862305\n",
      "Epoch 60 \t Batch 460 \t Training Loss: 46.48261247717816\n",
      "Epoch 60 \t Batch 480 \t Training Loss: 46.4448600769043\n",
      "Epoch 60 \t Batch 500 \t Training Loss: 46.39725360107422\n",
      "Epoch 60 \t Batch 520 \t Training Loss: 46.46736432589017\n",
      "Epoch 60 \t Batch 540 \t Training Loss: 46.42991982212773\n",
      "Epoch 60 \t Batch 560 \t Training Loss: 46.42100227219718\n",
      "Epoch 60 \t Batch 580 \t Training Loss: 46.393947377698176\n",
      "Epoch 60 \t Batch 600 \t Training Loss: 46.38969446818034\n",
      "Epoch 60 \t Batch 620 \t Training Loss: 46.33578573042347\n",
      "Epoch 60 \t Batch 640 \t Training Loss: 46.378189611434934\n",
      "Epoch 60 \t Batch 660 \t Training Loss: 46.35862349307898\n",
      "Epoch 60 \t Batch 680 \t Training Loss: 46.414035314672134\n",
      "Epoch 60 \t Batch 700 \t Training Loss: 46.42282198224749\n",
      "Epoch 60 \t Batch 720 \t Training Loss: 46.445698515574136\n",
      "Epoch 60 \t Batch 740 \t Training Loss: 46.39779884235279\n",
      "Epoch 60 \t Batch 760 \t Training Loss: 46.38865193818745\n",
      "Epoch 60 \t Batch 780 \t Training Loss: 46.35906226329315\n",
      "Epoch 60 \t Batch 800 \t Training Loss: 46.314857721328735\n",
      "Epoch 60 \t Batch 820 \t Training Loss: 46.31083771310202\n",
      "Epoch 60 \t Batch 840 \t Training Loss: 46.324248486473444\n",
      "Epoch 60 \t Batch 860 \t Training Loss: 46.323625480297\n",
      "Epoch 60 \t Batch 880 \t Training Loss: 46.31173347126354\n",
      "Epoch 60 \t Batch 900 \t Training Loss: 46.29843966166178\n",
      "Epoch 60 \t Batch 20 \t Validation Loss: 44.22873754501343\n",
      "Epoch 60 \t Batch 40 \t Validation Loss: 40.778733420372006\n",
      "Epoch 60 \t Batch 60 \t Validation Loss: 44.06905458768209\n",
      "Epoch 60 \t Batch 80 \t Validation Loss: 42.262086915969846\n",
      "Epoch 60 \t Batch 100 \t Validation Loss: 39.73396728515625\n",
      "Epoch 60 \t Batch 120 \t Validation Loss: 38.07271539370219\n",
      "Epoch 60 \t Batch 140 \t Validation Loss: 36.464071246555875\n",
      "Epoch 60 \t Batch 160 \t Validation Loss: 36.89826214313507\n",
      "Epoch 60 \t Batch 180 \t Validation Loss: 39.11108345455594\n",
      "Epoch 60 \t Batch 200 \t Validation Loss: 39.824836540222165\n",
      "Epoch 60 \t Batch 220 \t Validation Loss: 40.38022535497492\n",
      "Epoch 60 \t Batch 240 \t Validation Loss: 40.238696511586504\n",
      "Epoch 60 \t Batch 260 \t Validation Loss: 42.02354099200322\n",
      "Epoch 60 \t Batch 280 \t Validation Loss: 42.84706012521471\n",
      "Epoch 60 \t Batch 300 \t Validation Loss: 43.268373874028526\n",
      "Epoch 60 \t Batch 320 \t Validation Loss: 43.34525851309299\n",
      "Epoch 60 \t Batch 340 \t Validation Loss: 43.01381184914533\n",
      "Epoch 60 \t Batch 360 \t Validation Loss: 42.49937440554301\n",
      "Epoch 60 \t Batch 380 \t Validation Loss: 42.57844926934493\n",
      "Epoch 60 \t Batch 400 \t Validation Loss: 42.022485160827635\n",
      "Epoch 60 \t Batch 420 \t Validation Loss: 41.908762856892174\n",
      "Epoch 60 \t Batch 440 \t Validation Loss: 41.54308673901991\n",
      "Epoch 60 \t Batch 460 \t Validation Loss: 41.66953497969586\n",
      "Epoch 60 \t Batch 480 \t Validation Loss: 41.986354142427444\n",
      "Epoch 60 \t Batch 500 \t Validation Loss: 41.578819715499876\n",
      "Epoch 60 \t Batch 520 \t Validation Loss: 41.31405104490427\n",
      "Epoch 60 \t Batch 540 \t Validation Loss: 40.90272342187387\n",
      "Epoch 60 \t Batch 560 \t Validation Loss: 40.57566328389304\n",
      "Epoch 60 \t Batch 580 \t Validation Loss: 40.21871610509938\n",
      "Epoch 60 \t Batch 600 \t Validation Loss: 40.31433442751567\n",
      "Epoch 60 Training Loss: 46.265337895151056 Validation Loss: 40.860044643476414\n",
      "Epoch 60 completed\n",
      "Epoch 61 \t Batch 20 \t Training Loss: 45.90291881561279\n",
      "Epoch 61 \t Batch 40 \t Training Loss: 45.567787170410156\n",
      "Epoch 61 \t Batch 60 \t Training Loss: 45.824391555786136\n",
      "Epoch 61 \t Batch 80 \t Training Loss: 45.861309385299684\n",
      "Epoch 61 \t Batch 100 \t Training Loss: 46.03984840393066\n",
      "Epoch 61 \t Batch 120 \t Training Loss: 45.83350143432617\n",
      "Epoch 61 \t Batch 140 \t Training Loss: 45.85776985713414\n",
      "Epoch 61 \t Batch 160 \t Training Loss: 45.8821058511734\n",
      "Epoch 61 \t Batch 180 \t Training Loss: 46.078974872165254\n",
      "Epoch 61 \t Batch 200 \t Training Loss: 46.08541276931763\n",
      "Epoch 61 \t Batch 220 \t Training Loss: 46.22024695656516\n",
      "Epoch 61 \t Batch 240 \t Training Loss: 46.33429209391276\n",
      "Epoch 61 \t Batch 260 \t Training Loss: 46.297392243605394\n",
      "Epoch 61 \t Batch 280 \t Training Loss: 46.33425024577549\n",
      "Epoch 61 \t Batch 300 \t Training Loss: 46.31549987792969\n",
      "Epoch 61 \t Batch 320 \t Training Loss: 46.25976747274399\n",
      "Epoch 61 \t Batch 340 \t Training Loss: 46.261498148301065\n",
      "Epoch 61 \t Batch 360 \t Training Loss: 46.240993955400256\n",
      "Epoch 61 \t Batch 380 \t Training Loss: 46.174516818397926\n",
      "Epoch 61 \t Batch 400 \t Training Loss: 46.095866165161134\n",
      "Epoch 61 \t Batch 420 \t Training Loss: 46.087522497631255\n",
      "Epoch 61 \t Batch 440 \t Training Loss: 46.16108226776123\n",
      "Epoch 61 \t Batch 460 \t Training Loss: 46.17412777776303\n",
      "Epoch 61 \t Batch 480 \t Training Loss: 46.221792817115784\n",
      "Epoch 61 \t Batch 500 \t Training Loss: 46.32364091491699\n",
      "Epoch 61 \t Batch 520 \t Training Loss: 46.30657495351938\n",
      "Epoch 61 \t Batch 540 \t Training Loss: 46.293102992022476\n",
      "Epoch 61 \t Batch 560 \t Training Loss: 46.31755579539708\n",
      "Epoch 61 \t Batch 580 \t Training Loss: 46.28712500210466\n",
      "Epoch 61 \t Batch 600 \t Training Loss: 46.2714831161499\n",
      "Epoch 61 \t Batch 620 \t Training Loss: 46.2906893391763\n",
      "Epoch 61 \t Batch 640 \t Training Loss: 46.26819176077843\n",
      "Epoch 61 \t Batch 660 \t Training Loss: 46.22929818124482\n",
      "Epoch 61 \t Batch 680 \t Training Loss: 46.19310670740464\n",
      "Epoch 61 \t Batch 700 \t Training Loss: 46.21251344953264\n",
      "Epoch 61 \t Batch 720 \t Training Loss: 46.202130762736004\n",
      "Epoch 61 \t Batch 740 \t Training Loss: 46.21050647529396\n",
      "Epoch 61 \t Batch 760 \t Training Loss: 46.19412283646433\n",
      "Epoch 61 \t Batch 780 \t Training Loss: 46.224984036959135\n",
      "Epoch 61 \t Batch 800 \t Training Loss: 46.27978171348572\n",
      "Epoch 61 \t Batch 820 \t Training Loss: 46.25977367773289\n",
      "Epoch 61 \t Batch 840 \t Training Loss: 46.266877932775586\n",
      "Epoch 61 \t Batch 860 \t Training Loss: 46.25592734758244\n",
      "Epoch 61 \t Batch 880 \t Training Loss: 46.23261410106312\n",
      "Epoch 61 \t Batch 900 \t Training Loss: 46.22940839131673\n",
      "Epoch 61 \t Batch 20 \t Validation Loss: 39.9321891784668\n",
      "Epoch 61 \t Batch 40 \t Validation Loss: 37.248283219337466\n",
      "Epoch 61 \t Batch 60 \t Validation Loss: 39.39751731554667\n",
      "Epoch 61 \t Batch 80 \t Validation Loss: 38.255892717838286\n",
      "Epoch 61 \t Batch 100 \t Validation Loss: 36.200366716384885\n",
      "Epoch 61 \t Batch 120 \t Validation Loss: 35.01258556842804\n",
      "Epoch 61 \t Batch 140 \t Validation Loss: 33.82336530685425\n",
      "Epoch 61 \t Batch 160 \t Validation Loss: 34.65429828166962\n",
      "Epoch 61 \t Batch 180 \t Validation Loss: 37.227823416392006\n",
      "Epoch 61 \t Batch 200 \t Validation Loss: 38.242397422790525\n",
      "Epoch 61 \t Batch 220 \t Validation Loss: 38.94599890275435\n",
      "Epoch 61 \t Batch 240 \t Validation Loss: 38.91210552056631\n",
      "Epoch 61 \t Batch 260 \t Validation Loss: 40.82028556603652\n",
      "Epoch 61 \t Batch 280 \t Validation Loss: 41.7479903118951\n",
      "Epoch 61 \t Batch 300 \t Validation Loss: 42.30234884579976\n",
      "Epoch 61 \t Batch 320 \t Validation Loss: 42.45489367246628\n",
      "Epoch 61 \t Batch 340 \t Validation Loss: 42.180012557085824\n",
      "Epoch 61 \t Batch 360 \t Validation Loss: 41.76511229938931\n",
      "Epoch 61 \t Batch 380 \t Validation Loss: 41.937013440383105\n",
      "Epoch 61 \t Batch 400 \t Validation Loss: 41.49376623153687\n",
      "Epoch 61 \t Batch 420 \t Validation Loss: 41.44270103545416\n",
      "Epoch 61 \t Batch 440 \t Validation Loss: 41.21429330218922\n",
      "Epoch 61 \t Batch 460 \t Validation Loss: 41.42586674068285\n",
      "Epoch 61 \t Batch 480 \t Validation Loss: 41.781794369220734\n",
      "Epoch 61 \t Batch 500 \t Validation Loss: 41.41222037887573\n",
      "Epoch 61 \t Batch 520 \t Validation Loss: 41.28951316246619\n",
      "Epoch 61 \t Batch 540 \t Validation Loss: 40.9063497543335\n",
      "Epoch 61 \t Batch 560 \t Validation Loss: 40.6275092124939\n",
      "Epoch 61 \t Batch 580 \t Validation Loss: 40.37623285425121\n",
      "Epoch 61 \t Batch 600 \t Validation Loss: 40.47996750831604\n",
      "Epoch 61 Training Loss: 46.237107230116784 Validation Loss: 41.05220927201308\n",
      "Epoch 61 completed\n",
      "Epoch 62 \t Batch 20 \t Training Loss: 45.694589614868164\n",
      "Epoch 62 \t Batch 40 \t Training Loss: 46.01002235412598\n",
      "Epoch 62 \t Batch 60 \t Training Loss: 45.95360647837321\n",
      "Epoch 62 \t Batch 80 \t Training Loss: 46.075464725494385\n",
      "Epoch 62 \t Batch 100 \t Training Loss: 45.98282543182373\n",
      "Epoch 62 \t Batch 120 \t Training Loss: 46.0995376586914\n",
      "Epoch 62 \t Batch 140 \t Training Loss: 46.14640933445522\n",
      "Epoch 62 \t Batch 160 \t Training Loss: 46.24695537090302\n",
      "Epoch 62 \t Batch 180 \t Training Loss: 46.06319024827745\n",
      "Epoch 62 \t Batch 200 \t Training Loss: 45.956729373931886\n",
      "Epoch 62 \t Batch 220 \t Training Loss: 46.00528418801048\n",
      "Epoch 62 \t Batch 240 \t Training Loss: 46.05543437004089\n",
      "Epoch 62 \t Batch 260 \t Training Loss: 46.11440790616549\n",
      "Epoch 62 \t Batch 280 \t Training Loss: 46.152606446402416\n",
      "Epoch 62 \t Batch 300 \t Training Loss: 46.1986358388265\n",
      "Epoch 62 \t Batch 320 \t Training Loss: 46.149420487880704\n",
      "Epoch 62 \t Batch 340 \t Training Loss: 46.10762847451603\n",
      "Epoch 62 \t Batch 360 \t Training Loss: 46.11695949766371\n",
      "Epoch 62 \t Batch 380 \t Training Loss: 46.13754069679662\n",
      "Epoch 62 \t Batch 400 \t Training Loss: 46.13965027809143\n",
      "Epoch 62 \t Batch 420 \t Training Loss: 46.100015267871676\n",
      "Epoch 62 \t Batch 440 \t Training Loss: 46.130215393413195\n",
      "Epoch 62 \t Batch 460 \t Training Loss: 46.1568034545235\n",
      "Epoch 62 \t Batch 480 \t Training Loss: 46.18203524748484\n",
      "Epoch 62 \t Batch 500 \t Training Loss: 46.2274557723999\n",
      "Epoch 62 \t Batch 520 \t Training Loss: 46.24228689487164\n",
      "Epoch 62 \t Batch 540 \t Training Loss: 46.2170019502993\n",
      "Epoch 62 \t Batch 560 \t Training Loss: 46.19112042018345\n",
      "Epoch 62 \t Batch 580 \t Training Loss: 46.201790993789146\n",
      "Epoch 62 \t Batch 600 \t Training Loss: 46.22407316843669\n",
      "Epoch 62 \t Batch 620 \t Training Loss: 46.25326898021083\n",
      "Epoch 62 \t Batch 640 \t Training Loss: 46.26769054532051\n",
      "Epoch 62 \t Batch 660 \t Training Loss: 46.29717735521721\n",
      "Epoch 62 \t Batch 680 \t Training Loss: 46.25797034992891\n",
      "Epoch 62 \t Batch 700 \t Training Loss: 46.23568273271833\n",
      "Epoch 62 \t Batch 720 \t Training Loss: 46.22923410203722\n",
      "Epoch 62 \t Batch 740 \t Training Loss: 46.20281033902555\n",
      "Epoch 62 \t Batch 760 \t Training Loss: 46.22244453430176\n",
      "Epoch 62 \t Batch 780 \t Training Loss: 46.1946319042108\n",
      "Epoch 62 \t Batch 800 \t Training Loss: 46.242732439041134\n",
      "Epoch 62 \t Batch 820 \t Training Loss: 46.261616083470784\n",
      "Epoch 62 \t Batch 840 \t Training Loss: 46.2689343724932\n",
      "Epoch 62 \t Batch 860 \t Training Loss: 46.245747730343844\n",
      "Epoch 62 \t Batch 880 \t Training Loss: 46.247496756640345\n",
      "Epoch 62 \t Batch 900 \t Training Loss: 46.246081602308486\n",
      "Epoch 62 \t Batch 20 \t Validation Loss: 35.76819381713867\n",
      "Epoch 62 \t Batch 40 \t Validation Loss: 34.9998101234436\n",
      "Epoch 62 \t Batch 60 \t Validation Loss: 36.977399762471514\n",
      "Epoch 62 \t Batch 80 \t Validation Loss: 36.176464557647705\n",
      "Epoch 62 \t Batch 100 \t Validation Loss: 34.477415542602536\n",
      "Epoch 62 \t Batch 120 \t Validation Loss: 33.75604831377665\n",
      "Epoch 62 \t Batch 140 \t Validation Loss: 32.84574718475342\n",
      "Epoch 62 \t Batch 160 \t Validation Loss: 33.94975728988648\n",
      "Epoch 62 \t Batch 180 \t Validation Loss: 36.90035444895427\n",
      "Epoch 62 \t Batch 200 \t Validation Loss: 37.981452283859255\n",
      "Epoch 62 \t Batch 220 \t Validation Loss: 38.93089468695901\n",
      "Epoch 62 \t Batch 240 \t Validation Loss: 39.05953446626663\n",
      "Epoch 62 \t Batch 260 \t Validation Loss: 41.07766787455632\n",
      "Epoch 62 \t Batch 280 \t Validation Loss: 42.07702798843384\n",
      "Epoch 62 \t Batch 300 \t Validation Loss: 42.78761077880859\n",
      "Epoch 62 \t Batch 320 \t Validation Loss: 43.036495816707614\n",
      "Epoch 62 \t Batch 340 \t Validation Loss: 42.78374715692857\n",
      "Epoch 62 \t Batch 360 \t Validation Loss: 42.444777478112115\n",
      "Epoch 62 \t Batch 380 \t Validation Loss: 42.5566693406356\n",
      "Epoch 62 \t Batch 400 \t Validation Loss: 42.023220868110656\n",
      "Epoch 62 \t Batch 420 \t Validation Loss: 41.90033070700509\n",
      "Epoch 62 \t Batch 440 \t Validation Loss: 41.534658215262674\n",
      "Epoch 62 \t Batch 460 \t Validation Loss: 41.630424532683\n",
      "Epoch 62 \t Batch 480 \t Validation Loss: 41.98466914494832\n",
      "Epoch 62 \t Batch 500 \t Validation Loss: 41.60575094985962\n",
      "Epoch 62 \t Batch 520 \t Validation Loss: 41.34874143417065\n",
      "Epoch 62 \t Batch 540 \t Validation Loss: 40.977317889531456\n",
      "Epoch 62 \t Batch 560 \t Validation Loss: 40.68758224589484\n",
      "Epoch 62 \t Batch 580 \t Validation Loss: 40.40840234920896\n",
      "Epoch 62 \t Batch 600 \t Validation Loss: 40.514589511553446\n",
      "Epoch 62 Training Loss: 46.206729239959884 Validation Loss: 41.1086473852009\n",
      "Epoch 62 completed\n",
      "Epoch 63 \t Batch 20 \t Training Loss: 46.76076316833496\n",
      "Epoch 63 \t Batch 40 \t Training Loss: 46.31683340072632\n",
      "Epoch 63 \t Batch 60 \t Training Loss: 45.95249843597412\n",
      "Epoch 63 \t Batch 80 \t Training Loss: 45.90560631752014\n",
      "Epoch 63 \t Batch 100 \t Training Loss: 46.26091773986816\n",
      "Epoch 63 \t Batch 120 \t Training Loss: 46.18336868286133\n",
      "Epoch 63 \t Batch 140 \t Training Loss: 46.22602133069719\n",
      "Epoch 63 \t Batch 160 \t Training Loss: 46.2443210363388\n",
      "Epoch 63 \t Batch 180 \t Training Loss: 46.2291900422838\n",
      "Epoch 63 \t Batch 200 \t Training Loss: 46.11345993041992\n",
      "Epoch 63 \t Batch 220 \t Training Loss: 46.194098541953345\n",
      "Epoch 63 \t Batch 240 \t Training Loss: 46.22969512939453\n",
      "Epoch 63 \t Batch 260 \t Training Loss: 46.232403608468864\n",
      "Epoch 63 \t Batch 280 \t Training Loss: 46.196934645516535\n",
      "Epoch 63 \t Batch 300 \t Training Loss: 46.25934922536214\n",
      "Epoch 63 \t Batch 320 \t Training Loss: 46.278700053691864\n",
      "Epoch 63 \t Batch 340 \t Training Loss: 46.29309982972987\n",
      "Epoch 63 \t Batch 360 \t Training Loss: 46.288959725697836\n",
      "Epoch 63 \t Batch 380 \t Training Loss: 46.32200678775185\n",
      "Epoch 63 \t Batch 400 \t Training Loss: 46.20987632751465\n",
      "Epoch 63 \t Batch 420 \t Training Loss: 46.15960722423735\n",
      "Epoch 63 \t Batch 440 \t Training Loss: 46.10462666424838\n",
      "Epoch 63 \t Batch 460 \t Training Loss: 46.10604014189347\n",
      "Epoch 63 \t Batch 480 \t Training Loss: 46.09000135262807\n",
      "Epoch 63 \t Batch 500 \t Training Loss: 46.07572148132324\n",
      "Epoch 63 \t Batch 520 \t Training Loss: 46.04621741955097\n",
      "Epoch 63 \t Batch 540 \t Training Loss: 46.07010995370371\n",
      "Epoch 63 \t Batch 560 \t Training Loss: 46.06974502972194\n",
      "Epoch 63 \t Batch 580 \t Training Loss: 46.10522027508966\n",
      "Epoch 63 \t Batch 600 \t Training Loss: 46.07576658884684\n",
      "Epoch 63 \t Batch 620 \t Training Loss: 46.08980470472766\n",
      "Epoch 63 \t Batch 640 \t Training Loss: 46.11376231908798\n",
      "Epoch 63 \t Batch 660 \t Training Loss: 46.13261281215783\n",
      "Epoch 63 \t Batch 680 \t Training Loss: 46.15314379860373\n",
      "Epoch 63 \t Batch 700 \t Training Loss: 46.13845342908587\n",
      "Epoch 63 \t Batch 720 \t Training Loss: 46.13643906381395\n",
      "Epoch 63 \t Batch 740 \t Training Loss: 46.12852886303051\n",
      "Epoch 63 \t Batch 760 \t Training Loss: 46.10942043505217\n",
      "Epoch 63 \t Batch 780 \t Training Loss: 46.09968447562976\n",
      "Epoch 63 \t Batch 800 \t Training Loss: 46.091130204200745\n",
      "Epoch 63 \t Batch 820 \t Training Loss: 46.10713524702118\n",
      "Epoch 63 \t Batch 840 \t Training Loss: 46.12918844677153\n",
      "Epoch 63 \t Batch 860 \t Training Loss: 46.15604126065276\n",
      "Epoch 63 \t Batch 880 \t Training Loss: 46.179124637083575\n",
      "Epoch 63 \t Batch 900 \t Training Loss: 46.21455874549018\n",
      "Epoch 63 \t Batch 20 \t Validation Loss: 65.83622188568116\n",
      "Epoch 63 \t Batch 40 \t Validation Loss: 56.91313519477844\n",
      "Epoch 63 \t Batch 60 \t Validation Loss: 61.287091445922854\n",
      "Epoch 63 \t Batch 80 \t Validation Loss: 58.173530268669126\n",
      "Epoch 63 \t Batch 100 \t Validation Loss: 53.450346031188964\n",
      "Epoch 63 \t Batch 120 \t Validation Loss: 50.25606886545817\n",
      "Epoch 63 \t Batch 140 \t Validation Loss: 47.30305151258196\n",
      "Epoch 63 \t Batch 160 \t Validation Loss: 46.380874276161194\n",
      "Epoch 63 \t Batch 180 \t Validation Loss: 47.600756337907576\n",
      "Epoch 63 \t Batch 200 \t Validation Loss: 47.45367429733276\n",
      "Epoch 63 \t Batch 220 \t Validation Loss: 47.347456368533045\n",
      "Epoch 63 \t Batch 240 \t Validation Loss: 46.61585781176885\n",
      "Epoch 63 \t Batch 260 \t Validation Loss: 47.92363985135005\n",
      "Epoch 63 \t Batch 280 \t Validation Loss: 48.30642836775098\n",
      "Epoch 63 \t Batch 300 \t Validation Loss: 48.37867721875509\n",
      "Epoch 63 \t Batch 320 \t Validation Loss: 48.10993777215481\n",
      "Epoch 63 \t Batch 340 \t Validation Loss: 47.51298380178564\n",
      "Epoch 63 \t Batch 360 \t Validation Loss: 46.77999997668796\n",
      "Epoch 63 \t Batch 380 \t Validation Loss: 46.58619865869221\n",
      "Epoch 63 \t Batch 400 \t Validation Loss: 45.787023417949676\n",
      "Epoch 63 \t Batch 420 \t Validation Loss: 45.45369778587705\n",
      "Epoch 63 \t Batch 440 \t Validation Loss: 44.85801401571794\n",
      "Epoch 63 \t Batch 460 \t Validation Loss: 44.77155884452488\n",
      "Epoch 63 \t Batch 480 \t Validation Loss: 44.963967557748155\n",
      "Epoch 63 \t Batch 500 \t Validation Loss: 44.44083799362183\n",
      "Epoch 63 \t Batch 520 \t Validation Loss: 43.989165958991414\n",
      "Epoch 63 \t Batch 540 \t Validation Loss: 43.50382768136484\n",
      "Epoch 63 \t Batch 560 \t Validation Loss: 43.12914596966335\n",
      "Epoch 63 \t Batch 580 \t Validation Loss: 42.76619587602286\n",
      "Epoch 63 \t Batch 600 \t Validation Loss: 42.79808759689331\n",
      "Epoch 63 Training Loss: 46.18023220715372 Validation Loss: 43.28645838081063\n",
      "Epoch 63 completed\n",
      "Epoch 64 \t Batch 20 \t Training Loss: 45.50017871856689\n",
      "Epoch 64 \t Batch 40 \t Training Loss: 45.42036085128784\n",
      "Epoch 64 \t Batch 60 \t Training Loss: 45.572632026672366\n",
      "Epoch 64 \t Batch 80 \t Training Loss: 45.73063097000122\n",
      "Epoch 64 \t Batch 100 \t Training Loss: 45.8192561340332\n",
      "Epoch 64 \t Batch 120 \t Training Loss: 45.9103307723999\n",
      "Epoch 64 \t Batch 140 \t Training Loss: 45.78110839298793\n",
      "Epoch 64 \t Batch 160 \t Training Loss: 45.894286918640134\n",
      "Epoch 64 \t Batch 180 \t Training Loss: 45.938377168443466\n",
      "Epoch 64 \t Batch 200 \t Training Loss: 46.04667465209961\n",
      "Epoch 64 \t Batch 220 \t Training Loss: 46.113931257074526\n",
      "Epoch 64 \t Batch 240 \t Training Loss: 46.09588828086853\n",
      "Epoch 64 \t Batch 260 \t Training Loss: 46.09924105130709\n",
      "Epoch 64 \t Batch 280 \t Training Loss: 46.003079536982945\n",
      "Epoch 64 \t Batch 300 \t Training Loss: 46.01016845703125\n",
      "Epoch 64 \t Batch 320 \t Training Loss: 45.981244838237764\n",
      "Epoch 64 \t Batch 340 \t Training Loss: 46.003304851756376\n",
      "Epoch 64 \t Batch 360 \t Training Loss: 45.96977800793118\n",
      "Epoch 64 \t Batch 380 \t Training Loss: 46.056414493761565\n",
      "Epoch 64 \t Batch 400 \t Training Loss: 46.044962663650516\n",
      "Epoch 64 \t Batch 420 \t Training Loss: 46.06116075061617\n",
      "Epoch 64 \t Batch 440 \t Training Loss: 46.038515238328415\n",
      "Epoch 64 \t Batch 460 \t Training Loss: 46.039883315044904\n",
      "Epoch 64 \t Batch 480 \t Training Loss: 46.00327129364014\n",
      "Epoch 64 \t Batch 500 \t Training Loss: 46.02229853057862\n",
      "Epoch 64 \t Batch 520 \t Training Loss: 46.03063367696909\n",
      "Epoch 64 \t Batch 540 \t Training Loss: 46.032651046470356\n",
      "Epoch 64 \t Batch 560 \t Training Loss: 46.07897274834769\n",
      "Epoch 64 \t Batch 580 \t Training Loss: 46.1061587826959\n",
      "Epoch 64 \t Batch 600 \t Training Loss: 46.118037649790445\n",
      "Epoch 64 \t Batch 620 \t Training Loss: 46.12532589820123\n",
      "Epoch 64 \t Batch 640 \t Training Loss: 46.16148688793182\n",
      "Epoch 64 \t Batch 660 \t Training Loss: 46.11665705478553\n",
      "Epoch 64 \t Batch 680 \t Training Loss: 46.140817165374756\n",
      "Epoch 64 \t Batch 700 \t Training Loss: 46.13594650268555\n",
      "Epoch 64 \t Batch 720 \t Training Loss: 46.116711907916596\n",
      "Epoch 64 \t Batch 740 \t Training Loss: 46.11248327203699\n",
      "Epoch 64 \t Batch 760 \t Training Loss: 46.12661984594245\n",
      "Epoch 64 \t Batch 780 \t Training Loss: 46.14825408642108\n",
      "Epoch 64 \t Batch 800 \t Training Loss: 46.1429603433609\n",
      "Epoch 64 \t Batch 820 \t Training Loss: 46.162524804836366\n",
      "Epoch 64 \t Batch 840 \t Training Loss: 46.13107698077247\n",
      "Epoch 64 \t Batch 860 \t Training Loss: 46.11254538159038\n",
      "Epoch 64 \t Batch 880 \t Training Loss: 46.12328711856495\n",
      "Epoch 64 \t Batch 900 \t Training Loss: 46.13555382622613\n",
      "Epoch 64 \t Batch 20 \t Validation Loss: 48.9768590927124\n",
      "Epoch 64 \t Batch 40 \t Validation Loss: 43.094869995117186\n",
      "Epoch 64 \t Batch 60 \t Validation Loss: 45.98993539810181\n",
      "Epoch 64 \t Batch 80 \t Validation Loss: 43.88968417644501\n",
      "Epoch 64 \t Batch 100 \t Validation Loss: 40.98245279312134\n",
      "Epoch 64 \t Batch 120 \t Validation Loss: 39.19271020889282\n",
      "Epoch 64 \t Batch 140 \t Validation Loss: 37.44565824781145\n",
      "Epoch 64 \t Batch 160 \t Validation Loss: 37.65757457017899\n",
      "Epoch 64 \t Batch 180 \t Validation Loss: 39.39602540863885\n",
      "Epoch 64 \t Batch 200 \t Validation Loss: 39.859710087776186\n",
      "Epoch 64 \t Batch 220 \t Validation Loss: 40.1108763738112\n",
      "Epoch 64 \t Batch 240 \t Validation Loss: 39.77397529681524\n",
      "Epoch 64 \t Batch 260 \t Validation Loss: 41.299475981638984\n",
      "Epoch 64 \t Batch 280 \t Validation Loss: 41.913758121218\n",
      "Epoch 64 \t Batch 300 \t Validation Loss: 42.21503776550293\n",
      "Epoch 64 \t Batch 320 \t Validation Loss: 42.25281499326229\n",
      "Epoch 64 \t Batch 340 \t Validation Loss: 41.941550824221444\n",
      "Epoch 64 \t Batch 360 \t Validation Loss: 41.47881982326508\n",
      "Epoch 64 \t Batch 380 \t Validation Loss: 41.514495857138385\n",
      "Epoch 64 \t Batch 400 \t Validation Loss: 41.03098655462265\n",
      "Epoch 64 \t Batch 420 \t Validation Loss: 40.95934449150449\n",
      "Epoch 64 \t Batch 440 \t Validation Loss: 40.651992524753915\n",
      "Epoch 64 \t Batch 460 \t Validation Loss: 40.815907810045324\n",
      "Epoch 64 \t Batch 480 \t Validation Loss: 41.16985519329707\n",
      "Epoch 64 \t Batch 500 \t Validation Loss: 40.78469818115234\n",
      "Epoch 64 \t Batch 520 \t Validation Loss: 40.61140528642214\n",
      "Epoch 64 \t Batch 540 \t Validation Loss: 40.236709186765886\n",
      "Epoch 64 \t Batch 560 \t Validation Loss: 39.943367009503504\n",
      "Epoch 64 \t Batch 580 \t Validation Loss: 39.64940134739054\n",
      "Epoch 64 \t Batch 600 \t Validation Loss: 39.75998065471649\n",
      "Epoch 64 Training Loss: 46.14676342197697 Validation Loss: 40.34631790898063\n",
      "Epoch 64 completed\n",
      "Epoch 65 \t Batch 20 \t Training Loss: 44.670752143859865\n",
      "Epoch 65 \t Batch 40 \t Training Loss: 45.40883131027222\n",
      "Epoch 65 \t Batch 60 \t Training Loss: 45.99303258260091\n",
      "Epoch 65 \t Batch 80 \t Training Loss: 45.66729340553284\n",
      "Epoch 65 \t Batch 100 \t Training Loss: 45.66422855377197\n",
      "Epoch 65 \t Batch 120 \t Training Loss: 46.07775481541952\n",
      "Epoch 65 \t Batch 140 \t Training Loss: 46.15952548980713\n",
      "Epoch 65 \t Batch 160 \t Training Loss: 46.11495127677917\n",
      "Epoch 65 \t Batch 180 \t Training Loss: 46.10108856625027\n",
      "Epoch 65 \t Batch 200 \t Training Loss: 46.04361131668091\n",
      "Epoch 65 \t Batch 220 \t Training Loss: 46.195961969549\n",
      "Epoch 65 \t Batch 240 \t Training Loss: 46.348875538508096\n",
      "Epoch 65 \t Batch 260 \t Training Loss: 46.3636202445397\n",
      "Epoch 65 \t Batch 280 \t Training Loss: 46.32599390574864\n",
      "Epoch 65 \t Batch 300 \t Training Loss: 46.35393028259277\n",
      "Epoch 65 \t Batch 320 \t Training Loss: 46.31477265357971\n",
      "Epoch 65 \t Batch 340 \t Training Loss: 46.1869109209846\n",
      "Epoch 65 \t Batch 360 \t Training Loss: 46.205517037709555\n",
      "Epoch 65 \t Batch 380 \t Training Loss: 46.187265396118164\n",
      "Epoch 65 \t Batch 400 \t Training Loss: 46.17341551780701\n",
      "Epoch 65 \t Batch 420 \t Training Loss: 46.15970824105399\n",
      "Epoch 65 \t Batch 440 \t Training Loss: 46.157416395707564\n",
      "Epoch 65 \t Batch 460 \t Training Loss: 46.19762310359789\n",
      "Epoch 65 \t Batch 480 \t Training Loss: 46.190267300605775\n",
      "Epoch 65 \t Batch 500 \t Training Loss: 46.08772329711914\n",
      "Epoch 65 \t Batch 520 \t Training Loss: 46.08629692517794\n",
      "Epoch 65 \t Batch 540 \t Training Loss: 46.12188035470468\n",
      "Epoch 65 \t Batch 560 \t Training Loss: 46.11748031207493\n",
      "Epoch 65 \t Batch 580 \t Training Loss: 46.182820491133064\n",
      "Epoch 65 \t Batch 600 \t Training Loss: 46.16332694371541\n",
      "Epoch 65 \t Batch 620 \t Training Loss: 46.16070725840907\n",
      "Epoch 65 \t Batch 640 \t Training Loss: 46.15215426683426\n",
      "Epoch 65 \t Batch 660 \t Training Loss: 46.121005191224995\n",
      "Epoch 65 \t Batch 680 \t Training Loss: 46.119670205957746\n",
      "Epoch 65 \t Batch 700 \t Training Loss: 46.117439471653526\n",
      "Epoch 65 \t Batch 720 \t Training Loss: 46.09052408006456\n",
      "Epoch 65 \t Batch 740 \t Training Loss: 46.10102206565238\n",
      "Epoch 65 \t Batch 760 \t Training Loss: 46.11234677967272\n",
      "Epoch 65 \t Batch 780 \t Training Loss: 46.121792558523325\n",
      "Epoch 65 \t Batch 800 \t Training Loss: 46.11072448253631\n",
      "Epoch 65 \t Batch 820 \t Training Loss: 46.12872522865854\n",
      "Epoch 65 \t Batch 840 \t Training Loss: 46.1133410181318\n",
      "Epoch 65 \t Batch 860 \t Training Loss: 46.089135635730834\n",
      "Epoch 65 \t Batch 880 \t Training Loss: 46.09318455782804\n",
      "Epoch 65 \t Batch 900 \t Training Loss: 46.127293230692544\n",
      "Epoch 65 \t Batch 20 \t Validation Loss: 48.83613061904907\n",
      "Epoch 65 \t Batch 40 \t Validation Loss: 44.25294449329376\n",
      "Epoch 65 \t Batch 60 \t Validation Loss: 47.00292738278707\n",
      "Epoch 65 \t Batch 80 \t Validation Loss: 45.1451309800148\n",
      "Epoch 65 \t Batch 100 \t Validation Loss: 42.24709118843079\n",
      "Epoch 65 \t Batch 120 \t Validation Loss: 40.350659823417665\n",
      "Epoch 65 \t Batch 140 \t Validation Loss: 38.514929396765574\n",
      "Epoch 65 \t Batch 160 \t Validation Loss: 38.88967831730842\n",
      "Epoch 65 \t Batch 180 \t Validation Loss: 41.07764059172736\n",
      "Epoch 65 \t Batch 200 \t Validation Loss: 41.641136059761045\n",
      "Epoch 65 \t Batch 220 \t Validation Loss: 42.218142140995376\n",
      "Epoch 65 \t Batch 240 \t Validation Loss: 42.01324690977732\n",
      "Epoch 65 \t Batch 260 \t Validation Loss: 43.75943683110751\n",
      "Epoch 65 \t Batch 280 \t Validation Loss: 44.53996304103306\n",
      "Epoch 65 \t Batch 300 \t Validation Loss: 44.95679662068685\n",
      "Epoch 65 \t Batch 320 \t Validation Loss: 45.01006876826286\n",
      "Epoch 65 \t Batch 340 \t Validation Loss: 44.61944412343642\n",
      "Epoch 65 \t Batch 360 \t Validation Loss: 44.160658383369444\n",
      "Epoch 65 \t Batch 380 \t Validation Loss: 44.21356413740861\n",
      "Epoch 65 \t Batch 400 \t Validation Loss: 43.61145581483841\n",
      "Epoch 65 \t Batch 420 \t Validation Loss: 43.4293851466406\n",
      "Epoch 65 \t Batch 440 \t Validation Loss: 43.028631169145754\n",
      "Epoch 65 \t Batch 460 \t Validation Loss: 43.10226142924765\n",
      "Epoch 65 \t Batch 480 \t Validation Loss: 43.39937284588814\n",
      "Epoch 65 \t Batch 500 \t Validation Loss: 42.97928486442566\n",
      "Epoch 65 \t Batch 520 \t Validation Loss: 42.712962414668155\n",
      "Epoch 65 \t Batch 540 \t Validation Loss: 42.2844508630258\n",
      "Epoch 65 \t Batch 560 \t Validation Loss: 41.94014742715018\n",
      "Epoch 65 \t Batch 580 \t Validation Loss: 41.59582650743682\n",
      "Epoch 65 \t Batch 600 \t Validation Loss: 41.66136505762736\n",
      "Epoch 65 Training Loss: 46.15258091434658 Validation Loss: 42.197391955883475\n",
      "Epoch 65 completed\n",
      "Epoch 66 \t Batch 20 \t Training Loss: 45.4602294921875\n",
      "Epoch 66 \t Batch 40 \t Training Loss: 46.431256484985354\n",
      "Epoch 66 \t Batch 60 \t Training Loss: 46.52793337504069\n",
      "Epoch 66 \t Batch 80 \t Training Loss: 46.32045421600342\n",
      "Epoch 66 \t Batch 100 \t Training Loss: 46.324315567016605\n",
      "Epoch 66 \t Batch 120 \t Training Loss: 46.24948495229085\n",
      "Epoch 66 \t Batch 140 \t Training Loss: 46.23477578844343\n",
      "Epoch 66 \t Batch 160 \t Training Loss: 46.37434010505676\n",
      "Epoch 66 \t Batch 180 \t Training Loss: 46.397422557406955\n",
      "Epoch 66 \t Batch 200 \t Training Loss: 46.16336692810059\n",
      "Epoch 66 \t Batch 220 \t Training Loss: 46.1688139481978\n",
      "Epoch 66 \t Batch 240 \t Training Loss: 46.089240566889444\n",
      "Epoch 66 \t Batch 260 \t Training Loss: 46.102673780001126\n",
      "Epoch 66 \t Batch 280 \t Training Loss: 46.086694731031145\n",
      "Epoch 66 \t Batch 300 \t Training Loss: 46.078669471740724\n",
      "Epoch 66 \t Batch 320 \t Training Loss: 46.01666692495346\n",
      "Epoch 66 \t Batch 340 \t Training Loss: 46.13680691438563\n",
      "Epoch 66 \t Batch 360 \t Training Loss: 46.10224837197198\n",
      "Epoch 66 \t Batch 380 \t Training Loss: 46.17780217622456\n",
      "Epoch 66 \t Batch 400 \t Training Loss: 46.22422979354858\n",
      "Epoch 66 \t Batch 420 \t Training Loss: 46.215255219595775\n",
      "Epoch 66 \t Batch 440 \t Training Loss: 46.249383328177714\n",
      "Epoch 66 \t Batch 460 \t Training Loss: 46.237004064477006\n",
      "Epoch 66 \t Batch 480 \t Training Loss: 46.18842432498932\n",
      "Epoch 66 \t Batch 500 \t Training Loss: 46.16720092773438\n",
      "Epoch 66 \t Batch 520 \t Training Loss: 46.21161061066847\n",
      "Epoch 66 \t Batch 540 \t Training Loss: 46.26481690583406\n",
      "Epoch 66 \t Batch 560 \t Training Loss: 46.20998430252075\n",
      "Epoch 66 \t Batch 580 \t Training Loss: 46.19751798695531\n",
      "Epoch 66 \t Batch 600 \t Training Loss: 46.22849779129028\n",
      "Epoch 66 \t Batch 620 \t Training Loss: 46.27368666741156\n",
      "Epoch 66 \t Batch 640 \t Training Loss: 46.26899650096893\n",
      "Epoch 66 \t Batch 660 \t Training Loss: 46.295416762612085\n",
      "Epoch 66 \t Batch 680 \t Training Loss: 46.282421807681814\n",
      "Epoch 66 \t Batch 700 \t Training Loss: 46.268609401157924\n",
      "Epoch 66 \t Batch 720 \t Training Loss: 46.26366572909885\n",
      "Epoch 66 \t Batch 740 \t Training Loss: 46.222741147634146\n",
      "Epoch 66 \t Batch 760 \t Training Loss: 46.1993785556994\n",
      "Epoch 66 \t Batch 780 \t Training Loss: 46.21419940850674\n",
      "Epoch 66 \t Batch 800 \t Training Loss: 46.21141379833222\n",
      "Epoch 66 \t Batch 820 \t Training Loss: 46.19750513216344\n",
      "Epoch 66 \t Batch 840 \t Training Loss: 46.16262969516573\n",
      "Epoch 66 \t Batch 860 \t Training Loss: 46.15714826805647\n",
      "Epoch 66 \t Batch 880 \t Training Loss: 46.12374888766896\n",
      "Epoch 66 \t Batch 900 \t Training Loss: 46.10082714928521\n",
      "Epoch 66 \t Batch 20 \t Validation Loss: 53.40854263305664\n",
      "Epoch 66 \t Batch 40 \t Validation Loss: 50.21980757713318\n",
      "Epoch 66 \t Batch 60 \t Validation Loss: 52.054609807332355\n",
      "Epoch 66 \t Batch 80 \t Validation Loss: 50.11382329463959\n",
      "Epoch 66 \t Batch 100 \t Validation Loss: 46.034567546844485\n",
      "Epoch 66 \t Batch 120 \t Validation Loss: 43.46837593714396\n",
      "Epoch 66 \t Batch 140 \t Validation Loss: 41.22091144834246\n",
      "Epoch 66 \t Batch 160 \t Validation Loss: 41.18429070711136\n",
      "Epoch 66 \t Batch 180 \t Validation Loss: 42.73383778995938\n",
      "Epoch 66 \t Batch 200 \t Validation Loss: 43.00997576236725\n",
      "Epoch 66 \t Batch 220 \t Validation Loss: 43.15105346766385\n",
      "Epoch 66 \t Batch 240 \t Validation Loss: 42.70642853975296\n",
      "Epoch 66 \t Batch 260 \t Validation Loss: 44.15544985991258\n",
      "Epoch 66 \t Batch 280 \t Validation Loss: 44.72550791672298\n",
      "Epoch 66 \t Batch 300 \t Validation Loss: 44.919657939275105\n",
      "Epoch 66 \t Batch 320 \t Validation Loss: 44.86251889765263\n",
      "Epoch 66 \t Batch 340 \t Validation Loss: 44.45128836631775\n",
      "Epoch 66 \t Batch 360 \t Validation Loss: 43.949039697647095\n",
      "Epoch 66 \t Batch 380 \t Validation Loss: 43.98078310364171\n",
      "Epoch 66 \t Batch 400 \t Validation Loss: 43.550838453769686\n",
      "Epoch 66 \t Batch 420 \t Validation Loss: 43.42983152071635\n",
      "Epoch 66 \t Batch 440 \t Validation Loss: 43.188404857028615\n",
      "Epoch 66 \t Batch 460 \t Validation Loss: 43.39880519742551\n",
      "Epoch 66 \t Batch 480 \t Validation Loss: 43.711862530310945\n",
      "Epoch 66 \t Batch 500 \t Validation Loss: 43.28066604804992\n",
      "Epoch 66 \t Batch 520 \t Validation Loss: 43.18607275302593\n",
      "Epoch 66 \t Batch 540 \t Validation Loss: 42.81144829502812\n",
      "Epoch 66 \t Batch 560 \t Validation Loss: 42.48591317449297\n",
      "Epoch 66 \t Batch 580 \t Validation Loss: 42.21418353771341\n",
      "Epoch 66 \t Batch 600 \t Validation Loss: 42.28301163355509\n",
      "Epoch 66 Training Loss: 46.1053246817386 Validation Loss: 42.82776661971947\n",
      "Epoch 66 completed\n",
      "Epoch 67 \t Batch 20 \t Training Loss: 45.34924182891846\n",
      "Epoch 67 \t Batch 40 \t Training Loss: 46.4579776763916\n",
      "Epoch 67 \t Batch 60 \t Training Loss: 46.0740961710612\n",
      "Epoch 67 \t Batch 80 \t Training Loss: 46.13528141975403\n",
      "Epoch 67 \t Batch 100 \t Training Loss: 46.25209686279297\n",
      "Epoch 67 \t Batch 120 \t Training Loss: 46.010728136698404\n",
      "Epoch 67 \t Batch 140 \t Training Loss: 45.947812979561945\n",
      "Epoch 67 \t Batch 160 \t Training Loss: 46.02305066585541\n",
      "Epoch 67 \t Batch 180 \t Training Loss: 45.91779727935791\n",
      "Epoch 67 \t Batch 200 \t Training Loss: 45.92345592498779\n",
      "Epoch 67 \t Batch 220 \t Training Loss: 45.98457008708607\n",
      "Epoch 67 \t Batch 240 \t Training Loss: 46.05082262357076\n",
      "Epoch 67 \t Batch 260 \t Training Loss: 46.0502385359544\n",
      "Epoch 67 \t Batch 280 \t Training Loss: 45.982299137115476\n",
      "Epoch 67 \t Batch 300 \t Training Loss: 45.909334882100424\n",
      "Epoch 67 \t Batch 320 \t Training Loss: 45.92699900865555\n",
      "Epoch 67 \t Batch 340 \t Training Loss: 45.9668862398933\n",
      "Epoch 67 \t Batch 360 \t Training Loss: 45.993218909369574\n",
      "Epoch 67 \t Batch 380 \t Training Loss: 45.99334826218455\n",
      "Epoch 67 \t Batch 400 \t Training Loss: 46.00473077774048\n",
      "Epoch 67 \t Batch 420 \t Training Loss: 46.06342407408215\n",
      "Epoch 67 \t Batch 440 \t Training Loss: 46.06359534697099\n",
      "Epoch 67 \t Batch 460 \t Training Loss: 46.11412516884182\n",
      "Epoch 67 \t Batch 480 \t Training Loss: 46.1024657090505\n",
      "Epoch 67 \t Batch 500 \t Training Loss: 46.092066680908204\n",
      "Epoch 67 \t Batch 520 \t Training Loss: 46.05868040965154\n",
      "Epoch 67 \t Batch 540 \t Training Loss: 46.046119626363115\n",
      "Epoch 67 \t Batch 560 \t Training Loss: 46.05506640842983\n",
      "Epoch 67 \t Batch 580 \t Training Loss: 46.10496148076551\n",
      "Epoch 67 \t Batch 600 \t Training Loss: 46.09372374852499\n",
      "Epoch 67 \t Batch 620 \t Training Loss: 46.09335039815595\n",
      "Epoch 67 \t Batch 640 \t Training Loss: 46.12592443227768\n",
      "Epoch 67 \t Batch 660 \t Training Loss: 46.079119792129056\n",
      "Epoch 67 \t Batch 680 \t Training Loss: 46.04812511556289\n",
      "Epoch 67 \t Batch 700 \t Training Loss: 46.01049811771938\n",
      "Epoch 67 \t Batch 720 \t Training Loss: 45.99313521385193\n",
      "Epoch 67 \t Batch 740 \t Training Loss: 45.959510220708076\n",
      "Epoch 67 \t Batch 760 \t Training Loss: 45.94606337296335\n",
      "Epoch 67 \t Batch 780 \t Training Loss: 45.960970541147084\n",
      "Epoch 67 \t Batch 800 \t Training Loss: 45.976133270263674\n",
      "Epoch 67 \t Batch 820 \t Training Loss: 46.00239184309797\n",
      "Epoch 67 \t Batch 840 \t Training Loss: 46.03334077653431\n",
      "Epoch 67 \t Batch 860 \t Training Loss: 46.04861422694007\n",
      "Epoch 67 \t Batch 880 \t Training Loss: 46.04374189810319\n",
      "Epoch 67 \t Batch 900 \t Training Loss: 46.04790000491672\n",
      "Epoch 67 \t Batch 20 \t Validation Loss: 46.60104923248291\n",
      "Epoch 67 \t Batch 40 \t Validation Loss: 42.88911247253418\n",
      "Epoch 67 \t Batch 60 \t Validation Loss: 44.999018828074135\n",
      "Epoch 67 \t Batch 80 \t Validation Loss: 43.91188762187958\n",
      "Epoch 67 \t Batch 100 \t Validation Loss: 41.11473096847534\n",
      "Epoch 67 \t Batch 120 \t Validation Loss: 39.4402730623881\n",
      "Epoch 67 \t Batch 140 \t Validation Loss: 37.81085011618478\n",
      "Epoch 67 \t Batch 160 \t Validation Loss: 38.05133284330368\n",
      "Epoch 67 \t Batch 180 \t Validation Loss: 40.27787026299371\n",
      "Epoch 67 \t Batch 200 \t Validation Loss: 40.92173433303833\n",
      "Epoch 67 \t Batch 220 \t Validation Loss: 41.388072950189766\n",
      "Epoch 67 \t Batch 240 \t Validation Loss: 41.18025081157684\n",
      "Epoch 67 \t Batch 260 \t Validation Loss: 42.91908514683063\n",
      "Epoch 67 \t Batch 280 \t Validation Loss: 43.69615111691611\n",
      "Epoch 67 \t Batch 300 \t Validation Loss: 44.122424853642784\n",
      "Epoch 67 \t Batch 320 \t Validation Loss: 44.13356555998325\n",
      "Epoch 67 \t Batch 340 \t Validation Loss: 43.75192539271186\n",
      "Epoch 67 \t Batch 360 \t Validation Loss: 43.21154945956336\n",
      "Epoch 67 \t Batch 380 \t Validation Loss: 43.251458712628015\n",
      "Epoch 67 \t Batch 400 \t Validation Loss: 42.67374989271164\n",
      "Epoch 67 \t Batch 420 \t Validation Loss: 42.49098208972386\n",
      "Epoch 67 \t Batch 440 \t Validation Loss: 42.11981608434157\n",
      "Epoch 67 \t Batch 460 \t Validation Loss: 42.174529144038324\n",
      "Epoch 67 \t Batch 480 \t Validation Loss: 42.458669581015904\n",
      "Epoch 67 \t Batch 500 \t Validation Loss: 42.002769746780395\n",
      "Epoch 67 \t Batch 520 \t Validation Loss: 41.7396441422976\n",
      "Epoch 67 \t Batch 540 \t Validation Loss: 41.393759151741314\n",
      "Epoch 67 \t Batch 560 \t Validation Loss: 41.17923402445657\n",
      "Epoch 67 \t Batch 580 \t Validation Loss: 40.96032620462878\n",
      "Epoch 67 \t Batch 600 \t Validation Loss: 41.09251510302226\n",
      "Epoch 67 Training Loss: 46.0854642695403 Validation Loss: 41.675332883735756\n",
      "Epoch 67 completed\n",
      "Epoch 68 \t Batch 20 \t Training Loss: 47.216224098205565\n",
      "Epoch 68 \t Batch 40 \t Training Loss: 46.60356903076172\n",
      "Epoch 68 \t Batch 60 \t Training Loss: 46.15127042134603\n",
      "Epoch 68 \t Batch 80 \t Training Loss: 46.130575609207156\n",
      "Epoch 68 \t Batch 100 \t Training Loss: 46.319494819641115\n",
      "Epoch 68 \t Batch 120 \t Training Loss: 46.21595420837402\n",
      "Epoch 68 \t Batch 140 \t Training Loss: 46.24953259059361\n",
      "Epoch 68 \t Batch 160 \t Training Loss: 46.14087610244751\n",
      "Epoch 68 \t Batch 180 \t Training Loss: 46.17230623033311\n",
      "Epoch 68 \t Batch 200 \t Training Loss: 46.068778648376465\n",
      "Epoch 68 \t Batch 220 \t Training Loss: 46.150483564897016\n",
      "Epoch 68 \t Batch 240 \t Training Loss: 46.13504900932312\n",
      "Epoch 68 \t Batch 260 \t Training Loss: 46.11159973144531\n",
      "Epoch 68 \t Batch 280 \t Training Loss: 46.11974246161325\n",
      "Epoch 68 \t Batch 300 \t Training Loss: 46.10086279551188\n",
      "Epoch 68 \t Batch 320 \t Training Loss: 46.1614777803421\n",
      "Epoch 68 \t Batch 340 \t Training Loss: 46.136163801305436\n",
      "Epoch 68 \t Batch 360 \t Training Loss: 46.130742687649196\n",
      "Epoch 68 \t Batch 380 \t Training Loss: 46.080643292477255\n",
      "Epoch 68 \t Batch 400 \t Training Loss: 45.980062351226806\n",
      "Epoch 68 \t Batch 420 \t Training Loss: 45.97113937196278\n",
      "Epoch 68 \t Batch 440 \t Training Loss: 45.93429016633467\n",
      "Epoch 68 \t Batch 460 \t Training Loss: 45.985916743071186\n",
      "Epoch 68 \t Batch 480 \t Training Loss: 46.01436378955841\n",
      "Epoch 68 \t Batch 500 \t Training Loss: 45.99728624725342\n",
      "Epoch 68 \t Batch 520 \t Training Loss: 46.04537839155931\n",
      "Epoch 68 \t Batch 540 \t Training Loss: 46.10742313243725\n",
      "Epoch 68 \t Batch 560 \t Training Loss: 46.084411416734966\n",
      "Epoch 68 \t Batch 580 \t Training Loss: 46.078954775580044\n",
      "Epoch 68 \t Batch 600 \t Training Loss: 46.05750928243001\n",
      "Epoch 68 \t Batch 620 \t Training Loss: 46.06975855673513\n",
      "Epoch 68 \t Batch 640 \t Training Loss: 46.07290464043617\n",
      "Epoch 68 \t Batch 660 \t Training Loss: 46.09867678555575\n",
      "Epoch 68 \t Batch 680 \t Training Loss: 46.12482042312622\n",
      "Epoch 68 \t Batch 700 \t Training Loss: 46.15124433789934\n",
      "Epoch 68 \t Batch 720 \t Training Loss: 46.15262275801764\n",
      "Epoch 68 \t Batch 740 \t Training Loss: 46.167194649979876\n",
      "Epoch 68 \t Batch 760 \t Training Loss: 46.185692034269636\n",
      "Epoch 68 \t Batch 780 \t Training Loss: 46.182372973515434\n",
      "Epoch 68 \t Batch 800 \t Training Loss: 46.195240693092344\n",
      "Epoch 68 \t Batch 820 \t Training Loss: 46.176592752410144\n",
      "Epoch 68 \t Batch 840 \t Training Loss: 46.17244490214757\n",
      "Epoch 68 \t Batch 860 \t Training Loss: 46.15568882698236\n",
      "Epoch 68 \t Batch 880 \t Training Loss: 46.127972000295465\n",
      "Epoch 68 \t Batch 900 \t Training Loss: 46.08245509677463\n",
      "Epoch 68 \t Batch 20 \t Validation Loss: 42.744910430908206\n",
      "Epoch 68 \t Batch 40 \t Validation Loss: 39.824911785125735\n",
      "Epoch 68 \t Batch 60 \t Validation Loss: 41.96387742360433\n",
      "Epoch 68 \t Batch 80 \t Validation Loss: 40.66618721485138\n",
      "Epoch 68 \t Batch 100 \t Validation Loss: 38.27168787002564\n",
      "Epoch 68 \t Batch 120 \t Validation Loss: 36.880367549260455\n",
      "Epoch 68 \t Batch 140 \t Validation Loss: 35.497592026846746\n",
      "Epoch 68 \t Batch 160 \t Validation Loss: 36.14303172826767\n",
      "Epoch 68 \t Batch 180 \t Validation Loss: 38.51440821223789\n",
      "Epoch 68 \t Batch 200 \t Validation Loss: 39.36142562389374\n",
      "Epoch 68 \t Batch 220 \t Validation Loss: 39.940277233990756\n",
      "Epoch 68 \t Batch 240 \t Validation Loss: 39.84259211619695\n",
      "Epoch 68 \t Batch 260 \t Validation Loss: 41.6187027821174\n",
      "Epoch 68 \t Batch 280 \t Validation Loss: 42.43863229070391\n",
      "Epoch 68 \t Batch 300 \t Validation Loss: 42.93957911809286\n",
      "Epoch 68 \t Batch 320 \t Validation Loss: 43.055228835344316\n",
      "Epoch 68 \t Batch 340 \t Validation Loss: 42.75038362390855\n",
      "Epoch 68 \t Batch 360 \t Validation Loss: 42.33857638041179\n",
      "Epoch 68 \t Batch 380 \t Validation Loss: 42.48658457304302\n",
      "Epoch 68 \t Batch 400 \t Validation Loss: 42.08797675132752\n",
      "Epoch 68 \t Batch 420 \t Validation Loss: 42.00680385771252\n",
      "Epoch 68 \t Batch 440 \t Validation Loss: 41.849422402815385\n",
      "Epoch 68 \t Batch 460 \t Validation Loss: 42.04746700784435\n",
      "Epoch 68 \t Batch 480 \t Validation Loss: 42.38229628006617\n",
      "Epoch 68 \t Batch 500 \t Validation Loss: 41.973807678222656\n",
      "Epoch 68 \t Batch 520 \t Validation Loss: 41.93419095552885\n",
      "Epoch 68 \t Batch 540 \t Validation Loss: 41.59256125202885\n",
      "Epoch 68 \t Batch 560 \t Validation Loss: 41.34540308543614\n",
      "Epoch 68 \t Batch 580 \t Validation Loss: 41.10980764915203\n",
      "Epoch 68 \t Batch 600 \t Validation Loss: 41.225195258458456\n",
      "Epoch 68 Training Loss: 46.06057971384429 Validation Loss: 41.79440936484894\n",
      "Epoch 68 completed\n",
      "Epoch 69 \t Batch 20 \t Training Loss: 47.10922641754151\n",
      "Epoch 69 \t Batch 40 \t Training Loss: 46.163390731811525\n",
      "Epoch 69 \t Batch 60 \t Training Loss: 45.633561515808104\n",
      "Epoch 69 \t Batch 80 \t Training Loss: 45.76317949295044\n",
      "Epoch 69 \t Batch 100 \t Training Loss: 45.81197944641113\n",
      "Epoch 69 \t Batch 120 \t Training Loss: 45.871555137634275\n",
      "Epoch 69 \t Batch 140 \t Training Loss: 45.983424704415455\n",
      "Epoch 69 \t Batch 160 \t Training Loss: 46.189901447296144\n",
      "Epoch 69 \t Batch 180 \t Training Loss: 46.214890310499406\n",
      "Epoch 69 \t Batch 200 \t Training Loss: 45.988360176086424\n",
      "Epoch 69 \t Batch 220 \t Training Loss: 45.930728947032584\n",
      "Epoch 69 \t Batch 240 \t Training Loss: 45.93955979347229\n",
      "Epoch 69 \t Batch 260 \t Training Loss: 45.8199308688824\n",
      "Epoch 69 \t Batch 280 \t Training Loss: 45.84122280393328\n",
      "Epoch 69 \t Batch 300 \t Training Loss: 45.878468437194826\n",
      "Epoch 69 \t Batch 320 \t Training Loss: 45.9968642950058\n",
      "Epoch 69 \t Batch 340 \t Training Loss: 46.02561221403234\n",
      "Epoch 69 \t Batch 360 \t Training Loss: 46.16741272608439\n",
      "Epoch 69 \t Batch 380 \t Training Loss: 46.14912143506502\n",
      "Epoch 69 \t Batch 400 \t Training Loss: 46.2709020614624\n",
      "Epoch 69 \t Batch 420 \t Training Loss: 46.191720653715585\n",
      "Epoch 69 \t Batch 440 \t Training Loss: 46.14297113418579\n",
      "Epoch 69 \t Batch 460 \t Training Loss: 46.08526200004246\n",
      "Epoch 69 \t Batch 480 \t Training Loss: 46.157689253489174\n",
      "Epoch 69 \t Batch 500 \t Training Loss: 46.160004219055175\n",
      "Epoch 69 \t Batch 520 \t Training Loss: 46.15272581393902\n",
      "Epoch 69 \t Batch 540 \t Training Loss: 46.154485631872106\n",
      "Epoch 69 \t Batch 560 \t Training Loss: 46.14730290685381\n",
      "Epoch 69 \t Batch 580 \t Training Loss: 46.16310103843952\n",
      "Epoch 69 \t Batch 600 \t Training Loss: 46.149337793986\n",
      "Epoch 69 \t Batch 620 \t Training Loss: 46.189627105959\n",
      "Epoch 69 \t Batch 640 \t Training Loss: 46.21790754199028\n",
      "Epoch 69 \t Batch 660 \t Training Loss: 46.18706344835686\n",
      "Epoch 69 \t Batch 680 \t Training Loss: 46.173711271847\n",
      "Epoch 69 \t Batch 700 \t Training Loss: 46.12089676993234\n",
      "Epoch 69 \t Batch 720 \t Training Loss: 46.10274230109321\n",
      "Epoch 69 \t Batch 740 \t Training Loss: 46.08992351325782\n",
      "Epoch 69 \t Batch 760 \t Training Loss: 46.09445132205361\n",
      "Epoch 69 \t Batch 780 \t Training Loss: 46.0814981949635\n",
      "Epoch 69 \t Batch 800 \t Training Loss: 46.07974872112274\n",
      "Epoch 69 \t Batch 820 \t Training Loss: 46.06332613316978\n",
      "Epoch 69 \t Batch 840 \t Training Loss: 46.05187948771886\n",
      "Epoch 69 \t Batch 860 \t Training Loss: 46.085635455819066\n",
      "Epoch 69 \t Batch 880 \t Training Loss: 46.08680779283697\n",
      "Epoch 69 \t Batch 900 \t Training Loss: 46.06210644616021\n",
      "Epoch 69 \t Batch 20 \t Validation Loss: 64.09470424652099\n",
      "Epoch 69 \t Batch 40 \t Validation Loss: 57.11639518737793\n",
      "Epoch 69 \t Batch 60 \t Validation Loss: 60.335411898295085\n",
      "Epoch 69 \t Batch 80 \t Validation Loss: 57.66029002666473\n",
      "Epoch 69 \t Batch 100 \t Validation Loss: 52.29915218353271\n",
      "Epoch 69 \t Batch 120 \t Validation Loss: 48.87056805292765\n",
      "Epoch 69 \t Batch 140 \t Validation Loss: 45.891514096941265\n",
      "Epoch 69 \t Batch 160 \t Validation Loss: 45.22219018936157\n",
      "Epoch 69 \t Batch 180 \t Validation Loss: 46.62625158098009\n",
      "Epoch 69 \t Batch 200 \t Validation Loss: 46.5663223361969\n",
      "Epoch 69 \t Batch 220 \t Validation Loss: 46.57265857349743\n",
      "Epoch 69 \t Batch 240 \t Validation Loss: 45.976312394936876\n",
      "Epoch 69 \t Batch 260 \t Validation Loss: 47.342859506607056\n",
      "Epoch 69 \t Batch 280 \t Validation Loss: 47.79042145865304\n",
      "Epoch 69 \t Batch 300 \t Validation Loss: 47.93375358581543\n",
      "Epoch 69 \t Batch 320 \t Validation Loss: 47.75485470294952\n",
      "Epoch 69 \t Batch 340 \t Validation Loss: 47.17663434533512\n",
      "Epoch 69 \t Batch 360 \t Validation Loss: 46.48995091120402\n",
      "Epoch 69 \t Batch 380 \t Validation Loss: 46.376828417025116\n",
      "Epoch 69 \t Batch 400 \t Validation Loss: 45.63524390935898\n",
      "Epoch 69 \t Batch 420 \t Validation Loss: 45.33371574538095\n",
      "Epoch 69 \t Batch 440 \t Validation Loss: 44.78085711869326\n",
      "Epoch 69 \t Batch 460 \t Validation Loss: 44.8364621763644\n",
      "Epoch 69 \t Batch 480 \t Validation Loss: 45.03527461886406\n",
      "Epoch 69 \t Batch 500 \t Validation Loss: 44.51072069740295\n",
      "Epoch 69 \t Batch 520 \t Validation Loss: 44.183528390297525\n",
      "Epoch 69 \t Batch 540 \t Validation Loss: 43.73983279334174\n",
      "Epoch 69 \t Batch 560 \t Validation Loss: 43.331481317111425\n",
      "Epoch 69 \t Batch 580 \t Validation Loss: 42.904812454355174\n",
      "Epoch 69 \t Batch 600 \t Validation Loss: 42.9695368830363\n",
      "Epoch 69 Training Loss: 46.04885952865284 Validation Loss: 43.48131389432139\n",
      "Epoch 69 completed\n",
      "Epoch 70 \t Batch 20 \t Training Loss: 46.868493461608885\n",
      "Epoch 70 \t Batch 40 \t Training Loss: 46.556486415863034\n",
      "Epoch 70 \t Batch 60 \t Training Loss: 47.15709953308105\n",
      "Epoch 70 \t Batch 80 \t Training Loss: 46.544303226470944\n",
      "Epoch 70 \t Batch 100 \t Training Loss: 46.268633651733396\n",
      "Epoch 70 \t Batch 120 \t Training Loss: 46.14106953938802\n",
      "Epoch 70 \t Batch 140 \t Training Loss: 45.96115155901228\n",
      "Epoch 70 \t Batch 160 \t Training Loss: 46.13254535198212\n",
      "Epoch 70 \t Batch 180 \t Training Loss: 46.096507962544756\n",
      "Epoch 70 \t Batch 200 \t Training Loss: 46.084218692779544\n",
      "Epoch 70 \t Batch 220 \t Training Loss: 46.04664982882413\n",
      "Epoch 70 \t Batch 240 \t Training Loss: 45.98618399302165\n",
      "Epoch 70 \t Batch 260 \t Training Loss: 46.02468409905067\n",
      "Epoch 70 \t Batch 280 \t Training Loss: 46.07399915967669\n",
      "Epoch 70 \t Batch 300 \t Training Loss: 46.024838574727376\n",
      "Epoch 70 \t Batch 320 \t Training Loss: 45.9813395857811\n",
      "Epoch 70 \t Batch 340 \t Training Loss: 45.953989892847396\n",
      "Epoch 70 \t Batch 360 \t Training Loss: 45.98880430857341\n",
      "Epoch 70 \t Batch 380 \t Training Loss: 45.90344857667622\n",
      "Epoch 70 \t Batch 400 \t Training Loss: 45.874218444824216\n",
      "Epoch 70 \t Batch 420 \t Training Loss: 45.895491509210494\n",
      "Epoch 70 \t Batch 440 \t Training Loss: 45.89290357069536\n",
      "Epoch 70 \t Batch 460 \t Training Loss: 45.88107464002526\n",
      "Epoch 70 \t Batch 480 \t Training Loss: 45.896360603968304\n",
      "Epoch 70 \t Batch 500 \t Training Loss: 45.885799507141115\n",
      "Epoch 70 \t Batch 520 \t Training Loss: 45.84594736099243\n",
      "Epoch 70 \t Batch 540 \t Training Loss: 45.839409277174205\n",
      "Epoch 70 \t Batch 560 \t Training Loss: 45.88026142120361\n",
      "Epoch 70 \t Batch 580 \t Training Loss: 45.90437796362515\n",
      "Epoch 70 \t Batch 600 \t Training Loss: 45.89714422861735\n",
      "Epoch 70 \t Batch 620 \t Training Loss: 45.91804464401737\n",
      "Epoch 70 \t Batch 640 \t Training Loss: 45.930284798145294\n",
      "Epoch 70 \t Batch 660 \t Training Loss: 45.92091386968439\n",
      "Epoch 70 \t Batch 680 \t Training Loss: 45.953309603298415\n",
      "Epoch 70 \t Batch 700 \t Training Loss: 45.9633555657523\n",
      "Epoch 70 \t Batch 720 \t Training Loss: 45.93034205966526\n",
      "Epoch 70 \t Batch 740 \t Training Loss: 45.93519680951093\n",
      "Epoch 70 \t Batch 760 \t Training Loss: 45.96043788006431\n",
      "Epoch 70 \t Batch 780 \t Training Loss: 45.94609282566951\n",
      "Epoch 70 \t Batch 800 \t Training Loss: 45.94303222179413\n",
      "Epoch 70 \t Batch 820 \t Training Loss: 45.9795083348344\n",
      "Epoch 70 \t Batch 840 \t Training Loss: 46.002537695566815\n",
      "Epoch 70 \t Batch 860 \t Training Loss: 46.010648864923525\n",
      "Epoch 70 \t Batch 880 \t Training Loss: 45.97097999832847\n",
      "Epoch 70 \t Batch 900 \t Training Loss: 46.000355580647785\n",
      "Epoch 70 \t Batch 20 \t Validation Loss: 53.05127954483032\n",
      "Epoch 70 \t Batch 40 \t Validation Loss: 47.88127031326294\n",
      "Epoch 70 \t Batch 60 \t Validation Loss: 50.45742254257202\n",
      "Epoch 70 \t Batch 80 \t Validation Loss: 48.29339962005615\n",
      "Epoch 70 \t Batch 100 \t Validation Loss: 44.943035202026365\n",
      "Epoch 70 \t Batch 120 \t Validation Loss: 42.80845287640889\n",
      "Epoch 70 \t Batch 140 \t Validation Loss: 40.722154194968084\n",
      "Epoch 70 \t Batch 160 \t Validation Loss: 40.60379345417023\n",
      "Epoch 70 \t Batch 180 \t Validation Loss: 42.1488954755995\n",
      "Epoch 70 \t Batch 200 \t Validation Loss: 42.41393436431885\n",
      "Epoch 70 \t Batch 220 \t Validation Loss: 42.5283614765514\n",
      "Epoch 70 \t Batch 240 \t Validation Loss: 42.0821204106013\n",
      "Epoch 70 \t Batch 260 \t Validation Loss: 43.52440670453585\n",
      "Epoch 70 \t Batch 280 \t Validation Loss: 44.11810240064349\n",
      "Epoch 70 \t Batch 300 \t Validation Loss: 44.29660546620687\n",
      "Epoch 70 \t Batch 320 \t Validation Loss: 44.22763864099979\n",
      "Epoch 70 \t Batch 340 \t Validation Loss: 43.820023068259744\n",
      "Epoch 70 \t Batch 360 \t Validation Loss: 43.296179509162904\n",
      "Epoch 70 \t Batch 380 \t Validation Loss: 43.327768584301595\n",
      "Epoch 70 \t Batch 400 \t Validation Loss: 42.85016313314438\n",
      "Epoch 70 \t Batch 420 \t Validation Loss: 42.69025653884525\n",
      "Epoch 70 \t Batch 440 \t Validation Loss: 42.419291494109416\n",
      "Epoch 70 \t Batch 460 \t Validation Loss: 42.578988740755165\n",
      "Epoch 70 \t Batch 480 \t Validation Loss: 42.860368901491164\n",
      "Epoch 70 \t Batch 500 \t Validation Loss: 42.405175745010375\n",
      "Epoch 70 \t Batch 520 \t Validation Loss: 42.26031331465794\n",
      "Epoch 70 \t Batch 540 \t Validation Loss: 41.910112361554745\n",
      "Epoch 70 \t Batch 560 \t Validation Loss: 41.672297983510155\n",
      "Epoch 70 \t Batch 580 \t Validation Loss: 41.48484928361301\n",
      "Epoch 70 \t Batch 600 \t Validation Loss: 41.59542264461518\n",
      "Epoch 70 Training Loss: 46.011547995497644 Validation Loss: 42.160802084130125\n",
      "Epoch 70 completed\n",
      "Epoch 71 \t Batch 20 \t Training Loss: 46.03689765930176\n",
      "Epoch 71 \t Batch 40 \t Training Loss: 45.24753398895264\n",
      "Epoch 71 \t Batch 60 \t Training Loss: 45.34131514231364\n",
      "Epoch 71 \t Batch 80 \t Training Loss: 45.79683303833008\n",
      "Epoch 71 \t Batch 100 \t Training Loss: 45.86449775695801\n",
      "Epoch 71 \t Batch 120 \t Training Loss: 45.90939153035482\n",
      "Epoch 71 \t Batch 140 \t Training Loss: 45.81973114013672\n",
      "Epoch 71 \t Batch 160 \t Training Loss: 46.02879881858826\n",
      "Epoch 71 \t Batch 180 \t Training Loss: 46.11566630469428\n",
      "Epoch 71 \t Batch 200 \t Training Loss: 46.133950843811036\n",
      "Epoch 71 \t Batch 220 \t Training Loss: 46.16849077398127\n",
      "Epoch 71 \t Batch 240 \t Training Loss: 46.13253801663716\n",
      "Epoch 71 \t Batch 260 \t Training Loss: 46.11504697066087\n",
      "Epoch 71 \t Batch 280 \t Training Loss: 46.14142912455967\n",
      "Epoch 71 \t Batch 300 \t Training Loss: 46.08022293090821\n",
      "Epoch 71 \t Batch 320 \t Training Loss: 46.11488673686981\n",
      "Epoch 71 \t Batch 340 \t Training Loss: 46.060318812202006\n",
      "Epoch 71 \t Batch 360 \t Training Loss: 46.02695529725816\n",
      "Epoch 71 \t Batch 380 \t Training Loss: 45.98809252287212\n",
      "Epoch 71 \t Batch 400 \t Training Loss: 46.00533415794373\n",
      "Epoch 71 \t Batch 420 \t Training Loss: 46.06722928001767\n",
      "Epoch 71 \t Batch 440 \t Training Loss: 45.98826244527643\n",
      "Epoch 71 \t Batch 460 \t Training Loss: 46.054451643902325\n",
      "Epoch 71 \t Batch 480 \t Training Loss: 46.0790932337443\n",
      "Epoch 71 \t Batch 500 \t Training Loss: 46.08258264160156\n",
      "Epoch 71 \t Batch 520 \t Training Loss: 46.05310781185444\n",
      "Epoch 71 \t Batch 540 \t Training Loss: 45.98036053268998\n",
      "Epoch 71 \t Batch 560 \t Training Loss: 45.92428582736424\n",
      "Epoch 71 \t Batch 580 \t Training Loss: 45.90854867737869\n",
      "Epoch 71 \t Batch 600 \t Training Loss: 45.973019167582194\n",
      "Epoch 71 \t Batch 620 \t Training Loss: 45.99374454252182\n",
      "Epoch 71 \t Batch 640 \t Training Loss: 46.03557502031326\n",
      "Epoch 71 \t Batch 660 \t Training Loss: 46.06731430978486\n",
      "Epoch 71 \t Batch 680 \t Training Loss: 46.0233307614046\n",
      "Epoch 71 \t Batch 700 \t Training Loss: 45.99008712768555\n",
      "Epoch 71 \t Batch 720 \t Training Loss: 45.9992198255327\n",
      "Epoch 71 \t Batch 740 \t Training Loss: 46.003761585338694\n",
      "Epoch 71 \t Batch 760 \t Training Loss: 46.00050560298719\n",
      "Epoch 71 \t Batch 780 \t Training Loss: 45.99528586069743\n",
      "Epoch 71 \t Batch 800 \t Training Loss: 46.00192009449005\n",
      "Epoch 71 \t Batch 820 \t Training Loss: 45.99561047902921\n",
      "Epoch 71 \t Batch 840 \t Training Loss: 45.9794722647894\n",
      "Epoch 71 \t Batch 860 \t Training Loss: 46.00010896727096\n",
      "Epoch 71 \t Batch 880 \t Training Loss: 45.998941850662234\n",
      "Epoch 71 \t Batch 900 \t Training Loss: 46.00336380852593\n",
      "Epoch 71 \t Batch 20 \t Validation Loss: 45.3483642578125\n",
      "Epoch 71 \t Batch 40 \t Validation Loss: 41.94938547611237\n",
      "Epoch 71 \t Batch 60 \t Validation Loss: 43.603926769892375\n",
      "Epoch 71 \t Batch 80 \t Validation Loss: 42.282243001461026\n",
      "Epoch 71 \t Batch 100 \t Validation Loss: 39.38052010536194\n",
      "Epoch 71 \t Batch 120 \t Validation Loss: 37.74149973392487\n",
      "Epoch 71 \t Batch 140 \t Validation Loss: 36.17313750811986\n",
      "Epoch 71 \t Batch 160 \t Validation Loss: 36.531752449274066\n",
      "Epoch 71 \t Batch 180 \t Validation Loss: 38.28845590485467\n",
      "Epoch 71 \t Batch 200 \t Validation Loss: 38.83610082149506\n",
      "Epoch 71 \t Batch 220 \t Validation Loss: 39.119215900247745\n",
      "Epoch 71 \t Batch 240 \t Validation Loss: 38.826093403498334\n",
      "Epoch 71 \t Batch 260 \t Validation Loss: 40.35576583788945\n",
      "Epoch 71 \t Batch 280 \t Validation Loss: 40.98905885219574\n",
      "Epoch 71 \t Batch 300 \t Validation Loss: 41.29158612251282\n",
      "Epoch 71 \t Batch 320 \t Validation Loss: 41.3714735686779\n",
      "Epoch 71 \t Batch 340 \t Validation Loss: 41.10964069366455\n",
      "Epoch 71 \t Batch 360 \t Validation Loss: 40.68880519602034\n",
      "Epoch 71 \t Batch 380 \t Validation Loss: 40.762673199804205\n",
      "Epoch 71 \t Batch 400 \t Validation Loss: 40.34514387845993\n",
      "Epoch 71 \t Batch 420 \t Validation Loss: 40.32514652524676\n",
      "Epoch 71 \t Batch 440 \t Validation Loss: 40.07068560340188\n",
      "Epoch 71 \t Batch 460 \t Validation Loss: 40.3117462116739\n",
      "Epoch 71 \t Batch 480 \t Validation Loss: 40.69868399699529\n",
      "Epoch 71 \t Batch 500 \t Validation Loss: 40.33625631713867\n",
      "Epoch 71 \t Batch 520 \t Validation Loss: 40.220670184722316\n",
      "Epoch 71 \t Batch 540 \t Validation Loss: 39.86593009630839\n",
      "Epoch 71 \t Batch 560 \t Validation Loss: 39.5703491296087\n",
      "Epoch 71 \t Batch 580 \t Validation Loss: 39.25487035060751\n",
      "Epoch 71 \t Batch 600 \t Validation Loss: 39.38197599252065\n",
      "Epoch 71 Training Loss: 45.98546904851982 Validation Loss: 39.96238185368575\n",
      "Epoch 71 completed\n",
      "Epoch 72 \t Batch 20 \t Training Loss: 44.73641777038574\n",
      "Epoch 72 \t Batch 40 \t Training Loss: 45.459543991088864\n",
      "Epoch 72 \t Batch 60 \t Training Loss: 45.559891891479495\n",
      "Epoch 72 \t Batch 80 \t Training Loss: 45.694540929794314\n",
      "Epoch 72 \t Batch 100 \t Training Loss: 45.78417446136475\n",
      "Epoch 72 \t Batch 120 \t Training Loss: 45.89680836995443\n",
      "Epoch 72 \t Batch 140 \t Training Loss: 45.811583164760044\n",
      "Epoch 72 \t Batch 160 \t Training Loss: 45.78070907592773\n",
      "Epoch 72 \t Batch 180 \t Training Loss: 45.77612671322293\n",
      "Epoch 72 \t Batch 200 \t Training Loss: 45.767841243743895\n",
      "Epoch 72 \t Batch 220 \t Training Loss: 45.79718530828303\n",
      "Epoch 72 \t Batch 240 \t Training Loss: 45.73003520965576\n",
      "Epoch 72 \t Batch 260 \t Training Loss: 45.7652619435237\n",
      "Epoch 72 \t Batch 280 \t Training Loss: 45.796146515437535\n",
      "Epoch 72 \t Batch 300 \t Training Loss: 45.886518491109214\n",
      "Epoch 72 \t Batch 320 \t Training Loss: 46.00712183713913\n",
      "Epoch 72 \t Batch 340 \t Training Loss: 46.00841900320614\n",
      "Epoch 72 \t Batch 360 \t Training Loss: 46.045729054345024\n",
      "Epoch 72 \t Batch 380 \t Training Loss: 46.03635689584832\n",
      "Epoch 72 \t Batch 400 \t Training Loss: 46.10528024673462\n",
      "Epoch 72 \t Batch 420 \t Training Loss: 46.15163127354213\n",
      "Epoch 72 \t Batch 440 \t Training Loss: 46.16021109494296\n",
      "Epoch 72 \t Batch 460 \t Training Loss: 46.10168857574463\n",
      "Epoch 72 \t Batch 480 \t Training Loss: 46.050229438145955\n",
      "Epoch 72 \t Batch 500 \t Training Loss: 46.03253207397461\n",
      "Epoch 72 \t Batch 520 \t Training Loss: 46.050350189208984\n",
      "Epoch 72 \t Batch 540 \t Training Loss: 46.08795630137126\n",
      "Epoch 72 \t Batch 560 \t Training Loss: 46.08349272864206\n",
      "Epoch 72 \t Batch 580 \t Training Loss: 46.06701534534323\n",
      "Epoch 72 \t Batch 600 \t Training Loss: 46.07663971583049\n",
      "Epoch 72 \t Batch 620 \t Training Loss: 46.08579524255568\n",
      "Epoch 72 \t Batch 640 \t Training Loss: 46.03721686601639\n",
      "Epoch 72 \t Batch 660 \t Training Loss: 46.04293687415846\n",
      "Epoch 72 \t Batch 680 \t Training Loss: 45.993553520651425\n",
      "Epoch 72 \t Batch 700 \t Training Loss: 45.97195451463972\n",
      "Epoch 72 \t Batch 720 \t Training Loss: 45.98960252337986\n",
      "Epoch 72 \t Batch 740 \t Training Loss: 45.98682277782543\n",
      "Epoch 72 \t Batch 760 \t Training Loss: 46.016470020695735\n",
      "Epoch 72 \t Batch 780 \t Training Loss: 46.004191198104465\n",
      "Epoch 72 \t Batch 800 \t Training Loss: 45.99791552066803\n",
      "Epoch 72 \t Batch 820 \t Training Loss: 45.98283771421851\n",
      "Epoch 72 \t Batch 840 \t Training Loss: 45.979079832349505\n",
      "Epoch 72 \t Batch 860 \t Training Loss: 45.96392106344533\n",
      "Epoch 72 \t Batch 880 \t Training Loss: 45.9572509245439\n",
      "Epoch 72 \t Batch 900 \t Training Loss: 45.98694524553087\n",
      "Epoch 72 \t Batch 20 \t Validation Loss: 43.86787738800049\n",
      "Epoch 72 \t Batch 40 \t Validation Loss: 39.688903999328616\n",
      "Epoch 72 \t Batch 60 \t Validation Loss: 41.75823265711467\n",
      "Epoch 72 \t Batch 80 \t Validation Loss: 40.254228031635286\n",
      "Epoch 72 \t Batch 100 \t Validation Loss: 37.78997353553772\n",
      "Epoch 72 \t Batch 120 \t Validation Loss: 36.379423220952354\n",
      "Epoch 72 \t Batch 140 \t Validation Loss: 34.98952234813145\n",
      "Epoch 72 \t Batch 160 \t Validation Loss: 35.52307279109955\n",
      "Epoch 72 \t Batch 180 \t Validation Loss: 37.741849252912736\n",
      "Epoch 72 \t Batch 200 \t Validation Loss: 38.49773648262024\n",
      "Epoch 72 \t Batch 220 \t Validation Loss: 39.02127653468739\n",
      "Epoch 72 \t Batch 240 \t Validation Loss: 38.906425805886585\n",
      "Epoch 72 \t Batch 260 \t Validation Loss: 40.65878183658307\n",
      "Epoch 72 \t Batch 280 \t Validation Loss: 41.46919080529894\n",
      "Epoch 72 \t Batch 300 \t Validation Loss: 41.87156886100769\n",
      "Epoch 72 \t Batch 320 \t Validation Loss: 41.97559668123722\n",
      "Epoch 72 \t Batch 340 \t Validation Loss: 41.68306333036984\n",
      "Epoch 72 \t Batch 360 \t Validation Loss: 41.250243208143445\n",
      "Epoch 72 \t Batch 380 \t Validation Loss: 41.404452835886104\n",
      "Epoch 72 \t Batch 400 \t Validation Loss: 40.960612444877626\n",
      "Epoch 72 \t Batch 420 \t Validation Loss: 40.890167554219566\n",
      "Epoch 72 \t Batch 440 \t Validation Loss: 40.62034766890786\n",
      "Epoch 72 \t Batch 460 \t Validation Loss: 40.86233029158219\n",
      "Epoch 72 \t Batch 480 \t Validation Loss: 41.21841744184494\n",
      "Epoch 72 \t Batch 500 \t Validation Loss: 40.82324566268921\n",
      "Epoch 72 \t Batch 520 \t Validation Loss: 40.730868555949286\n",
      "Epoch 72 \t Batch 540 \t Validation Loss: 40.38710805045234\n",
      "Epoch 72 \t Batch 560 \t Validation Loss: 40.110185858181545\n",
      "Epoch 72 \t Batch 580 \t Validation Loss: 39.82276569563767\n",
      "Epoch 72 \t Batch 600 \t Validation Loss: 39.952360445658364\n",
      "Epoch 72 Training Loss: 45.9572703404151 Validation Loss: 40.54332684541677\n",
      "Epoch 72 completed\n",
      "Epoch 73 \t Batch 20 \t Training Loss: 45.94826679229736\n",
      "Epoch 73 \t Batch 40 \t Training Loss: 45.86731748580932\n",
      "Epoch 73 \t Batch 60 \t Training Loss: 46.07740338643392\n",
      "Epoch 73 \t Batch 80 \t Training Loss: 45.98977856636047\n",
      "Epoch 73 \t Batch 100 \t Training Loss: 46.20778697967529\n",
      "Epoch 73 \t Batch 120 \t Training Loss: 46.20832929611206\n",
      "Epoch 73 \t Batch 140 \t Training Loss: 46.32655942099435\n",
      "Epoch 73 \t Batch 160 \t Training Loss: 46.30212378501892\n",
      "Epoch 73 \t Batch 180 \t Training Loss: 46.052744420369464\n",
      "Epoch 73 \t Batch 200 \t Training Loss: 46.03560657501221\n",
      "Epoch 73 \t Batch 220 \t Training Loss: 45.945606803894044\n",
      "Epoch 73 \t Batch 240 \t Training Loss: 45.933267498016356\n",
      "Epoch 73 \t Batch 260 \t Training Loss: 45.97934614328238\n",
      "Epoch 73 \t Batch 280 \t Training Loss: 45.934708731515066\n",
      "Epoch 73 \t Batch 300 \t Training Loss: 45.966294326782226\n",
      "Epoch 73 \t Batch 320 \t Training Loss: 46.00327408313751\n",
      "Epoch 73 \t Batch 340 \t Training Loss: 46.02634088852826\n",
      "Epoch 73 \t Batch 360 \t Training Loss: 46.00950761371189\n",
      "Epoch 73 \t Batch 380 \t Training Loss: 46.02682278281764\n",
      "Epoch 73 \t Batch 400 \t Training Loss: 46.052553482055664\n",
      "Epoch 73 \t Batch 420 \t Training Loss: 46.061675880068826\n",
      "Epoch 73 \t Batch 440 \t Training Loss: 46.088566303253174\n",
      "Epoch 73 \t Batch 460 \t Training Loss: 46.10346856324569\n",
      "Epoch 73 \t Batch 480 \t Training Loss: 46.07683117389679\n",
      "Epoch 73 \t Batch 500 \t Training Loss: 46.10276531982422\n",
      "Epoch 73 \t Batch 520 \t Training Loss: 46.07695494431716\n",
      "Epoch 73 \t Batch 540 \t Training Loss: 46.112968628494826\n",
      "Epoch 73 \t Batch 560 \t Training Loss: 46.11891042164394\n",
      "Epoch 73 \t Batch 580 \t Training Loss: 46.134327165011705\n",
      "Epoch 73 \t Batch 600 \t Training Loss: 46.096107425689695\n",
      "Epoch 73 \t Batch 620 \t Training Loss: 46.099197449222686\n",
      "Epoch 73 \t Batch 640 \t Training Loss: 46.06402086019516\n",
      "Epoch 73 \t Batch 660 \t Training Loss: 46.04083835717404\n",
      "Epoch 73 \t Batch 680 \t Training Loss: 46.0830758599674\n",
      "Epoch 73 \t Batch 700 \t Training Loss: 46.11140492575509\n",
      "Epoch 73 \t Batch 720 \t Training Loss: 46.10436165597704\n",
      "Epoch 73 \t Batch 740 \t Training Loss: 46.08732532295021\n",
      "Epoch 73 \t Batch 760 \t Training Loss: 46.08622971082988\n",
      "Epoch 73 \t Batch 780 \t Training Loss: 46.0926639850323\n",
      "Epoch 73 \t Batch 800 \t Training Loss: 46.07486613750458\n",
      "Epoch 73 \t Batch 820 \t Training Loss: 46.064927608210866\n",
      "Epoch 73 \t Batch 840 \t Training Loss: 46.050826277051655\n",
      "Epoch 73 \t Batch 860 \t Training Loss: 46.00778495877288\n",
      "Epoch 73 \t Batch 880 \t Training Loss: 45.99758154262196\n",
      "Epoch 73 \t Batch 900 \t Training Loss: 45.94838255564372\n",
      "Epoch 73 \t Batch 20 \t Validation Loss: 56.50671968460083\n",
      "Epoch 73 \t Batch 40 \t Validation Loss: 50.01832756996155\n",
      "Epoch 73 \t Batch 60 \t Validation Loss: 53.52026786804199\n",
      "Epoch 73 \t Batch 80 \t Validation Loss: 50.82578411102295\n",
      "Epoch 73 \t Batch 100 \t Validation Loss: 46.38600366592407\n",
      "Epoch 73 \t Batch 120 \t Validation Loss: 43.682091824213664\n",
      "Epoch 73 \t Batch 140 \t Validation Loss: 41.41353258405413\n",
      "Epoch 73 \t Batch 160 \t Validation Loss: 41.36942547559738\n",
      "Epoch 73 \t Batch 180 \t Validation Loss: 43.28665251202054\n",
      "Epoch 73 \t Batch 200 \t Validation Loss: 43.698084344863894\n",
      "Epoch 73 \t Batch 220 \t Validation Loss: 43.98679421164773\n",
      "Epoch 73 \t Batch 240 \t Validation Loss: 43.630348312854764\n",
      "Epoch 73 \t Batch 260 \t Validation Loss: 45.218228985713075\n",
      "Epoch 73 \t Batch 280 \t Validation Loss: 45.87076973574502\n",
      "Epoch 73 \t Batch 300 \t Validation Loss: 46.17687673250834\n",
      "Epoch 73 \t Batch 320 \t Validation Loss: 46.11601404249668\n",
      "Epoch 73 \t Batch 340 \t Validation Loss: 45.680046336791094\n",
      "Epoch 73 \t Batch 360 \t Validation Loss: 45.14707083437178\n",
      "Epoch 73 \t Batch 380 \t Validation Loss: 45.153907236300014\n",
      "Epoch 73 \t Batch 400 \t Validation Loss: 44.625690972805025\n",
      "Epoch 73 \t Batch 420 \t Validation Loss: 44.43668965612139\n",
      "Epoch 73 \t Batch 440 \t Validation Loss: 44.12972713600505\n",
      "Epoch 73 \t Batch 460 \t Validation Loss: 44.21346979348556\n",
      "Epoch 73 \t Batch 480 \t Validation Loss: 44.45840547680855\n",
      "Epoch 73 \t Batch 500 \t Validation Loss: 43.98115540885925\n",
      "Epoch 73 \t Batch 520 \t Validation Loss: 43.79138706097236\n",
      "Epoch 73 \t Batch 540 \t Validation Loss: 43.37980471893593\n",
      "Epoch 73 \t Batch 560 \t Validation Loss: 43.06060963528497\n",
      "Epoch 73 \t Batch 580 \t Validation Loss: 42.777552836516804\n",
      "Epoch 73 \t Batch 600 \t Validation Loss: 42.82247288544973\n",
      "Epoch 73 Training Loss: 45.96210714774927 Validation Loss: 43.36195776214847\n",
      "Epoch 73 completed\n",
      "Epoch 74 \t Batch 20 \t Training Loss: 46.60233688354492\n",
      "Epoch 74 \t Batch 40 \t Training Loss: 46.363613796234134\n",
      "Epoch 74 \t Batch 60 \t Training Loss: 46.39270699818929\n",
      "Epoch 74 \t Batch 80 \t Training Loss: 46.110353994369504\n",
      "Epoch 74 \t Batch 100 \t Training Loss: 45.90906700134278\n",
      "Epoch 74 \t Batch 120 \t Training Loss: 45.58296820322673\n",
      "Epoch 74 \t Batch 140 \t Training Loss: 45.772660636901854\n",
      "Epoch 74 \t Batch 160 \t Training Loss: 45.87148780822754\n",
      "Epoch 74 \t Batch 180 \t Training Loss: 45.97704010009765\n",
      "Epoch 74 \t Batch 200 \t Training Loss: 45.89309461593628\n",
      "Epoch 74 \t Batch 220 \t Training Loss: 45.80597685033625\n",
      "Epoch 74 \t Batch 240 \t Training Loss: 45.77942646344503\n",
      "Epoch 74 \t Batch 260 \t Training Loss: 45.824927374032825\n",
      "Epoch 74 \t Batch 280 \t Training Loss: 45.77796662194388\n",
      "Epoch 74 \t Batch 300 \t Training Loss: 45.800986506144206\n",
      "Epoch 74 \t Batch 320 \t Training Loss: 45.76450902223587\n",
      "Epoch 74 \t Batch 340 \t Training Loss: 45.746144204981185\n",
      "Epoch 74 \t Batch 360 \t Training Loss: 45.745896106296115\n",
      "Epoch 74 \t Batch 380 \t Training Loss: 45.79125800885652\n",
      "Epoch 74 \t Batch 400 \t Training Loss: 45.80095910072327\n",
      "Epoch 74 \t Batch 420 \t Training Loss: 45.865327571687246\n",
      "Epoch 74 \t Batch 440 \t Training Loss: 45.86357979340987\n",
      "Epoch 74 \t Batch 460 \t Training Loss: 45.88536143924879\n",
      "Epoch 74 \t Batch 480 \t Training Loss: 45.84895248413086\n",
      "Epoch 74 \t Batch 500 \t Training Loss: 45.89218877410889\n",
      "Epoch 74 \t Batch 520 \t Training Loss: 45.894642602480374\n",
      "Epoch 74 \t Batch 540 \t Training Loss: 45.89700584411621\n",
      "Epoch 74 \t Batch 560 \t Training Loss: 45.91854387692043\n",
      "Epoch 74 \t Batch 580 \t Training Loss: 45.93002121366303\n",
      "Epoch 74 \t Batch 600 \t Training Loss: 45.91988151550293\n",
      "Epoch 74 \t Batch 620 \t Training Loss: 45.918831763728974\n",
      "Epoch 74 \t Batch 640 \t Training Loss: 45.94777507185936\n",
      "Epoch 74 \t Batch 660 \t Training Loss: 45.9822446938717\n",
      "Epoch 74 \t Batch 680 \t Training Loss: 45.99675225089578\n",
      "Epoch 74 \t Batch 700 \t Training Loss: 45.99649242401123\n",
      "Epoch 74 \t Batch 720 \t Training Loss: 45.99396060307821\n",
      "Epoch 74 \t Batch 740 \t Training Loss: 45.944699256484576\n",
      "Epoch 74 \t Batch 760 \t Training Loss: 45.94425892578928\n",
      "Epoch 74 \t Batch 780 \t Training Loss: 45.94909720787635\n",
      "Epoch 74 \t Batch 800 \t Training Loss: 45.93313734531402\n",
      "Epoch 74 \t Batch 820 \t Training Loss: 45.93650114943342\n",
      "Epoch 74 \t Batch 840 \t Training Loss: 45.938163807278585\n",
      "Epoch 74 \t Batch 860 \t Training Loss: 45.93494325682175\n",
      "Epoch 74 \t Batch 880 \t Training Loss: 45.93972631801258\n",
      "Epoch 74 \t Batch 900 \t Training Loss: 45.932710316975914\n",
      "Epoch 74 \t Batch 20 \t Validation Loss: 49.69353513717651\n",
      "Epoch 74 \t Batch 40 \t Validation Loss: 43.93710196018219\n",
      "Epoch 74 \t Batch 60 \t Validation Loss: 46.7149249235789\n",
      "Epoch 74 \t Batch 80 \t Validation Loss: 44.91547682285309\n",
      "Epoch 74 \t Batch 100 \t Validation Loss: 41.98700578689575\n",
      "Epoch 74 \t Batch 120 \t Validation Loss: 40.28623838424683\n",
      "Epoch 74 \t Batch 140 \t Validation Loss: 38.54691159384591\n",
      "Epoch 74 \t Batch 160 \t Validation Loss: 39.001706910133365\n",
      "Epoch 74 \t Batch 180 \t Validation Loss: 41.25992573632134\n",
      "Epoch 74 \t Batch 200 \t Validation Loss: 41.988846416473386\n",
      "Epoch 74 \t Batch 220 \t Validation Loss: 42.46945256319913\n",
      "Epoch 74 \t Batch 240 \t Validation Loss: 42.211986207962035\n",
      "Epoch 74 \t Batch 260 \t Validation Loss: 43.96310428839463\n",
      "Epoch 74 \t Batch 280 \t Validation Loss: 44.759107385362896\n",
      "Epoch 74 \t Batch 300 \t Validation Loss: 45.23250649134318\n",
      "Epoch 74 \t Batch 320 \t Validation Loss: 45.302483102679254\n",
      "Epoch 74 \t Batch 340 \t Validation Loss: 44.930268610225006\n",
      "Epoch 74 \t Batch 360 \t Validation Loss: 44.50115927325355\n",
      "Epoch 74 \t Batch 380 \t Validation Loss: 44.58308120275799\n",
      "Epoch 74 \t Batch 400 \t Validation Loss: 44.031524603366854\n",
      "Epoch 74 \t Batch 420 \t Validation Loss: 43.88338015647162\n",
      "Epoch 74 \t Batch 440 \t Validation Loss: 43.584752830592066\n",
      "Epoch 74 \t Batch 460 \t Validation Loss: 43.73835272789002\n",
      "Epoch 74 \t Batch 480 \t Validation Loss: 44.045670225222906\n",
      "Epoch 74 \t Batch 500 \t Validation Loss: 43.6432981929779\n",
      "Epoch 74 \t Batch 520 \t Validation Loss: 43.48603293162126\n",
      "Epoch 74 \t Batch 540 \t Validation Loss: 43.036788903342355\n",
      "Epoch 74 \t Batch 560 \t Validation Loss: 42.68731691326414\n",
      "Epoch 74 \t Batch 580 \t Validation Loss: 42.335649406498874\n",
      "Epoch 74 \t Batch 600 \t Validation Loss: 42.385364972750345\n",
      "Epoch 74 Training Loss: 45.91811259778272 Validation Loss: 42.9024027771764\n",
      "Epoch 74 completed\n",
      "Epoch 75 \t Batch 20 \t Training Loss: 47.42444820404053\n",
      "Epoch 75 \t Batch 40 \t Training Loss: 46.53914384841919\n",
      "Epoch 75 \t Batch 60 \t Training Loss: 46.07906277974447\n",
      "Epoch 75 \t Batch 80 \t Training Loss: 46.10985088348389\n",
      "Epoch 75 \t Batch 100 \t Training Loss: 46.36825256347656\n",
      "Epoch 75 \t Batch 120 \t Training Loss: 46.41363649368286\n",
      "Epoch 75 \t Batch 140 \t Training Loss: 46.19777175358364\n",
      "Epoch 75 \t Batch 160 \t Training Loss: 46.12910542488098\n",
      "Epoch 75 \t Batch 180 \t Training Loss: 46.03189152611627\n",
      "Epoch 75 \t Batch 200 \t Training Loss: 45.8050332069397\n",
      "Epoch 75 \t Batch 220 \t Training Loss: 45.59075639898127\n",
      "Epoch 75 \t Batch 240 \t Training Loss: 45.649921083450316\n",
      "Epoch 75 \t Batch 260 \t Training Loss: 45.62470839573787\n",
      "Epoch 75 \t Batch 280 \t Training Loss: 45.5753637450082\n",
      "Epoch 75 \t Batch 300 \t Training Loss: 45.54405245463053\n",
      "Epoch 75 \t Batch 320 \t Training Loss: 45.629015839099885\n",
      "Epoch 75 \t Batch 340 \t Training Loss: 45.66330894021427\n",
      "Epoch 75 \t Batch 360 \t Training Loss: 45.54180695215861\n",
      "Epoch 75 \t Batch 380 \t Training Loss: 45.527887645520664\n",
      "Epoch 75 \t Batch 400 \t Training Loss: 45.597659540176394\n",
      "Epoch 75 \t Batch 420 \t Training Loss: 45.554566428774876\n",
      "Epoch 75 \t Batch 440 \t Training Loss: 45.59178771972656\n",
      "Epoch 75 \t Batch 460 \t Training Loss: 45.59712275629458\n",
      "Epoch 75 \t Batch 480 \t Training Loss: 45.6461617231369\n",
      "Epoch 75 \t Batch 500 \t Training Loss: 45.6531443862915\n",
      "Epoch 75 \t Batch 520 \t Training Loss: 45.64985008973342\n",
      "Epoch 75 \t Batch 540 \t Training Loss: 45.68219914612947\n",
      "Epoch 75 \t Batch 560 \t Training Loss: 45.720117705208914\n",
      "Epoch 75 \t Batch 580 \t Training Loss: 45.74143378816802\n",
      "Epoch 75 \t Batch 600 \t Training Loss: 45.76738552093506\n",
      "Epoch 75 \t Batch 620 \t Training Loss: 45.77142360441147\n",
      "Epoch 75 \t Batch 640 \t Training Loss: 45.838636833429334\n",
      "Epoch 75 \t Batch 660 \t Training Loss: 45.82843720985181\n",
      "Epoch 75 \t Batch 680 \t Training Loss: 45.81026025098913\n",
      "Epoch 75 \t Batch 700 \t Training Loss: 45.8224586214338\n",
      "Epoch 75 \t Batch 720 \t Training Loss: 45.83346835242377\n",
      "Epoch 75 \t Batch 740 \t Training Loss: 45.88047326062177\n",
      "Epoch 75 \t Batch 760 \t Training Loss: 45.856189762918575\n",
      "Epoch 75 \t Batch 780 \t Training Loss: 45.86039535815899\n",
      "Epoch 75 \t Batch 800 \t Training Loss: 45.863990483284\n",
      "Epoch 75 \t Batch 820 \t Training Loss: 45.86141216464159\n",
      "Epoch 75 \t Batch 840 \t Training Loss: 45.873934214455744\n",
      "Epoch 75 \t Batch 860 \t Training Loss: 45.8802234028661\n",
      "Epoch 75 \t Batch 880 \t Training Loss: 45.8735699003393\n",
      "Epoch 75 \t Batch 900 \t Training Loss: 45.88168499840631\n",
      "Epoch 75 \t Batch 20 \t Validation Loss: 41.17781591415405\n",
      "Epoch 75 \t Batch 40 \t Validation Loss: 38.20043897628784\n",
      "Epoch 75 \t Batch 60 \t Validation Loss: 40.82186454137166\n",
      "Epoch 75 \t Batch 80 \t Validation Loss: 39.377343440055846\n",
      "Epoch 75 \t Batch 100 \t Validation Loss: 37.44690515518189\n",
      "Epoch 75 \t Batch 120 \t Validation Loss: 36.15722845395406\n",
      "Epoch 75 \t Batch 140 \t Validation Loss: 34.83714051927839\n",
      "Epoch 75 \t Batch 160 \t Validation Loss: 35.64391106367111\n",
      "Epoch 75 \t Batch 180 \t Validation Loss: 38.141896671719024\n",
      "Epoch 75 \t Batch 200 \t Validation Loss: 39.08042045593262\n",
      "Epoch 75 \t Batch 220 \t Validation Loss: 39.80213662927801\n",
      "Epoch 75 \t Batch 240 \t Validation Loss: 39.758200633525846\n",
      "Epoch 75 \t Batch 260 \t Validation Loss: 41.62089976897607\n",
      "Epoch 75 \t Batch 280 \t Validation Loss: 42.51587827205658\n",
      "Epoch 75 \t Batch 300 \t Validation Loss: 43.04766324679057\n",
      "Epoch 75 \t Batch 320 \t Validation Loss: 43.1782539755106\n",
      "Epoch 75 \t Batch 340 \t Validation Loss: 42.89772090070388\n",
      "Epoch 75 \t Batch 360 \t Validation Loss: 42.523797668351065\n",
      "Epoch 75 \t Batch 380 \t Validation Loss: 42.70637256471734\n",
      "Epoch 75 \t Batch 400 \t Validation Loss: 42.33673277616501\n",
      "Epoch 75 \t Batch 420 \t Validation Loss: 42.2887143316723\n",
      "Epoch 75 \t Batch 440 \t Validation Loss: 42.21642499837009\n",
      "Epoch 75 \t Batch 460 \t Validation Loss: 42.40775800373243\n",
      "Epoch 75 \t Batch 480 \t Validation Loss: 42.75895245869955\n",
      "Epoch 75 \t Batch 500 \t Validation Loss: 42.36063893508911\n",
      "Epoch 75 \t Batch 520 \t Validation Loss: 42.281681178166316\n",
      "Epoch 75 \t Batch 540 \t Validation Loss: 41.871191554599335\n",
      "Epoch 75 \t Batch 560 \t Validation Loss: 41.56358360903604\n",
      "Epoch 75 \t Batch 580 \t Validation Loss: 41.22541696285379\n",
      "Epoch 75 \t Batch 600 \t Validation Loss: 41.298472274144494\n",
      "Epoch 75 Training Loss: 45.90516568582217 Validation Loss: 41.855406027335626\n",
      "Epoch 75 completed\n",
      "Epoch 76 \t Batch 20 \t Training Loss: 44.45944766998291\n",
      "Epoch 76 \t Batch 40 \t Training Loss: 45.717822265625\n",
      "Epoch 76 \t Batch 60 \t Training Loss: 45.439394060770674\n",
      "Epoch 76 \t Batch 80 \t Training Loss: 45.48046221733093\n",
      "Epoch 76 \t Batch 100 \t Training Loss: 45.86270439147949\n",
      "Epoch 76 \t Batch 120 \t Training Loss: 45.95143613815308\n",
      "Epoch 76 \t Batch 140 \t Training Loss: 45.95735833304269\n",
      "Epoch 76 \t Batch 160 \t Training Loss: 46.085514307022095\n",
      "Epoch 76 \t Batch 180 \t Training Loss: 46.034419504801434\n",
      "Epoch 76 \t Batch 200 \t Training Loss: 45.94044111251831\n",
      "Epoch 76 \t Batch 220 \t Training Loss: 45.91341074163263\n",
      "Epoch 76 \t Batch 240 \t Training Loss: 45.83616088231405\n",
      "Epoch 76 \t Batch 260 \t Training Loss: 45.877115205618054\n",
      "Epoch 76 \t Batch 280 \t Training Loss: 45.79502574375697\n",
      "Epoch 76 \t Batch 300 \t Training Loss: 45.85495239257813\n",
      "Epoch 76 \t Batch 320 \t Training Loss: 45.856428277492526\n",
      "Epoch 76 \t Batch 340 \t Training Loss: 45.79123914382037\n",
      "Epoch 76 \t Batch 360 \t Training Loss: 45.73620921240912\n",
      "Epoch 76 \t Batch 380 \t Training Loss: 45.70514794399864\n",
      "Epoch 76 \t Batch 400 \t Training Loss: 45.77848327636719\n",
      "Epoch 76 \t Batch 420 \t Training Loss: 45.75121676127116\n",
      "Epoch 76 \t Batch 440 \t Training Loss: 45.71676197918978\n",
      "Epoch 76 \t Batch 460 \t Training Loss: 45.660679717685866\n",
      "Epoch 76 \t Batch 480 \t Training Loss: 45.61448565324147\n",
      "Epoch 76 \t Batch 500 \t Training Loss: 45.584443809509274\n",
      "Epoch 76 \t Batch 520 \t Training Loss: 45.580693765786975\n",
      "Epoch 76 \t Batch 540 \t Training Loss: 45.573763600102176\n",
      "Epoch 76 \t Batch 560 \t Training Loss: 45.56335849761963\n",
      "Epoch 76 \t Batch 580 \t Training Loss: 45.64550727318073\n",
      "Epoch 76 \t Batch 600 \t Training Loss: 45.666824448903405\n",
      "Epoch 76 \t Batch 620 \t Training Loss: 45.67267852290984\n",
      "Epoch 76 \t Batch 640 \t Training Loss: 45.69752349257469\n",
      "Epoch 76 \t Batch 660 \t Training Loss: 45.7274697563865\n",
      "Epoch 76 \t Batch 680 \t Training Loss: 45.71580067241893\n",
      "Epoch 76 \t Batch 700 \t Training Loss: 45.69796538761684\n",
      "Epoch 76 \t Batch 720 \t Training Loss: 45.690717702441745\n",
      "Epoch 76 \t Batch 740 \t Training Loss: 45.73004448349411\n",
      "Epoch 76 \t Batch 760 \t Training Loss: 45.78695023185328\n",
      "Epoch 76 \t Batch 780 \t Training Loss: 45.79707140800281\n",
      "Epoch 76 \t Batch 800 \t Training Loss: 45.79387675762177\n",
      "Epoch 76 \t Batch 820 \t Training Loss: 45.79300409177454\n",
      "Epoch 76 \t Batch 840 \t Training Loss: 45.81247896466937\n",
      "Epoch 76 \t Batch 860 \t Training Loss: 45.81916495922\n",
      "Epoch 76 \t Batch 880 \t Training Loss: 45.83611087799072\n",
      "Epoch 76 \t Batch 900 \t Training Loss: 45.84993203481039\n",
      "Epoch 76 \t Batch 20 \t Validation Loss: 50.433033084869386\n",
      "Epoch 76 \t Batch 40 \t Validation Loss: 45.58690328598023\n",
      "Epoch 76 \t Batch 60 \t Validation Loss: 48.32313067118327\n",
      "Epoch 76 \t Batch 80 \t Validation Loss: 45.76849865913391\n",
      "Epoch 76 \t Batch 100 \t Validation Loss: 42.60341108322144\n",
      "Epoch 76 \t Batch 120 \t Validation Loss: 40.74066058794657\n",
      "Epoch 76 \t Batch 140 \t Validation Loss: 38.86810278211321\n",
      "Epoch 76 \t Batch 160 \t Validation Loss: 39.1337210059166\n",
      "Epoch 76 \t Batch 180 \t Validation Loss: 41.20512920485602\n",
      "Epoch 76 \t Batch 200 \t Validation Loss: 41.79271493434906\n",
      "Epoch 76 \t Batch 220 \t Validation Loss: 42.200707361914894\n",
      "Epoch 76 \t Batch 240 \t Validation Loss: 41.93585360447566\n",
      "Epoch 76 \t Batch 260 \t Validation Loss: 43.57704473275405\n",
      "Epoch 76 \t Batch 280 \t Validation Loss: 44.31691724572863\n",
      "Epoch 76 \t Batch 300 \t Validation Loss: 44.6736683177948\n",
      "Epoch 76 \t Batch 320 \t Validation Loss: 44.7000172317028\n",
      "Epoch 76 \t Batch 340 \t Validation Loss: 44.31980905532837\n",
      "Epoch 76 \t Batch 360 \t Validation Loss: 43.83989165623983\n",
      "Epoch 76 \t Batch 380 \t Validation Loss: 43.8906402638084\n",
      "Epoch 76 \t Batch 400 \t Validation Loss: 43.443019337654114\n",
      "Epoch 76 \t Batch 420 \t Validation Loss: 43.31734176817395\n",
      "Epoch 76 \t Batch 440 \t Validation Loss: 43.06422844800082\n",
      "Epoch 76 \t Batch 460 \t Validation Loss: 43.20622902745786\n",
      "Epoch 76 \t Batch 480 \t Validation Loss: 43.49590506156286\n",
      "Epoch 76 \t Batch 500 \t Validation Loss: 43.06487656784058\n",
      "Epoch 76 \t Batch 520 \t Validation Loss: 42.90641325437105\n",
      "Epoch 76 \t Batch 540 \t Validation Loss: 42.52229377605297\n",
      "Epoch 76 \t Batch 560 \t Validation Loss: 42.24803458281926\n",
      "Epoch 76 \t Batch 580 \t Validation Loss: 41.98419658068953\n",
      "Epoch 76 \t Batch 600 \t Validation Loss: 42.07365060170491\n",
      "Epoch 76 Training Loss: 45.86429741546398 Validation Loss: 42.63621859116988\n",
      "Epoch 76 completed\n",
      "Epoch 77 \t Batch 20 \t Training Loss: 46.6008487701416\n",
      "Epoch 77 \t Batch 40 \t Training Loss: 45.957017230987546\n",
      "Epoch 77 \t Batch 60 \t Training Loss: 45.96320552825928\n",
      "Epoch 77 \t Batch 80 \t Training Loss: 46.03008856773376\n",
      "Epoch 77 \t Batch 100 \t Training Loss: 45.93437702178955\n",
      "Epoch 77 \t Batch 120 \t Training Loss: 46.04648536046346\n",
      "Epoch 77 \t Batch 140 \t Training Loss: 45.8653135572161\n",
      "Epoch 77 \t Batch 160 \t Training Loss: 45.789432644844055\n",
      "Epoch 77 \t Batch 180 \t Training Loss: 45.76973951127794\n",
      "Epoch 77 \t Batch 200 \t Training Loss: 45.802380809783934\n",
      "Epoch 77 \t Batch 220 \t Training Loss: 45.836638710715555\n",
      "Epoch 77 \t Batch 240 \t Training Loss: 45.945509354273476\n",
      "Epoch 77 \t Batch 260 \t Training Loss: 46.06759848961463\n",
      "Epoch 77 \t Batch 280 \t Training Loss: 46.038172476632255\n",
      "Epoch 77 \t Batch 300 \t Training Loss: 46.114847335815426\n",
      "Epoch 77 \t Batch 320 \t Training Loss: 46.16352149248123\n",
      "Epoch 77 \t Batch 340 \t Training Loss: 46.12566553003648\n",
      "Epoch 77 \t Batch 360 \t Training Loss: 46.02608530256483\n",
      "Epoch 77 \t Batch 380 \t Training Loss: 46.04300878424394\n",
      "Epoch 77 \t Batch 400 \t Training Loss: 45.97156346321106\n",
      "Epoch 77 \t Batch 420 \t Training Loss: 45.93621073223296\n",
      "Epoch 77 \t Batch 440 \t Training Loss: 45.87735457853837\n",
      "Epoch 77 \t Batch 460 \t Training Loss: 45.871264524045195\n",
      "Epoch 77 \t Batch 480 \t Training Loss: 45.84087079366048\n",
      "Epoch 77 \t Batch 500 \t Training Loss: 45.7925283126831\n",
      "Epoch 77 \t Batch 520 \t Training Loss: 45.78247038767888\n",
      "Epoch 77 \t Batch 540 \t Training Loss: 45.78729118064598\n",
      "Epoch 77 \t Batch 560 \t Training Loss: 45.819789736611504\n",
      "Epoch 77 \t Batch 580 \t Training Loss: 45.831700094814956\n",
      "Epoch 77 \t Batch 600 \t Training Loss: 45.80098609288534\n",
      "Epoch 77 \t Batch 620 \t Training Loss: 45.81527824401856\n",
      "Epoch 77 \t Batch 640 \t Training Loss: 45.83136918544769\n",
      "Epoch 77 \t Batch 660 \t Training Loss: 45.87319403561679\n",
      "Epoch 77 \t Batch 680 \t Training Loss: 45.836686981425565\n",
      "Epoch 77 \t Batch 700 \t Training Loss: 45.849035742623464\n",
      "Epoch 77 \t Batch 720 \t Training Loss: 45.8459980222914\n",
      "Epoch 77 \t Batch 740 \t Training Loss: 45.79036417265196\n",
      "Epoch 77 \t Batch 760 \t Training Loss: 45.79639778137207\n",
      "Epoch 77 \t Batch 780 \t Training Loss: 45.7994863656851\n",
      "Epoch 77 \t Batch 800 \t Training Loss: 45.76669697284699\n",
      "Epoch 77 \t Batch 820 \t Training Loss: 45.75573168033507\n",
      "Epoch 77 \t Batch 840 \t Training Loss: 45.774939873105005\n",
      "Epoch 77 \t Batch 860 \t Training Loss: 45.76967257122661\n",
      "Epoch 77 \t Batch 880 \t Training Loss: 45.80001378059387\n",
      "Epoch 77 \t Batch 900 \t Training Loss: 45.831910824245874\n",
      "Epoch 77 \t Batch 20 \t Validation Loss: 40.13012285232544\n",
      "Epoch 77 \t Batch 40 \t Validation Loss: 38.1809499502182\n",
      "Epoch 77 \t Batch 60 \t Validation Loss: 39.876504723231\n",
      "Epoch 77 \t Batch 80 \t Validation Loss: 38.83014920949936\n",
      "Epoch 77 \t Batch 100 \t Validation Loss: 36.62052500724793\n",
      "Epoch 77 \t Batch 120 \t Validation Loss: 35.491419315338135\n",
      "Epoch 77 \t Batch 140 \t Validation Loss: 34.240383175441195\n",
      "Epoch 77 \t Batch 160 \t Validation Loss: 34.83363697528839\n",
      "Epoch 77 \t Batch 180 \t Validation Loss: 37.11736407809787\n",
      "Epoch 77 \t Batch 200 \t Validation Loss: 37.89146216392517\n",
      "Epoch 77 \t Batch 220 \t Validation Loss: 38.44300236268477\n",
      "Epoch 77 \t Batch 240 \t Validation Loss: 38.34501367807388\n",
      "Epoch 77 \t Batch 260 \t Validation Loss: 40.11460087482746\n",
      "Epoch 77 \t Batch 280 \t Validation Loss: 40.94735055991581\n",
      "Epoch 77 \t Batch 300 \t Validation Loss: 41.36609328269959\n",
      "Epoch 77 \t Batch 320 \t Validation Loss: 41.48846039772034\n",
      "Epoch 77 \t Batch 340 \t Validation Loss: 41.222980914396395\n",
      "Epoch 77 \t Batch 360 \t Validation Loss: 40.805747434828014\n",
      "Epoch 77 \t Batch 380 \t Validation Loss: 40.88505539643137\n",
      "Epoch 77 \t Batch 400 \t Validation Loss: 40.437084674835205\n",
      "Epoch 77 \t Batch 420 \t Validation Loss: 40.35769768669492\n",
      "Epoch 77 \t Batch 440 \t Validation Loss: 40.05622842528603\n",
      "Epoch 77 \t Batch 460 \t Validation Loss: 40.20880185417507\n",
      "Epoch 77 \t Batch 480 \t Validation Loss: 40.5820015390714\n",
      "Epoch 77 \t Batch 500 \t Validation Loss: 40.18328594207764\n",
      "Epoch 77 \t Batch 520 \t Validation Loss: 39.97451353623317\n",
      "Epoch 77 \t Batch 540 \t Validation Loss: 39.629896296395195\n",
      "Epoch 77 \t Batch 560 \t Validation Loss: 39.36634522676468\n",
      "Epoch 77 \t Batch 580 \t Validation Loss: 39.09398147155498\n",
      "Epoch 77 \t Batch 600 \t Validation Loss: 39.22146917819977\n",
      "Epoch 77 Training Loss: 45.80684725512855 Validation Loss: 39.84712877985719\n",
      "Epoch 77 completed\n",
      "Epoch 78 \t Batch 20 \t Training Loss: 44.721936988830564\n",
      "Epoch 78 \t Batch 40 \t Training Loss: 44.69268932342529\n",
      "Epoch 78 \t Batch 60 \t Training Loss: 44.82746550242106\n",
      "Epoch 78 \t Batch 80 \t Training Loss: 45.38160648345947\n",
      "Epoch 78 \t Batch 100 \t Training Loss: 45.6112283706665\n",
      "Epoch 78 \t Batch 120 \t Training Loss: 45.52946510314941\n",
      "Epoch 78 \t Batch 140 \t Training Loss: 45.590180560520714\n",
      "Epoch 78 \t Batch 160 \t Training Loss: 45.5890228509903\n",
      "Epoch 78 \t Batch 180 \t Training Loss: 45.68858534495036\n",
      "Epoch 78 \t Batch 200 \t Training Loss: 45.762383193969725\n",
      "Epoch 78 \t Batch 220 \t Training Loss: 45.84233523282138\n",
      "Epoch 78 \t Batch 240 \t Training Loss: 45.771756664911905\n",
      "Epoch 78 \t Batch 260 \t Training Loss: 45.803049204899715\n",
      "Epoch 78 \t Batch 280 \t Training Loss: 45.78877829142979\n",
      "Epoch 78 \t Batch 300 \t Training Loss: 45.84331484476725\n",
      "Epoch 78 \t Batch 320 \t Training Loss: 45.797967648506166\n",
      "Epoch 78 \t Batch 340 \t Training Loss: 45.808046475578756\n",
      "Epoch 78 \t Batch 360 \t Training Loss: 45.773474046919084\n",
      "Epoch 78 \t Batch 380 \t Training Loss: 45.70509517067357\n",
      "Epoch 78 \t Batch 400 \t Training Loss: 45.71836957931519\n",
      "Epoch 78 \t Batch 420 \t Training Loss: 45.75430014474051\n",
      "Epoch 78 \t Batch 440 \t Training Loss: 45.76978493603793\n",
      "Epoch 78 \t Batch 460 \t Training Loss: 45.74358402749766\n",
      "Epoch 78 \t Batch 480 \t Training Loss: 45.810317277908325\n",
      "Epoch 78 \t Batch 500 \t Training Loss: 45.80424947357178\n",
      "Epoch 78 \t Batch 520 \t Training Loss: 45.788135829338664\n",
      "Epoch 78 \t Batch 540 \t Training Loss: 45.76805227774161\n",
      "Epoch 78 \t Batch 560 \t Training Loss: 45.76360094206674\n",
      "Epoch 78 \t Batch 580 \t Training Loss: 45.75774426953546\n",
      "Epoch 78 \t Batch 600 \t Training Loss: 45.756620496114095\n",
      "Epoch 78 \t Batch 620 \t Training Loss: 45.75331748224074\n",
      "Epoch 78 \t Batch 640 \t Training Loss: 45.77815688252449\n",
      "Epoch 78 \t Batch 660 \t Training Loss: 45.77282701550108\n",
      "Epoch 78 \t Batch 680 \t Training Loss: 45.8268700094784\n",
      "Epoch 78 \t Batch 700 \t Training Loss: 45.8391920035226\n",
      "Epoch 78 \t Batch 720 \t Training Loss: 45.79071174197727\n",
      "Epoch 78 \t Batch 740 \t Training Loss: 45.79525758897936\n",
      "Epoch 78 \t Batch 760 \t Training Loss: 45.7995716396131\n",
      "Epoch 78 \t Batch 780 \t Training Loss: 45.79733792329446\n",
      "Epoch 78 \t Batch 800 \t Training Loss: 45.79717391967773\n",
      "Epoch 78 \t Batch 820 \t Training Loss: 45.79843009390482\n",
      "Epoch 78 \t Batch 840 \t Training Loss: 45.848314489637104\n",
      "Epoch 78 \t Batch 860 \t Training Loss: 45.86778933502907\n",
      "Epoch 78 \t Batch 880 \t Training Loss: 45.86878387711265\n",
      "Epoch 78 \t Batch 900 \t Training Loss: 45.841368124220104\n",
      "Epoch 78 \t Batch 20 \t Validation Loss: 49.05527935028076\n",
      "Epoch 78 \t Batch 40 \t Validation Loss: 44.66688306331635\n",
      "Epoch 78 \t Batch 60 \t Validation Loss: 47.08599359194438\n",
      "Epoch 78 \t Batch 80 \t Validation Loss: 45.52920521497727\n",
      "Epoch 78 \t Batch 100 \t Validation Loss: 42.43108788490295\n",
      "Epoch 78 \t Batch 120 \t Validation Loss: 40.56760458151499\n",
      "Epoch 78 \t Batch 140 \t Validation Loss: 38.74880689212254\n",
      "Epoch 78 \t Batch 160 \t Validation Loss: 39.129453629255295\n",
      "Epoch 78 \t Batch 180 \t Validation Loss: 41.635510449939304\n",
      "Epoch 78 \t Batch 200 \t Validation Loss: 42.36182312488556\n",
      "Epoch 78 \t Batch 220 \t Validation Loss: 43.00114840160717\n",
      "Epoch 78 \t Batch 240 \t Validation Loss: 42.85225207010905\n",
      "Epoch 78 \t Batch 260 \t Validation Loss: 44.64602613449097\n",
      "Epoch 78 \t Batch 280 \t Validation Loss: 45.43915198871068\n",
      "Epoch 78 \t Batch 300 \t Validation Loss: 45.986308320363364\n",
      "Epoch 78 \t Batch 320 \t Validation Loss: 46.054202711582185\n",
      "Epoch 78 \t Batch 340 \t Validation Loss: 45.636958756166344\n",
      "Epoch 78 \t Batch 360 \t Validation Loss: 45.14234859678481\n",
      "Epoch 78 \t Batch 380 \t Validation Loss: 45.176993861951324\n",
      "Epoch 78 \t Batch 400 \t Validation Loss: 44.55377213001251\n",
      "Epoch 78 \t Batch 420 \t Validation Loss: 44.33662111872719\n",
      "Epoch 78 \t Batch 440 \t Validation Loss: 43.91233748739416\n",
      "Epoch 78 \t Batch 460 \t Validation Loss: 43.929818292286086\n",
      "Epoch 78 \t Batch 480 \t Validation Loss: 44.19288591742516\n",
      "Epoch 78 \t Batch 500 \t Validation Loss: 43.73117051506043\n",
      "Epoch 78 \t Batch 520 \t Validation Loss: 43.46276527001307\n",
      "Epoch 78 \t Batch 540 \t Validation Loss: 43.00127515969453\n",
      "Epoch 78 \t Batch 560 \t Validation Loss: 42.640809442315785\n",
      "Epoch 78 \t Batch 580 \t Validation Loss: 42.25597526122784\n",
      "Epoch 78 \t Batch 600 \t Validation Loss: 42.30785920937856\n",
      "Epoch 78 Training Loss: 45.819568163956006 Validation Loss: 42.87777462098506\n",
      "Epoch 78 completed\n",
      "Epoch 79 \t Batch 20 \t Training Loss: 44.774671363830564\n",
      "Epoch 79 \t Batch 40 \t Training Loss: 45.49641723632813\n",
      "Epoch 79 \t Batch 60 \t Training Loss: 45.61413307189942\n",
      "Epoch 79 \t Batch 80 \t Training Loss: 45.809695625305174\n",
      "Epoch 79 \t Batch 100 \t Training Loss: 45.756530418396\n",
      "Epoch 79 \t Batch 120 \t Training Loss: 45.74257154464722\n",
      "Epoch 79 \t Batch 140 \t Training Loss: 45.69571176256452\n",
      "Epoch 79 \t Batch 160 \t Training Loss: 45.6636405467987\n",
      "Epoch 79 \t Batch 180 \t Training Loss: 45.65430359310574\n",
      "Epoch 79 \t Batch 200 \t Training Loss: 45.69632318496704\n",
      "Epoch 79 \t Batch 220 \t Training Loss: 45.79777785214511\n",
      "Epoch 79 \t Batch 240 \t Training Loss: 45.85765705108643\n",
      "Epoch 79 \t Batch 260 \t Training Loss: 45.897797613877515\n",
      "Epoch 79 \t Batch 280 \t Training Loss: 45.87686849321638\n",
      "Epoch 79 \t Batch 300 \t Training Loss: 45.9034080251058\n",
      "Epoch 79 \t Batch 320 \t Training Loss: 45.912164413928984\n",
      "Epoch 79 \t Batch 340 \t Training Loss: 45.96556984396542\n",
      "Epoch 79 \t Batch 360 \t Training Loss: 46.00296868218316\n",
      "Epoch 79 \t Batch 380 \t Training Loss: 45.94761351535195\n",
      "Epoch 79 \t Batch 400 \t Training Loss: 45.89055332183838\n",
      "Epoch 79 \t Batch 420 \t Training Loss: 45.88762375967843\n",
      "Epoch 79 \t Batch 440 \t Training Loss: 45.90616041530262\n",
      "Epoch 79 \t Batch 460 \t Training Loss: 45.86575520556906\n",
      "Epoch 79 \t Batch 480 \t Training Loss: 45.87458738485972\n",
      "Epoch 79 \t Batch 500 \t Training Loss: 45.91353347015381\n",
      "Epoch 79 \t Batch 520 \t Training Loss: 45.910359756763164\n",
      "Epoch 79 \t Batch 540 \t Training Loss: 45.906134344030306\n",
      "Epoch 79 \t Batch 560 \t Training Loss: 45.93620651108878\n",
      "Epoch 79 \t Batch 580 \t Training Loss: 45.9164049806266\n",
      "Epoch 79 \t Batch 600 \t Training Loss: 45.95031998316447\n",
      "Epoch 79 \t Batch 620 \t Training Loss: 45.954518619660405\n",
      "Epoch 79 \t Batch 640 \t Training Loss: 45.91156648993492\n",
      "Epoch 79 \t Batch 660 \t Training Loss: 45.896411485383005\n",
      "Epoch 79 \t Batch 680 \t Training Loss: 45.863525878681855\n",
      "Epoch 79 \t Batch 700 \t Training Loss: 45.85614362444196\n",
      "Epoch 79 \t Batch 720 \t Training Loss: 45.83825739754571\n",
      "Epoch 79 \t Batch 740 \t Training Loss: 45.85182160042428\n",
      "Epoch 79 \t Batch 760 \t Training Loss: 45.842064501109874\n",
      "Epoch 79 \t Batch 780 \t Training Loss: 45.84586526919634\n",
      "Epoch 79 \t Batch 800 \t Training Loss: 45.83955155849457\n",
      "Epoch 79 \t Batch 820 \t Training Loss: 45.843783667029406\n",
      "Epoch 79 \t Batch 840 \t Training Loss: 45.83953239804222\n",
      "Epoch 79 \t Batch 860 \t Training Loss: 45.828967781954034\n",
      "Epoch 79 \t Batch 880 \t Training Loss: 45.8148258599368\n",
      "Epoch 79 \t Batch 900 \t Training Loss: 45.79899310641819\n",
      "Epoch 79 \t Batch 20 \t Validation Loss: 57.96601047515869\n",
      "Epoch 79 \t Batch 40 \t Validation Loss: 50.04183485507965\n",
      "Epoch 79 \t Batch 60 \t Validation Loss: 52.86976431210836\n",
      "Epoch 79 \t Batch 80 \t Validation Loss: 50.18110743761063\n",
      "Epoch 79 \t Batch 100 \t Validation Loss: 45.92319628715515\n",
      "Epoch 79 \t Batch 120 \t Validation Loss: 43.36580495039622\n",
      "Epoch 79 \t Batch 140 \t Validation Loss: 41.04908764021737\n",
      "Epoch 79 \t Batch 160 \t Validation Loss: 40.976388305425644\n",
      "Epoch 79 \t Batch 180 \t Validation Loss: 43.03752876917521\n",
      "Epoch 79 \t Batch 200 \t Validation Loss: 43.458676433563234\n",
      "Epoch 79 \t Batch 220 \t Validation Loss: 43.77494290091774\n",
      "Epoch 79 \t Batch 240 \t Validation Loss: 43.42724286317825\n",
      "Epoch 79 \t Batch 260 \t Validation Loss: 45.080559275700494\n",
      "Epoch 79 \t Batch 280 \t Validation Loss: 45.7624582869666\n",
      "Epoch 79 \t Batch 300 \t Validation Loss: 46.14386351585388\n",
      "Epoch 79 \t Batch 320 \t Validation Loss: 46.132747837901114\n",
      "Epoch 79 \t Batch 340 \t Validation Loss: 45.67139924834756\n",
      "Epoch 79 \t Batch 360 \t Validation Loss: 45.101647239261204\n",
      "Epoch 79 \t Batch 380 \t Validation Loss: 45.063080923180834\n",
      "Epoch 79 \t Batch 400 \t Validation Loss: 44.361035089492795\n",
      "Epoch 79 \t Batch 420 \t Validation Loss: 44.10329359826588\n",
      "Epoch 79 \t Batch 440 \t Validation Loss: 43.594893869486725\n",
      "Epoch 79 \t Batch 460 \t Validation Loss: 43.629229091561356\n",
      "Epoch 79 \t Batch 480 \t Validation Loss: 43.886716304222745\n",
      "Epoch 79 \t Batch 500 \t Validation Loss: 43.41631286430359\n",
      "Epoch 79 \t Batch 520 \t Validation Loss: 43.100232067474955\n",
      "Epoch 79 \t Batch 540 \t Validation Loss: 42.68023810386658\n",
      "Epoch 79 \t Batch 560 \t Validation Loss: 42.36544973679951\n",
      "Epoch 79 \t Batch 580 \t Validation Loss: 42.0791648190597\n",
      "Epoch 79 \t Batch 600 \t Validation Loss: 42.159094014167785\n",
      "Epoch 79 Training Loss: 45.79409901526398 Validation Loss: 42.692401466431555\n",
      "Epoch 79 completed\n",
      "Epoch 80 \t Batch 20 \t Training Loss: 44.988953018188475\n",
      "Epoch 80 \t Batch 40 \t Training Loss: 45.4764009475708\n",
      "Epoch 80 \t Batch 60 \t Training Loss: 45.56665744781494\n",
      "Epoch 80 \t Batch 80 \t Training Loss: 45.4197922706604\n",
      "Epoch 80 \t Batch 100 \t Training Loss: 45.68604179382324\n",
      "Epoch 80 \t Batch 120 \t Training Loss: 45.86092468897502\n",
      "Epoch 80 \t Batch 140 \t Training Loss: 45.97669947487967\n",
      "Epoch 80 \t Batch 160 \t Training Loss: 45.943518495559694\n",
      "Epoch 80 \t Batch 180 \t Training Loss: 45.78750851949056\n",
      "Epoch 80 \t Batch 200 \t Training Loss: 45.69546726226807\n",
      "Epoch 80 \t Batch 220 \t Training Loss: 45.83359031677246\n",
      "Epoch 80 \t Batch 240 \t Training Loss: 45.90835719108581\n",
      "Epoch 80 \t Batch 260 \t Training Loss: 46.0150945516733\n",
      "Epoch 80 \t Batch 280 \t Training Loss: 46.03178066526141\n",
      "Epoch 80 \t Batch 300 \t Training Loss: 45.92770158131918\n",
      "Epoch 80 \t Batch 320 \t Training Loss: 45.929848515987395\n",
      "Epoch 80 \t Batch 340 \t Training Loss: 45.96164940104765\n",
      "Epoch 80 \t Batch 360 \t Training Loss: 45.9591913541158\n",
      "Epoch 80 \t Batch 380 \t Training Loss: 45.973446735582854\n",
      "Epoch 80 \t Batch 400 \t Training Loss: 45.950347099304196\n",
      "Epoch 80 \t Batch 420 \t Training Loss: 45.93280972072056\n",
      "Epoch 80 \t Batch 440 \t Training Loss: 45.93148105794733\n",
      "Epoch 80 \t Batch 460 \t Training Loss: 45.88347041917884\n",
      "Epoch 80 \t Batch 480 \t Training Loss: 45.83903478781382\n",
      "Epoch 80 \t Batch 500 \t Training Loss: 45.888820388793945\n",
      "Epoch 80 \t Batch 520 \t Training Loss: 45.82875350805429\n",
      "Epoch 80 \t Batch 540 \t Training Loss: 45.86249451107449\n",
      "Epoch 80 \t Batch 560 \t Training Loss: 45.880106353759764\n",
      "Epoch 80 \t Batch 580 \t Training Loss: 45.863762441174735\n",
      "Epoch 80 \t Batch 600 \t Training Loss: 45.861907393137614\n",
      "Epoch 80 \t Batch 620 \t Training Loss: 45.85206313594695\n",
      "Epoch 80 \t Batch 640 \t Training Loss: 45.82586543560028\n",
      "Epoch 80 \t Batch 660 \t Training Loss: 45.78140828681715\n",
      "Epoch 80 \t Batch 680 \t Training Loss: 45.82333244435927\n",
      "Epoch 80 \t Batch 700 \t Training Loss: 45.81861653464181\n",
      "Epoch 80 \t Batch 720 \t Training Loss: 45.816790379418265\n",
      "Epoch 80 \t Batch 740 \t Training Loss: 45.81188413774645\n",
      "Epoch 80 \t Batch 760 \t Training Loss: 45.79230014399478\n",
      "Epoch 80 \t Batch 780 \t Training Loss: 45.78154612810184\n",
      "Epoch 80 \t Batch 800 \t Training Loss: 45.7750215959549\n",
      "Epoch 80 \t Batch 820 \t Training Loss: 45.7842750921482\n",
      "Epoch 80 \t Batch 840 \t Training Loss: 45.784130001068114\n",
      "Epoch 80 \t Batch 860 \t Training Loss: 45.76272989317428\n",
      "Epoch 80 \t Batch 880 \t Training Loss: 45.767447671023284\n",
      "Epoch 80 \t Batch 900 \t Training Loss: 45.777305433485246\n",
      "Epoch 80 \t Batch 20 \t Validation Loss: 52.02645025253296\n",
      "Epoch 80 \t Batch 40 \t Validation Loss: 44.83577864170074\n",
      "Epoch 80 \t Batch 60 \t Validation Loss: 48.23007559776306\n",
      "Epoch 80 \t Batch 80 \t Validation Loss: 45.91275005340576\n",
      "Epoch 80 \t Batch 100 \t Validation Loss: 42.54616256713867\n",
      "Epoch 80 \t Batch 120 \t Validation Loss: 40.4863072236379\n",
      "Epoch 80 \t Batch 140 \t Validation Loss: 38.5845591204507\n",
      "Epoch 80 \t Batch 160 \t Validation Loss: 38.93331063389778\n",
      "Epoch 80 \t Batch 180 \t Validation Loss: 41.304816521538626\n",
      "Epoch 80 \t Batch 200 \t Validation Loss: 42.06713391304016\n",
      "Epoch 80 \t Batch 220 \t Validation Loss: 42.57419020912864\n",
      "Epoch 80 \t Batch 240 \t Validation Loss: 42.3639826575915\n",
      "Epoch 80 \t Batch 260 \t Validation Loss: 44.09210824966431\n",
      "Epoch 80 \t Batch 280 \t Validation Loss: 44.90927909782955\n",
      "Epoch 80 \t Batch 300 \t Validation Loss: 45.42851771990458\n",
      "Epoch 80 \t Batch 320 \t Validation Loss: 45.49195992052555\n",
      "Epoch 80 \t Batch 340 \t Validation Loss: 45.1528428217944\n",
      "Epoch 80 \t Batch 360 \t Validation Loss: 44.63880506886376\n",
      "Epoch 80 \t Batch 380 \t Validation Loss: 44.614150260624136\n",
      "Epoch 80 \t Batch 400 \t Validation Loss: 43.989895102977755\n",
      "Epoch 80 \t Batch 420 \t Validation Loss: 43.84403783934457\n",
      "Epoch 80 \t Batch 440 \t Validation Loss: 43.391128095713526\n",
      "Epoch 80 \t Batch 460 \t Validation Loss: 43.40171837806702\n",
      "Epoch 80 \t Batch 480 \t Validation Loss: 43.702797394990924\n",
      "Epoch 80 \t Batch 500 \t Validation Loss: 43.303816179275515\n",
      "Epoch 80 \t Batch 520 \t Validation Loss: 42.90899070776426\n",
      "Epoch 80 \t Batch 540 \t Validation Loss: 42.42971712924816\n",
      "Epoch 80 \t Batch 560 \t Validation Loss: 42.06501521212714\n",
      "Epoch 80 \t Batch 580 \t Validation Loss: 41.666203760278634\n",
      "Epoch 80 \t Batch 600 \t Validation Loss: 41.70804531892141\n",
      "Epoch 80 Training Loss: 45.761495301091685 Validation Loss: 42.22663326851733\n",
      "Epoch 80 completed\n",
      "Epoch 81 \t Batch 20 \t Training Loss: 44.29522476196289\n",
      "Epoch 81 \t Batch 40 \t Training Loss: 45.0510591506958\n",
      "Epoch 81 \t Batch 60 \t Training Loss: 45.13629411061605\n",
      "Epoch 81 \t Batch 80 \t Training Loss: 45.33477573394775\n",
      "Epoch 81 \t Batch 100 \t Training Loss: 45.2438595199585\n",
      "Epoch 81 \t Batch 120 \t Training Loss: 45.42000780105591\n",
      "Epoch 81 \t Batch 140 \t Training Loss: 45.438490649632044\n",
      "Epoch 81 \t Batch 160 \t Training Loss: 45.515640544891355\n",
      "Epoch 81 \t Batch 180 \t Training Loss: 45.51886874304877\n",
      "Epoch 81 \t Batch 200 \t Training Loss: 45.45248435974121\n",
      "Epoch 81 \t Batch 220 \t Training Loss: 45.41399997364391\n",
      "Epoch 81 \t Batch 240 \t Training Loss: 45.49415663083394\n",
      "Epoch 81 \t Batch 260 \t Training Loss: 45.521575164794925\n",
      "Epoch 81 \t Batch 280 \t Training Loss: 45.6074266569955\n",
      "Epoch 81 \t Batch 300 \t Training Loss: 45.61081778208415\n",
      "Epoch 81 \t Batch 320 \t Training Loss: 45.731092476844786\n",
      "Epoch 81 \t Batch 340 \t Training Loss: 45.72774220634909\n",
      "Epoch 81 \t Batch 360 \t Training Loss: 45.64749572541979\n",
      "Epoch 81 \t Batch 380 \t Training Loss: 45.58939427827534\n",
      "Epoch 81 \t Batch 400 \t Training Loss: 45.61097681999207\n",
      "Epoch 81 \t Batch 420 \t Training Loss: 45.56720769973028\n",
      "Epoch 81 \t Batch 440 \t Training Loss: 45.58671739751642\n",
      "Epoch 81 \t Batch 460 \t Training Loss: 45.59121256289275\n",
      "Epoch 81 \t Batch 480 \t Training Loss: 45.62728265126546\n",
      "Epoch 81 \t Batch 500 \t Training Loss: 45.64677153015137\n",
      "Epoch 81 \t Batch 520 \t Training Loss: 45.659734696608325\n",
      "Epoch 81 \t Batch 540 \t Training Loss: 45.664932780795624\n",
      "Epoch 81 \t Batch 560 \t Training Loss: 45.700097690309796\n",
      "Epoch 81 \t Batch 580 \t Training Loss: 45.730420329652986\n",
      "Epoch 81 \t Batch 600 \t Training Loss: 45.72403230667114\n",
      "Epoch 81 \t Batch 620 \t Training Loss: 45.673508471827354\n",
      "Epoch 81 \t Batch 640 \t Training Loss: 45.70582134127617\n",
      "Epoch 81 \t Batch 660 \t Training Loss: 45.700892598701245\n",
      "Epoch 81 \t Batch 680 \t Training Loss: 45.650949068630446\n",
      "Epoch 81 \t Batch 700 \t Training Loss: 45.666495895385744\n",
      "Epoch 81 \t Batch 720 \t Training Loss: 45.72391809357537\n",
      "Epoch 81 \t Batch 740 \t Training Loss: 45.71128142588847\n",
      "Epoch 81 \t Batch 760 \t Training Loss: 45.73170686019095\n",
      "Epoch 81 \t Batch 780 \t Training Loss: 45.747530780694426\n",
      "Epoch 81 \t Batch 800 \t Training Loss: 45.74779495239258\n",
      "Epoch 81 \t Batch 820 \t Training Loss: 45.7216629540048\n",
      "Epoch 81 \t Batch 840 \t Training Loss: 45.718453398204986\n",
      "Epoch 81 \t Batch 860 \t Training Loss: 45.72481815426848\n",
      "Epoch 81 \t Batch 880 \t Training Loss: 45.718071972240104\n",
      "Epoch 81 \t Batch 900 \t Training Loss: 45.72104822370741\n",
      "Epoch 81 \t Batch 20 \t Validation Loss: 38.09975337982178\n",
      "Epoch 81 \t Batch 40 \t Validation Loss: 35.60930037498474\n",
      "Epoch 81 \t Batch 60 \t Validation Loss: 36.925026162465414\n",
      "Epoch 81 \t Batch 80 \t Validation Loss: 36.197318243980405\n",
      "Epoch 81 \t Batch 100 \t Validation Loss: 34.58605766296387\n",
      "Epoch 81 \t Batch 120 \t Validation Loss: 33.690001503626505\n",
      "Epoch 81 \t Batch 140 \t Validation Loss: 32.73357297352382\n",
      "Epoch 81 \t Batch 160 \t Validation Loss: 33.775802600383756\n",
      "Epoch 81 \t Batch 180 \t Validation Loss: 36.67015980084737\n",
      "Epoch 81 \t Batch 200 \t Validation Loss: 37.724961199760436\n",
      "Epoch 81 \t Batch 220 \t Validation Loss: 38.655777580087836\n",
      "Epoch 81 \t Batch 240 \t Validation Loss: 38.76604754130046\n",
      "Epoch 81 \t Batch 260 \t Validation Loss: 40.79976151906527\n",
      "Epoch 81 \t Batch 280 \t Validation Loss: 41.80974967820304\n",
      "Epoch 81 \t Batch 300 \t Validation Loss: 42.47940473556518\n",
      "Epoch 81 \t Batch 320 \t Validation Loss: 42.70798425078392\n",
      "Epoch 81 \t Batch 340 \t Validation Loss: 42.471033174851364\n",
      "Epoch 81 \t Batch 360 \t Validation Loss: 42.102152705192566\n",
      "Epoch 81 \t Batch 380 \t Validation Loss: 42.18455797747562\n",
      "Epoch 81 \t Batch 400 \t Validation Loss: 41.59328997850418\n",
      "Epoch 81 \t Batch 420 \t Validation Loss: 41.46570769037519\n",
      "Epoch 81 \t Batch 440 \t Validation Loss: 41.01087479808114\n",
      "Epoch 81 \t Batch 460 \t Validation Loss: 41.10020050587861\n",
      "Epoch 81 \t Batch 480 \t Validation Loss: 41.46868618528048\n",
      "Epoch 81 \t Batch 500 \t Validation Loss: 41.112010431289676\n",
      "Epoch 81 \t Batch 520 \t Validation Loss: 40.77231219915243\n",
      "Epoch 81 \t Batch 540 \t Validation Loss: 40.37608288129171\n",
      "Epoch 81 \t Batch 560 \t Validation Loss: 40.07314123255866\n",
      "Epoch 81 \t Batch 580 \t Validation Loss: 39.78624543650397\n",
      "Epoch 81 \t Batch 600 \t Validation Loss: 39.895390706062315\n",
      "Epoch 81 Training Loss: 45.731912406996265 Validation Loss: 40.46541375154025\n",
      "Epoch 81 completed\n",
      "Epoch 82 \t Batch 20 \t Training Loss: 47.89868812561035\n",
      "Epoch 82 \t Batch 40 \t Training Loss: 45.268922328948975\n",
      "Epoch 82 \t Batch 60 \t Training Loss: 45.42960885365804\n",
      "Epoch 82 \t Batch 80 \t Training Loss: 45.68772296905517\n",
      "Epoch 82 \t Batch 100 \t Training Loss: 45.711132774353025\n",
      "Epoch 82 \t Batch 120 \t Training Loss: 45.74663546880086\n",
      "Epoch 82 \t Batch 140 \t Training Loss: 46.0319698878697\n",
      "Epoch 82 \t Batch 160 \t Training Loss: 46.03924851417541\n",
      "Epoch 82 \t Batch 180 \t Training Loss: 46.10801696777344\n",
      "Epoch 82 \t Batch 200 \t Training Loss: 46.04539630889892\n",
      "Epoch 82 \t Batch 220 \t Training Loss: 46.11039035103538\n",
      "Epoch 82 \t Batch 240 \t Training Loss: 46.034213892618816\n",
      "Epoch 82 \t Batch 260 \t Training Loss: 46.08546102963961\n",
      "Epoch 82 \t Batch 280 \t Training Loss: 46.11807505743844\n",
      "Epoch 82 \t Batch 300 \t Training Loss: 46.047739855448405\n",
      "Epoch 82 \t Batch 320 \t Training Loss: 46.003397333621976\n",
      "Epoch 82 \t Batch 340 \t Training Loss: 45.94751877504237\n",
      "Epoch 82 \t Batch 360 \t Training Loss: 45.94396183225844\n",
      "Epoch 82 \t Batch 380 \t Training Loss: 45.917892335590565\n",
      "Epoch 82 \t Batch 400 \t Training Loss: 45.88261332511902\n",
      "Epoch 82 \t Batch 420 \t Training Loss: 45.89802894592285\n",
      "Epoch 82 \t Batch 440 \t Training Loss: 45.932962018793276\n",
      "Epoch 82 \t Batch 460 \t Training Loss: 45.89005391494088\n",
      "Epoch 82 \t Batch 480 \t Training Loss: 45.922637414932254\n",
      "Epoch 82 \t Batch 500 \t Training Loss: 45.95449880218506\n",
      "Epoch 82 \t Batch 520 \t Training Loss: 45.945957139822156\n",
      "Epoch 82 \t Batch 540 \t Training Loss: 45.95435177131935\n",
      "Epoch 82 \t Batch 560 \t Training Loss: 45.922886984688894\n",
      "Epoch 82 \t Batch 580 \t Training Loss: 45.95867919921875\n",
      "Epoch 82 \t Batch 600 \t Training Loss: 45.962369448343914\n",
      "Epoch 82 \t Batch 620 \t Training Loss: 45.96316749818863\n",
      "Epoch 82 \t Batch 640 \t Training Loss: 45.888152688741684\n",
      "Epoch 82 \t Batch 660 \t Training Loss: 45.83542283954042\n",
      "Epoch 82 \t Batch 680 \t Training Loss: 45.80668010150685\n",
      "Epoch 82 \t Batch 700 \t Training Loss: 45.77269412449428\n",
      "Epoch 82 \t Batch 720 \t Training Loss: 45.737927209006415\n",
      "Epoch 82 \t Batch 740 \t Training Loss: 45.724160715051596\n",
      "Epoch 82 \t Batch 760 \t Training Loss: 45.72598924134907\n",
      "Epoch 82 \t Batch 780 \t Training Loss: 45.71334853050036\n",
      "Epoch 82 \t Batch 800 \t Training Loss: 45.74207817554474\n",
      "Epoch 82 \t Batch 820 \t Training Loss: 45.77874675378567\n",
      "Epoch 82 \t Batch 840 \t Training Loss: 45.77005087080456\n",
      "Epoch 82 \t Batch 860 \t Training Loss: 45.77817751418713\n",
      "Epoch 82 \t Batch 880 \t Training Loss: 45.76776481108232\n",
      "Epoch 82 \t Batch 900 \t Training Loss: 45.75298776838515\n",
      "Epoch 82 \t Batch 20 \t Validation Loss: 54.61390209197998\n",
      "Epoch 82 \t Batch 40 \t Validation Loss: 47.860179376602176\n",
      "Epoch 82 \t Batch 60 \t Validation Loss: 51.29087934494019\n",
      "Epoch 82 \t Batch 80 \t Validation Loss: 48.775701665878294\n",
      "Epoch 82 \t Batch 100 \t Validation Loss: 45.41579822540283\n",
      "Epoch 82 \t Batch 120 \t Validation Loss: 43.30867284138997\n",
      "Epoch 82 \t Batch 140 \t Validation Loss: 41.241340337480814\n",
      "Epoch 82 \t Batch 160 \t Validation Loss: 41.23774130344391\n",
      "Epoch 82 \t Batch 180 \t Validation Loss: 43.06780446370443\n",
      "Epoch 82 \t Batch 200 \t Validation Loss: 43.44352784156799\n",
      "Epoch 82 \t Batch 220 \t Validation Loss: 43.68221283825961\n",
      "Epoch 82 \t Batch 240 \t Validation Loss: 43.271387894948326\n",
      "Epoch 82 \t Batch 260 \t Validation Loss: 44.815359845528235\n",
      "Epoch 82 \t Batch 280 \t Validation Loss: 45.43268529687609\n",
      "Epoch 82 \t Batch 300 \t Validation Loss: 45.748307371139525\n",
      "Epoch 82 \t Batch 320 \t Validation Loss: 45.733646312355994\n",
      "Epoch 82 \t Batch 340 \t Validation Loss: 45.280654663198135\n",
      "Epoch 82 \t Batch 360 \t Validation Loss: 44.78391529719035\n",
      "Epoch 82 \t Batch 380 \t Validation Loss: 44.84518785476685\n",
      "Epoch 82 \t Batch 400 \t Validation Loss: 44.33593479156494\n",
      "Epoch 82 \t Batch 420 \t Validation Loss: 44.168611585526236\n",
      "Epoch 82 \t Batch 440 \t Validation Loss: 43.88570337295532\n",
      "Epoch 82 \t Batch 460 \t Validation Loss: 44.06824180354243\n",
      "Epoch 82 \t Batch 480 \t Validation Loss: 44.35256962776184\n",
      "Epoch 82 \t Batch 500 \t Validation Loss: 43.88579671859741\n",
      "Epoch 82 \t Batch 520 \t Validation Loss: 43.80278741029593\n",
      "Epoch 82 \t Batch 540 \t Validation Loss: 43.385012517152006\n",
      "Epoch 82 \t Batch 560 \t Validation Loss: 43.02713737828391\n",
      "Epoch 82 \t Batch 580 \t Validation Loss: 42.697400481125406\n",
      "Epoch 82 \t Batch 600 \t Validation Loss: 42.75100539207458\n",
      "Epoch 82 Training Loss: 45.72280360880179 Validation Loss: 43.26401721966731\n",
      "Epoch 82 completed\n",
      "Epoch 83 \t Batch 20 \t Training Loss: 43.80500812530518\n",
      "Epoch 83 \t Batch 40 \t Training Loss: 45.487058353424075\n",
      "Epoch 83 \t Batch 60 \t Training Loss: 45.3914878209432\n",
      "Epoch 83 \t Batch 80 \t Training Loss: 45.56046533584595\n",
      "Epoch 83 \t Batch 100 \t Training Loss: 45.32367889404297\n",
      "Epoch 83 \t Batch 120 \t Training Loss: 45.546899509429934\n",
      "Epoch 83 \t Batch 140 \t Training Loss: 45.50308824266706\n",
      "Epoch 83 \t Batch 160 \t Training Loss: 45.71721723079681\n",
      "Epoch 83 \t Batch 180 \t Training Loss: 45.682618692186146\n",
      "Epoch 83 \t Batch 200 \t Training Loss: 45.75465860366821\n",
      "Epoch 83 \t Batch 220 \t Training Loss: 45.7700946634466\n",
      "Epoch 83 \t Batch 240 \t Training Loss: 45.650869369506836\n",
      "Epoch 83 \t Batch 260 \t Training Loss: 45.70735086294321\n",
      "Epoch 83 \t Batch 280 \t Training Loss: 45.66558696201869\n",
      "Epoch 83 \t Batch 300 \t Training Loss: 45.66096373240153\n",
      "Epoch 83 \t Batch 320 \t Training Loss: 45.741985988616946\n",
      "Epoch 83 \t Batch 340 \t Training Loss: 45.77357843062457\n",
      "Epoch 83 \t Batch 360 \t Training Loss: 45.80950560039944\n",
      "Epoch 83 \t Batch 380 \t Training Loss: 45.770870600248635\n",
      "Epoch 83 \t Batch 400 \t Training Loss: 45.74153193473816\n",
      "Epoch 83 \t Batch 420 \t Training Loss: 45.70936592647008\n",
      "Epoch 83 \t Batch 440 \t Training Loss: 45.69638199372725\n",
      "Epoch 83 \t Batch 460 \t Training Loss: 45.70944795193879\n",
      "Epoch 83 \t Batch 480 \t Training Loss: 45.690434161822004\n",
      "Epoch 83 \t Batch 500 \t Training Loss: 45.599054962158206\n",
      "Epoch 83 \t Batch 520 \t Training Loss: 45.583564450190615\n",
      "Epoch 83 \t Batch 540 \t Training Loss: 45.62468788712113\n",
      "Epoch 83 \t Batch 560 \t Training Loss: 45.638025113514495\n",
      "Epoch 83 \t Batch 580 \t Training Loss: 45.688128445066255\n",
      "Epoch 83 \t Batch 600 \t Training Loss: 45.72299101511637\n",
      "Epoch 83 \t Batch 620 \t Training Loss: 45.677637124830675\n",
      "Epoch 83 \t Batch 640 \t Training Loss: 45.72393838763237\n",
      "Epoch 83 \t Batch 660 \t Training Loss: 45.72110870823715\n",
      "Epoch 83 \t Batch 680 \t Training Loss: 45.702034338782816\n",
      "Epoch 83 \t Batch 700 \t Training Loss: 45.70109965188163\n",
      "Epoch 83 \t Batch 720 \t Training Loss: 45.669159687889945\n",
      "Epoch 83 \t Batch 740 \t Training Loss: 45.69633124067977\n",
      "Epoch 83 \t Batch 760 \t Training Loss: 45.65963974500957\n",
      "Epoch 83 \t Batch 780 \t Training Loss: 45.660669752267694\n",
      "Epoch 83 \t Batch 800 \t Training Loss: 45.71116656780243\n",
      "Epoch 83 \t Batch 820 \t Training Loss: 45.69503077995486\n",
      "Epoch 83 \t Batch 840 \t Training Loss: 45.64934536161877\n",
      "Epoch 83 \t Batch 860 \t Training Loss: 45.66707463375358\n",
      "Epoch 83 \t Batch 880 \t Training Loss: 45.645031521537085\n",
      "Epoch 83 \t Batch 900 \t Training Loss: 45.65591234842936\n",
      "Epoch 83 \t Batch 20 \t Validation Loss: 60.801585006713864\n",
      "Epoch 83 \t Batch 40 \t Validation Loss: 52.270024394989015\n",
      "Epoch 83 \t Batch 60 \t Validation Loss: 55.7633100827535\n",
      "Epoch 83 \t Batch 80 \t Validation Loss: 53.192828464508054\n",
      "Epoch 83 \t Batch 100 \t Validation Loss: 48.75584627151489\n",
      "Epoch 83 \t Batch 120 \t Validation Loss: 45.873195012410484\n",
      "Epoch 83 \t Batch 140 \t Validation Loss: 43.24643729073661\n",
      "Epoch 83 \t Batch 160 \t Validation Loss: 42.70334795713425\n",
      "Epoch 83 \t Batch 180 \t Validation Loss: 44.14389241006639\n",
      "Epoch 83 \t Batch 200 \t Validation Loss: 44.284197735786435\n",
      "Epoch 83 \t Batch 220 \t Validation Loss: 44.24499167095531\n",
      "Epoch 83 \t Batch 240 \t Validation Loss: 43.62691813707352\n",
      "Epoch 83 \t Batch 260 \t Validation Loss: 45.03433747291565\n",
      "Epoch 83 \t Batch 280 \t Validation Loss: 45.53553944315229\n",
      "Epoch 83 \t Batch 300 \t Validation Loss: 45.66447634379069\n",
      "Epoch 83 \t Batch 320 \t Validation Loss: 45.518001744151114\n",
      "Epoch 83 \t Batch 340 \t Validation Loss: 45.01956870415631\n",
      "Epoch 83 \t Batch 360 \t Validation Loss: 44.36614948908488\n",
      "Epoch 83 \t Batch 380 \t Validation Loss: 44.2955448702762\n",
      "Epoch 83 \t Batch 400 \t Validation Loss: 43.65079537391662\n",
      "Epoch 83 \t Batch 420 \t Validation Loss: 43.428933198111395\n",
      "Epoch 83 \t Batch 440 \t Validation Loss: 42.997285288030454\n",
      "Epoch 83 \t Batch 460 \t Validation Loss: 43.068408924600355\n",
      "Epoch 83 \t Batch 480 \t Validation Loss: 43.32339967886607\n",
      "Epoch 83 \t Batch 500 \t Validation Loss: 42.83042583465576\n",
      "Epoch 83 \t Batch 520 \t Validation Loss: 42.55601074512188\n",
      "Epoch 83 \t Batch 540 \t Validation Loss: 42.104974301656085\n",
      "Epoch 83 \t Batch 560 \t Validation Loss: 41.754132751056126\n",
      "Epoch 83 \t Batch 580 \t Validation Loss: 41.41484520024267\n",
      "Epoch 83 \t Batch 600 \t Validation Loss: 41.476603733698525\n",
      "Epoch 83 Training Loss: 45.67016433083504 Validation Loss: 41.996544531413484\n",
      "Epoch 83 completed\n",
      "Epoch 84 \t Batch 20 \t Training Loss: 43.08340473175049\n",
      "Epoch 84 \t Batch 40 \t Training Loss: 44.549374675750734\n",
      "Epoch 84 \t Batch 60 \t Training Loss: 44.71756801605225\n",
      "Epoch 84 \t Batch 80 \t Training Loss: 45.09144005775452\n",
      "Epoch 84 \t Batch 100 \t Training Loss: 44.903293724060056\n",
      "Epoch 84 \t Batch 120 \t Training Loss: 45.027629884084064\n",
      "Epoch 84 \t Batch 140 \t Training Loss: 45.06228763035366\n",
      "Epoch 84 \t Batch 160 \t Training Loss: 45.117434072494504\n",
      "Epoch 84 \t Batch 180 \t Training Loss: 45.17085461086697\n",
      "Epoch 84 \t Batch 200 \t Training Loss: 45.213957138061524\n",
      "Epoch 84 \t Batch 220 \t Training Loss: 45.358892094005235\n",
      "Epoch 84 \t Batch 240 \t Training Loss: 45.39317169189453\n",
      "Epoch 84 \t Batch 260 \t Training Loss: 45.39147142263559\n",
      "Epoch 84 \t Batch 280 \t Training Loss: 45.46235570907593\n",
      "Epoch 84 \t Batch 300 \t Training Loss: 45.49925415039063\n",
      "Epoch 84 \t Batch 320 \t Training Loss: 45.488166904449464\n",
      "Epoch 84 \t Batch 340 \t Training Loss: 45.482290851368624\n",
      "Epoch 84 \t Batch 360 \t Training Loss: 45.52273293601142\n",
      "Epoch 84 \t Batch 380 \t Training Loss: 45.481527218065764\n",
      "Epoch 84 \t Batch 400 \t Training Loss: 45.48258785247803\n",
      "Epoch 84 \t Batch 420 \t Training Loss: 45.53212740761893\n",
      "Epoch 84 \t Batch 440 \t Training Loss: 45.50901355743408\n",
      "Epoch 84 \t Batch 460 \t Training Loss: 45.45101776123047\n",
      "Epoch 84 \t Batch 480 \t Training Loss: 45.44009548823039\n",
      "Epoch 84 \t Batch 500 \t Training Loss: 45.47120874786377\n",
      "Epoch 84 \t Batch 520 \t Training Loss: 45.49272877619817\n",
      "Epoch 84 \t Batch 540 \t Training Loss: 45.51430079848678\n",
      "Epoch 84 \t Batch 560 \t Training Loss: 45.52574987411499\n",
      "Epoch 84 \t Batch 580 \t Training Loss: 45.53844476897141\n",
      "Epoch 84 \t Batch 600 \t Training Loss: 45.5396786181132\n",
      "Epoch 84 \t Batch 620 \t Training Loss: 45.571765278231716\n",
      "Epoch 84 \t Batch 640 \t Training Loss: 45.59273452162743\n",
      "Epoch 84 \t Batch 660 \t Training Loss: 45.56313715270071\n",
      "Epoch 84 \t Batch 680 \t Training Loss: 45.53139991199269\n",
      "Epoch 84 \t Batch 700 \t Training Loss: 45.57393853868757\n",
      "Epoch 84 \t Batch 720 \t Training Loss: 45.56137736638387\n",
      "Epoch 84 \t Batch 740 \t Training Loss: 45.582458774463554\n",
      "Epoch 84 \t Batch 760 \t Training Loss: 45.61315718199077\n",
      "Epoch 84 \t Batch 780 \t Training Loss: 45.61994514954396\n",
      "Epoch 84 \t Batch 800 \t Training Loss: 45.6050984621048\n",
      "Epoch 84 \t Batch 820 \t Training Loss: 45.60702159230302\n",
      "Epoch 84 \t Batch 840 \t Training Loss: 45.597231560661676\n",
      "Epoch 84 \t Batch 860 \t Training Loss: 45.59888115816338\n",
      "Epoch 84 \t Batch 880 \t Training Loss: 45.59751730398698\n",
      "Epoch 84 \t Batch 900 \t Training Loss: 45.617959111531576\n",
      "Epoch 84 \t Batch 20 \t Validation Loss: 63.76561479568481\n",
      "Epoch 84 \t Batch 40 \t Validation Loss: 55.157397174835204\n",
      "Epoch 84 \t Batch 60 \t Validation Loss: 58.92624775568644\n",
      "Epoch 84 \t Batch 80 \t Validation Loss: 56.17659389972687\n",
      "Epoch 84 \t Batch 100 \t Validation Loss: 51.3059224319458\n",
      "Epoch 84 \t Batch 120 \t Validation Loss: 48.12345790863037\n",
      "Epoch 84 \t Batch 140 \t Validation Loss: 45.30214443206787\n",
      "Epoch 84 \t Batch 160 \t Validation Loss: 44.80989683866501\n",
      "Epoch 84 \t Batch 180 \t Validation Loss: 46.39484249220954\n",
      "Epoch 84 \t Batch 200 \t Validation Loss: 46.52888393878937\n",
      "Epoch 84 \t Batch 220 \t Validation Loss: 46.49431269819086\n",
      "Epoch 84 \t Batch 240 \t Validation Loss: 45.86661171515782\n",
      "Epoch 84 \t Batch 260 \t Validation Loss: 47.24826410733736\n",
      "Epoch 84 \t Batch 280 \t Validation Loss: 47.72042110647474\n",
      "Epoch 84 \t Batch 300 \t Validation Loss: 47.99716401100159\n",
      "Epoch 84 \t Batch 320 \t Validation Loss: 47.88607352077961\n",
      "Epoch 84 \t Batch 340 \t Validation Loss: 47.33867901353275\n",
      "Epoch 84 \t Batch 360 \t Validation Loss: 46.721134543418884\n",
      "Epoch 84 \t Batch 380 \t Validation Loss: 46.664936043086804\n",
      "Epoch 84 \t Batch 400 \t Validation Loss: 45.99635314702988\n",
      "Epoch 84 \t Batch 420 \t Validation Loss: 45.73493696394421\n",
      "Epoch 84 \t Batch 440 \t Validation Loss: 45.31577822945335\n",
      "Epoch 84 \t Batch 460 \t Validation Loss: 45.43015368503073\n",
      "Epoch 84 \t Batch 480 \t Validation Loss: 45.6442560950915\n",
      "Epoch 84 \t Batch 500 \t Validation Loss: 45.16609813690186\n",
      "Epoch 84 \t Batch 520 \t Validation Loss: 45.02030234336853\n",
      "Epoch 84 \t Batch 540 \t Validation Loss: 44.4605532752143\n",
      "Epoch 84 \t Batch 560 \t Validation Loss: 44.00327947480338\n",
      "Epoch 84 \t Batch 580 \t Validation Loss: 43.5440528475005\n",
      "Epoch 84 \t Batch 600 \t Validation Loss: 43.52436544418335\n",
      "Epoch 84 Training Loss: 45.65013915033871 Validation Loss: 43.98929042630381\n",
      "Epoch 84 completed\n",
      "Epoch 85 \t Batch 20 \t Training Loss: 46.612915229797366\n",
      "Epoch 85 \t Batch 40 \t Training Loss: 46.213415908813474\n",
      "Epoch 85 \t Batch 60 \t Training Loss: 46.45363114674886\n",
      "Epoch 85 \t Batch 80 \t Training Loss: 46.01220712661743\n",
      "Epoch 85 \t Batch 100 \t Training Loss: 45.709041061401365\n",
      "Epoch 85 \t Batch 120 \t Training Loss: 45.45298719406128\n",
      "Epoch 85 \t Batch 140 \t Training Loss: 45.73666812351772\n",
      "Epoch 85 \t Batch 160 \t Training Loss: 45.56961529254913\n",
      "Epoch 85 \t Batch 180 \t Training Loss: 45.54723858303494\n",
      "Epoch 85 \t Batch 200 \t Training Loss: 45.492119731903074\n",
      "Epoch 85 \t Batch 220 \t Training Loss: 45.57416184165261\n",
      "Epoch 85 \t Batch 240 \t Training Loss: 45.56176241238912\n",
      "Epoch 85 \t Batch 260 \t Training Loss: 45.52895694145789\n",
      "Epoch 85 \t Batch 280 \t Training Loss: 45.49883436475481\n",
      "Epoch 85 \t Batch 300 \t Training Loss: 45.469986305236816\n",
      "Epoch 85 \t Batch 320 \t Training Loss: 45.41743150949478\n",
      "Epoch 85 \t Batch 340 \t Training Loss: 45.31293340570787\n",
      "Epoch 85 \t Batch 360 \t Training Loss: 45.28398165173001\n",
      "Epoch 85 \t Batch 380 \t Training Loss: 45.2961552870901\n",
      "Epoch 85 \t Batch 400 \t Training Loss: 45.27058554649353\n",
      "Epoch 85 \t Batch 420 \t Training Loss: 45.3125618707566\n",
      "Epoch 85 \t Batch 440 \t Training Loss: 45.364659430763936\n",
      "Epoch 85 \t Batch 460 \t Training Loss: 45.424571692425275\n",
      "Epoch 85 \t Batch 480 \t Training Loss: 45.454739507039385\n",
      "Epoch 85 \t Batch 500 \t Training Loss: 45.55734881591797\n",
      "Epoch 85 \t Batch 520 \t Training Loss: 45.56977820763221\n",
      "Epoch 85 \t Batch 540 \t Training Loss: 45.59007959012632\n",
      "Epoch 85 \t Batch 560 \t Training Loss: 45.586664356504166\n",
      "Epoch 85 \t Batch 580 \t Training Loss: 45.58335079982363\n",
      "Epoch 85 \t Batch 600 \t Training Loss: 45.64525955835978\n",
      "Epoch 85 \t Batch 620 \t Training Loss: 45.63970659317509\n",
      "Epoch 85 \t Batch 640 \t Training Loss: 45.65565083026886\n",
      "Epoch 85 \t Batch 660 \t Training Loss: 45.66749639222116\n",
      "Epoch 85 \t Batch 680 \t Training Loss: 45.65247649024515\n",
      "Epoch 85 \t Batch 700 \t Training Loss: 45.68260577610561\n",
      "Epoch 85 \t Batch 720 \t Training Loss: 45.70345985094706\n",
      "Epoch 85 \t Batch 740 \t Training Loss: 45.66647544036041\n",
      "Epoch 85 \t Batch 760 \t Training Loss: 45.626929770017924\n",
      "Epoch 85 \t Batch 780 \t Training Loss: 45.62915504162128\n",
      "Epoch 85 \t Batch 800 \t Training Loss: 45.61995484828949\n",
      "Epoch 85 \t Batch 820 \t Training Loss: 45.620813523269284\n",
      "Epoch 85 \t Batch 840 \t Training Loss: 45.62388645807902\n",
      "Epoch 85 \t Batch 860 \t Training Loss: 45.60467937824338\n",
      "Epoch 85 \t Batch 880 \t Training Loss: 45.590001565759835\n",
      "Epoch 85 \t Batch 900 \t Training Loss: 45.60372612423367\n",
      "Epoch 85 \t Batch 20 \t Validation Loss: 39.274137783050534\n",
      "Epoch 85 \t Batch 40 \t Validation Loss: 36.40857458114624\n",
      "Epoch 85 \t Batch 60 \t Validation Loss: 38.184870115915935\n",
      "Epoch 85 \t Batch 80 \t Validation Loss: 37.40221029520035\n",
      "Epoch 85 \t Batch 100 \t Validation Loss: 35.71365498542786\n",
      "Epoch 85 \t Batch 120 \t Validation Loss: 34.69129962126414\n",
      "Epoch 85 \t Batch 140 \t Validation Loss: 33.62590700558254\n",
      "Epoch 85 \t Batch 160 \t Validation Loss: 34.69105703234673\n",
      "Epoch 85 \t Batch 180 \t Validation Loss: 37.639304394192166\n",
      "Epoch 85 \t Batch 200 \t Validation Loss: 38.77980738639832\n",
      "Epoch 85 \t Batch 220 \t Validation Loss: 39.706548291986635\n",
      "Epoch 85 \t Batch 240 \t Validation Loss: 39.79663632710775\n",
      "Epoch 85 \t Batch 260 \t Validation Loss: 41.84360140286959\n",
      "Epoch 85 \t Batch 280 \t Validation Loss: 42.836524970190865\n",
      "Epoch 85 \t Batch 300 \t Validation Loss: 43.55646830240885\n",
      "Epoch 85 \t Batch 320 \t Validation Loss: 43.7585952758789\n",
      "Epoch 85 \t Batch 340 \t Validation Loss: 43.4910777316374\n",
      "Epoch 85 \t Batch 360 \t Validation Loss: 43.12398858070374\n",
      "Epoch 85 \t Batch 380 \t Validation Loss: 43.27630758787456\n",
      "Epoch 85 \t Batch 400 \t Validation Loss: 42.73467702865601\n",
      "Epoch 85 \t Batch 420 \t Validation Loss: 42.621608777273266\n",
      "Epoch 85 \t Batch 440 \t Validation Loss: 42.28476476019079\n",
      "Epoch 85 \t Batch 460 \t Validation Loss: 42.41604985983476\n",
      "Epoch 85 \t Batch 480 \t Validation Loss: 42.761480289697644\n",
      "Epoch 85 \t Batch 500 \t Validation Loss: 42.38875274467468\n",
      "Epoch 85 \t Batch 520 \t Validation Loss: 42.20155922632951\n",
      "Epoch 85 \t Batch 540 \t Validation Loss: 41.74969219808225\n",
      "Epoch 85 \t Batch 560 \t Validation Loss: 41.40845719235284\n",
      "Epoch 85 \t Batch 580 \t Validation Loss: 41.06752439531787\n",
      "Epoch 85 \t Batch 600 \t Validation Loss: 41.12649083296458\n",
      "Epoch 85 Training Loss: 45.627224134904864 Validation Loss: 41.6722391379344\n",
      "Epoch 85 completed\n",
      "Epoch 86 \t Batch 20 \t Training Loss: 45.20099029541016\n",
      "Epoch 86 \t Batch 40 \t Training Loss: 45.41507701873779\n",
      "Epoch 86 \t Batch 60 \t Training Loss: 45.582532501220705\n",
      "Epoch 86 \t Batch 80 \t Training Loss: 45.4307888507843\n",
      "Epoch 86 \t Batch 100 \t Training Loss: 45.27285671234131\n",
      "Epoch 86 \t Batch 120 \t Training Loss: 45.47214771906535\n",
      "Epoch 86 \t Batch 140 \t Training Loss: 45.50194723946708\n",
      "Epoch 86 \t Batch 160 \t Training Loss: 45.496098804473874\n",
      "Epoch 86 \t Batch 180 \t Training Loss: 45.507665401034885\n",
      "Epoch 86 \t Batch 200 \t Training Loss: 45.50306221008301\n",
      "Epoch 86 \t Batch 220 \t Training Loss: 45.58302102522416\n",
      "Epoch 86 \t Batch 240 \t Training Loss: 45.70314707756042\n",
      "Epoch 86 \t Batch 260 \t Training Loss: 45.72236373607929\n",
      "Epoch 86 \t Batch 280 \t Training Loss: 45.72467607770648\n",
      "Epoch 86 \t Batch 300 \t Training Loss: 45.73437834421794\n",
      "Epoch 86 \t Batch 320 \t Training Loss: 45.666970098018645\n",
      "Epoch 86 \t Batch 340 \t Training Loss: 45.643760209925034\n",
      "Epoch 86 \t Batch 360 \t Training Loss: 45.66235008239746\n",
      "Epoch 86 \t Batch 380 \t Training Loss: 45.74529833542673\n",
      "Epoch 86 \t Batch 400 \t Training Loss: 45.730919647216794\n",
      "Epoch 86 \t Batch 420 \t Training Loss: 45.72031548817952\n",
      "Epoch 86 \t Batch 440 \t Training Loss: 45.658967252211134\n",
      "Epoch 86 \t Batch 460 \t Training Loss: 45.704219229325005\n",
      "Epoch 86 \t Batch 480 \t Training Loss: 45.639217074712114\n",
      "Epoch 86 \t Batch 500 \t Training Loss: 45.67308443450928\n",
      "Epoch 86 \t Batch 520 \t Training Loss: 45.6161004286546\n",
      "Epoch 86 \t Batch 540 \t Training Loss: 45.613689528571236\n",
      "Epoch 86 \t Batch 560 \t Training Loss: 45.626248420987814\n",
      "Epoch 86 \t Batch 580 \t Training Loss: 45.61567559077822\n",
      "Epoch 86 \t Batch 600 \t Training Loss: 45.639744466145835\n",
      "Epoch 86 \t Batch 620 \t Training Loss: 45.667006148061446\n",
      "Epoch 86 \t Batch 640 \t Training Loss: 45.64743882417679\n",
      "Epoch 86 \t Batch 660 \t Training Loss: 45.641598441384055\n",
      "Epoch 86 \t Batch 680 \t Training Loss: 45.638731047686406\n",
      "Epoch 86 \t Batch 700 \t Training Loss: 45.673544540405274\n",
      "Epoch 86 \t Batch 720 \t Training Loss: 45.698912138409085\n",
      "Epoch 86 \t Batch 740 \t Training Loss: 45.71342407432763\n",
      "Epoch 86 \t Batch 760 \t Training Loss: 45.72882535332128\n",
      "Epoch 86 \t Batch 780 \t Training Loss: 45.74825874719864\n",
      "Epoch 86 \t Batch 800 \t Training Loss: 45.71140525341034\n",
      "Epoch 86 \t Batch 820 \t Training Loss: 45.73185073573415\n",
      "Epoch 86 \t Batch 840 \t Training Loss: 45.732897581372946\n",
      "Epoch 86 \t Batch 860 \t Training Loss: 45.687365607328196\n",
      "Epoch 86 \t Batch 880 \t Training Loss: 45.65864377021789\n",
      "Epoch 86 \t Batch 900 \t Training Loss: 45.651710238986546\n",
      "Epoch 86 \t Batch 20 \t Validation Loss: 46.67574377059937\n",
      "Epoch 86 \t Batch 40 \t Validation Loss: 42.07331948280334\n",
      "Epoch 86 \t Batch 60 \t Validation Loss: 45.160171794891355\n",
      "Epoch 86 \t Batch 80 \t Validation Loss: 42.87029733657837\n",
      "Epoch 86 \t Batch 100 \t Validation Loss: 40.057996921539306\n",
      "Epoch 86 \t Batch 120 \t Validation Loss: 38.552241786321005\n",
      "Epoch 86 \t Batch 140 \t Validation Loss: 36.9124030658177\n",
      "Epoch 86 \t Batch 160 \t Validation Loss: 37.52469470500946\n",
      "Epoch 86 \t Batch 180 \t Validation Loss: 40.12007786962721\n",
      "Epoch 86 \t Batch 200 \t Validation Loss: 41.06373031139374\n",
      "Epoch 86 \t Batch 220 \t Validation Loss: 41.74444257129323\n",
      "Epoch 86 \t Batch 240 \t Validation Loss: 41.683626917998\n",
      "Epoch 86 \t Batch 260 \t Validation Loss: 43.55915573927072\n",
      "Epoch 86 \t Batch 280 \t Validation Loss: 44.41976783616202\n",
      "Epoch 86 \t Batch 300 \t Validation Loss: 45.00440338134766\n",
      "Epoch 86 \t Batch 320 \t Validation Loss: 45.10364270210266\n",
      "Epoch 86 \t Batch 340 \t Validation Loss: 44.74757295496323\n",
      "Epoch 86 \t Batch 360 \t Validation Loss: 44.24654644330342\n",
      "Epoch 86 \t Batch 380 \t Validation Loss: 44.38056117107994\n",
      "Epoch 86 \t Batch 400 \t Validation Loss: 43.90010235309601\n",
      "Epoch 86 \t Batch 420 \t Validation Loss: 43.78741311799912\n",
      "Epoch 86 \t Batch 440 \t Validation Loss: 43.594635904919016\n",
      "Epoch 86 \t Batch 460 \t Validation Loss: 43.72639491661735\n",
      "Epoch 86 \t Batch 480 \t Validation Loss: 44.01391870776812\n",
      "Epoch 86 \t Batch 500 \t Validation Loss: 43.58536677742004\n",
      "Epoch 86 \t Batch 520 \t Validation Loss: 43.45858768499814\n",
      "Epoch 86 \t Batch 540 \t Validation Loss: 43.0166285249922\n",
      "Epoch 86 \t Batch 560 \t Validation Loss: 42.71710696731295\n",
      "Epoch 86 \t Batch 580 \t Validation Loss: 42.37232355742619\n",
      "Epoch 86 \t Batch 600 \t Validation Loss: 42.43871016025543\n",
      "Epoch 86 Training Loss: 45.61894515608493 Validation Loss: 42.97341561472261\n",
      "Epoch 86 completed\n",
      "Epoch 87 \t Batch 20 \t Training Loss: 46.36462516784668\n",
      "Epoch 87 \t Batch 40 \t Training Loss: 44.93734645843506\n",
      "Epoch 87 \t Batch 60 \t Training Loss: 45.151272710164385\n",
      "Epoch 87 \t Batch 80 \t Training Loss: 45.524530506134035\n",
      "Epoch 87 \t Batch 100 \t Training Loss: 45.665173034667966\n",
      "Epoch 87 \t Batch 120 \t Training Loss: 45.50589602788289\n",
      "Epoch 87 \t Batch 140 \t Training Loss: 45.435264996119905\n",
      "Epoch 87 \t Batch 160 \t Training Loss: 45.49984080791474\n",
      "Epoch 87 \t Batch 180 \t Training Loss: 45.39422208997939\n",
      "Epoch 87 \t Batch 200 \t Training Loss: 45.46701509475708\n",
      "Epoch 87 \t Batch 220 \t Training Loss: 45.536558983542704\n",
      "Epoch 87 \t Batch 240 \t Training Loss: 45.639658292134605\n",
      "Epoch 87 \t Batch 260 \t Training Loss: 45.633223460270806\n",
      "Epoch 87 \t Batch 280 \t Training Loss: 45.669382558550154\n",
      "Epoch 87 \t Batch 300 \t Training Loss: 45.67977602640788\n",
      "Epoch 87 \t Batch 320 \t Training Loss: 45.67269991636276\n",
      "Epoch 87 \t Batch 340 \t Training Loss: 45.5986800923067\n",
      "Epoch 87 \t Batch 360 \t Training Loss: 45.618319574991865\n",
      "Epoch 87 \t Batch 380 \t Training Loss: 45.49945646587171\n",
      "Epoch 87 \t Batch 400 \t Training Loss: 45.46329899787903\n",
      "Epoch 87 \t Batch 420 \t Training Loss: 45.44538087390718\n",
      "Epoch 87 \t Batch 440 \t Training Loss: 45.40390141227029\n",
      "Epoch 87 \t Batch 460 \t Training Loss: 45.411381680032484\n",
      "Epoch 87 \t Batch 480 \t Training Loss: 45.4075586160024\n",
      "Epoch 87 \t Batch 500 \t Training Loss: 45.38239179229736\n",
      "Epoch 87 \t Batch 520 \t Training Loss: 45.40986850445087\n",
      "Epoch 87 \t Batch 540 \t Training Loss: 45.41233169414379\n",
      "Epoch 87 \t Batch 560 \t Training Loss: 45.45797439302717\n",
      "Epoch 87 \t Batch 580 \t Training Loss: 45.43739372779583\n",
      "Epoch 87 \t Batch 600 \t Training Loss: 45.410796839396156\n",
      "Epoch 87 \t Batch 620 \t Training Loss: 45.38875466623614\n",
      "Epoch 87 \t Batch 640 \t Training Loss: 45.369562584161756\n",
      "Epoch 87 \t Batch 660 \t Training Loss: 45.41146589336973\n",
      "Epoch 87 \t Batch 680 \t Training Loss: 45.43580651002772\n",
      "Epoch 87 \t Batch 700 \t Training Loss: 45.49647909436907\n",
      "Epoch 87 \t Batch 720 \t Training Loss: 45.48512130843268\n",
      "Epoch 87 \t Batch 740 \t Training Loss: 45.48112474905478\n",
      "Epoch 87 \t Batch 760 \t Training Loss: 45.455727306165194\n",
      "Epoch 87 \t Batch 780 \t Training Loss: 45.46841246776092\n",
      "Epoch 87 \t Batch 800 \t Training Loss: 45.49440643310547\n",
      "Epoch 87 \t Batch 820 \t Training Loss: 45.53326149452023\n",
      "Epoch 87 \t Batch 840 \t Training Loss: 45.57036102385748\n",
      "Epoch 87 \t Batch 860 \t Training Loss: 45.58877142529155\n",
      "Epoch 87 \t Batch 880 \t Training Loss: 45.602171863209115\n",
      "Epoch 87 \t Batch 900 \t Training Loss: 45.59840067969428\n",
      "Epoch 87 \t Batch 20 \t Validation Loss: 56.39951982498169\n",
      "Epoch 87 \t Batch 40 \t Validation Loss: 48.86234874725342\n",
      "Epoch 87 \t Batch 60 \t Validation Loss: 51.38574504852295\n",
      "Epoch 87 \t Batch 80 \t Validation Loss: 48.63015284538269\n",
      "Epoch 87 \t Batch 100 \t Validation Loss: 44.78266471862793\n",
      "Epoch 87 \t Batch 120 \t Validation Loss: 42.32623578707377\n",
      "Epoch 87 \t Batch 140 \t Validation Loss: 40.1939832006182\n",
      "Epoch 87 \t Batch 160 \t Validation Loss: 40.42925658226013\n",
      "Epoch 87 \t Batch 180 \t Validation Loss: 42.595394680235124\n",
      "Epoch 87 \t Batch 200 \t Validation Loss: 43.183230671882626\n",
      "Epoch 87 \t Batch 220 \t Validation Loss: 43.639600697430694\n",
      "Epoch 87 \t Batch 240 \t Validation Loss: 43.390554209550224\n",
      "Epoch 87 \t Batch 260 \t Validation Loss: 45.101060148385855\n",
      "Epoch 87 \t Batch 280 \t Validation Loss: 45.8379736321313\n",
      "Epoch 87 \t Batch 300 \t Validation Loss: 46.26927957216898\n",
      "Epoch 87 \t Batch 320 \t Validation Loss: 46.28266750276089\n",
      "Epoch 87 \t Batch 340 \t Validation Loss: 45.84136164328631\n",
      "Epoch 87 \t Batch 360 \t Validation Loss: 45.32533796363407\n",
      "Epoch 87 \t Batch 380 \t Validation Loss: 45.40616116021809\n",
      "Epoch 87 \t Batch 400 \t Validation Loss: 44.88175726175308\n",
      "Epoch 87 \t Batch 420 \t Validation Loss: 44.691287315459476\n",
      "Epoch 87 \t Batch 440 \t Validation Loss: 44.430189442634585\n",
      "Epoch 87 \t Batch 460 \t Validation Loss: 44.54120797489001\n",
      "Epoch 87 \t Batch 480 \t Validation Loss: 44.80853601495425\n",
      "Epoch 87 \t Batch 500 \t Validation Loss: 44.35062728691101\n",
      "Epoch 87 \t Batch 520 \t Validation Loss: 44.24494030475616\n",
      "Epoch 87 \t Batch 540 \t Validation Loss: 43.82876421433908\n",
      "Epoch 87 \t Batch 560 \t Validation Loss: 43.53048794780459\n",
      "Epoch 87 \t Batch 580 \t Validation Loss: 43.25554399654783\n",
      "Epoch 87 \t Batch 600 \t Validation Loss: 43.318571778933205\n",
      "Epoch 87 Training Loss: 45.58306725022577 Validation Loss: 43.90647928900533\n",
      "Epoch 87 completed\n",
      "Epoch 88 \t Batch 20 \t Training Loss: 44.37900848388672\n",
      "Epoch 88 \t Batch 40 \t Training Loss: 45.03287076950073\n",
      "Epoch 88 \t Batch 60 \t Training Loss: 44.62033265431722\n",
      "Epoch 88 \t Batch 80 \t Training Loss: 44.62970080375671\n",
      "Epoch 88 \t Batch 100 \t Training Loss: 44.847117805480956\n",
      "Epoch 88 \t Batch 120 \t Training Loss: 44.93503405253092\n",
      "Epoch 88 \t Batch 140 \t Training Loss: 45.00864998953683\n",
      "Epoch 88 \t Batch 160 \t Training Loss: 44.98355576992035\n",
      "Epoch 88 \t Batch 180 \t Training Loss: 45.000617048475476\n",
      "Epoch 88 \t Batch 200 \t Training Loss: 44.986028842926025\n",
      "Epoch 88 \t Batch 220 \t Training Loss: 44.99377805536444\n",
      "Epoch 88 \t Batch 240 \t Training Loss: 44.89113717079162\n",
      "Epoch 88 \t Batch 260 \t Training Loss: 44.99391384124756\n",
      "Epoch 88 \t Batch 280 \t Training Loss: 45.131331770760674\n",
      "Epoch 88 \t Batch 300 \t Training Loss: 45.16200449625651\n",
      "Epoch 88 \t Batch 320 \t Training Loss: 45.14958697557449\n",
      "Epoch 88 \t Batch 340 \t Training Loss: 45.26568741517909\n",
      "Epoch 88 \t Batch 360 \t Training Loss: 45.26802772945828\n",
      "Epoch 88 \t Batch 380 \t Training Loss: 45.22749610700105\n",
      "Epoch 88 \t Batch 400 \t Training Loss: 45.31877036094666\n",
      "Epoch 88 \t Batch 420 \t Training Loss: 45.33145901816232\n",
      "Epoch 88 \t Batch 440 \t Training Loss: 45.3243634744124\n",
      "Epoch 88 \t Batch 460 \t Training Loss: 45.324624807938285\n",
      "Epoch 88 \t Batch 480 \t Training Loss: 45.326073892911275\n",
      "Epoch 88 \t Batch 500 \t Training Loss: 45.346661079406736\n",
      "Epoch 88 \t Batch 520 \t Training Loss: 45.34160202466524\n",
      "Epoch 88 \t Batch 540 \t Training Loss: 45.3775746734054\n",
      "Epoch 88 \t Batch 560 \t Training Loss: 45.39613199234009\n",
      "Epoch 88 \t Batch 580 \t Training Loss: 45.393954112611965\n",
      "Epoch 88 \t Batch 600 \t Training Loss: 45.40655260086059\n",
      "Epoch 88 \t Batch 620 \t Training Loss: 45.4234404840777\n",
      "Epoch 88 \t Batch 640 \t Training Loss: 45.46399274468422\n",
      "Epoch 88 \t Batch 660 \t Training Loss: 45.46494481057832\n",
      "Epoch 88 \t Batch 680 \t Training Loss: 45.44770162806791\n",
      "Epoch 88 \t Batch 700 \t Training Loss: 45.42411332266671\n",
      "Epoch 88 \t Batch 720 \t Training Loss: 45.439421065648396\n",
      "Epoch 88 \t Batch 740 \t Training Loss: 45.44584343884442\n",
      "Epoch 88 \t Batch 760 \t Training Loss: 45.45175225609227\n",
      "Epoch 88 \t Batch 780 \t Training Loss: 45.48503491817377\n",
      "Epoch 88 \t Batch 800 \t Training Loss: 45.50720641613007\n",
      "Epoch 88 \t Batch 820 \t Training Loss: 45.51160878902528\n",
      "Epoch 88 \t Batch 840 \t Training Loss: 45.50969569342477\n",
      "Epoch 88 \t Batch 860 \t Training Loss: 45.51345234804375\n",
      "Epoch 88 \t Batch 880 \t Training Loss: 45.51697485230186\n",
      "Epoch 88 \t Batch 900 \t Training Loss: 45.51870530022515\n",
      "Epoch 88 \t Batch 20 \t Validation Loss: 39.204720211029056\n",
      "Epoch 88 \t Batch 40 \t Validation Loss: 35.84259929656982\n",
      "Epoch 88 \t Batch 60 \t Validation Loss: 37.80657842954\n",
      "Epoch 88 \t Batch 80 \t Validation Loss: 36.652223563194276\n",
      "Epoch 88 \t Batch 100 \t Validation Loss: 35.20265209197998\n",
      "Epoch 88 \t Batch 120 \t Validation Loss: 34.39883910814921\n",
      "Epoch 88 \t Batch 140 \t Validation Loss: 33.40655457632882\n",
      "Epoch 88 \t Batch 160 \t Validation Loss: 34.43104301691055\n",
      "Epoch 88 \t Batch 180 \t Validation Loss: 37.44660028351678\n",
      "Epoch 88 \t Batch 200 \t Validation Loss: 38.483479599952695\n",
      "Epoch 88 \t Batch 220 \t Validation Loss: 39.4647789218209\n",
      "Epoch 88 \t Batch 240 \t Validation Loss: 39.60786270697911\n",
      "Epoch 88 \t Batch 260 \t Validation Loss: 41.62614212402931\n",
      "Epoch 88 \t Batch 280 \t Validation Loss: 42.59898758275168\n",
      "Epoch 88 \t Batch 300 \t Validation Loss: 43.347820863723754\n",
      "Epoch 88 \t Batch 320 \t Validation Loss: 43.59692196547985\n",
      "Epoch 88 \t Batch 340 \t Validation Loss: 43.29304692604963\n",
      "Epoch 88 \t Batch 360 \t Validation Loss: 42.917000744077896\n",
      "Epoch 88 \t Batch 380 \t Validation Loss: 43.035739145780866\n",
      "Epoch 88 \t Batch 400 \t Validation Loss: 42.429733624458315\n",
      "Epoch 88 \t Batch 420 \t Validation Loss: 42.27713949339731\n",
      "Epoch 88 \t Batch 440 \t Validation Loss: 41.850273047794\n",
      "Epoch 88 \t Batch 460 \t Validation Loss: 41.949043906253316\n",
      "Epoch 88 \t Batch 480 \t Validation Loss: 42.2855839629968\n",
      "Epoch 88 \t Batch 500 \t Validation Loss: 41.884422662734984\n",
      "Epoch 88 \t Batch 520 \t Validation Loss: 41.623921645604646\n",
      "Epoch 88 \t Batch 540 \t Validation Loss: 41.22690274803727\n",
      "Epoch 88 \t Batch 560 \t Validation Loss: 40.917337763309476\n",
      "Epoch 88 \t Batch 580 \t Validation Loss: 40.626008305056345\n",
      "Epoch 88 \t Batch 600 \t Validation Loss: 40.715238280296326\n",
      "Epoch 88 Training Loss: 45.53024031993737 Validation Loss: 41.265007447886774\n",
      "Epoch 88 completed\n",
      "Epoch 89 \t Batch 20 \t Training Loss: 45.04635047912598\n",
      "Epoch 89 \t Batch 40 \t Training Loss: 45.149227142333984\n",
      "Epoch 89 \t Batch 60 \t Training Loss: 45.05881907145182\n",
      "Epoch 89 \t Batch 80 \t Training Loss: 44.940305376052855\n",
      "Epoch 89 \t Batch 100 \t Training Loss: 45.31877960205078\n",
      "Epoch 89 \t Batch 120 \t Training Loss: 45.24160385131836\n",
      "Epoch 89 \t Batch 140 \t Training Loss: 45.16639532361712\n",
      "Epoch 89 \t Batch 160 \t Training Loss: 45.344540691375734\n",
      "Epoch 89 \t Batch 180 \t Training Loss: 45.39440392388238\n",
      "Epoch 89 \t Batch 200 \t Training Loss: 45.3665559387207\n",
      "Epoch 89 \t Batch 220 \t Training Loss: 45.52550709464333\n",
      "Epoch 89 \t Batch 240 \t Training Loss: 45.543282699584964\n",
      "Epoch 89 \t Batch 260 \t Training Loss: 45.44814098064716\n",
      "Epoch 89 \t Batch 280 \t Training Loss: 45.51168283735003\n",
      "Epoch 89 \t Batch 300 \t Training Loss: 45.62562419891358\n",
      "Epoch 89 \t Batch 320 \t Training Loss: 45.55449302196503\n",
      "Epoch 89 \t Batch 340 \t Training Loss: 45.61059255038991\n",
      "Epoch 89 \t Batch 360 \t Training Loss: 45.63936412599352\n",
      "Epoch 89 \t Batch 380 \t Training Loss: 45.66428228679456\n",
      "Epoch 89 \t Batch 400 \t Training Loss: 45.68385415077209\n",
      "Epoch 89 \t Batch 420 \t Training Loss: 45.69830810001918\n",
      "Epoch 89 \t Batch 440 \t Training Loss: 45.71068404804576\n",
      "Epoch 89 \t Batch 460 \t Training Loss: 45.718419489653215\n",
      "Epoch 89 \t Batch 480 \t Training Loss: 45.70321637789409\n",
      "Epoch 89 \t Batch 500 \t Training Loss: 45.74895359802246\n",
      "Epoch 89 \t Batch 520 \t Training Loss: 45.70795826545128\n",
      "Epoch 89 \t Batch 540 \t Training Loss: 45.70132750051993\n",
      "Epoch 89 \t Batch 560 \t Training Loss: 45.70585654122489\n",
      "Epoch 89 \t Batch 580 \t Training Loss: 45.68681242712613\n",
      "Epoch 89 \t Batch 600 \t Training Loss: 45.73193563461304\n",
      "Epoch 89 \t Batch 620 \t Training Loss: 45.69108327434909\n",
      "Epoch 89 \t Batch 640 \t Training Loss: 45.683403241634366\n",
      "Epoch 89 \t Batch 660 \t Training Loss: 45.67483985785282\n",
      "Epoch 89 \t Batch 680 \t Training Loss: 45.67336066750919\n",
      "Epoch 89 \t Batch 700 \t Training Loss: 45.69136150360107\n",
      "Epoch 89 \t Batch 720 \t Training Loss: 45.674542840321855\n",
      "Epoch 89 \t Batch 740 \t Training Loss: 45.649418588586755\n",
      "Epoch 89 \t Batch 760 \t Training Loss: 45.61783810665733\n",
      "Epoch 89 \t Batch 780 \t Training Loss: 45.60263861142672\n",
      "Epoch 89 \t Batch 800 \t Training Loss: 45.61407594680786\n",
      "Epoch 89 \t Batch 820 \t Training Loss: 45.57188188971543\n",
      "Epoch 89 \t Batch 840 \t Training Loss: 45.58017667134603\n",
      "Epoch 89 \t Batch 860 \t Training Loss: 45.558106196203894\n",
      "Epoch 89 \t Batch 880 \t Training Loss: 45.573860853368586\n",
      "Epoch 89 \t Batch 900 \t Training Loss: 45.54567088656955\n",
      "Epoch 89 \t Batch 20 \t Validation Loss: 58.65026340484619\n",
      "Epoch 89 \t Batch 40 \t Validation Loss: 51.451076698303225\n",
      "Epoch 89 \t Batch 60 \t Validation Loss: 54.154381942749026\n",
      "Epoch 89 \t Batch 80 \t Validation Loss: 51.46106054782867\n",
      "Epoch 89 \t Batch 100 \t Validation Loss: 47.12045143127441\n",
      "Epoch 89 \t Batch 120 \t Validation Loss: 44.54973553021749\n",
      "Epoch 89 \t Batch 140 \t Validation Loss: 42.158901119232176\n",
      "Epoch 89 \t Batch 160 \t Validation Loss: 41.93265982866287\n",
      "Epoch 89 \t Batch 180 \t Validation Loss: 43.62500130865309\n",
      "Epoch 89 \t Batch 200 \t Validation Loss: 43.97703432559967\n",
      "Epoch 89 \t Batch 220 \t Validation Loss: 44.125064091248944\n",
      "Epoch 89 \t Batch 240 \t Validation Loss: 43.64097106854121\n",
      "Epoch 89 \t Batch 260 \t Validation Loss: 45.18031013195331\n",
      "Epoch 89 \t Batch 280 \t Validation Loss: 45.77394483089447\n",
      "Epoch 89 \t Batch 300 \t Validation Loss: 46.01021096229553\n",
      "Epoch 89 \t Batch 320 \t Validation Loss: 45.92685635983944\n",
      "Epoch 89 \t Batch 340 \t Validation Loss: 45.4690018345328\n",
      "Epoch 89 \t Batch 360 \t Validation Loss: 44.903260080019635\n",
      "Epoch 89 \t Batch 380 \t Validation Loss: 44.89761415029827\n",
      "Epoch 89 \t Batch 400 \t Validation Loss: 44.32808645963669\n",
      "Epoch 89 \t Batch 420 \t Validation Loss: 44.13392202286493\n",
      "Epoch 89 \t Batch 440 \t Validation Loss: 43.774524480646306\n",
      "Epoch 89 \t Batch 460 \t Validation Loss: 43.83863940446273\n",
      "Epoch 89 \t Batch 480 \t Validation Loss: 44.11189113855362\n",
      "Epoch 89 \t Batch 500 \t Validation Loss: 43.64442950820923\n",
      "Epoch 89 \t Batch 520 \t Validation Loss: 43.43939236127413\n",
      "Epoch 89 \t Batch 540 \t Validation Loss: 42.98812298598113\n",
      "Epoch 89 \t Batch 560 \t Validation Loss: 42.64321740014213\n",
      "Epoch 89 \t Batch 580 \t Validation Loss: 42.34074917168453\n",
      "Epoch 89 \t Batch 600 \t Validation Loss: 42.38694670995076\n",
      "Epoch 89 Training Loss: 45.533155364324635 Validation Loss: 42.925345665448674\n",
      "Epoch 89 completed\n",
      "Epoch 90 \t Batch 20 \t Training Loss: 44.17793083190918\n",
      "Epoch 90 \t Batch 40 \t Training Loss: 44.46798400878906\n",
      "Epoch 90 \t Batch 60 \t Training Loss: 44.73781916300456\n",
      "Epoch 90 \t Batch 80 \t Training Loss: 45.32243671417236\n",
      "Epoch 90 \t Batch 100 \t Training Loss: 45.229443359375\n",
      "Epoch 90 \t Batch 120 \t Training Loss: 45.07141758600871\n",
      "Epoch 90 \t Batch 140 \t Training Loss: 45.35817999158587\n",
      "Epoch 90 \t Batch 160 \t Training Loss: 45.27039532661438\n",
      "Epoch 90 \t Batch 180 \t Training Loss: 45.27736407385932\n",
      "Epoch 90 \t Batch 200 \t Training Loss: 45.39242597579956\n",
      "Epoch 90 \t Batch 220 \t Training Loss: 45.4475210016424\n",
      "Epoch 90 \t Batch 240 \t Training Loss: 45.49663823445638\n",
      "Epoch 90 \t Batch 260 \t Training Loss: 45.44848722311167\n",
      "Epoch 90 \t Batch 280 \t Training Loss: 45.4158800806318\n",
      "Epoch 90 \t Batch 300 \t Training Loss: 45.4761315536499\n",
      "Epoch 90 \t Batch 320 \t Training Loss: 45.43724080324173\n",
      "Epoch 90 \t Batch 340 \t Training Loss: 45.432679030474496\n",
      "Epoch 90 \t Batch 360 \t Training Loss: 45.34600826899211\n",
      "Epoch 90 \t Batch 380 \t Training Loss: 45.41811682048597\n",
      "Epoch 90 \t Batch 400 \t Training Loss: 45.444536180496215\n",
      "Epoch 90 \t Batch 420 \t Training Loss: 45.416303670974\n",
      "Epoch 90 \t Batch 440 \t Training Loss: 45.40909538269043\n",
      "Epoch 90 \t Batch 460 \t Training Loss: 45.46381769594939\n",
      "Epoch 90 \t Batch 480 \t Training Loss: 45.418728510538735\n",
      "Epoch 90 \t Batch 500 \t Training Loss: 45.40407593536377\n",
      "Epoch 90 \t Batch 520 \t Training Loss: 45.35326629785391\n",
      "Epoch 90 \t Batch 540 \t Training Loss: 45.3948589960734\n",
      "Epoch 90 \t Batch 560 \t Training Loss: 45.38525867462158\n",
      "Epoch 90 \t Batch 580 \t Training Loss: 45.401558047327505\n",
      "Epoch 90 \t Batch 600 \t Training Loss: 45.39911022822062\n",
      "Epoch 90 \t Batch 620 \t Training Loss: 45.43359030446699\n",
      "Epoch 90 \t Batch 640 \t Training Loss: 45.43413244485855\n",
      "Epoch 90 \t Batch 660 \t Training Loss: 45.469106223366474\n",
      "Epoch 90 \t Batch 680 \t Training Loss: 45.46011503443999\n",
      "Epoch 90 \t Batch 700 \t Training Loss: 45.48685868399484\n",
      "Epoch 90 \t Batch 720 \t Training Loss: 45.458989678488834\n",
      "Epoch 90 \t Batch 740 \t Training Loss: 45.469832750268885\n",
      "Epoch 90 \t Batch 760 \t Training Loss: 45.47168534931384\n",
      "Epoch 90 \t Batch 780 \t Training Loss: 45.48963850461519\n",
      "Epoch 90 \t Batch 800 \t Training Loss: 45.457861547470095\n",
      "Epoch 90 \t Batch 820 \t Training Loss: 45.46320828693669\n",
      "Epoch 90 \t Batch 840 \t Training Loss: 45.47584457397461\n",
      "Epoch 90 \t Batch 860 \t Training Loss: 45.47734009498774\n",
      "Epoch 90 \t Batch 880 \t Training Loss: 45.492138693549414\n",
      "Epoch 90 \t Batch 900 \t Training Loss: 45.50415742238363\n",
      "Epoch 90 \t Batch 20 \t Validation Loss: 55.29404525756836\n",
      "Epoch 90 \t Batch 40 \t Validation Loss: 48.4026421546936\n",
      "Epoch 90 \t Batch 60 \t Validation Loss: 50.7636536916097\n",
      "Epoch 90 \t Batch 80 \t Validation Loss: 48.92957510948181\n",
      "Epoch 90 \t Batch 100 \t Validation Loss: 45.216791229248045\n",
      "Epoch 90 \t Batch 120 \t Validation Loss: 42.7790522813797\n",
      "Epoch 90 \t Batch 140 \t Validation Loss: 40.568046481268745\n",
      "Epoch 90 \t Batch 160 \t Validation Loss: 40.56550131440163\n",
      "Epoch 90 \t Batch 180 \t Validation Loss: 42.395543808407254\n",
      "Epoch 90 \t Batch 200 \t Validation Loss: 42.885223684310915\n",
      "Epoch 90 \t Batch 220 \t Validation Loss: 43.18070433356545\n",
      "Epoch 90 \t Batch 240 \t Validation Loss: 42.82981056372325\n",
      "Epoch 90 \t Batch 260 \t Validation Loss: 44.441401958465576\n",
      "Epoch 90 \t Batch 280 \t Validation Loss: 45.122387640816825\n",
      "Epoch 90 \t Batch 300 \t Validation Loss: 45.41330317179362\n",
      "Epoch 90 \t Batch 320 \t Validation Loss: 45.388706842064856\n",
      "Epoch 90 \t Batch 340 \t Validation Loss: 44.982179795994476\n",
      "Epoch 90 \t Batch 360 \t Validation Loss: 44.40326412783729\n",
      "Epoch 90 \t Batch 380 \t Validation Loss: 44.426423845793074\n",
      "Epoch 90 \t Batch 400 \t Validation Loss: 43.840847415924074\n",
      "Epoch 90 \t Batch 420 \t Validation Loss: 43.70818557966323\n",
      "Epoch 90 \t Batch 440 \t Validation Loss: 43.33744598952207\n",
      "Epoch 90 \t Batch 460 \t Validation Loss: 43.45278272006823\n",
      "Epoch 90 \t Batch 480 \t Validation Loss: 43.73954474727313\n",
      "Epoch 90 \t Batch 500 \t Validation Loss: 43.30574669456482\n",
      "Epoch 90 \t Batch 520 \t Validation Loss: 43.062356195083034\n",
      "Epoch 90 \t Batch 540 \t Validation Loss: 42.577510877891825\n",
      "Epoch 90 \t Batch 560 \t Validation Loss: 42.17410750899996\n",
      "Epoch 90 \t Batch 580 \t Validation Loss: 41.737067447859665\n",
      "Epoch 90 \t Batch 600 \t Validation Loss: 41.77731683890025\n",
      "Epoch 90 Training Loss: 45.49814064916114 Validation Loss: 42.28844196610636\n",
      "Epoch 90 completed\n",
      "Epoch 91 \t Batch 20 \t Training Loss: 44.774264907836915\n",
      "Epoch 91 \t Batch 40 \t Training Loss: 44.69446086883545\n",
      "Epoch 91 \t Batch 60 \t Training Loss: 44.82749843597412\n",
      "Epoch 91 \t Batch 80 \t Training Loss: 45.06775970458985\n",
      "Epoch 91 \t Batch 100 \t Training Loss: 45.102231407165526\n",
      "Epoch 91 \t Batch 120 \t Training Loss: 45.13076903025309\n",
      "Epoch 91 \t Batch 140 \t Training Loss: 45.092934308733255\n",
      "Epoch 91 \t Batch 160 \t Training Loss: 45.36045830249786\n",
      "Epoch 91 \t Batch 180 \t Training Loss: 45.2800287882487\n",
      "Epoch 91 \t Batch 200 \t Training Loss: 45.20614736557007\n",
      "Epoch 91 \t Batch 220 \t Training Loss: 45.025968222184616\n",
      "Epoch 91 \t Batch 240 \t Training Loss: 45.13104530970256\n",
      "Epoch 91 \t Batch 260 \t Training Loss: 45.147640110896184\n",
      "Epoch 91 \t Batch 280 \t Training Loss: 45.27576870237078\n",
      "Epoch 91 \t Batch 300 \t Training Loss: 45.36044718424479\n",
      "Epoch 91 \t Batch 320 \t Training Loss: 45.3289214015007\n",
      "Epoch 91 \t Batch 340 \t Training Loss: 45.25083698945887\n",
      "Epoch 91 \t Batch 360 \t Training Loss: 45.35112329059177\n",
      "Epoch 91 \t Batch 380 \t Training Loss: 45.338172149658206\n",
      "Epoch 91 \t Batch 400 \t Training Loss: 45.33714297294617\n",
      "Epoch 91 \t Batch 420 \t Training Loss: 45.357955160595125\n",
      "Epoch 91 \t Batch 440 \t Training Loss: 45.37675842805342\n",
      "Epoch 91 \t Batch 460 \t Training Loss: 45.42819821730904\n",
      "Epoch 91 \t Batch 480 \t Training Loss: 45.40239838759104\n",
      "Epoch 91 \t Batch 500 \t Training Loss: 45.405347442626955\n",
      "Epoch 91 \t Batch 520 \t Training Loss: 45.41865500670213\n",
      "Epoch 91 \t Batch 540 \t Training Loss: 45.479747998272934\n",
      "Epoch 91 \t Batch 560 \t Training Loss: 45.492297410964966\n",
      "Epoch 91 \t Batch 580 \t Training Loss: 45.39929074912236\n",
      "Epoch 91 \t Batch 600 \t Training Loss: 45.426787268320716\n",
      "Epoch 91 \t Batch 620 \t Training Loss: 45.407444037160566\n",
      "Epoch 91 \t Batch 640 \t Training Loss: 45.37900960445404\n",
      "Epoch 91 \t Batch 660 \t Training Loss: 45.355712150804926\n",
      "Epoch 91 \t Batch 680 \t Training Loss: 45.39959268569946\n",
      "Epoch 91 \t Batch 700 \t Training Loss: 45.38017136710031\n",
      "Epoch 91 \t Batch 720 \t Training Loss: 45.367020728853014\n",
      "Epoch 91 \t Batch 740 \t Training Loss: 45.39520862166946\n",
      "Epoch 91 \t Batch 760 \t Training Loss: 45.41261872241372\n",
      "Epoch 91 \t Batch 780 \t Training Loss: 45.41873899606558\n",
      "Epoch 91 \t Batch 800 \t Training Loss: 45.44885561943054\n",
      "Epoch 91 \t Batch 820 \t Training Loss: 45.45839367843256\n",
      "Epoch 91 \t Batch 840 \t Training Loss: 45.454071453639436\n",
      "Epoch 91 \t Batch 860 \t Training Loss: 45.450485069807186\n",
      "Epoch 91 \t Batch 880 \t Training Loss: 45.46718565333973\n",
      "Epoch 91 \t Batch 900 \t Training Loss: 45.47532823350694\n",
      "Epoch 91 \t Batch 20 \t Validation Loss: 49.524318599700926\n",
      "Epoch 91 \t Batch 40 \t Validation Loss: 43.254812121391296\n",
      "Epoch 91 \t Batch 60 \t Validation Loss: 46.082732343673705\n",
      "Epoch 91 \t Batch 80 \t Validation Loss: 44.15444114208221\n",
      "Epoch 91 \t Batch 100 \t Validation Loss: 40.913979568481444\n",
      "Epoch 91 \t Batch 120 \t Validation Loss: 38.96490630308787\n",
      "Epoch 91 \t Batch 140 \t Validation Loss: 37.18073454584394\n",
      "Epoch 91 \t Batch 160 \t Validation Loss: 37.46477851271629\n",
      "Epoch 91 \t Batch 180 \t Validation Loss: 39.603975809945\n",
      "Epoch 91 \t Batch 200 \t Validation Loss: 40.235211672782896\n",
      "Epoch 91 \t Batch 220 \t Validation Loss: 40.72809321230108\n",
      "Epoch 91 \t Batch 240 \t Validation Loss: 40.553696711858116\n",
      "Epoch 91 \t Batch 260 \t Validation Loss: 42.235577535629275\n",
      "Epoch 91 \t Batch 280 \t Validation Loss: 42.97925661631993\n",
      "Epoch 91 \t Batch 300 \t Validation Loss: 43.40502335230509\n",
      "Epoch 91 \t Batch 320 \t Validation Loss: 43.479223895072934\n",
      "Epoch 91 \t Batch 340 \t Validation Loss: 43.15830168443568\n",
      "Epoch 91 \t Batch 360 \t Validation Loss: 42.64607565138075\n",
      "Epoch 91 \t Batch 380 \t Validation Loss: 42.650129054722036\n",
      "Epoch 91 \t Batch 400 \t Validation Loss: 42.08669681787491\n",
      "Epoch 91 \t Batch 420 \t Validation Loss: 41.97922099885486\n",
      "Epoch 91 \t Batch 440 \t Validation Loss: 41.59040222601457\n",
      "Epoch 91 \t Batch 460 \t Validation Loss: 41.658978926617166\n",
      "Epoch 91 \t Batch 480 \t Validation Loss: 41.984275511900584\n",
      "Epoch 91 \t Batch 500 \t Validation Loss: 41.58164748764038\n",
      "Epoch 91 \t Batch 520 \t Validation Loss: 41.27204316579378\n",
      "Epoch 91 \t Batch 540 \t Validation Loss: 40.83400959085535\n",
      "Epoch 91 \t Batch 560 \t Validation Loss: 40.49070649828229\n",
      "Epoch 91 \t Batch 580 \t Validation Loss: 40.09310234497333\n",
      "Epoch 91 \t Batch 600 \t Validation Loss: 40.18015499591827\n",
      "Epoch 91 Training Loss: 45.4725692149987 Validation Loss: 40.733743486466345\n",
      "Epoch 91 completed\n",
      "Epoch 92 \t Batch 20 \t Training Loss: 45.49095783233643\n",
      "Epoch 92 \t Batch 40 \t Training Loss: 45.09731616973877\n",
      "Epoch 92 \t Batch 60 \t Training Loss: 45.334662755330406\n",
      "Epoch 92 \t Batch 80 \t Training Loss: 45.17321228981018\n",
      "Epoch 92 \t Batch 100 \t Training Loss: 45.211082000732425\n",
      "Epoch 92 \t Batch 120 \t Training Loss: 45.35592784881592\n",
      "Epoch 92 \t Batch 140 \t Training Loss: 45.446999658857074\n",
      "Epoch 92 \t Batch 160 \t Training Loss: 45.42634913921356\n",
      "Epoch 92 \t Batch 180 \t Training Loss: 45.25966924031575\n",
      "Epoch 92 \t Batch 200 \t Training Loss: 45.21622983932495\n",
      "Epoch 92 \t Batch 220 \t Training Loss: 45.09592016393488\n",
      "Epoch 92 \t Batch 240 \t Training Loss: 45.15547119776408\n",
      "Epoch 92 \t Batch 260 \t Training Loss: 45.19786828848032\n",
      "Epoch 92 \t Batch 280 \t Training Loss: 45.19281316484724\n",
      "Epoch 92 \t Batch 300 \t Training Loss: 45.1149941889445\n",
      "Epoch 92 \t Batch 320 \t Training Loss: 45.141381680965424\n",
      "Epoch 92 \t Batch 340 \t Training Loss: 45.143855835409724\n",
      "Epoch 92 \t Batch 360 \t Training Loss: 45.104896015591095\n",
      "Epoch 92 \t Batch 380 \t Training Loss: 45.1058905149761\n",
      "Epoch 92 \t Batch 400 \t Training Loss: 45.10839672088623\n",
      "Epoch 92 \t Batch 420 \t Training Loss: 45.104060163952056\n",
      "Epoch 92 \t Batch 440 \t Training Loss: 45.14123778776689\n",
      "Epoch 92 \t Batch 460 \t Training Loss: 45.18217553677766\n",
      "Epoch 92 \t Batch 480 \t Training Loss: 45.17063915729523\n",
      "Epoch 92 \t Batch 500 \t Training Loss: 45.192072547912595\n",
      "Epoch 92 \t Batch 520 \t Training Loss: 45.262631445664624\n",
      "Epoch 92 \t Batch 540 \t Training Loss: 45.30618008507623\n",
      "Epoch 92 \t Batch 560 \t Training Loss: 45.31594812529428\n",
      "Epoch 92 \t Batch 580 \t Training Loss: 45.39153372665931\n",
      "Epoch 92 \t Batch 600 \t Training Loss: 45.384117902119954\n",
      "Epoch 92 \t Batch 620 \t Training Loss: 45.33325465417678\n",
      "Epoch 92 \t Batch 640 \t Training Loss: 45.33806616067886\n",
      "Epoch 92 \t Batch 660 \t Training Loss: 45.300945871526544\n",
      "Epoch 92 \t Batch 680 \t Training Loss: 45.28956286486457\n",
      "Epoch 92 \t Batch 700 \t Training Loss: 45.317513852800644\n",
      "Epoch 92 \t Batch 720 \t Training Loss: 45.321068143844606\n",
      "Epoch 92 \t Batch 740 \t Training Loss: 45.344837616585394\n",
      "Epoch 92 \t Batch 760 \t Training Loss: 45.34383610675209\n",
      "Epoch 92 \t Batch 780 \t Training Loss: 45.35595899728628\n",
      "Epoch 92 \t Batch 800 \t Training Loss: 45.38533221244812\n",
      "Epoch 92 \t Batch 820 \t Training Loss: 45.38035435560273\n",
      "Epoch 92 \t Batch 840 \t Training Loss: 45.388740789322625\n",
      "Epoch 92 \t Batch 860 \t Training Loss: 45.42134391873382\n",
      "Epoch 92 \t Batch 880 \t Training Loss: 45.43346990238536\n",
      "Epoch 92 \t Batch 900 \t Training Loss: 45.43846210479737\n",
      "Epoch 92 \t Batch 20 \t Validation Loss: 44.754346656799314\n",
      "Epoch 92 \t Batch 40 \t Validation Loss: 41.251250696182254\n",
      "Epoch 92 \t Batch 60 \t Validation Loss: 43.22863105138143\n",
      "Epoch 92 \t Batch 80 \t Validation Loss: 41.8055717587471\n",
      "Epoch 92 \t Batch 100 \t Validation Loss: 39.32546647071838\n",
      "Epoch 92 \t Batch 120 \t Validation Loss: 37.85878260135651\n",
      "Epoch 92 \t Batch 140 \t Validation Loss: 36.40038721220834\n",
      "Epoch 92 \t Batch 160 \t Validation Loss: 36.98733883500099\n",
      "Epoch 92 \t Batch 180 \t Validation Loss: 39.36407504611545\n",
      "Epoch 92 \t Batch 200 \t Validation Loss: 40.14668901443481\n",
      "Epoch 92 \t Batch 220 \t Validation Loss: 40.715166412700306\n",
      "Epoch 92 \t Batch 240 \t Validation Loss: 40.58673986196518\n",
      "Epoch 92 \t Batch 260 \t Validation Loss: 42.40047634931711\n",
      "Epoch 92 \t Batch 280 \t Validation Loss: 43.22371920517513\n",
      "Epoch 92 \t Batch 300 \t Validation Loss: 43.72058678627014\n",
      "Epoch 92 \t Batch 320 \t Validation Loss: 43.83415552973747\n",
      "Epoch 92 \t Batch 340 \t Validation Loss: 43.5026065938613\n",
      "Epoch 92 \t Batch 360 \t Validation Loss: 43.093762175242105\n",
      "Epoch 92 \t Batch 380 \t Validation Loss: 43.196118023521024\n",
      "Epoch 92 \t Batch 400 \t Validation Loss: 42.678069128990174\n",
      "Epoch 92 \t Batch 420 \t Validation Loss: 42.547819101242794\n",
      "Epoch 92 \t Batch 440 \t Validation Loss: 42.212321719256316\n",
      "Epoch 92 \t Batch 460 \t Validation Loss: 42.36175112931625\n",
      "Epoch 92 \t Batch 480 \t Validation Loss: 42.68651919364929\n",
      "Epoch 92 \t Batch 500 \t Validation Loss: 42.27185775756836\n",
      "Epoch 92 \t Batch 520 \t Validation Loss: 42.10698076394888\n",
      "Epoch 92 \t Batch 540 \t Validation Loss: 41.753359667460124\n",
      "Epoch 92 \t Batch 560 \t Validation Loss: 41.516540438788276\n",
      "Epoch 92 \t Batch 580 \t Validation Loss: 41.318446616468755\n",
      "Epoch 92 \t Batch 600 \t Validation Loss: 41.43007863044739\n",
      "Epoch 92 Training Loss: 45.44392864264735 Validation Loss: 42.005178240986616\n",
      "Epoch 92 completed\n",
      "Epoch 93 \t Batch 20 \t Training Loss: 46.1436092376709\n",
      "Epoch 93 \t Batch 40 \t Training Loss: 45.568807220458986\n",
      "Epoch 93 \t Batch 60 \t Training Loss: 45.491628328959145\n",
      "Epoch 93 \t Batch 80 \t Training Loss: 45.49539165496826\n",
      "Epoch 93 \t Batch 100 \t Training Loss: 45.32691318511963\n",
      "Epoch 93 \t Batch 120 \t Training Loss: 45.48907998402913\n",
      "Epoch 93 \t Batch 140 \t Training Loss: 45.433328001839776\n",
      "Epoch 93 \t Batch 160 \t Training Loss: 45.319446778297426\n",
      "Epoch 93 \t Batch 180 \t Training Loss: 45.514358584086104\n",
      "Epoch 93 \t Batch 200 \t Training Loss: 45.523793144226076\n",
      "Epoch 93 \t Batch 220 \t Training Loss: 45.49334203546697\n",
      "Epoch 93 \t Batch 240 \t Training Loss: 45.45103138287862\n",
      "Epoch 93 \t Batch 260 \t Training Loss: 45.3943206346952\n",
      "Epoch 93 \t Batch 280 \t Training Loss: 45.23859404155186\n",
      "Epoch 93 \t Batch 300 \t Training Loss: 45.20346791585286\n",
      "Epoch 93 \t Batch 320 \t Training Loss: 45.18210834264755\n",
      "Epoch 93 \t Batch 340 \t Training Loss: 45.21738966773538\n",
      "Epoch 93 \t Batch 360 \t Training Loss: 45.18806431028578\n",
      "Epoch 93 \t Batch 380 \t Training Loss: 45.22832167776007\n",
      "Epoch 93 \t Batch 400 \t Training Loss: 45.17278841018677\n",
      "Epoch 93 \t Batch 420 \t Training Loss: 45.110955338251024\n",
      "Epoch 93 \t Batch 440 \t Training Loss: 45.150641337308016\n",
      "Epoch 93 \t Batch 460 \t Training Loss: 45.18607749938965\n",
      "Epoch 93 \t Batch 480 \t Training Loss: 45.220398902893066\n",
      "Epoch 93 \t Batch 500 \t Training Loss: 45.21488330078125\n",
      "Epoch 93 \t Batch 520 \t Training Loss: 45.23105813539945\n",
      "Epoch 93 \t Batch 540 \t Training Loss: 45.23924287160238\n",
      "Epoch 93 \t Batch 560 \t Training Loss: 45.21272862298148\n",
      "Epoch 93 \t Batch 580 \t Training Loss: 45.28938740039694\n",
      "Epoch 93 \t Batch 600 \t Training Loss: 45.272394466400144\n",
      "Epoch 93 \t Batch 620 \t Training Loss: 45.267738520714545\n",
      "Epoch 93 \t Batch 640 \t Training Loss: 45.29112375378609\n",
      "Epoch 93 \t Batch 660 \t Training Loss: 45.29753658988259\n",
      "Epoch 93 \t Batch 680 \t Training Loss: 45.353817126330206\n",
      "Epoch 93 \t Batch 700 \t Training Loss: 45.4039847946167\n",
      "Epoch 93 \t Batch 720 \t Training Loss: 45.42464836438497\n",
      "Epoch 93 \t Batch 740 \t Training Loss: 45.45477196976945\n",
      "Epoch 93 \t Batch 760 \t Training Loss: 45.434171671616404\n",
      "Epoch 93 \t Batch 780 \t Training Loss: 45.420088489239035\n",
      "Epoch 93 \t Batch 800 \t Training Loss: 45.45659202098847\n",
      "Epoch 93 \t Batch 820 \t Training Loss: 45.45458872260117\n",
      "Epoch 93 \t Batch 840 \t Training Loss: 45.43352138428461\n",
      "Epoch 93 \t Batch 860 \t Training Loss: 45.46522144051485\n",
      "Epoch 93 \t Batch 880 \t Training Loss: 45.44439399459145\n",
      "Epoch 93 \t Batch 900 \t Training Loss: 45.434148457845055\n",
      "Epoch 93 \t Batch 20 \t Validation Loss: 47.12413158416748\n",
      "Epoch 93 \t Batch 40 \t Validation Loss: 43.7793931722641\n",
      "Epoch 93 \t Batch 60 \t Validation Loss: 45.45870823860169\n",
      "Epoch 93 \t Batch 80 \t Validation Loss: 44.289527118206024\n",
      "Epoch 93 \t Batch 100 \t Validation Loss: 41.33849669456482\n",
      "Epoch 93 \t Batch 120 \t Validation Loss: 39.42744277318319\n",
      "Epoch 93 \t Batch 140 \t Validation Loss: 37.74369040897914\n",
      "Epoch 93 \t Batch 160 \t Validation Loss: 38.20826303958893\n",
      "Epoch 93 \t Batch 180 \t Validation Loss: 40.52987706926134\n",
      "Epoch 93 \t Batch 200 \t Validation Loss: 41.26915322303772\n",
      "Epoch 93 \t Batch 220 \t Validation Loss: 41.81217540394176\n",
      "Epoch 93 \t Batch 240 \t Validation Loss: 41.63882619539897\n",
      "Epoch 93 \t Batch 260 \t Validation Loss: 43.43240958360525\n",
      "Epoch 93 \t Batch 280 \t Validation Loss: 44.24900084563664\n",
      "Epoch 93 \t Batch 300 \t Validation Loss: 44.71804632504781\n",
      "Epoch 93 \t Batch 320 \t Validation Loss: 44.78345985114574\n",
      "Epoch 93 \t Batch 340 \t Validation Loss: 44.41446186794954\n",
      "Epoch 93 \t Batch 360 \t Validation Loss: 43.96236184702979\n",
      "Epoch 93 \t Batch 380 \t Validation Loss: 44.05663757575186\n",
      "Epoch 93 \t Batch 400 \t Validation Loss: 43.53201400995255\n",
      "Epoch 93 \t Batch 420 \t Validation Loss: 43.375377407528106\n",
      "Epoch 93 \t Batch 440 \t Validation Loss: 43.05914523168044\n",
      "Epoch 93 \t Batch 460 \t Validation Loss: 43.18505191181017\n",
      "Epoch 93 \t Batch 480 \t Validation Loss: 43.49584258993467\n",
      "Epoch 93 \t Batch 500 \t Validation Loss: 43.06202929115295\n",
      "Epoch 93 \t Batch 520 \t Validation Loss: 42.92233907076029\n",
      "Epoch 93 \t Batch 540 \t Validation Loss: 42.5503739727868\n",
      "Epoch 93 \t Batch 560 \t Validation Loss: 42.300663779463086\n",
      "Epoch 93 \t Batch 580 \t Validation Loss: 42.06453146934509\n",
      "Epoch 93 \t Batch 600 \t Validation Loss: 42.16777869701386\n",
      "Epoch 93 Training Loss: 45.41620458771384 Validation Loss: 42.71144309136775\n",
      "Epoch 93 completed\n",
      "Epoch 94 \t Batch 20 \t Training Loss: 44.90663719177246\n",
      "Epoch 94 \t Batch 40 \t Training Loss: 45.83015089035034\n",
      "Epoch 94 \t Batch 60 \t Training Loss: 45.67531255086263\n",
      "Epoch 94 \t Batch 80 \t Training Loss: 45.27019166946411\n",
      "Epoch 94 \t Batch 100 \t Training Loss: 45.17640785217285\n",
      "Epoch 94 \t Batch 120 \t Training Loss: 45.25193004608154\n",
      "Epoch 94 \t Batch 140 \t Training Loss: 45.23690956660679\n",
      "Epoch 94 \t Batch 160 \t Training Loss: 45.41825785636902\n",
      "Epoch 94 \t Batch 180 \t Training Loss: 45.49500109354655\n",
      "Epoch 94 \t Batch 200 \t Training Loss: 45.384889698028566\n",
      "Epoch 94 \t Batch 220 \t Training Loss: 45.405257537148216\n",
      "Epoch 94 \t Batch 240 \t Training Loss: 45.351965761184694\n",
      "Epoch 94 \t Batch 260 \t Training Loss: 45.28646614368145\n",
      "Epoch 94 \t Batch 280 \t Training Loss: 45.241297817230226\n",
      "Epoch 94 \t Batch 300 \t Training Loss: 45.27644945780436\n",
      "Epoch 94 \t Batch 320 \t Training Loss: 45.30204442739487\n",
      "Epoch 94 \t Batch 340 \t Training Loss: 45.34509505103616\n",
      "Epoch 94 \t Batch 360 \t Training Loss: 45.33547169367473\n",
      "Epoch 94 \t Batch 380 \t Training Loss: 45.37732188576146\n",
      "Epoch 94 \t Batch 400 \t Training Loss: 45.30034704208374\n",
      "Epoch 94 \t Batch 420 \t Training Loss: 45.30002846490769\n",
      "Epoch 94 \t Batch 440 \t Training Loss: 45.301986347545274\n",
      "Epoch 94 \t Batch 460 \t Training Loss: 45.30429932967476\n",
      "Epoch 94 \t Batch 480 \t Training Loss: 45.33650454680125\n",
      "Epoch 94 \t Batch 500 \t Training Loss: 45.347066772460934\n",
      "Epoch 94 \t Batch 520 \t Training Loss: 45.35265320264376\n",
      "Epoch 94 \t Batch 540 \t Training Loss: 45.38032541628237\n",
      "Epoch 94 \t Batch 560 \t Training Loss: 45.41167091642107\n",
      "Epoch 94 \t Batch 580 \t Training Loss: 45.385551630217456\n",
      "Epoch 94 \t Batch 600 \t Training Loss: 45.37986649195353\n",
      "Epoch 94 \t Batch 620 \t Training Loss: 45.39554383062547\n",
      "Epoch 94 \t Batch 640 \t Training Loss: 45.34178311824799\n",
      "Epoch 94 \t Batch 660 \t Training Loss: 45.35062639063055\n",
      "Epoch 94 \t Batch 680 \t Training Loss: 45.37253890318029\n",
      "Epoch 94 \t Batch 700 \t Training Loss: 45.359928310939246\n",
      "Epoch 94 \t Batch 720 \t Training Loss: 45.37349010043674\n",
      "Epoch 94 \t Batch 740 \t Training Loss: 45.38896929251181\n",
      "Epoch 94 \t Batch 760 \t Training Loss: 45.39183535826834\n",
      "Epoch 94 \t Batch 780 \t Training Loss: 45.354314828530335\n",
      "Epoch 94 \t Batch 800 \t Training Loss: 45.340505294799804\n",
      "Epoch 94 \t Batch 820 \t Training Loss: 45.34005380490931\n",
      "Epoch 94 \t Batch 840 \t Training Loss: 45.331859988257996\n",
      "Epoch 94 \t Batch 860 \t Training Loss: 45.362442327100176\n",
      "Epoch 94 \t Batch 880 \t Training Loss: 45.36433269327337\n",
      "Epoch 94 \t Batch 900 \t Training Loss: 45.36314789242215\n",
      "Epoch 94 \t Batch 20 \t Validation Loss: 54.98318977355957\n",
      "Epoch 94 \t Batch 40 \t Validation Loss: 47.915882563591005\n",
      "Epoch 94 \t Batch 60 \t Validation Loss: 51.3030846118927\n",
      "Epoch 94 \t Batch 80 \t Validation Loss: 48.36353167295456\n",
      "Epoch 94 \t Batch 100 \t Validation Loss: 44.476946916580204\n",
      "Epoch 94 \t Batch 120 \t Validation Loss: 42.04270301659902\n",
      "Epoch 94 \t Batch 140 \t Validation Loss: 39.92756248201643\n",
      "Epoch 94 \t Batch 160 \t Validation Loss: 40.0985018491745\n",
      "Epoch 94 \t Batch 180 \t Validation Loss: 42.22624608675639\n",
      "Epoch 94 \t Batch 200 \t Validation Loss: 42.812426953315736\n",
      "Epoch 94 \t Batch 220 \t Validation Loss: 43.217984437942505\n",
      "Epoch 94 \t Batch 240 \t Validation Loss: 42.89921644528707\n",
      "Epoch 94 \t Batch 260 \t Validation Loss: 44.63751063713661\n",
      "Epoch 94 \t Batch 280 \t Validation Loss: 45.396551745278494\n",
      "Epoch 94 \t Batch 300 \t Validation Loss: 45.7671236038208\n",
      "Epoch 94 \t Batch 320 \t Validation Loss: 45.739400586485864\n",
      "Epoch 94 \t Batch 340 \t Validation Loss: 45.29343029751497\n",
      "Epoch 94 \t Batch 360 \t Validation Loss: 44.76948240333133\n",
      "Epoch 94 \t Batch 380 \t Validation Loss: 44.845064672670865\n",
      "Epoch 94 \t Batch 400 \t Validation Loss: 44.296223752498626\n",
      "Epoch 94 \t Batch 420 \t Validation Loss: 44.128868813741775\n",
      "Epoch 94 \t Batch 440 \t Validation Loss: 43.79581357565793\n",
      "Epoch 94 \t Batch 460 \t Validation Loss: 43.92973606897437\n",
      "Epoch 94 \t Batch 480 \t Validation Loss: 44.20331923762957\n",
      "Epoch 94 \t Batch 500 \t Validation Loss: 43.754129446029665\n",
      "Epoch 94 \t Batch 520 \t Validation Loss: 43.633550950197076\n",
      "Epoch 94 \t Batch 540 \t Validation Loss: 43.177093818452626\n",
      "Epoch 94 \t Batch 560 \t Validation Loss: 42.81369057212557\n",
      "Epoch 94 \t Batch 580 \t Validation Loss: 42.49031300215886\n",
      "Epoch 94 \t Batch 600 \t Validation Loss: 42.52704461892446\n",
      "Epoch 94 Training Loss: 45.37363286990766 Validation Loss: 43.07912323536811\n",
      "Epoch 94 completed\n",
      "Epoch 95 \t Batch 20 \t Training Loss: 43.72440528869629\n",
      "Epoch 95 \t Batch 40 \t Training Loss: 44.37661027908325\n",
      "Epoch 95 \t Batch 60 \t Training Loss: 44.81854070027669\n",
      "Epoch 95 \t Batch 80 \t Training Loss: 44.913527965545654\n",
      "Epoch 95 \t Batch 100 \t Training Loss: 44.89759422302246\n",
      "Epoch 95 \t Batch 120 \t Training Loss: 44.88394727706909\n",
      "Epoch 95 \t Batch 140 \t Training Loss: 44.89516846793038\n",
      "Epoch 95 \t Batch 160 \t Training Loss: 45.15214152336121\n",
      "Epoch 95 \t Batch 180 \t Training Loss: 45.074571969774034\n",
      "Epoch 95 \t Batch 200 \t Training Loss: 45.10164196014404\n",
      "Epoch 95 \t Batch 220 \t Training Loss: 45.06389591910622\n",
      "Epoch 95 \t Batch 240 \t Training Loss: 44.97930822372437\n",
      "Epoch 95 \t Batch 260 \t Training Loss: 45.02572419093205\n",
      "Epoch 95 \t Batch 280 \t Training Loss: 45.144254847935265\n",
      "Epoch 95 \t Batch 300 \t Training Loss: 45.101925137837725\n",
      "Epoch 95 \t Batch 320 \t Training Loss: 45.14844523668289\n",
      "Epoch 95 \t Batch 340 \t Training Loss: 45.19571446811452\n",
      "Epoch 95 \t Batch 360 \t Training Loss: 45.2204899681939\n",
      "Epoch 95 \t Batch 380 \t Training Loss: 45.2173162962261\n",
      "Epoch 95 \t Batch 400 \t Training Loss: 45.24891705513001\n",
      "Epoch 95 \t Batch 420 \t Training Loss: 45.20991513388498\n",
      "Epoch 95 \t Batch 440 \t Training Loss: 45.19575729370117\n",
      "Epoch 95 \t Batch 460 \t Training Loss: 45.262388146441914\n",
      "Epoch 95 \t Batch 480 \t Training Loss: 45.28835155169169\n",
      "Epoch 95 \t Batch 500 \t Training Loss: 45.3206401977539\n",
      "Epoch 95 \t Batch 520 \t Training Loss: 45.34886229588435\n",
      "Epoch 95 \t Batch 540 \t Training Loss: 45.342378877710416\n",
      "Epoch 95 \t Batch 560 \t Training Loss: 45.33953076090131\n",
      "Epoch 95 \t Batch 580 \t Training Loss: 45.329656298407194\n",
      "Epoch 95 \t Batch 600 \t Training Loss: 45.320214932759605\n",
      "Epoch 95 \t Batch 620 \t Training Loss: 45.36831477380568\n",
      "Epoch 95 \t Batch 640 \t Training Loss: 45.32892534136772\n",
      "Epoch 95 \t Batch 660 \t Training Loss: 45.31100423986261\n",
      "Epoch 95 \t Batch 680 \t Training Loss: 45.334340948217054\n",
      "Epoch 95 \t Batch 700 \t Training Loss: 45.36202583857945\n",
      "Epoch 95 \t Batch 720 \t Training Loss: 45.37941660881042\n",
      "Epoch 95 \t Batch 740 \t Training Loss: 45.40452304530788\n",
      "Epoch 95 \t Batch 760 \t Training Loss: 45.399542667991234\n",
      "Epoch 95 \t Batch 780 \t Training Loss: 45.388567885374414\n",
      "Epoch 95 \t Batch 800 \t Training Loss: 45.36626482486725\n",
      "Epoch 95 \t Batch 820 \t Training Loss: 45.35427498235935\n",
      "Epoch 95 \t Batch 840 \t Training Loss: 45.37385434196109\n",
      "Epoch 95 \t Batch 860 \t Training Loss: 45.36290406071863\n",
      "Epoch 95 \t Batch 880 \t Training Loss: 45.37839429161765\n",
      "Epoch 95 \t Batch 900 \t Training Loss: 45.361781938340926\n",
      "Epoch 95 \t Batch 20 \t Validation Loss: 49.839761066436765\n",
      "Epoch 95 \t Batch 40 \t Validation Loss: 45.02586653232574\n",
      "Epoch 95 \t Batch 60 \t Validation Loss: 47.45700402259827\n",
      "Epoch 95 \t Batch 80 \t Validation Loss: 46.07194749116898\n",
      "Epoch 95 \t Batch 100 \t Validation Loss: 42.88220213890076\n",
      "Epoch 95 \t Batch 120 \t Validation Loss: 40.771763865153\n",
      "Epoch 95 \t Batch 140 \t Validation Loss: 38.85426357814244\n",
      "Epoch 95 \t Batch 160 \t Validation Loss: 38.861696255207065\n",
      "Epoch 95 \t Batch 180 \t Validation Loss: 40.62302201588948\n",
      "Epoch 95 \t Batch 200 \t Validation Loss: 40.99347394943237\n",
      "Epoch 95 \t Batch 220 \t Validation Loss: 41.23859614458951\n",
      "Epoch 95 \t Batch 240 \t Validation Loss: 40.88529394865036\n",
      "Epoch 95 \t Batch 260 \t Validation Loss: 42.43043790597182\n",
      "Epoch 95 \t Batch 280 \t Validation Loss: 43.10323750972748\n",
      "Epoch 95 \t Batch 300 \t Validation Loss: 43.34180414517721\n",
      "Epoch 95 \t Batch 320 \t Validation Loss: 43.30977277159691\n",
      "Epoch 95 \t Batch 340 \t Validation Loss: 42.958746444477754\n",
      "Epoch 95 \t Batch 360 \t Validation Loss: 42.45782959726122\n",
      "Epoch 95 \t Batch 380 \t Validation Loss: 42.427972351877315\n",
      "Epoch 95 \t Batch 400 \t Validation Loss: 41.88827771663666\n",
      "Epoch 95 \t Batch 420 \t Validation Loss: 41.75160485222226\n",
      "Epoch 95 \t Batch 440 \t Validation Loss: 41.376389278065076\n",
      "Epoch 95 \t Batch 460 \t Validation Loss: 41.44015981010769\n",
      "Epoch 95 \t Batch 480 \t Validation Loss: 41.76206029256185\n",
      "Epoch 95 \t Batch 500 \t Validation Loss: 41.33630610275269\n",
      "Epoch 95 \t Batch 520 \t Validation Loss: 41.07026179020222\n",
      "Epoch 95 \t Batch 540 \t Validation Loss: 40.77059922748142\n",
      "Epoch 95 \t Batch 560 \t Validation Loss: 40.57700123786926\n",
      "Epoch 95 \t Batch 580 \t Validation Loss: 40.40069800409778\n",
      "Epoch 95 \t Batch 600 \t Validation Loss: 40.55458665211995\n",
      "Epoch 95 Training Loss: 45.36778607883266 Validation Loss: 41.14440545788059\n",
      "Epoch 95 completed\n",
      "Epoch 96 \t Batch 20 \t Training Loss: 45.86485195159912\n",
      "Epoch 96 \t Batch 40 \t Training Loss: 45.507336902618405\n",
      "Epoch 96 \t Batch 60 \t Training Loss: 45.91075763702393\n",
      "Epoch 96 \t Batch 80 \t Training Loss: 45.74278860092163\n",
      "Epoch 96 \t Batch 100 \t Training Loss: 45.476794204711915\n",
      "Epoch 96 \t Batch 120 \t Training Loss: 45.33760134379069\n",
      "Epoch 96 \t Batch 140 \t Training Loss: 45.39747428894043\n",
      "Epoch 96 \t Batch 160 \t Training Loss: 45.415605306625366\n",
      "Epoch 96 \t Batch 180 \t Training Loss: 45.326126437717015\n",
      "Epoch 96 \t Batch 200 \t Training Loss: 45.23110898971558\n",
      "Epoch 96 \t Batch 220 \t Training Loss: 45.27666808041659\n",
      "Epoch 96 \t Batch 240 \t Training Loss: 45.256858285268144\n",
      "Epoch 96 \t Batch 260 \t Training Loss: 45.35415421999418\n",
      "Epoch 96 \t Batch 280 \t Training Loss: 45.48032131195068\n",
      "Epoch 96 \t Batch 300 \t Training Loss: 45.44338928222656\n",
      "Epoch 96 \t Batch 320 \t Training Loss: 45.49225798845291\n",
      "Epoch 96 \t Batch 340 \t Training Loss: 45.49087340411018\n",
      "Epoch 96 \t Batch 360 \t Training Loss: 45.558106093936495\n",
      "Epoch 96 \t Batch 380 \t Training Loss: 45.47663293135793\n",
      "Epoch 96 \t Batch 400 \t Training Loss: 45.512658710479734\n",
      "Epoch 96 \t Batch 420 \t Training Loss: 45.473932738531204\n",
      "Epoch 96 \t Batch 440 \t Training Loss: 45.54427793676203\n",
      "Epoch 96 \t Batch 460 \t Training Loss: 45.52648618946905\n",
      "Epoch 96 \t Batch 480 \t Training Loss: 45.492015568415326\n",
      "Epoch 96 \t Batch 500 \t Training Loss: 45.42237899017334\n",
      "Epoch 96 \t Batch 520 \t Training Loss: 45.39533535150381\n",
      "Epoch 96 \t Batch 540 \t Training Loss: 45.43106463396991\n",
      "Epoch 96 \t Batch 560 \t Training Loss: 45.43870699746268\n",
      "Epoch 96 \t Batch 580 \t Training Loss: 45.368460207972035\n",
      "Epoch 96 \t Batch 600 \t Training Loss: 45.3557986386617\n",
      "Epoch 96 \t Batch 620 \t Training Loss: 45.35826888545867\n",
      "Epoch 96 \t Batch 640 \t Training Loss: 45.336229622364044\n",
      "Epoch 96 \t Batch 660 \t Training Loss: 45.37018123395515\n",
      "Epoch 96 \t Batch 680 \t Training Loss: 45.36598311031566\n",
      "Epoch 96 \t Batch 700 \t Training Loss: 45.376263149806434\n",
      "Epoch 96 \t Batch 720 \t Training Loss: 45.43614772690667\n",
      "Epoch 96 \t Batch 740 \t Training Loss: 45.41193128018766\n",
      "Epoch 96 \t Batch 760 \t Training Loss: 45.44866720500745\n",
      "Epoch 96 \t Batch 780 \t Training Loss: 45.40397403423603\n",
      "Epoch 96 \t Batch 800 \t Training Loss: 45.35668256282806\n",
      "Epoch 96 \t Batch 820 \t Training Loss: 45.33979222367449\n",
      "Epoch 96 \t Batch 840 \t Training Loss: 45.30709505081177\n",
      "Epoch 96 \t Batch 860 \t Training Loss: 45.289609066275666\n",
      "Epoch 96 \t Batch 880 \t Training Loss: 45.30368998700922\n",
      "Epoch 96 \t Batch 900 \t Training Loss: 45.287592845492895\n",
      "Epoch 96 \t Batch 20 \t Validation Loss: 50.844215965271\n",
      "Epoch 96 \t Batch 40 \t Validation Loss: 44.325617003440854\n",
      "Epoch 96 \t Batch 60 \t Validation Loss: 47.36566170056661\n",
      "Epoch 96 \t Batch 80 \t Validation Loss: 45.32767168283463\n",
      "Epoch 96 \t Batch 100 \t Validation Loss: 42.04455279350281\n",
      "Epoch 96 \t Batch 120 \t Validation Loss: 40.08125461737315\n",
      "Epoch 96 \t Batch 140 \t Validation Loss: 38.28781987598964\n",
      "Epoch 96 \t Batch 160 \t Validation Loss: 38.650199151039125\n",
      "Epoch 96 \t Batch 180 \t Validation Loss: 40.78539073732164\n",
      "Epoch 96 \t Batch 200 \t Validation Loss: 41.384617519378665\n",
      "Epoch 96 \t Batch 220 \t Validation Loss: 41.81565416509455\n",
      "Epoch 96 \t Batch 240 \t Validation Loss: 41.5445122440656\n",
      "Epoch 96 \t Batch 260 \t Validation Loss: 43.228955386235164\n",
      "Epoch 96 \t Batch 280 \t Validation Loss: 43.9902218409947\n",
      "Epoch 96 \t Batch 300 \t Validation Loss: 44.40771059672038\n",
      "Epoch 96 \t Batch 320 \t Validation Loss: 44.44452478289604\n",
      "Epoch 96 \t Batch 340 \t Validation Loss: 44.082496356964114\n",
      "Epoch 96 \t Batch 360 \t Validation Loss: 43.658577643500436\n",
      "Epoch 96 \t Batch 380 \t Validation Loss: 43.74346797842728\n",
      "Epoch 96 \t Batch 400 \t Validation Loss: 43.27596691131592\n",
      "Epoch 96 \t Batch 420 \t Validation Loss: 43.158978684743246\n",
      "Epoch 96 \t Batch 440 \t Validation Loss: 42.92061152458191\n",
      "Epoch 96 \t Batch 460 \t Validation Loss: 43.09082622942717\n",
      "Epoch 96 \t Batch 480 \t Validation Loss: 43.40756562948227\n",
      "Epoch 96 \t Batch 500 \t Validation Loss: 42.98518255233765\n",
      "Epoch 96 \t Batch 520 \t Validation Loss: 42.85521366779621\n",
      "Epoch 96 \t Batch 540 \t Validation Loss: 42.4799990300779\n",
      "Epoch 96 \t Batch 560 \t Validation Loss: 42.1938909632819\n",
      "Epoch 96 \t Batch 580 \t Validation Loss: 41.91032526082006\n",
      "Epoch 96 \t Batch 600 \t Validation Loss: 41.99481902758281\n",
      "Epoch 96 Training Loss: 45.315889526999506 Validation Loss: 42.54633416757955\n",
      "Epoch 96 completed\n",
      "Epoch 97 \t Batch 20 \t Training Loss: 46.05878677368164\n",
      "Epoch 97 \t Batch 40 \t Training Loss: 45.63741064071655\n",
      "Epoch 97 \t Batch 60 \t Training Loss: 45.42528476715088\n",
      "Epoch 97 \t Batch 80 \t Training Loss: 45.3782820224762\n",
      "Epoch 97 \t Batch 100 \t Training Loss: 45.29320846557617\n",
      "Epoch 97 \t Batch 120 \t Training Loss: 45.638885434468584\n",
      "Epoch 97 \t Batch 140 \t Training Loss: 45.54178338732038\n",
      "Epoch 97 \t Batch 160 \t Training Loss: 45.54659295082092\n",
      "Epoch 97 \t Batch 180 \t Training Loss: 45.45947704315186\n",
      "Epoch 97 \t Batch 200 \t Training Loss: 45.26858568191528\n",
      "Epoch 97 \t Batch 220 \t Training Loss: 45.245814202048564\n",
      "Epoch 97 \t Batch 240 \t Training Loss: 45.1543755531311\n",
      "Epoch 97 \t Batch 260 \t Training Loss: 45.13814370081975\n",
      "Epoch 97 \t Batch 280 \t Training Loss: 45.156804711478095\n",
      "Epoch 97 \t Batch 300 \t Training Loss: 45.196026255289716\n",
      "Epoch 97 \t Batch 320 \t Training Loss: 45.17534608840943\n",
      "Epoch 97 \t Batch 340 \t Training Loss: 45.20719134386848\n",
      "Epoch 97 \t Batch 360 \t Training Loss: 45.20424281226264\n",
      "Epoch 97 \t Batch 380 \t Training Loss: 45.20319451783833\n",
      "Epoch 97 \t Batch 400 \t Training Loss: 45.23084422111511\n",
      "Epoch 97 \t Batch 420 \t Training Loss: 45.16553327469599\n",
      "Epoch 97 \t Batch 440 \t Training Loss: 45.271151898124\n",
      "Epoch 97 \t Batch 460 \t Training Loss: 45.30546607971191\n",
      "Epoch 97 \t Batch 480 \t Training Loss: 45.359581637382504\n",
      "Epoch 97 \t Batch 500 \t Training Loss: 45.318956184387204\n",
      "Epoch 97 \t Batch 520 \t Training Loss: 45.32046276239248\n",
      "Epoch 97 \t Batch 540 \t Training Loss: 45.23361114925808\n",
      "Epoch 97 \t Batch 560 \t Training Loss: 45.24856331007821\n",
      "Epoch 97 \t Batch 580 \t Training Loss: 45.213915778850684\n",
      "Epoch 97 \t Batch 600 \t Training Loss: 45.214338099161786\n",
      "Epoch 97 \t Batch 620 \t Training Loss: 45.24298582999937\n",
      "Epoch 97 \t Batch 640 \t Training Loss: 45.268632131814954\n",
      "Epoch 97 \t Batch 660 \t Training Loss: 45.24389001672918\n",
      "Epoch 97 \t Batch 680 \t Training Loss: 45.25372323989868\n",
      "Epoch 97 \t Batch 700 \t Training Loss: 45.241985735212054\n",
      "Epoch 97 \t Batch 720 \t Training Loss: 45.29770786497328\n",
      "Epoch 97 \t Batch 740 \t Training Loss: 45.27479298050339\n",
      "Epoch 97 \t Batch 760 \t Training Loss: 45.29854967217696\n",
      "Epoch 97 \t Batch 780 \t Training Loss: 45.33200392111754\n",
      "Epoch 97 \t Batch 800 \t Training Loss: 45.347066655159\n",
      "Epoch 97 \t Batch 820 \t Training Loss: 45.336667219022424\n",
      "Epoch 97 \t Batch 840 \t Training Loss: 45.36284531638736\n",
      "Epoch 97 \t Batch 860 \t Training Loss: 45.325883191130885\n",
      "Epoch 97 \t Batch 880 \t Training Loss: 45.33764064095237\n",
      "Epoch 97 \t Batch 900 \t Training Loss: 45.336764920552575\n",
      "Epoch 97 \t Batch 20 \t Validation Loss: 56.98996086120606\n",
      "Epoch 97 \t Batch 40 \t Validation Loss: 49.28656506538391\n",
      "Epoch 97 \t Batch 60 \t Validation Loss: 52.50175406138102\n",
      "Epoch 97 \t Batch 80 \t Validation Loss: 50.41932022571564\n",
      "Epoch 97 \t Batch 100 \t Validation Loss: 46.641893787384035\n",
      "Epoch 97 \t Batch 120 \t Validation Loss: 44.061327123641966\n",
      "Epoch 97 \t Batch 140 \t Validation Loss: 41.72615237917219\n",
      "Epoch 97 \t Batch 160 \t Validation Loss: 41.56754797697067\n",
      "Epoch 97 \t Batch 180 \t Validation Loss: 43.27812271647983\n",
      "Epoch 97 \t Batch 200 \t Validation Loss: 43.64550614833832\n",
      "Epoch 97 \t Batch 220 \t Validation Loss: 43.794560089978305\n",
      "Epoch 97 \t Batch 240 \t Validation Loss: 43.318637688954674\n",
      "Epoch 97 \t Batch 260 \t Validation Loss: 44.8825009896205\n",
      "Epoch 97 \t Batch 280 \t Validation Loss: 45.50482685225351\n",
      "Epoch 97 \t Batch 300 \t Validation Loss: 45.78102910995484\n",
      "Epoch 97 \t Batch 320 \t Validation Loss: 45.71381880640983\n",
      "Epoch 97 \t Batch 340 \t Validation Loss: 45.27236122243545\n",
      "Epoch 97 \t Batch 360 \t Validation Loss: 44.7207750611835\n",
      "Epoch 97 \t Batch 380 \t Validation Loss: 44.661785037894\n",
      "Epoch 97 \t Batch 400 \t Validation Loss: 43.9848167681694\n",
      "Epoch 97 \t Batch 420 \t Validation Loss: 43.784110602878386\n",
      "Epoch 97 \t Batch 440 \t Validation Loss: 43.3043622862209\n",
      "Epoch 97 \t Batch 460 \t Validation Loss: 43.34744058484616\n",
      "Epoch 97 \t Batch 480 \t Validation Loss: 43.642998896042506\n",
      "Epoch 97 \t Batch 500 \t Validation Loss: 43.19981654167175\n",
      "Epoch 97 \t Batch 520 \t Validation Loss: 42.88108251094818\n",
      "Epoch 97 \t Batch 540 \t Validation Loss: 42.48994596092789\n",
      "Epoch 97 \t Batch 560 \t Validation Loss: 42.20929946388517\n",
      "Epoch 97 \t Batch 580 \t Validation Loss: 41.98523183855517\n",
      "Epoch 97 \t Batch 600 \t Validation Loss: 42.08851318518321\n",
      "Epoch 97 Training Loss: 45.32637454067218 Validation Loss: 42.66351961005818\n",
      "Epoch 97 completed\n",
      "Epoch 98 \t Batch 20 \t Training Loss: 43.83404178619385\n",
      "Epoch 98 \t Batch 40 \t Training Loss: 44.30173034667969\n",
      "Epoch 98 \t Batch 60 \t Training Loss: 44.31695041656494\n",
      "Epoch 98 \t Batch 80 \t Training Loss: 44.27576947212219\n",
      "Epoch 98 \t Batch 100 \t Training Loss: 44.13350666046143\n",
      "Epoch 98 \t Batch 120 \t Training Loss: 44.490053208669025\n",
      "Epoch 98 \t Batch 140 \t Training Loss: 44.62534345899309\n",
      "Epoch 98 \t Batch 160 \t Training Loss: 44.64821283817291\n",
      "Epoch 98 \t Batch 180 \t Training Loss: 44.746982447306316\n",
      "Epoch 98 \t Batch 200 \t Training Loss: 44.95756628036499\n",
      "Epoch 98 \t Batch 220 \t Training Loss: 44.9758961244063\n",
      "Epoch 98 \t Batch 240 \t Training Loss: 45.017222038904826\n",
      "Epoch 98 \t Batch 260 \t Training Loss: 44.988855112515964\n",
      "Epoch 98 \t Batch 280 \t Training Loss: 44.94228313991002\n",
      "Epoch 98 \t Batch 300 \t Training Loss: 44.96102771759033\n",
      "Epoch 98 \t Batch 320 \t Training Loss: 45.102310228347775\n",
      "Epoch 98 \t Batch 340 \t Training Loss: 45.07652107687557\n",
      "Epoch 98 \t Batch 360 \t Training Loss: 45.06435907151964\n",
      "Epoch 98 \t Batch 380 \t Training Loss: 45.05403209485506\n",
      "Epoch 98 \t Batch 400 \t Training Loss: 45.00639402389526\n",
      "Epoch 98 \t Batch 420 \t Training Loss: 44.9962495077224\n",
      "Epoch 98 \t Batch 440 \t Training Loss: 45.02477374510332\n",
      "Epoch 98 \t Batch 460 \t Training Loss: 45.06774685901144\n",
      "Epoch 98 \t Batch 480 \t Training Loss: 45.109806958834334\n",
      "Epoch 98 \t Batch 500 \t Training Loss: 45.11219394683838\n",
      "Epoch 98 \t Batch 520 \t Training Loss: 45.151750381176285\n",
      "Epoch 98 \t Batch 540 \t Training Loss: 45.13511124363652\n",
      "Epoch 98 \t Batch 560 \t Training Loss: 45.10346658570426\n",
      "Epoch 98 \t Batch 580 \t Training Loss: 45.10843450612035\n",
      "Epoch 98 \t Batch 600 \t Training Loss: 45.14888324101766\n",
      "Epoch 98 \t Batch 620 \t Training Loss: 45.11480752883419\n",
      "Epoch 98 \t Batch 640 \t Training Loss: 45.09553903341293\n",
      "Epoch 98 \t Batch 660 \t Training Loss: 45.069258788137724\n",
      "Epoch 98 \t Batch 680 \t Training Loss: 45.087196103264304\n",
      "Epoch 98 \t Batch 700 \t Training Loss: 45.1326224572318\n",
      "Epoch 98 \t Batch 720 \t Training Loss: 45.17982914182875\n",
      "Epoch 98 \t Batch 740 \t Training Loss: 45.19444983198836\n",
      "Epoch 98 \t Batch 760 \t Training Loss: 45.226670571377404\n",
      "Epoch 98 \t Batch 780 \t Training Loss: 45.23177746014717\n",
      "Epoch 98 \t Batch 800 \t Training Loss: 45.216841187477115\n",
      "Epoch 98 \t Batch 820 \t Training Loss: 45.19061094144495\n",
      "Epoch 98 \t Batch 840 \t Training Loss: 45.16768123081752\n",
      "Epoch 98 \t Batch 860 \t Training Loss: 45.17291946854702\n",
      "Epoch 98 \t Batch 880 \t Training Loss: 45.22088353850625\n",
      "Epoch 98 \t Batch 900 \t Training Loss: 45.24618651919895\n",
      "Epoch 98 \t Batch 20 \t Validation Loss: 61.39755363464356\n",
      "Epoch 98 \t Batch 40 \t Validation Loss: 53.81366107463837\n",
      "Epoch 98 \t Batch 60 \t Validation Loss: 57.46005997657776\n",
      "Epoch 98 \t Batch 80 \t Validation Loss: 55.20111356973648\n",
      "Epoch 98 \t Batch 100 \t Validation Loss: 50.72011773109436\n",
      "Epoch 98 \t Batch 120 \t Validation Loss: 47.71635622183482\n",
      "Epoch 98 \t Batch 140 \t Validation Loss: 45.01297372409275\n",
      "Epoch 98 \t Batch 160 \t Validation Loss: 44.448617643117906\n",
      "Epoch 98 \t Batch 180 \t Validation Loss: 45.85690905253092\n",
      "Epoch 98 \t Batch 200 \t Validation Loss: 45.85673571586609\n",
      "Epoch 98 \t Batch 220 \t Validation Loss: 45.87950136878273\n",
      "Epoch 98 \t Batch 240 \t Validation Loss: 45.3000972032547\n",
      "Epoch 98 \t Batch 260 \t Validation Loss: 46.70807529229384\n",
      "Epoch 98 \t Batch 280 \t Validation Loss: 47.21368111201695\n",
      "Epoch 98 \t Batch 300 \t Validation Loss: 47.36454787572225\n",
      "Epoch 98 \t Batch 320 \t Validation Loss: 47.21706985235214\n",
      "Epoch 98 \t Batch 340 \t Validation Loss: 46.6699893839219\n",
      "Epoch 98 \t Batch 360 \t Validation Loss: 46.02368890444438\n",
      "Epoch 98 \t Batch 380 \t Validation Loss: 45.98955786353663\n",
      "Epoch 98 \t Batch 400 \t Validation Loss: 45.33614735603332\n",
      "Epoch 98 \t Batch 420 \t Validation Loss: 45.11121196292696\n",
      "Epoch 98 \t Batch 440 \t Validation Loss: 44.70767081000588\n",
      "Epoch 98 \t Batch 460 \t Validation Loss: 44.81290128127388\n",
      "Epoch 98 \t Batch 480 \t Validation Loss: 45.03850561777751\n",
      "Epoch 98 \t Batch 500 \t Validation Loss: 44.53824598312378\n",
      "Epoch 98 \t Batch 520 \t Validation Loss: 44.29853208248432\n",
      "Epoch 98 \t Batch 540 \t Validation Loss: 43.81632498281974\n",
      "Epoch 98 \t Batch 560 \t Validation Loss: 43.403736097472056\n",
      "Epoch 98 \t Batch 580 \t Validation Loss: 42.998525675411884\n",
      "Epoch 98 \t Batch 600 \t Validation Loss: 43.023679161071776\n",
      "Epoch 98 Training Loss: 45.256310732310055 Validation Loss: 43.495657366591615\n",
      "Epoch 98 completed\n",
      "Epoch 99 \t Batch 20 \t Training Loss: 43.74575061798096\n",
      "Epoch 99 \t Batch 40 \t Training Loss: 45.13279228210449\n",
      "Epoch 99 \t Batch 60 \t Training Loss: 45.3712828318278\n",
      "Epoch 99 \t Batch 80 \t Training Loss: 45.298997497558595\n",
      "Epoch 99 \t Batch 100 \t Training Loss: 45.6963695526123\n",
      "Epoch 99 \t Batch 120 \t Training Loss: 45.50833279291789\n",
      "Epoch 99 \t Batch 140 \t Training Loss: 45.566241782052174\n",
      "Epoch 99 \t Batch 160 \t Training Loss: 45.629114627838135\n",
      "Epoch 99 \t Batch 180 \t Training Loss: 45.583215692308215\n",
      "Epoch 99 \t Batch 200 \t Training Loss: 45.625227756500244\n",
      "Epoch 99 \t Batch 220 \t Training Loss: 45.602512845126064\n",
      "Epoch 99 \t Batch 240 \t Training Loss: 45.54652400016785\n",
      "Epoch 99 \t Batch 260 \t Training Loss: 45.47860085414006\n",
      "Epoch 99 \t Batch 280 \t Training Loss: 45.41128753934588\n",
      "Epoch 99 \t Batch 300 \t Training Loss: 45.47359559377035\n",
      "Epoch 99 \t Batch 320 \t Training Loss: 45.46391450166702\n",
      "Epoch 99 \t Batch 340 \t Training Loss: 45.371595876357134\n",
      "Epoch 99 \t Batch 360 \t Training Loss: 45.393509006500246\n",
      "Epoch 99 \t Batch 380 \t Training Loss: 45.38244847749409\n",
      "Epoch 99 \t Batch 400 \t Training Loss: 45.30621817588806\n",
      "Epoch 99 \t Batch 420 \t Training Loss: 45.393307704017275\n",
      "Epoch 99 \t Batch 440 \t Training Loss: 45.32686562971635\n",
      "Epoch 99 \t Batch 460 \t Training Loss: 45.33923389600671\n",
      "Epoch 99 \t Batch 480 \t Training Loss: 45.288489468892415\n",
      "Epoch 99 \t Batch 500 \t Training Loss: 45.27832308959961\n",
      "Epoch 99 \t Batch 520 \t Training Loss: 45.26073988400973\n",
      "Epoch 99 \t Batch 540 \t Training Loss: 45.23999623192681\n",
      "Epoch 99 \t Batch 560 \t Training Loss: 45.252606105804446\n",
      "Epoch 99 \t Batch 580 \t Training Loss: 45.256373734309754\n",
      "Epoch 99 \t Batch 600 \t Training Loss: 45.25286652247111\n",
      "Epoch 99 \t Batch 620 \t Training Loss: 45.202586217080395\n",
      "Epoch 99 \t Batch 640 \t Training Loss: 45.2594074845314\n",
      "Epoch 99 \t Batch 660 \t Training Loss: 45.23967905333548\n",
      "Epoch 99 \t Batch 680 \t Training Loss: 45.25912922129911\n",
      "Epoch 99 \t Batch 700 \t Training Loss: 45.23066351209368\n",
      "Epoch 99 \t Batch 720 \t Training Loss: 45.23317339155409\n",
      "Epoch 99 \t Batch 740 \t Training Loss: 45.275663417094464\n",
      "Epoch 99 \t Batch 760 \t Training Loss: 45.28523795479222\n",
      "Epoch 99 \t Batch 780 \t Training Loss: 45.3055475968581\n",
      "Epoch 99 \t Batch 800 \t Training Loss: 45.28656783103943\n",
      "Epoch 99 \t Batch 820 \t Training Loss: 45.29021625053592\n",
      "Epoch 99 \t Batch 840 \t Training Loss: 45.27794935589745\n",
      "Epoch 99 \t Batch 860 \t Training Loss: 45.270277382606686\n",
      "Epoch 99 \t Batch 880 \t Training Loss: 45.25792360739275\n",
      "Epoch 99 \t Batch 900 \t Training Loss: 45.2259908718533\n",
      "Epoch 99 \t Batch 20 \t Validation Loss: 32.74492540359497\n",
      "Epoch 99 \t Batch 40 \t Validation Loss: 31.976336789131164\n",
      "Epoch 99 \t Batch 60 \t Validation Loss: 32.61002799669902\n",
      "Epoch 99 \t Batch 80 \t Validation Loss: 32.58093749284744\n",
      "Epoch 99 \t Batch 100 \t Validation Loss: 31.804541006088257\n",
      "Epoch 99 \t Batch 120 \t Validation Loss: 31.505681586265563\n",
      "Epoch 99 \t Batch 140 \t Validation Loss: 30.934539243153164\n",
      "Epoch 99 \t Batch 160 \t Validation Loss: 32.29933435320854\n",
      "Epoch 99 \t Batch 180 \t Validation Loss: 35.56450390815735\n",
      "Epoch 99 \t Batch 200 \t Validation Loss: 36.85136519908905\n",
      "Epoch 99 \t Batch 220 \t Validation Loss: 37.94648939045993\n",
      "Epoch 99 \t Batch 240 \t Validation Loss: 38.20955174763997\n",
      "Epoch 99 \t Batch 260 \t Validation Loss: 40.39862609276405\n",
      "Epoch 99 \t Batch 280 \t Validation Loss: 41.549204444885255\n",
      "Epoch 99 \t Batch 300 \t Validation Loss: 42.348269850413004\n",
      "Epoch 99 \t Batch 320 \t Validation Loss: 42.65461698770523\n",
      "Epoch 99 \t Batch 340 \t Validation Loss: 42.44626908582799\n",
      "Epoch 99 \t Batch 360 \t Validation Loss: 42.11634185579088\n",
      "Epoch 99 \t Batch 380 \t Validation Loss: 42.29685967595954\n",
      "Epoch 99 \t Batch 400 \t Validation Loss: 41.785275535583494\n",
      "Epoch 99 \t Batch 420 \t Validation Loss: 41.7090277331216\n",
      "Epoch 99 \t Batch 440 \t Validation Loss: 41.37250609181144\n",
      "Epoch 99 \t Batch 460 \t Validation Loss: 41.49946176487467\n",
      "Epoch 99 \t Batch 480 \t Validation Loss: 41.867630352576576\n",
      "Epoch 99 \t Batch 500 \t Validation Loss: 41.50005695152283\n",
      "Epoch 99 \t Batch 520 \t Validation Loss: 41.251946531809295\n",
      "Epoch 99 \t Batch 540 \t Validation Loss: 40.91816584445812\n",
      "Epoch 99 \t Batch 560 \t Validation Loss: 40.683855494431086\n",
      "Epoch 99 \t Batch 580 \t Validation Loss: 40.43436739526946\n",
      "Epoch 99 \t Batch 600 \t Validation Loss: 40.578950708707175\n",
      "Epoch 99 Training Loss: 45.230705169711015 Validation Loss: 41.163352144228945\n",
      "Epoch 99 completed\n",
      "Epoch 100 \t Batch 20 \t Training Loss: 45.7701919555664\n",
      "Epoch 100 \t Batch 40 \t Training Loss: 45.345016193389895\n",
      "Epoch 100 \t Batch 60 \t Training Loss: 45.26783402760824\n",
      "Epoch 100 \t Batch 80 \t Training Loss: 45.35008454322815\n",
      "Epoch 100 \t Batch 100 \t Training Loss: 45.34950042724609\n",
      "Epoch 100 \t Batch 120 \t Training Loss: 45.58574542999268\n",
      "Epoch 100 \t Batch 140 \t Training Loss: 45.596429334368025\n",
      "Epoch 100 \t Batch 160 \t Training Loss: 45.36941018104553\n",
      "Epoch 100 \t Batch 180 \t Training Loss: 45.355033217536075\n",
      "Epoch 100 \t Batch 200 \t Training Loss: 45.284925594329835\n",
      "Epoch 100 \t Batch 220 \t Training Loss: 45.26586827364835\n",
      "Epoch 100 \t Batch 240 \t Training Loss: 45.20262028376261\n",
      "Epoch 100 \t Batch 260 \t Training Loss: 45.34849940079909\n",
      "Epoch 100 \t Batch 280 \t Training Loss: 45.32123666490827\n",
      "Epoch 100 \t Batch 300 \t Training Loss: 45.29774523417155\n",
      "Epoch 100 \t Batch 320 \t Training Loss: 45.214326453208926\n",
      "Epoch 100 \t Batch 340 \t Training Loss: 45.24914406047148\n",
      "Epoch 100 \t Batch 360 \t Training Loss: 45.242757638295494\n",
      "Epoch 100 \t Batch 380 \t Training Loss: 45.241248964008534\n",
      "Epoch 100 \t Batch 400 \t Training Loss: 45.20693771362305\n",
      "Epoch 100 \t Batch 420 \t Training Loss: 45.17723094395229\n",
      "Epoch 100 \t Batch 440 \t Training Loss: 45.19331042549827\n",
      "Epoch 100 \t Batch 460 \t Training Loss: 45.218510262862495\n",
      "Epoch 100 \t Batch 480 \t Training Loss: 45.21449248790741\n",
      "Epoch 100 \t Batch 500 \t Training Loss: 45.21068338775635\n",
      "Epoch 100 \t Batch 520 \t Training Loss: 45.16963704916147\n",
      "Epoch 100 \t Batch 540 \t Training Loss: 45.17063433329265\n",
      "Epoch 100 \t Batch 560 \t Training Loss: 45.174477863311765\n",
      "Epoch 100 \t Batch 580 \t Training Loss: 45.1936380846747\n",
      "Epoch 100 \t Batch 600 \t Training Loss: 45.172985572814945\n",
      "Epoch 100 \t Batch 620 \t Training Loss: 45.145386948124056\n",
      "Epoch 100 \t Batch 640 \t Training Loss: 45.17128714323044\n",
      "Epoch 100 \t Batch 660 \t Training Loss: 45.20272660110936\n",
      "Epoch 100 \t Batch 680 \t Training Loss: 45.16003272673663\n",
      "Epoch 100 \t Batch 700 \t Training Loss: 45.186832624162946\n",
      "Epoch 100 \t Batch 720 \t Training Loss: 45.20141806602478\n",
      "Epoch 100 \t Batch 740 \t Training Loss: 45.22472773371516\n",
      "Epoch 100 \t Batch 760 \t Training Loss: 45.259139136264196\n",
      "Epoch 100 \t Batch 780 \t Training Loss: 45.23115966014373\n",
      "Epoch 100 \t Batch 800 \t Training Loss: 45.21540687561035\n",
      "Epoch 100 \t Batch 820 \t Training Loss: 45.20206783573802\n",
      "Epoch 100 \t Batch 840 \t Training Loss: 45.21354856036958\n",
      "Epoch 100 \t Batch 860 \t Training Loss: 45.22691628655722\n",
      "Epoch 100 \t Batch 880 \t Training Loss: 45.22118201689287\n",
      "Epoch 100 \t Batch 900 \t Training Loss: 45.21629736582438\n",
      "Epoch 100 \t Batch 20 \t Validation Loss: 58.50142211914063\n",
      "Epoch 100 \t Batch 40 \t Validation Loss: 50.363459300994876\n",
      "Epoch 100 \t Batch 60 \t Validation Loss: 53.440900866190596\n",
      "Epoch 100 \t Batch 80 \t Validation Loss: 50.92642657756805\n",
      "Epoch 100 \t Batch 100 \t Validation Loss: 46.463797931671145\n",
      "Epoch 100 \t Batch 120 \t Validation Loss: 43.64862772623698\n",
      "Epoch 100 \t Batch 140 \t Validation Loss: 41.30533240182059\n",
      "Epoch 100 \t Batch 160 \t Validation Loss: 41.39690489768982\n",
      "Epoch 100 \t Batch 180 \t Validation Loss: 43.35901537471347\n",
      "Epoch 100 \t Batch 200 \t Validation Loss: 43.788057703971866\n",
      "Epoch 100 \t Batch 220 \t Validation Loss: 44.12412711490284\n",
      "Epoch 100 \t Batch 240 \t Validation Loss: 43.74198867082596\n",
      "Epoch 100 \t Batch 260 \t Validation Loss: 45.382276061865\n",
      "Epoch 100 \t Batch 280 \t Validation Loss: 46.05063843386514\n",
      "Epoch 100 \t Batch 300 \t Validation Loss: 46.43626446088155\n",
      "Epoch 100 \t Batch 320 \t Validation Loss: 46.41539036929608\n",
      "Epoch 100 \t Batch 340 \t Validation Loss: 45.95170424685759\n",
      "Epoch 100 \t Batch 360 \t Validation Loss: 45.42122218343947\n",
      "Epoch 100 \t Batch 380 \t Validation Loss: 45.49043142419112\n",
      "Epoch 100 \t Batch 400 \t Validation Loss: 44.92213210105896\n",
      "Epoch 100 \t Batch 420 \t Validation Loss: 44.76253047897702\n",
      "Epoch 100 \t Batch 440 \t Validation Loss: 44.42197679823095\n",
      "Epoch 100 \t Batch 460 \t Validation Loss: 44.584274891148446\n",
      "Epoch 100 \t Batch 480 \t Validation Loss: 44.86900246342023\n",
      "Epoch 100 \t Batch 500 \t Validation Loss: 44.429366914749146\n",
      "Epoch 100 \t Batch 520 \t Validation Loss: 44.30207694310408\n",
      "Epoch 100 \t Batch 540 \t Validation Loss: 43.82946815667329\n",
      "Epoch 100 \t Batch 560 \t Validation Loss: 43.45941368000848\n",
      "Epoch 100 \t Batch 580 \t Validation Loss: 43.10560047708709\n",
      "Epoch 100 \t Batch 600 \t Validation Loss: 43.13463057676951\n",
      "Epoch 100 Training Loss: 45.20615234707798 Validation Loss: 43.63330683924935\n",
      "Epoch 100 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataloader import *\n",
    "\n",
    "model = SimpleFCN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "class RMSE(nn.Module):\n",
    "    \"\"\" \n",
    "        Weighted RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.mse = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def __call__(self, prediction, target, weights = 1):\n",
    "        # prediction = prediction[:, 0]\n",
    "        return torch.sqrt(torch.mean(weights * self.mse(prediction,target)))\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_nonan_0-5.h5', 'data_nonan_1-5.h5', 'data_nonan_2-5.h5', 'data_nonan_3-5.h5', 'data_nonan_4-5.h5']\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(100):  # 10 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model2.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n",
      "Epoch 1 \t Batch 20 \t Training Loss: 112.12943458557129\n",
      "Epoch 1 \t Batch 40 \t Training Loss: 112.21190738677979\n",
      "Epoch 1 \t Batch 60 \t Training Loss: 111.21090558369954\n",
      "Epoch 1 \t Batch 80 \t Training Loss: 110.75978736877441\n",
      "Epoch 1 \t Batch 100 \t Training Loss: 109.97114784240722\n",
      "Epoch 1 \t Batch 120 \t Training Loss: 109.01233189900717\n",
      "Epoch 1 \t Batch 140 \t Training Loss: 107.7627934047154\n",
      "Epoch 1 \t Batch 160 \t Training Loss: 106.75223798751831\n",
      "Epoch 1 \t Batch 180 \t Training Loss: 105.37087622748481\n",
      "Epoch 1 \t Batch 200 \t Training Loss: 103.99563133239747\n",
      "Epoch 1 \t Batch 220 \t Training Loss: 102.65635986328125\n",
      "Epoch 1 \t Batch 240 \t Training Loss: 101.05740426381429\n",
      "Epoch 1 \t Batch 260 \t Training Loss: 99.47655927217923\n",
      "Epoch 1 \t Batch 280 \t Training Loss: 97.89582693917411\n",
      "Epoch 1 \t Batch 300 \t Training Loss: 96.28677233378093\n",
      "Epoch 1 \t Batch 320 \t Training Loss: 94.60417748689652\n",
      "Epoch 1 \t Batch 340 \t Training Loss: 93.07127366907456\n",
      "Epoch 1 \t Batch 360 \t Training Loss: 91.48208877775404\n",
      "Epoch 1 \t Batch 380 \t Training Loss: 89.95236511230469\n",
      "Epoch 1 \t Batch 400 \t Training Loss: 88.51921175003052\n",
      "Epoch 1 \t Batch 420 \t Training Loss: 87.16011092776344\n",
      "Epoch 1 \t Batch 440 \t Training Loss: 85.82411619533192\n",
      "Epoch 1 \t Batch 460 \t Training Loss: 84.55738542805547\n",
      "Epoch 1 \t Batch 480 \t Training Loss: 83.30267479419709\n",
      "Epoch 1 \t Batch 500 \t Training Loss: 82.14062731170654\n",
      "Epoch 1 \t Batch 520 \t Training Loss: 81.04883143351628\n",
      "Epoch 1 \t Batch 540 \t Training Loss: 80.0131725805777\n",
      "Epoch 1 \t Batch 560 \t Training Loss: 79.05721007074628\n",
      "Epoch 1 \t Batch 580 \t Training Loss: 78.1871903255068\n",
      "Epoch 1 \t Batch 600 \t Training Loss: 77.37179204940796\n",
      "Epoch 1 \t Batch 620 \t Training Loss: 76.6192326576479\n",
      "Epoch 1 \t Batch 640 \t Training Loss: 75.89130150079727\n",
      "Epoch 1 \t Batch 660 \t Training Loss: 75.12957282210841\n",
      "Epoch 1 \t Batch 680 \t Training Loss: 74.48037387062521\n",
      "Epoch 1 \t Batch 700 \t Training Loss: 73.84528317042759\n",
      "Epoch 1 \t Batch 720 \t Training Loss: 73.23989269998339\n",
      "Epoch 1 \t Batch 740 \t Training Loss: 72.62910919704953\n",
      "Epoch 1 \t Batch 760 \t Training Loss: 72.0768214225769\n",
      "Epoch 1 \t Batch 780 \t Training Loss: 71.53183026925112\n",
      "Epoch 1 \t Batch 800 \t Training Loss: 71.01410408496857\n",
      "Epoch 1 \t Batch 820 \t Training Loss: 70.52789906757634\n",
      "Epoch 1 \t Batch 840 \t Training Loss: 70.0356752486456\n",
      "Epoch 1 \t Batch 860 \t Training Loss: 69.59739839864332\n",
      "Epoch 1 \t Batch 880 \t Training Loss: 69.18551211790604\n",
      "Epoch 1 \t Batch 900 \t Training Loss: 68.7873000547621\n",
      "Epoch 1 \t Batch 20 \t Validation Loss: 13.723996448516846\n",
      "Epoch 1 \t Batch 40 \t Validation Loss: 17.751290106773375\n",
      "Epoch 1 \t Batch 60 \t Validation Loss: 17.608386977513632\n",
      "Epoch 1 \t Batch 80 \t Validation Loss: 18.513602328300475\n",
      "Epoch 1 \t Batch 100 \t Validation Loss: 20.443156280517577\n",
      "Epoch 1 \t Batch 120 \t Validation Loss: 22.009487438201905\n",
      "Epoch 1 \t Batch 140 \t Validation Loss: 22.902521160670688\n",
      "Epoch 1 \t Batch 160 \t Validation Loss: 25.38401652574539\n",
      "Epoch 1 \t Batch 180 \t Validation Loss: 29.070214700698852\n",
      "Epoch 1 \t Batch 200 \t Validation Loss: 30.573100714683534\n",
      "Epoch 1 \t Batch 220 \t Validation Loss: 32.100952629609544\n",
      "Epoch 1 \t Batch 240 \t Validation Loss: 32.749311633904775\n",
      "Epoch 1 \t Batch 260 \t Validation Loss: 34.82637760088994\n",
      "Epoch 1 \t Batch 280 \t Validation Loss: 35.97847503934588\n",
      "Epoch 1 \t Batch 300 \t Validation Loss: 37.22536875406901\n",
      "Epoch 1 \t Batch 320 \t Validation Loss: 37.849123692512514\n",
      "Epoch 1 \t Batch 340 \t Validation Loss: 37.89394904304953\n",
      "Epoch 1 \t Batch 360 \t Validation Loss: 37.953864643308854\n",
      "Epoch 1 \t Batch 380 \t Validation Loss: 38.306160168898735\n",
      "Epoch 1 \t Batch 400 \t Validation Loss: 38.030850234031675\n",
      "Epoch 1 \t Batch 420 \t Validation Loss: 38.15553075245449\n",
      "Epoch 1 \t Batch 440 \t Validation Loss: 37.951318155635484\n",
      "Epoch 1 \t Batch 460 \t Validation Loss: 38.22541082630987\n",
      "Epoch 1 \t Batch 480 \t Validation Loss: 38.769250957171124\n",
      "Epoch 1 \t Batch 500 \t Validation Loss: 38.564983856201174\n",
      "Epoch 1 \t Batch 520 \t Validation Loss: 38.399783361875095\n",
      "Epoch 1 \t Batch 540 \t Validation Loss: 38.0921792878045\n",
      "Epoch 1 \t Batch 560 \t Validation Loss: 37.82694109167372\n",
      "Epoch 1 \t Batch 580 \t Validation Loss: 37.528831853537724\n",
      "Epoch 1 \t Batch 600 \t Validation Loss: 37.70681098937988\n",
      "Epoch 1 Training Loss: 68.48498061751071 Validation Loss: 38.26811544616501\n",
      "Validation Loss Decreased(inf--->23573.159114837646) Saving The Model\n",
      "Epoch 1 completed\n",
      "Epoch 2 \t Batch 20 \t Training Loss: 51.81571578979492\n",
      "Epoch 2 \t Batch 40 \t Training Loss: 50.121859073638916\n",
      "Epoch 2 \t Batch 60 \t Training Loss: 50.501243654886885\n",
      "Epoch 2 \t Batch 80 \t Training Loss: 50.64871745109558\n",
      "Epoch 2 \t Batch 100 \t Training Loss: 50.781623649597165\n",
      "Epoch 2 \t Batch 120 \t Training Loss: 50.947065862019855\n",
      "Epoch 2 \t Batch 140 \t Training Loss: 51.01469135284424\n",
      "Epoch 2 \t Batch 160 \t Training Loss: 50.96592288017273\n",
      "Epoch 2 \t Batch 180 \t Training Loss: 51.01679562462701\n",
      "Epoch 2 \t Batch 200 \t Training Loss: 51.05754405975342\n",
      "Epoch 2 \t Batch 220 \t Training Loss: 50.97307909185236\n",
      "Epoch 2 \t Batch 240 \t Training Loss: 51.10301084518433\n",
      "Epoch 2 \t Batch 260 \t Training Loss: 51.06730600503775\n",
      "Epoch 2 \t Batch 280 \t Training Loss: 51.02930910927909\n",
      "Epoch 2 \t Batch 300 \t Training Loss: 51.07011679331462\n",
      "Epoch 2 \t Batch 320 \t Training Loss: 50.99718353748322\n",
      "Epoch 2 \t Batch 340 \t Training Loss: 51.01562945422004\n",
      "Epoch 2 \t Batch 360 \t Training Loss: 51.03450986014472\n",
      "Epoch 2 \t Batch 380 \t Training Loss: 50.97037653672068\n",
      "Epoch 2 \t Batch 400 \t Training Loss: 50.96318603515625\n",
      "Epoch 2 \t Batch 420 \t Training Loss: 50.98032242002941\n",
      "Epoch 2 \t Batch 440 \t Training Loss: 50.92261845848777\n",
      "Epoch 2 \t Batch 460 \t Training Loss: 50.94731488435165\n",
      "Epoch 2 \t Batch 480 \t Training Loss: 50.905800779660545\n",
      "Epoch 2 \t Batch 500 \t Training Loss: 50.89761740875244\n",
      "Epoch 2 \t Batch 520 \t Training Loss: 50.925401694958026\n",
      "Epoch 2 \t Batch 540 \t Training Loss: 50.863525214018644\n",
      "Epoch 2 \t Batch 560 \t Training Loss: 50.85865127699716\n",
      "Epoch 2 \t Batch 580 \t Training Loss: 50.84674650389573\n",
      "Epoch 2 \t Batch 600 \t Training Loss: 50.87797779719035\n",
      "Epoch 2 \t Batch 620 \t Training Loss: 50.89483812393681\n",
      "Epoch 2 \t Batch 640 \t Training Loss: 50.894781643152236\n",
      "Epoch 2 \t Batch 660 \t Training Loss: 50.86805329178319\n",
      "Epoch 2 \t Batch 680 \t Training Loss: 50.85485890893375\n",
      "Epoch 2 \t Batch 700 \t Training Loss: 50.83969614846366\n",
      "Epoch 2 \t Batch 720 \t Training Loss: 50.87093425856696\n",
      "Epoch 2 \t Batch 740 \t Training Loss: 50.85144669816301\n",
      "Epoch 2 \t Batch 760 \t Training Loss: 50.83121473412765\n",
      "Epoch 2 \t Batch 780 \t Training Loss: 50.78326014005221\n",
      "Epoch 2 \t Batch 800 \t Training Loss: 50.76442133903503\n",
      "Epoch 2 \t Batch 820 \t Training Loss: 50.79082719291129\n",
      "Epoch 2 \t Batch 840 \t Training Loss: 50.73971763338361\n",
      "Epoch 2 \t Batch 860 \t Training Loss: 50.70191288881524\n",
      "Epoch 2 \t Batch 880 \t Training Loss: 50.676482898538765\n",
      "Epoch 2 \t Batch 900 \t Training Loss: 50.65108392927382\n",
      "Epoch 2 \t Batch 20 \t Validation Loss: 16.925513076782227\n",
      "Epoch 2 \t Batch 40 \t Validation Loss: 20.05988175868988\n",
      "Epoch 2 \t Batch 60 \t Validation Loss: 19.757032728195192\n",
      "Epoch 2 \t Batch 80 \t Validation Loss: 20.596212327480316\n",
      "Epoch 2 \t Batch 100 \t Validation Loss: 21.724861001968385\n",
      "Epoch 2 \t Batch 120 \t Validation Loss: 23.01500169436137\n",
      "Epoch 2 \t Batch 140 \t Validation Loss: 23.61197977747236\n",
      "Epoch 2 \t Batch 160 \t Validation Loss: 25.791662281751634\n",
      "Epoch 2 \t Batch 180 \t Validation Loss: 29.07337219980028\n",
      "Epoch 2 \t Batch 200 \t Validation Loss: 30.381180930137635\n",
      "Epoch 2 \t Batch 220 \t Validation Loss: 31.684787598523226\n",
      "Epoch 2 \t Batch 240 \t Validation Loss: 32.16951963504155\n",
      "Epoch 2 \t Batch 260 \t Validation Loss: 34.14903697233934\n",
      "Epoch 2 \t Batch 280 \t Validation Loss: 35.21088581766401\n",
      "Epoch 2 \t Batch 300 \t Validation Loss: 36.29120875676473\n",
      "Epoch 2 \t Batch 320 \t Validation Loss: 36.798097014427185\n",
      "Epoch 2 \t Batch 340 \t Validation Loss: 36.834288260516\n",
      "Epoch 2 \t Batch 360 \t Validation Loss: 36.82825810114543\n",
      "Epoch 2 \t Batch 380 \t Validation Loss: 37.15904971172935\n",
      "Epoch 2 \t Batch 400 \t Validation Loss: 36.911566605567934\n",
      "Epoch 2 \t Batch 420 \t Validation Loss: 37.04554843902588\n",
      "Epoch 2 \t Batch 440 \t Validation Loss: 36.868138113888826\n",
      "Epoch 2 \t Batch 460 \t Validation Loss: 37.1369308181431\n",
      "Epoch 2 \t Batch 480 \t Validation Loss: 37.67922090689341\n",
      "Epoch 2 \t Batch 500 \t Validation Loss: 37.45908853530884\n",
      "Epoch 2 \t Batch 520 \t Validation Loss: 37.31257819762597\n",
      "Epoch 2 \t Batch 540 \t Validation Loss: 37.044319541366015\n",
      "Epoch 2 \t Batch 560 \t Validation Loss: 36.855100761141095\n",
      "Epoch 2 \t Batch 580 \t Validation Loss: 36.682175471864895\n",
      "Epoch 2 \t Batch 600 \t Validation Loss: 36.87261156400045\n",
      "Epoch 2 Training Loss: 50.615879799313475 Validation Loss: 37.500111146406695\n",
      "Validation Loss Decreased(23573.159114837646--->23100.068466186523) Saving The Model\n",
      "Epoch 2 completed\n",
      "Epoch 3 \t Batch 20 \t Training Loss: 49.16120510101318\n",
      "Epoch 3 \t Batch 40 \t Training Loss: 50.4053918838501\n",
      "Epoch 3 \t Batch 60 \t Training Loss: 50.334363047281904\n",
      "Epoch 3 \t Batch 80 \t Training Loss: 50.09670753479004\n",
      "Epoch 3 \t Batch 100 \t Training Loss: 50.12393238067627\n",
      "Epoch 3 \t Batch 120 \t Training Loss: 49.97300732930501\n",
      "Epoch 3 \t Batch 140 \t Training Loss: 49.97800421033587\n",
      "Epoch 3 \t Batch 160 \t Training Loss: 50.03927476406098\n",
      "Epoch 3 \t Batch 180 \t Training Loss: 50.008285692003035\n",
      "Epoch 3 \t Batch 200 \t Training Loss: 50.00698522567749\n",
      "Epoch 3 \t Batch 220 \t Training Loss: 50.14341094277122\n",
      "Epoch 3 \t Batch 240 \t Training Loss: 50.1519850730896\n",
      "Epoch 3 \t Batch 260 \t Training Loss: 50.164611655015214\n",
      "Epoch 3 \t Batch 280 \t Training Loss: 50.004047407422746\n",
      "Epoch 3 \t Batch 300 \t Training Loss: 50.00554504394531\n",
      "Epoch 3 \t Batch 320 \t Training Loss: 49.97645673751831\n",
      "Epoch 3 \t Batch 340 \t Training Loss: 49.99297664866728\n",
      "Epoch 3 \t Batch 360 \t Training Loss: 49.92819539176093\n",
      "Epoch 3 \t Batch 380 \t Training Loss: 49.99872587103593\n",
      "Epoch 3 \t Batch 400 \t Training Loss: 49.96780583381653\n",
      "Epoch 3 \t Batch 420 \t Training Loss: 50.00095313844227\n",
      "Epoch 3 \t Batch 440 \t Training Loss: 50.0297078739513\n",
      "Epoch 3 \t Batch 460 \t Training Loss: 49.961200680940046\n",
      "Epoch 3 \t Batch 480 \t Training Loss: 49.981594840685524\n",
      "Epoch 3 \t Batch 500 \t Training Loss: 50.00689242553711\n",
      "Epoch 3 \t Batch 520 \t Training Loss: 49.99240655899048\n",
      "Epoch 3 \t Batch 540 \t Training Loss: 50.00809989505344\n",
      "Epoch 3 \t Batch 560 \t Training Loss: 49.967288746152605\n",
      "Epoch 3 \t Batch 580 \t Training Loss: 49.950288785737136\n",
      "Epoch 3 \t Batch 600 \t Training Loss: 49.86211566289266\n",
      "Epoch 3 \t Batch 620 \t Training Loss: 49.858373888077274\n",
      "Epoch 3 \t Batch 640 \t Training Loss: 49.85456637144089\n",
      "Epoch 3 \t Batch 660 \t Training Loss: 49.85996191429369\n",
      "Epoch 3 \t Batch 680 \t Training Loss: 49.85425675897037\n",
      "Epoch 3 \t Batch 700 \t Training Loss: 49.863180405753\n",
      "Epoch 3 \t Batch 720 \t Training Loss: 49.87074261771308\n",
      "Epoch 3 \t Batch 740 \t Training Loss: 49.86702472068168\n",
      "Epoch 3 \t Batch 760 \t Training Loss: 49.810531209644516\n",
      "Epoch 3 \t Batch 780 \t Training Loss: 49.792540158980934\n",
      "Epoch 3 \t Batch 800 \t Training Loss: 49.79604595184326\n",
      "Epoch 3 \t Batch 820 \t Training Loss: 49.80005249395603\n",
      "Epoch 3 \t Batch 840 \t Training Loss: 49.80308611733573\n",
      "Epoch 3 \t Batch 860 \t Training Loss: 49.775764358875364\n",
      "Epoch 3 \t Batch 880 \t Training Loss: 49.8131453297355\n",
      "Epoch 3 \t Batch 900 \t Training Loss: 49.78164847056071\n",
      "Epoch 3 \t Batch 20 \t Validation Loss: 15.780649781227112\n",
      "Epoch 3 \t Batch 40 \t Validation Loss: 19.330253112316132\n",
      "Epoch 3 \t Batch 60 \t Validation Loss: 19.58543221950531\n",
      "Epoch 3 \t Batch 80 \t Validation Loss: 20.44908168911934\n",
      "Epoch 3 \t Batch 100 \t Validation Loss: 22.02263594150543\n",
      "Epoch 3 \t Batch 120 \t Validation Loss: 23.349712979793548\n",
      "Epoch 3 \t Batch 140 \t Validation Loss: 24.06300506932395\n",
      "Epoch 3 \t Batch 160 \t Validation Loss: 26.33784741461277\n",
      "Epoch 3 \t Batch 180 \t Validation Loss: 29.95436641110314\n",
      "Epoch 3 \t Batch 200 \t Validation Loss: 31.547407953739167\n",
      "Epoch 3 \t Batch 220 \t Validation Loss: 33.02779903628609\n",
      "Epoch 3 \t Batch 240 \t Validation Loss: 33.593863719701766\n",
      "Epoch 3 \t Batch 260 \t Validation Loss: 35.77505495548248\n",
      "Epoch 3 \t Batch 280 \t Validation Loss: 36.94404360396521\n",
      "Epoch 3 \t Batch 300 \t Validation Loss: 38.01474818706512\n",
      "Epoch 3 \t Batch 320 \t Validation Loss: 38.50555009394884\n",
      "Epoch 3 \t Batch 340 \t Validation Loss: 38.491370449346654\n",
      "Epoch 3 \t Batch 360 \t Validation Loss: 38.44559880495071\n",
      "Epoch 3 \t Batch 380 \t Validation Loss: 38.736844626225924\n",
      "Epoch 3 \t Batch 400 \t Validation Loss: 38.38128439545631\n",
      "Epoch 3 \t Batch 420 \t Validation Loss: 38.45538335187094\n",
      "Epoch 3 \t Batch 440 \t Validation Loss: 38.195189797878264\n",
      "Epoch 3 \t Batch 460 \t Validation Loss: 38.44365837366685\n",
      "Epoch 3 \t Batch 480 \t Validation Loss: 38.94042913218339\n",
      "Epoch 3 \t Batch 500 \t Validation Loss: 38.68004075717926\n",
      "Epoch 3 \t Batch 520 \t Validation Loss: 38.48444990469859\n",
      "Epoch 3 \t Batch 540 \t Validation Loss: 38.23970645357061\n",
      "Epoch 3 \t Batch 560 \t Validation Loss: 38.016127771139146\n",
      "Epoch 3 \t Batch 580 \t Validation Loss: 37.82425808824342\n",
      "Epoch 3 \t Batch 600 \t Validation Loss: 38.020438592433926\n",
      "Epoch 3 Training Loss: 49.81294795445034 Validation Loss: 38.61228811044197\n",
      "Epoch 3 completed\n",
      "Epoch 4 \t Batch 20 \t Training Loss: 50.17104778289795\n",
      "Epoch 4 \t Batch 40 \t Training Loss: 50.19603834152222\n",
      "Epoch 4 \t Batch 60 \t Training Loss: 50.1979585647583\n",
      "Epoch 4 \t Batch 80 \t Training Loss: 49.95917372703552\n",
      "Epoch 4 \t Batch 100 \t Training Loss: 49.91991870880127\n",
      "Epoch 4 \t Batch 120 \t Training Loss: 49.63244864145915\n",
      "Epoch 4 \t Batch 140 \t Training Loss: 49.59002167837961\n",
      "Epoch 4 \t Batch 160 \t Training Loss: 49.75449478626251\n",
      "Epoch 4 \t Batch 180 \t Training Loss: 49.786170408460826\n",
      "Epoch 4 \t Batch 200 \t Training Loss: 49.703476428985596\n",
      "Epoch 4 \t Batch 220 \t Training Loss: 49.76074718128551\n",
      "Epoch 4 \t Batch 240 \t Training Loss: 49.85936891237895\n",
      "Epoch 4 \t Batch 260 \t Training Loss: 49.86662945380578\n",
      "Epoch 4 \t Batch 280 \t Training Loss: 49.85342194693429\n",
      "Epoch 4 \t Batch 300 \t Training Loss: 49.86004136403402\n",
      "Epoch 4 \t Batch 320 \t Training Loss: 49.80114287137985\n",
      "Epoch 4 \t Batch 340 \t Training Loss: 49.71957504048067\n",
      "Epoch 4 \t Batch 360 \t Training Loss: 49.720229064093694\n",
      "Epoch 4 \t Batch 380 \t Training Loss: 49.68794349871184\n",
      "Epoch 4 \t Batch 400 \t Training Loss: 49.61900493621826\n",
      "Epoch 4 \t Batch 420 \t Training Loss: 49.66073243277413\n",
      "Epoch 4 \t Batch 440 \t Training Loss: 49.61548372615467\n",
      "Epoch 4 \t Batch 460 \t Training Loss: 49.61443452420442\n",
      "Epoch 4 \t Batch 480 \t Training Loss: 49.553638641039534\n",
      "Epoch 4 \t Batch 500 \t Training Loss: 49.510877891540524\n",
      "Epoch 4 \t Batch 520 \t Training Loss: 49.54532008537879\n",
      "Epoch 4 \t Batch 540 \t Training Loss: 49.551049889458554\n",
      "Epoch 4 \t Batch 560 \t Training Loss: 49.57249845777239\n",
      "Epoch 4 \t Batch 580 \t Training Loss: 49.52484660970754\n",
      "Epoch 4 \t Batch 600 \t Training Loss: 49.51180629094442\n",
      "Epoch 4 \t Batch 620 \t Training Loss: 49.46468841798844\n",
      "Epoch 4 \t Batch 640 \t Training Loss: 49.49153472781181\n",
      "Epoch 4 \t Batch 660 \t Training Loss: 49.45061420093883\n",
      "Epoch 4 \t Batch 680 \t Training Loss: 49.44432327046114\n",
      "Epoch 4 \t Batch 700 \t Training Loss: 49.416067548479354\n",
      "Epoch 4 \t Batch 720 \t Training Loss: 49.41965187390645\n",
      "Epoch 4 \t Batch 740 \t Training Loss: 49.43338885951687\n",
      "Epoch 4 \t Batch 760 \t Training Loss: 49.42507159584447\n",
      "Epoch 4 \t Batch 780 \t Training Loss: 49.4274655464368\n",
      "Epoch 4 \t Batch 800 \t Training Loss: 49.42113431930542\n",
      "Epoch 4 \t Batch 820 \t Training Loss: 49.44179294400099\n",
      "Epoch 4 \t Batch 840 \t Training Loss: 49.43300739469982\n",
      "Epoch 4 \t Batch 860 \t Training Loss: 49.42014098500096\n",
      "Epoch 4 \t Batch 880 \t Training Loss: 49.42693450234153\n",
      "Epoch 4 \t Batch 900 \t Training Loss: 49.42736427730984\n",
      "Epoch 4 \t Batch 20 \t Validation Loss: 14.456584405899047\n",
      "Epoch 4 \t Batch 40 \t Validation Loss: 17.41357045173645\n",
      "Epoch 4 \t Batch 60 \t Validation Loss: 17.459520387649537\n",
      "Epoch 4 \t Batch 80 \t Validation Loss: 18.28681563138962\n",
      "Epoch 4 \t Batch 100 \t Validation Loss: 20.12319603919983\n",
      "Epoch 4 \t Batch 120 \t Validation Loss: 21.809259072939554\n",
      "Epoch 4 \t Batch 140 \t Validation Loss: 22.699418122427804\n",
      "Epoch 4 \t Batch 160 \t Validation Loss: 25.072777223587035\n",
      "Epoch 4 \t Batch 180 \t Validation Loss: 28.76980553732978\n",
      "Epoch 4 \t Batch 200 \t Validation Loss: 30.520789680480956\n",
      "Epoch 4 \t Batch 220 \t Validation Loss: 32.061813311143354\n",
      "Epoch 4 \t Batch 240 \t Validation Loss: 32.652829162279765\n",
      "Epoch 4 \t Batch 260 \t Validation Loss: 34.894113757060126\n",
      "Epoch 4 \t Batch 280 \t Validation Loss: 36.153439450263974\n",
      "Epoch 4 \t Batch 300 \t Validation Loss: 37.2643488407135\n",
      "Epoch 4 \t Batch 320 \t Validation Loss: 37.76717502176761\n",
      "Epoch 4 \t Batch 340 \t Validation Loss: 37.79990959447973\n",
      "Epoch 4 \t Batch 360 \t Validation Loss: 37.788494573699104\n",
      "Epoch 4 \t Batch 380 \t Validation Loss: 38.12326485232303\n",
      "Epoch 4 \t Batch 400 \t Validation Loss: 37.82636486291885\n",
      "Epoch 4 \t Batch 420 \t Validation Loss: 37.892055232184276\n",
      "Epoch 4 \t Batch 440 \t Validation Loss: 37.67949373722077\n",
      "Epoch 4 \t Batch 460 \t Validation Loss: 37.90759527994239\n",
      "Epoch 4 \t Batch 480 \t Validation Loss: 38.41251584092776\n",
      "Epoch 4 \t Batch 500 \t Validation Loss: 38.146821844100955\n",
      "Epoch 4 \t Batch 520 \t Validation Loss: 38.00549697692578\n",
      "Epoch 4 \t Batch 540 \t Validation Loss: 37.793868469308926\n",
      "Epoch 4 \t Batch 560 \t Validation Loss: 37.63933086565563\n",
      "Epoch 4 \t Batch 580 \t Validation Loss: 37.545114900325906\n",
      "Epoch 4 \t Batch 600 \t Validation Loss: 37.76877370039622\n",
      "Epoch 4 Training Loss: 49.43600369019753 Validation Loss: 38.46953435842093\n",
      "Epoch 4 completed\n",
      "Epoch 5 \t Batch 20 \t Training Loss: 48.33051166534424\n",
      "Epoch 5 \t Batch 40 \t Training Loss: 48.48695268630981\n",
      "Epoch 5 \t Batch 60 \t Training Loss: 49.51099058787028\n",
      "Epoch 5 \t Batch 80 \t Training Loss: 49.55747790336609\n",
      "Epoch 5 \t Batch 100 \t Training Loss: 49.612753791809084\n",
      "Epoch 5 \t Batch 120 \t Training Loss: 49.11602300008138\n",
      "Epoch 5 \t Batch 140 \t Training Loss: 49.03558589390346\n",
      "Epoch 5 \t Batch 160 \t Training Loss: 48.97970023155212\n",
      "Epoch 5 \t Batch 180 \t Training Loss: 48.98023906283908\n",
      "Epoch 5 \t Batch 200 \t Training Loss: 49.00072124481201\n",
      "Epoch 5 \t Batch 220 \t Training Loss: 48.99792062586004\n",
      "Epoch 5 \t Batch 240 \t Training Loss: 49.08332654635112\n",
      "Epoch 5 \t Batch 260 \t Training Loss: 49.05524947826679\n",
      "Epoch 5 \t Batch 280 \t Training Loss: 49.08651673453195\n",
      "Epoch 5 \t Batch 300 \t Training Loss: 49.14830135345459\n",
      "Epoch 5 \t Batch 320 \t Training Loss: 49.06488995552063\n",
      "Epoch 5 \t Batch 340 \t Training Loss: 49.17438991771025\n",
      "Epoch 5 \t Batch 360 \t Training Loss: 49.28956283993191\n",
      "Epoch 5 \t Batch 380 \t Training Loss: 49.313838667618604\n",
      "Epoch 5 \t Batch 400 \t Training Loss: 49.24166694641113\n",
      "Epoch 5 \t Batch 420 \t Training Loss: 49.2115295228504\n",
      "Epoch 5 \t Batch 440 \t Training Loss: 49.22742730053989\n",
      "Epoch 5 \t Batch 460 \t Training Loss: 49.25917249762494\n",
      "Epoch 5 \t Batch 480 \t Training Loss: 49.284149424235025\n",
      "Epoch 5 \t Batch 500 \t Training Loss: 49.2967914428711\n",
      "Epoch 5 \t Batch 520 \t Training Loss: 49.32704003407405\n",
      "Epoch 5 \t Batch 540 \t Training Loss: 49.31042118778935\n",
      "Epoch 5 \t Batch 560 \t Training Loss: 49.34422272273472\n",
      "Epoch 5 \t Batch 580 \t Training Loss: 49.394863161547434\n",
      "Epoch 5 \t Batch 600 \t Training Loss: 49.38505818049113\n",
      "Epoch 5 \t Batch 620 \t Training Loss: 49.37390958724483\n",
      "Epoch 5 \t Batch 640 \t Training Loss: 49.347127044200896\n",
      "Epoch 5 \t Batch 660 \t Training Loss: 49.3037485816262\n",
      "Epoch 5 \t Batch 680 \t Training Loss: 49.27793221193201\n",
      "Epoch 5 \t Batch 700 \t Training Loss: 49.265532575334824\n",
      "Epoch 5 \t Batch 720 \t Training Loss: 49.22599373393589\n",
      "Epoch 5 \t Batch 740 \t Training Loss: 49.16932192106505\n",
      "Epoch 5 \t Batch 760 \t Training Loss: 49.153447472421746\n",
      "Epoch 5 \t Batch 780 \t Training Loss: 49.161514433836324\n",
      "Epoch 5 \t Batch 800 \t Training Loss: 49.19134191036224\n",
      "Epoch 5 \t Batch 820 \t Training Loss: 49.15957739295029\n",
      "Epoch 5 \t Batch 840 \t Training Loss: 49.168096601395376\n",
      "Epoch 5 \t Batch 860 \t Training Loss: 49.17884972594505\n",
      "Epoch 5 \t Batch 880 \t Training Loss: 49.15920645540411\n",
      "Epoch 5 \t Batch 900 \t Training Loss: 49.17022364298503\n",
      "Epoch 5 \t Batch 20 \t Validation Loss: 17.93508815765381\n",
      "Epoch 5 \t Batch 40 \t Validation Loss: 21.235318899154663\n",
      "Epoch 5 \t Batch 60 \t Validation Loss: 21.205316718419393\n",
      "Epoch 5 \t Batch 80 \t Validation Loss: 22.066556489467622\n",
      "Epoch 5 \t Batch 100 \t Validation Loss: 23.07012942314148\n",
      "Epoch 5 \t Batch 120 \t Validation Loss: 24.167429176966348\n",
      "Epoch 5 \t Batch 140 \t Validation Loss: 24.672507313319613\n",
      "Epoch 5 \t Batch 160 \t Validation Loss: 26.61337152719498\n",
      "Epoch 5 \t Batch 180 \t Validation Loss: 29.478550889756946\n",
      "Epoch 5 \t Batch 200 \t Validation Loss: 30.700938692092894\n",
      "Epoch 5 \t Batch 220 \t Validation Loss: 31.832017811861906\n",
      "Epoch 5 \t Batch 240 \t Validation Loss: 32.1543879588445\n",
      "Epoch 5 \t Batch 260 \t Validation Loss: 34.06011591691237\n",
      "Epoch 5 \t Batch 280 \t Validation Loss: 35.04807786941528\n",
      "Epoch 5 \t Batch 300 \t Validation Loss: 35.947559668223064\n",
      "Epoch 5 \t Batch 320 \t Validation Loss: 36.350639688968656\n",
      "Epoch 5 \t Batch 340 \t Validation Loss: 36.36946067249074\n",
      "Epoch 5 \t Batch 360 \t Validation Loss: 36.35129302077823\n",
      "Epoch 5 \t Batch 380 \t Validation Loss: 36.65943997282731\n",
      "Epoch 5 \t Batch 400 \t Validation Loss: 36.417450640201565\n",
      "Epoch 5 \t Batch 420 \t Validation Loss: 36.528956197556994\n",
      "Epoch 5 \t Batch 440 \t Validation Loss: 36.37092589248311\n",
      "Epoch 5 \t Batch 460 \t Validation Loss: 36.64580553096274\n",
      "Epoch 5 \t Batch 480 \t Validation Loss: 37.19512235919635\n",
      "Epoch 5 \t Batch 500 \t Validation Loss: 36.95083384513855\n",
      "Epoch 5 \t Batch 520 \t Validation Loss: 36.841590613585254\n",
      "Epoch 5 \t Batch 540 \t Validation Loss: 36.67464832729763\n",
      "Epoch 5 \t Batch 560 \t Validation Loss: 36.57764517239162\n",
      "Epoch 5 \t Batch 580 \t Validation Loss: 36.553612505156416\n",
      "Epoch 5 \t Batch 600 \t Validation Loss: 36.81929696083069\n",
      "Epoch 5 Training Loss: 49.1687237064607 Validation Loss: 37.50354808027094\n",
      "Epoch 5 completed\n",
      "Epoch 6 \t Batch 20 \t Training Loss: 49.95852775573731\n",
      "Epoch 6 \t Batch 40 \t Training Loss: 49.289150142669676\n",
      "Epoch 6 \t Batch 60 \t Training Loss: 49.45431238810222\n",
      "Epoch 6 \t Batch 80 \t Training Loss: 49.32721829414368\n",
      "Epoch 6 \t Batch 100 \t Training Loss: 49.47238758087158\n",
      "Epoch 6 \t Batch 120 \t Training Loss: 49.47342176437378\n",
      "Epoch 6 \t Batch 140 \t Training Loss: 49.42940265110561\n",
      "Epoch 6 \t Batch 160 \t Training Loss: 49.2818763256073\n",
      "Epoch 6 \t Batch 180 \t Training Loss: 49.310835965474446\n",
      "Epoch 6 \t Batch 200 \t Training Loss: 49.25259469985962\n",
      "Epoch 6 \t Batch 220 \t Training Loss: 49.12854083668102\n",
      "Epoch 6 \t Batch 240 \t Training Loss: 49.13731886545817\n",
      "Epoch 6 \t Batch 260 \t Training Loss: 49.09492447192852\n",
      "Epoch 6 \t Batch 280 \t Training Loss: 49.152140862601144\n",
      "Epoch 6 \t Batch 300 \t Training Loss: 49.08282855987549\n",
      "Epoch 6 \t Batch 320 \t Training Loss: 49.04680415391922\n",
      "Epoch 6 \t Batch 340 \t Training Loss: 48.994003666148465\n",
      "Epoch 6 \t Batch 360 \t Training Loss: 48.92705702251858\n",
      "Epoch 6 \t Batch 380 \t Training Loss: 48.96257992794639\n",
      "Epoch 6 \t Batch 400 \t Training Loss: 48.9971262550354\n",
      "Epoch 6 \t Batch 420 \t Training Loss: 48.970413071768625\n",
      "Epoch 6 \t Batch 440 \t Training Loss: 48.98337063355879\n",
      "Epoch 6 \t Batch 460 \t Training Loss: 48.98737151933753\n",
      "Epoch 6 \t Batch 480 \t Training Loss: 48.955986285209654\n",
      "Epoch 6 \t Batch 500 \t Training Loss: 48.967429107666014\n",
      "Epoch 6 \t Batch 520 \t Training Loss: 49.00387348761925\n",
      "Epoch 6 \t Batch 540 \t Training Loss: 48.99766808262578\n",
      "Epoch 6 \t Batch 560 \t Training Loss: 49.02546657153538\n",
      "Epoch 6 \t Batch 580 \t Training Loss: 49.02727396734829\n",
      "Epoch 6 \t Batch 600 \t Training Loss: 49.011702613830565\n",
      "Epoch 6 \t Batch 620 \t Training Loss: 49.00690609101326\n",
      "Epoch 6 \t Batch 640 \t Training Loss: 48.96566299796105\n",
      "Epoch 6 \t Batch 660 \t Training Loss: 48.98381074269613\n",
      "Epoch 6 \t Batch 680 \t Training Loss: 49.054756725535675\n",
      "Epoch 6 \t Batch 700 \t Training Loss: 49.00153860364642\n",
      "Epoch 6 \t Batch 720 \t Training Loss: 48.99702772564358\n",
      "Epoch 6 \t Batch 740 \t Training Loss: 48.98259083515889\n",
      "Epoch 6 \t Batch 760 \t Training Loss: 49.013707165969045\n",
      "Epoch 6 \t Batch 780 \t Training Loss: 49.01758921207526\n",
      "Epoch 6 \t Batch 800 \t Training Loss: 49.014780459403994\n",
      "Epoch 6 \t Batch 820 \t Training Loss: 48.98110915858571\n",
      "Epoch 6 \t Batch 840 \t Training Loss: 48.971444411504834\n",
      "Epoch 6 \t Batch 860 \t Training Loss: 48.98411884751431\n",
      "Epoch 6 \t Batch 880 \t Training Loss: 48.98994554172862\n",
      "Epoch 6 \t Batch 900 \t Training Loss: 49.00069598303901\n",
      "Epoch 6 \t Batch 20 \t Validation Loss: 18.891108655929564\n",
      "Epoch 6 \t Batch 40 \t Validation Loss: 21.03420250415802\n",
      "Epoch 6 \t Batch 60 \t Validation Loss: 21.292147302627562\n",
      "Epoch 6 \t Batch 80 \t Validation Loss: 22.094551956653596\n",
      "Epoch 6 \t Batch 100 \t Validation Loss: 22.97288229942322\n",
      "Epoch 6 \t Batch 120 \t Validation Loss: 24.037058424949645\n",
      "Epoch 6 \t Batch 140 \t Validation Loss: 24.49931880405971\n",
      "Epoch 6 \t Batch 160 \t Validation Loss: 26.48912332057953\n",
      "Epoch 6 \t Batch 180 \t Validation Loss: 29.614354123009576\n",
      "Epoch 6 \t Batch 200 \t Validation Loss: 31.043168430328368\n",
      "Epoch 6 \t Batch 220 \t Validation Loss: 32.3000826142051\n",
      "Epoch 6 \t Batch 240 \t Validation Loss: 32.66960649092992\n",
      "Epoch 6 \t Batch 260 \t Validation Loss: 34.701334447127124\n",
      "Epoch 6 \t Batch 280 \t Validation Loss: 35.817065943990436\n",
      "Epoch 6 \t Batch 300 \t Validation Loss: 36.72970510164897\n",
      "Epoch 6 \t Batch 320 \t Validation Loss: 37.137214604020116\n",
      "Epoch 6 \t Batch 340 \t Validation Loss: 37.14404302765341\n",
      "Epoch 6 \t Batch 360 \t Validation Loss: 37.05257600943248\n",
      "Epoch 6 \t Batch 380 \t Validation Loss: 37.35530783251712\n",
      "Epoch 6 \t Batch 400 \t Validation Loss: 37.05445325136185\n",
      "Epoch 6 \t Batch 420 \t Validation Loss: 37.167273864291964\n",
      "Epoch 6 \t Batch 440 \t Validation Loss: 36.94838193763386\n",
      "Epoch 6 \t Batch 460 \t Validation Loss: 37.20585663629615\n",
      "Epoch 6 \t Batch 480 \t Validation Loss: 37.72657612363498\n",
      "Epoch 6 \t Batch 500 \t Validation Loss: 37.480229459762576\n",
      "Epoch 6 \t Batch 520 \t Validation Loss: 37.27734368947836\n",
      "Epoch 6 \t Batch 540 \t Validation Loss: 37.05152939160665\n",
      "Epoch 6 \t Batch 560 \t Validation Loss: 36.91429904699326\n",
      "Epoch 6 \t Batch 580 \t Validation Loss: 36.78175068723744\n",
      "Epoch 6 \t Batch 600 \t Validation Loss: 37.01619807402293\n",
      "Epoch 6 Training Loss: 49.011221333444446 Validation Loss: 37.6888342724218\n",
      "Epoch 6 completed\n",
      "Epoch 7 \t Batch 20 \t Training Loss: 48.69656391143799\n",
      "Epoch 7 \t Batch 40 \t Training Loss: 48.49977502822876\n",
      "Epoch 7 \t Batch 60 \t Training Loss: 48.75353132883708\n",
      "Epoch 7 \t Batch 80 \t Training Loss: 48.74226846694946\n",
      "Epoch 7 \t Batch 100 \t Training Loss: 48.77975215911865\n",
      "Epoch 7 \t Batch 120 \t Training Loss: 48.69531799952189\n",
      "Epoch 7 \t Batch 140 \t Training Loss: 48.68821599142892\n",
      "Epoch 7 \t Batch 160 \t Training Loss: 48.69675316810608\n",
      "Epoch 7 \t Batch 180 \t Training Loss: 48.68167152404785\n",
      "Epoch 7 \t Batch 200 \t Training Loss: 48.54310977935791\n",
      "Epoch 7 \t Batch 220 \t Training Loss: 48.57821329290216\n",
      "Epoch 7 \t Batch 240 \t Training Loss: 48.599692757924394\n",
      "Epoch 7 \t Batch 260 \t Training Loss: 48.62127540294941\n",
      "Epoch 7 \t Batch 280 \t Training Loss: 48.687701443263464\n",
      "Epoch 7 \t Batch 300 \t Training Loss: 48.72578020731608\n",
      "Epoch 7 \t Batch 320 \t Training Loss: 48.68943303823471\n",
      "Epoch 7 \t Batch 340 \t Training Loss: 48.74229073243983\n",
      "Epoch 7 \t Batch 360 \t Training Loss: 48.71374013688829\n",
      "Epoch 7 \t Batch 380 \t Training Loss: 48.72494067141884\n",
      "Epoch 7 \t Batch 400 \t Training Loss: 48.72363063812256\n",
      "Epoch 7 \t Batch 420 \t Training Loss: 48.69538824898856\n",
      "Epoch 7 \t Batch 440 \t Training Loss: 48.73155482899059\n",
      "Epoch 7 \t Batch 460 \t Training Loss: 48.743597511623214\n",
      "Epoch 7 \t Batch 480 \t Training Loss: 48.789023383458456\n",
      "Epoch 7 \t Batch 500 \t Training Loss: 48.73443930053711\n",
      "Epoch 7 \t Batch 520 \t Training Loss: 48.743040715731105\n",
      "Epoch 7 \t Batch 540 \t Training Loss: 48.731383344862195\n",
      "Epoch 7 \t Batch 560 \t Training Loss: 48.73122910772051\n",
      "Epoch 7 \t Batch 580 \t Training Loss: 48.72589734833816\n",
      "Epoch 7 \t Batch 600 \t Training Loss: 48.73608828862508\n",
      "Epoch 7 \t Batch 620 \t Training Loss: 48.751258431711506\n",
      "Epoch 7 \t Batch 640 \t Training Loss: 48.768809801340105\n",
      "Epoch 7 \t Batch 660 \t Training Loss: 48.80991764068604\n",
      "Epoch 7 \t Batch 680 \t Training Loss: 48.82382572398466\n",
      "Epoch 7 \t Batch 700 \t Training Loss: 48.81022672925677\n",
      "Epoch 7 \t Batch 720 \t Training Loss: 48.75965379609002\n",
      "Epoch 7 \t Batch 740 \t Training Loss: 48.73785317395185\n",
      "Epoch 7 \t Batch 760 \t Training Loss: 48.73592182962518\n",
      "Epoch 7 \t Batch 780 \t Training Loss: 48.75360316741161\n",
      "Epoch 7 \t Batch 800 \t Training Loss: 48.776004643440245\n",
      "Epoch 7 \t Batch 820 \t Training Loss: 48.78801526325505\n",
      "Epoch 7 \t Batch 840 \t Training Loss: 48.771139812469485\n",
      "Epoch 7 \t Batch 860 \t Training Loss: 48.798222013961436\n",
      "Epoch 7 \t Batch 880 \t Training Loss: 48.810610537095506\n",
      "Epoch 7 \t Batch 900 \t Training Loss: 48.82816482543945\n",
      "Epoch 7 \t Batch 20 \t Validation Loss: 26.187445068359374\n",
      "Epoch 7 \t Batch 40 \t Validation Loss: 27.093010687828063\n",
      "Epoch 7 \t Batch 60 \t Validation Loss: 27.916803693771364\n",
      "Epoch 7 \t Batch 80 \t Validation Loss: 27.97636660337448\n",
      "Epoch 7 \t Batch 100 \t Validation Loss: 28.04297085762024\n",
      "Epoch 7 \t Batch 120 \t Validation Loss: 28.4184832016627\n",
      "Epoch 7 \t Batch 140 \t Validation Loss: 28.41055667740958\n",
      "Epoch 7 \t Batch 160 \t Validation Loss: 30.118400102853776\n",
      "Epoch 7 \t Batch 180 \t Validation Loss: 33.34307763311598\n",
      "Epoch 7 \t Batch 200 \t Validation Loss: 34.64049364566803\n",
      "Epoch 7 \t Batch 220 \t Validation Loss: 35.829239242727105\n",
      "Epoch 7 \t Batch 240 \t Validation Loss: 36.13483992020289\n",
      "Epoch 7 \t Batch 260 \t Validation Loss: 38.13586199100201\n",
      "Epoch 7 \t Batch 280 \t Validation Loss: 39.158402293069024\n",
      "Epoch 7 \t Batch 300 \t Validation Loss: 40.14558597564697\n",
      "Epoch 7 \t Batch 320 \t Validation Loss: 40.517531377077106\n",
      "Epoch 7 \t Batch 340 \t Validation Loss: 40.391220681807575\n",
      "Epoch 7 \t Batch 360 \t Validation Loss: 40.258320127593144\n",
      "Epoch 7 \t Batch 380 \t Validation Loss: 40.450696551172356\n",
      "Epoch 7 \t Batch 400 \t Validation Loss: 40.030118944644926\n",
      "Epoch 7 \t Batch 420 \t Validation Loss: 39.982654056094944\n",
      "Epoch 7 \t Batch 440 \t Validation Loss: 39.66631850112568\n",
      "Epoch 7 \t Batch 460 \t Validation Loss: 39.83726576307546\n",
      "Epoch 7 \t Batch 480 \t Validation Loss: 40.27722487250964\n",
      "Epoch 7 \t Batch 500 \t Validation Loss: 39.94137414741516\n",
      "Epoch 7 \t Batch 520 \t Validation Loss: 39.73486207998716\n",
      "Epoch 7 \t Batch 540 \t Validation Loss: 39.47709431295042\n",
      "Epoch 7 \t Batch 560 \t Validation Loss: 39.26441778285163\n",
      "Epoch 7 \t Batch 580 \t Validation Loss: 39.094549552325546\n",
      "Epoch 7 \t Batch 600 \t Validation Loss: 39.275778880119326\n",
      "Epoch 7 Training Loss: 48.85834296007063 Validation Loss: 39.90477619078252\n",
      "Epoch 7 completed\n",
      "Epoch 8 \t Batch 20 \t Training Loss: 48.56328983306885\n",
      "Epoch 8 \t Batch 40 \t Training Loss: 48.98389120101929\n",
      "Epoch 8 \t Batch 60 \t Training Loss: 49.45917485555013\n",
      "Epoch 8 \t Batch 80 \t Training Loss: 48.806512689590456\n",
      "Epoch 8 \t Batch 100 \t Training Loss: 48.81771148681641\n",
      "Epoch 8 \t Batch 120 \t Training Loss: 48.73657417297363\n",
      "Epoch 8 \t Batch 140 \t Training Loss: 48.767807606288365\n",
      "Epoch 8 \t Batch 160 \t Training Loss: 48.85875656604767\n",
      "Epoch 8 \t Batch 180 \t Training Loss: 48.94319727155897\n",
      "Epoch 8 \t Batch 200 \t Training Loss: 48.988058185577394\n",
      "Epoch 8 \t Batch 220 \t Training Loss: 49.075835384022106\n",
      "Epoch 8 \t Batch 240 \t Training Loss: 49.09563821156819\n",
      "Epoch 8 \t Batch 260 \t Training Loss: 49.12708354363075\n",
      "Epoch 8 \t Batch 280 \t Training Loss: 49.097417504446845\n",
      "Epoch 8 \t Batch 300 \t Training Loss: 48.94174617767334\n",
      "Epoch 8 \t Batch 320 \t Training Loss: 48.92942873239517\n",
      "Epoch 8 \t Batch 340 \t Training Loss: 48.81302486868466\n",
      "Epoch 8 \t Batch 360 \t Training Loss: 48.85219802856445\n",
      "Epoch 8 \t Batch 380 \t Training Loss: 48.81584681460732\n",
      "Epoch 8 \t Batch 400 \t Training Loss: 48.80710343360901\n",
      "Epoch 8 \t Batch 420 \t Training Loss: 48.77130440303257\n",
      "Epoch 8 \t Batch 440 \t Training Loss: 48.685180230574176\n",
      "Epoch 8 \t Batch 460 \t Training Loss: 48.66462361708931\n",
      "Epoch 8 \t Batch 480 \t Training Loss: 48.69045936266581\n",
      "Epoch 8 \t Batch 500 \t Training Loss: 48.61685237121582\n",
      "Epoch 8 \t Batch 520 \t Training Loss: 48.605187372060925\n",
      "Epoch 8 \t Batch 540 \t Training Loss: 48.60088646500199\n",
      "Epoch 8 \t Batch 560 \t Training Loss: 48.59151062965393\n",
      "Epoch 8 \t Batch 580 \t Training Loss: 48.59778479872079\n",
      "Epoch 8 \t Batch 600 \t Training Loss: 48.62437904993693\n",
      "Epoch 8 \t Batch 620 \t Training Loss: 48.58802236741589\n",
      "Epoch 8 \t Batch 640 \t Training Loss: 48.62443680763245\n",
      "Epoch 8 \t Batch 660 \t Training Loss: 48.63399861653646\n",
      "Epoch 8 \t Batch 680 \t Training Loss: 48.64863652621999\n",
      "Epoch 8 \t Batch 700 \t Training Loss: 48.63481676374163\n",
      "Epoch 8 \t Batch 720 \t Training Loss: 48.67093641493056\n",
      "Epoch 8 \t Batch 740 \t Training Loss: 48.66301042711412\n",
      "Epoch 8 \t Batch 760 \t Training Loss: 48.64142783817492\n",
      "Epoch 8 \t Batch 780 \t Training Loss: 48.64423463772505\n",
      "Epoch 8 \t Batch 800 \t Training Loss: 48.69134962081909\n",
      "Epoch 8 \t Batch 820 \t Training Loss: 48.715084890039954\n",
      "Epoch 8 \t Batch 840 \t Training Loss: 48.70071707225981\n",
      "Epoch 8 \t Batch 860 \t Training Loss: 48.68390219932379\n",
      "Epoch 8 \t Batch 880 \t Training Loss: 48.69889711466703\n",
      "Epoch 8 \t Batch 900 \t Training Loss: 48.70976097954644\n",
      "Epoch 8 \t Batch 20 \t Validation Loss: 19.907374572753906\n",
      "Epoch 8 \t Batch 40 \t Validation Loss: 21.934432888031004\n",
      "Epoch 8 \t Batch 60 \t Validation Loss: 22.144661124547323\n",
      "Epoch 8 \t Batch 80 \t Validation Loss: 22.710052406787874\n",
      "Epoch 8 \t Batch 100 \t Validation Loss: 23.463312978744508\n",
      "Epoch 8 \t Batch 120 \t Validation Loss: 24.499095344543456\n",
      "Epoch 8 \t Batch 140 \t Validation Loss: 24.9075888633728\n",
      "Epoch 8 \t Batch 160 \t Validation Loss: 27.015223908424378\n",
      "Epoch 8 \t Batch 180 \t Validation Loss: 30.542380650838215\n",
      "Epoch 8 \t Batch 200 \t Validation Loss: 32.03005602836609\n",
      "Epoch 8 \t Batch 220 \t Validation Loss: 33.478852488777854\n",
      "Epoch 8 \t Batch 240 \t Validation Loss: 33.97863204081853\n",
      "Epoch 8 \t Batch 260 \t Validation Loss: 36.138838544258704\n",
      "Epoch 8 \t Batch 280 \t Validation Loss: 37.32752125263214\n",
      "Epoch 8 \t Batch 300 \t Validation Loss: 38.40162340482076\n",
      "Epoch 8 \t Batch 320 \t Validation Loss: 38.88284117877483\n",
      "Epoch 8 \t Batch 340 \t Validation Loss: 38.84808217497433\n",
      "Epoch 8 \t Batch 360 \t Validation Loss: 38.76054980225033\n",
      "Epoch 8 \t Batch 380 \t Validation Loss: 39.035738254848276\n",
      "Epoch 8 \t Batch 400 \t Validation Loss: 38.66360548734665\n",
      "Epoch 8 \t Batch 420 \t Validation Loss: 38.69649826458522\n",
      "Epoch 8 \t Batch 440 \t Validation Loss: 38.422142993320115\n",
      "Epoch 8 \t Batch 460 \t Validation Loss: 38.631780779880025\n",
      "Epoch 8 \t Batch 480 \t Validation Loss: 39.10393080910047\n",
      "Epoch 8 \t Batch 500 \t Validation Loss: 38.82776828575134\n",
      "Epoch 8 \t Batch 520 \t Validation Loss: 38.60441952485304\n",
      "Epoch 8 \t Batch 540 \t Validation Loss: 38.312261259997335\n",
      "Epoch 8 \t Batch 560 \t Validation Loss: 38.08983963217054\n",
      "Epoch 8 \t Batch 580 \t Validation Loss: 37.85162710321361\n",
      "Epoch 8 \t Batch 600 \t Validation Loss: 38.02862913767497\n",
      "Epoch 8 Training Loss: 48.72185631588735 Validation Loss: 38.63878604034325\n",
      "Epoch 8 completed\n",
      "Epoch 9 \t Batch 20 \t Training Loss: 48.58048839569092\n",
      "Epoch 9 \t Batch 40 \t Training Loss: 48.83626365661621\n",
      "Epoch 9 \t Batch 60 \t Training Loss: 48.14904441833496\n",
      "Epoch 9 \t Batch 80 \t Training Loss: 48.201851797103885\n",
      "Epoch 9 \t Batch 100 \t Training Loss: 47.87767444610596\n",
      "Epoch 9 \t Batch 120 \t Training Loss: 48.27282346089681\n",
      "Epoch 9 \t Batch 140 \t Training Loss: 48.49385534014021\n",
      "Epoch 9 \t Batch 160 \t Training Loss: 48.390478658676145\n",
      "Epoch 9 \t Batch 180 \t Training Loss: 48.50861246320937\n",
      "Epoch 9 \t Batch 200 \t Training Loss: 48.69388792037964\n",
      "Epoch 9 \t Batch 220 \t Training Loss: 48.91186932650479\n",
      "Epoch 9 \t Batch 240 \t Training Loss: 48.933515548706055\n",
      "Epoch 9 \t Batch 260 \t Training Loss: 48.83203667860765\n",
      "Epoch 9 \t Batch 280 \t Training Loss: 48.80424690246582\n",
      "Epoch 9 \t Batch 300 \t Training Loss: 48.831523729960125\n",
      "Epoch 9 \t Batch 320 \t Training Loss: 48.811368787288664\n",
      "Epoch 9 \t Batch 340 \t Training Loss: 48.79859034594367\n",
      "Epoch 9 \t Batch 360 \t Training Loss: 48.802203475104434\n",
      "Epoch 9 \t Batch 380 \t Training Loss: 48.73711156343159\n",
      "Epoch 9 \t Batch 400 \t Training Loss: 48.8133696269989\n",
      "Epoch 9 \t Batch 420 \t Training Loss: 48.80074307577951\n",
      "Epoch 9 \t Batch 440 \t Training Loss: 48.8045242396268\n",
      "Epoch 9 \t Batch 460 \t Training Loss: 48.76397340194039\n",
      "Epoch 9 \t Batch 480 \t Training Loss: 48.70651883284251\n",
      "Epoch 9 \t Batch 500 \t Training Loss: 48.73842029571533\n",
      "Epoch 9 \t Batch 520 \t Training Loss: 48.743596546466534\n",
      "Epoch 9 \t Batch 540 \t Training Loss: 48.796745194329155\n",
      "Epoch 9 \t Batch 560 \t Training Loss: 48.79471917833601\n",
      "Epoch 9 \t Batch 580 \t Training Loss: 48.79035294631432\n",
      "Epoch 9 \t Batch 600 \t Training Loss: 48.76732196172078\n",
      "Epoch 9 \t Batch 620 \t Training Loss: 48.729562070292815\n",
      "Epoch 9 \t Batch 640 \t Training Loss: 48.71331552267075\n",
      "Epoch 9 \t Batch 660 \t Training Loss: 48.70728860334916\n",
      "Epoch 9 \t Batch 680 \t Training Loss: 48.7434580634622\n",
      "Epoch 9 \t Batch 700 \t Training Loss: 48.78267763410296\n",
      "Epoch 9 \t Batch 720 \t Training Loss: 48.763598346710204\n",
      "Epoch 9 \t Batch 740 \t Training Loss: 48.78560248194514\n",
      "Epoch 9 \t Batch 760 \t Training Loss: 48.79802555786936\n",
      "Epoch 9 \t Batch 780 \t Training Loss: 48.79243805225079\n",
      "Epoch 9 \t Batch 800 \t Training Loss: 48.8201629447937\n",
      "Epoch 9 \t Batch 820 \t Training Loss: 48.80269737708859\n",
      "Epoch 9 \t Batch 840 \t Training Loss: 48.73313838413784\n",
      "Epoch 9 \t Batch 860 \t Training Loss: 48.735794475466705\n",
      "Epoch 9 \t Batch 880 \t Training Loss: 48.665093153173274\n",
      "Epoch 9 \t Batch 900 \t Training Loss: 48.642902776930065\n",
      "Epoch 9 \t Batch 20 \t Validation Loss: 31.699079608917238\n",
      "Epoch 9 \t Batch 40 \t Validation Loss: 30.551040649414062\n",
      "Epoch 9 \t Batch 60 \t Validation Loss: 31.933695395787556\n",
      "Epoch 9 \t Batch 80 \t Validation Loss: 31.776054120063783\n",
      "Epoch 9 \t Batch 100 \t Validation Loss: 30.777845706939697\n",
      "Epoch 9 \t Batch 120 \t Validation Loss: 30.540073998769124\n",
      "Epoch 9 \t Batch 140 \t Validation Loss: 30.111576672962734\n",
      "Epoch 9 \t Batch 160 \t Validation Loss: 31.539044386148454\n",
      "Epoch 9 \t Batch 180 \t Validation Loss: 34.59630820486281\n",
      "Epoch 9 \t Batch 200 \t Validation Loss: 35.65900395870209\n",
      "Epoch 9 \t Batch 220 \t Validation Loss: 36.7519725669514\n",
      "Epoch 9 \t Batch 240 \t Validation Loss: 36.978289465109505\n",
      "Epoch 9 \t Batch 260 \t Validation Loss: 38.87391003095187\n",
      "Epoch 9 \t Batch 280 \t Validation Loss: 39.7819425548826\n",
      "Epoch 9 \t Batch 300 \t Validation Loss: 40.71395265261332\n",
      "Epoch 9 \t Batch 320 \t Validation Loss: 41.05573903620243\n",
      "Epoch 9 \t Batch 340 \t Validation Loss: 40.877054065816544\n",
      "Epoch 9 \t Batch 360 \t Validation Loss: 40.67986302110884\n",
      "Epoch 9 \t Batch 380 \t Validation Loss: 40.812785693218835\n",
      "Epoch 9 \t Batch 400 \t Validation Loss: 40.31982983827591\n",
      "Epoch 9 \t Batch 420 \t Validation Loss: 40.24311124483744\n",
      "Epoch 9 \t Batch 440 \t Validation Loss: 39.85715387301011\n",
      "Epoch 9 \t Batch 460 \t Validation Loss: 39.99452162410902\n",
      "Epoch 9 \t Batch 480 \t Validation Loss: 40.40420855085055\n",
      "Epoch 9 \t Batch 500 \t Validation Loss: 40.060591161727906\n",
      "Epoch 9 \t Batch 520 \t Validation Loss: 39.76181436868814\n",
      "Epoch 9 \t Batch 540 \t Validation Loss: 39.4243580200054\n",
      "Epoch 9 \t Batch 560 \t Validation Loss: 39.13804056814739\n",
      "Epoch 9 \t Batch 580 \t Validation Loss: 38.85187690504666\n",
      "Epoch 9 \t Batch 600 \t Validation Loss: 38.99539203484853\n",
      "Epoch 9 Training Loss: 48.62256757513647 Validation Loss: 39.569933765894405\n",
      "Epoch 9 completed\n",
      "Epoch 10 \t Batch 20 \t Training Loss: 48.694142532348636\n",
      "Epoch 10 \t Batch 40 \t Training Loss: 48.52541990280152\n",
      "Epoch 10 \t Batch 60 \t Training Loss: 48.30848477681478\n",
      "Epoch 10 \t Batch 80 \t Training Loss: 48.7861147403717\n",
      "Epoch 10 \t Batch 100 \t Training Loss: 48.76507480621338\n",
      "Epoch 10 \t Batch 120 \t Training Loss: 48.8030956586202\n",
      "Epoch 10 \t Batch 140 \t Training Loss: 48.45568049294608\n",
      "Epoch 10 \t Batch 160 \t Training Loss: 48.46234457492828\n",
      "Epoch 10 \t Batch 180 \t Training Loss: 48.43642260233561\n",
      "Epoch 10 \t Batch 200 \t Training Loss: 48.39431489944458\n",
      "Epoch 10 \t Batch 220 \t Training Loss: 48.41738381819292\n",
      "Epoch 10 \t Batch 240 \t Training Loss: 48.47688366572062\n",
      "Epoch 10 \t Batch 260 \t Training Loss: 48.41282094808725\n",
      "Epoch 10 \t Batch 280 \t Training Loss: 48.39489003590175\n",
      "Epoch 10 \t Batch 300 \t Training Loss: 48.327870483398435\n",
      "Epoch 10 \t Batch 320 \t Training Loss: 48.3747705578804\n",
      "Epoch 10 \t Batch 340 \t Training Loss: 48.43367866067325\n",
      "Epoch 10 \t Batch 360 \t Training Loss: 48.48206624984741\n",
      "Epoch 10 \t Batch 380 \t Training Loss: 48.46265269831607\n",
      "Epoch 10 \t Batch 400 \t Training Loss: 48.503483057022095\n",
      "Epoch 10 \t Batch 420 \t Training Loss: 48.527228246416364\n",
      "Epoch 10 \t Batch 440 \t Training Loss: 48.469946445118296\n",
      "Epoch 10 \t Batch 460 \t Training Loss: 48.49039116735044\n",
      "Epoch 10 \t Batch 480 \t Training Loss: 48.49812973340352\n",
      "Epoch 10 \t Batch 500 \t Training Loss: 48.45279188537598\n",
      "Epoch 10 \t Batch 520 \t Training Loss: 48.40195576594426\n",
      "Epoch 10 \t Batch 540 \t Training Loss: 48.430858548482256\n",
      "Epoch 10 \t Batch 560 \t Training Loss: 48.434944949831284\n",
      "Epoch 10 \t Batch 580 \t Training Loss: 48.4652884647764\n",
      "Epoch 10 \t Batch 600 \t Training Loss: 48.45530610402425\n",
      "Epoch 10 \t Batch 620 \t Training Loss: 48.432494077374855\n",
      "Epoch 10 \t Batch 640 \t Training Loss: 48.43170484304428\n",
      "Epoch 10 \t Batch 660 \t Training Loss: 48.44117250153513\n",
      "Epoch 10 \t Batch 680 \t Training Loss: 48.44255548925961\n",
      "Epoch 10 \t Batch 700 \t Training Loss: 48.481679431370324\n",
      "Epoch 10 \t Batch 720 \t Training Loss: 48.50673936208089\n",
      "Epoch 10 \t Batch 740 \t Training Loss: 48.54135989627323\n",
      "Epoch 10 \t Batch 760 \t Training Loss: 48.55303352757504\n",
      "Epoch 10 \t Batch 780 \t Training Loss: 48.55416321387658\n",
      "Epoch 10 \t Batch 800 \t Training Loss: 48.5459556055069\n",
      "Epoch 10 \t Batch 820 \t Training Loss: 48.541371508342465\n",
      "Epoch 10 \t Batch 840 \t Training Loss: 48.55729559489659\n",
      "Epoch 10 \t Batch 860 \t Training Loss: 48.55456156619759\n",
      "Epoch 10 \t Batch 880 \t Training Loss: 48.54098052545027\n",
      "Epoch 10 \t Batch 900 \t Training Loss: 48.54700158860948\n",
      "Epoch 10 \t Batch 20 \t Validation Loss: 24.517983627319335\n",
      "Epoch 10 \t Batch 40 \t Validation Loss: 25.83105217218399\n",
      "Epoch 10 \t Batch 60 \t Validation Loss: 26.36772073904673\n",
      "Epoch 10 \t Batch 80 \t Validation Loss: 26.522646313905717\n",
      "Epoch 10 \t Batch 100 \t Validation Loss: 26.558582367897035\n",
      "Epoch 10 \t Batch 120 \t Validation Loss: 26.983725837866466\n",
      "Epoch 10 \t Batch 140 \t Validation Loss: 27.03237498487745\n",
      "Epoch 10 \t Batch 160 \t Validation Loss: 28.80972869694233\n",
      "Epoch 10 \t Batch 180 \t Validation Loss: 32.055151671833464\n",
      "Epoch 10 \t Batch 200 \t Validation Loss: 33.37949471235275\n",
      "Epoch 10 \t Batch 220 \t Validation Loss: 34.64200146198273\n",
      "Epoch 10 \t Batch 240 \t Validation Loss: 34.98056182265282\n",
      "Epoch 10 \t Batch 260 \t Validation Loss: 37.049551774905275\n",
      "Epoch 10 \t Batch 280 \t Validation Loss: 38.16367353882109\n",
      "Epoch 10 \t Batch 300 \t Validation Loss: 39.12205206076304\n",
      "Epoch 10 \t Batch 320 \t Validation Loss: 39.519264586269855\n",
      "Epoch 10 \t Batch 340 \t Validation Loss: 39.434181663569284\n",
      "Epoch 10 \t Batch 360 \t Validation Loss: 39.28247547811932\n",
      "Epoch 10 \t Batch 380 \t Validation Loss: 39.50533251134973\n",
      "Epoch 10 \t Batch 400 \t Validation Loss: 39.05765870690346\n",
      "Epoch 10 \t Batch 420 \t Validation Loss: 39.03664065883273\n",
      "Epoch 10 \t Batch 440 \t Validation Loss: 38.69709588506005\n",
      "Epoch 10 \t Batch 460 \t Validation Loss: 38.8575741653857\n",
      "Epoch 10 \t Batch 480 \t Validation Loss: 39.307562443614\n",
      "Epoch 10 \t Batch 500 \t Validation Loss: 38.999713219642636\n",
      "Epoch 10 \t Batch 520 \t Validation Loss: 38.718068008239456\n",
      "Epoch 10 \t Batch 540 \t Validation Loss: 38.43819335213414\n",
      "Epoch 10 \t Batch 560 \t Validation Loss: 38.248676823718206\n",
      "Epoch 10 \t Batch 580 \t Validation Loss: 38.091566604581374\n",
      "Epoch 10 \t Batch 600 \t Validation Loss: 38.28181965112686\n",
      "Epoch 10 Training Loss: 48.513025876609944 Validation Loss: 38.889343078260296\n",
      "Epoch 10 completed\n",
      "Epoch 11 \t Batch 20 \t Training Loss: 47.479105186462405\n",
      "Epoch 11 \t Batch 40 \t Training Loss: 48.89282054901123\n",
      "Epoch 11 \t Batch 60 \t Training Loss: 48.60466505686442\n",
      "Epoch 11 \t Batch 80 \t Training Loss: 48.79239444732666\n",
      "Epoch 11 \t Batch 100 \t Training Loss: 48.897755813598636\n",
      "Epoch 11 \t Batch 120 \t Training Loss: 48.94202499389648\n",
      "Epoch 11 \t Batch 140 \t Training Loss: 49.127968651907786\n",
      "Epoch 11 \t Batch 160 \t Training Loss: 48.93887915611267\n",
      "Epoch 11 \t Batch 180 \t Training Loss: 48.767969597710504\n",
      "Epoch 11 \t Batch 200 \t Training Loss: 48.88243465423584\n",
      "Epoch 11 \t Batch 220 \t Training Loss: 48.935319848494096\n",
      "Epoch 11 \t Batch 240 \t Training Loss: 48.805328893661496\n",
      "Epoch 11 \t Batch 260 \t Training Loss: 48.85869147960956\n",
      "Epoch 11 \t Batch 280 \t Training Loss: 48.761748150416786\n",
      "Epoch 11 \t Batch 300 \t Training Loss: 48.87840178171793\n",
      "Epoch 11 \t Batch 320 \t Training Loss: 48.772482299804686\n",
      "Epoch 11 \t Batch 340 \t Training Loss: 48.6452153822955\n",
      "Epoch 11 \t Batch 360 \t Training Loss: 48.54163096745809\n",
      "Epoch 11 \t Batch 380 \t Training Loss: 48.49959227913305\n",
      "Epoch 11 \t Batch 400 \t Training Loss: 48.531596956253054\n",
      "Epoch 11 \t Batch 420 \t Training Loss: 48.532819439115976\n",
      "Epoch 11 \t Batch 440 \t Training Loss: 48.4840691132979\n",
      "Epoch 11 \t Batch 460 \t Training Loss: 48.36781561478325\n",
      "Epoch 11 \t Batch 480 \t Training Loss: 48.29660652478536\n",
      "Epoch 11 \t Batch 500 \t Training Loss: 48.268222145080564\n",
      "Epoch 11 \t Batch 520 \t Training Loss: 48.2543344937838\n",
      "Epoch 11 \t Batch 540 \t Training Loss: 48.263985089902526\n",
      "Epoch 11 \t Batch 560 \t Training Loss: 48.292712572642735\n",
      "Epoch 11 \t Batch 580 \t Training Loss: 48.349802418412835\n",
      "Epoch 11 \t Batch 600 \t Training Loss: 48.32040277481079\n",
      "Epoch 11 \t Batch 620 \t Training Loss: 48.33886830729823\n",
      "Epoch 11 \t Batch 640 \t Training Loss: 48.29299558401108\n",
      "Epoch 11 \t Batch 660 \t Training Loss: 48.35950142253529\n",
      "Epoch 11 \t Batch 680 \t Training Loss: 48.37099607131061\n",
      "Epoch 11 \t Batch 700 \t Training Loss: 48.370830399649485\n",
      "Epoch 11 \t Batch 720 \t Training Loss: 48.36829153696696\n",
      "Epoch 11 \t Batch 740 \t Training Loss: 48.37297978787809\n",
      "Epoch 11 \t Batch 760 \t Training Loss: 48.379377877084835\n",
      "Epoch 11 \t Batch 780 \t Training Loss: 48.41357349004501\n",
      "Epoch 11 \t Batch 800 \t Training Loss: 48.37712858200073\n",
      "Epoch 11 \t Batch 820 \t Training Loss: 48.37781557222692\n",
      "Epoch 11 \t Batch 840 \t Training Loss: 48.386228397914344\n",
      "Epoch 11 \t Batch 860 \t Training Loss: 48.387249715938125\n",
      "Epoch 11 \t Batch 880 \t Training Loss: 48.39674108245156\n",
      "Epoch 11 \t Batch 900 \t Training Loss: 48.43193707360162\n",
      "Epoch 11 \t Batch 20 \t Validation Loss: 23.463914918899537\n",
      "Epoch 11 \t Batch 40 \t Validation Loss: 23.58611719608307\n",
      "Epoch 11 \t Batch 60 \t Validation Loss: 24.79402979214986\n",
      "Epoch 11 \t Batch 80 \t Validation Loss: 25.005870032310487\n",
      "Epoch 11 \t Batch 100 \t Validation Loss: 25.17583861351013\n",
      "Epoch 11 \t Batch 120 \t Validation Loss: 25.776675868034364\n",
      "Epoch 11 \t Batch 140 \t Validation Loss: 25.96600224631173\n",
      "Epoch 11 \t Batch 160 \t Validation Loss: 27.993600487709045\n",
      "Epoch 11 \t Batch 180 \t Validation Loss: 31.426606962415907\n",
      "Epoch 11 \t Batch 200 \t Validation Loss: 32.867549629211425\n",
      "Epoch 11 \t Batch 220 \t Validation Loss: 34.24607046300714\n",
      "Epoch 11 \t Batch 240 \t Validation Loss: 34.68525127967199\n",
      "Epoch 11 \t Batch 260 \t Validation Loss: 36.80615589802082\n",
      "Epoch 11 \t Batch 280 \t Validation Loss: 37.94134210518428\n",
      "Epoch 11 \t Batch 300 \t Validation Loss: 38.97879325548808\n",
      "Epoch 11 \t Batch 320 \t Validation Loss: 39.432902929186824\n",
      "Epoch 11 \t Batch 340 \t Validation Loss: 39.38019751660964\n",
      "Epoch 11 \t Batch 360 \t Validation Loss: 39.26453858216603\n",
      "Epoch 11 \t Batch 380 \t Validation Loss: 39.50022223623176\n",
      "Epoch 11 \t Batch 400 \t Validation Loss: 39.087120113372805\n",
      "Epoch 11 \t Batch 420 \t Validation Loss: 39.11476208368937\n",
      "Epoch 11 \t Batch 440 \t Validation Loss: 38.814978066357696\n",
      "Epoch 11 \t Batch 460 \t Validation Loss: 38.99104396571284\n",
      "Epoch 11 \t Batch 480 \t Validation Loss: 39.480647734800975\n",
      "Epoch 11 \t Batch 500 \t Validation Loss: 39.21079172897339\n",
      "Epoch 11 \t Batch 520 \t Validation Loss: 38.93664600298955\n",
      "Epoch 11 \t Batch 540 \t Validation Loss: 38.65534882368865\n",
      "Epoch 11 \t Batch 560 \t Validation Loss: 38.4554539510182\n",
      "Epoch 11 \t Batch 580 \t Validation Loss: 38.25508852662711\n",
      "Epoch 11 \t Batch 600 \t Validation Loss: 38.437495412826536\n",
      "Epoch 11 Training Loss: 48.42180818247821 Validation Loss: 39.06495283795642\n",
      "Epoch 11 completed\n",
      "Epoch 12 \t Batch 20 \t Training Loss: 46.58959331512451\n",
      "Epoch 12 \t Batch 40 \t Training Loss: 47.086593341827395\n",
      "Epoch 12 \t Batch 60 \t Training Loss: 47.87526969909668\n",
      "Epoch 12 \t Batch 80 \t Training Loss: 47.83680543899536\n",
      "Epoch 12 \t Batch 100 \t Training Loss: 48.14748996734619\n",
      "Epoch 12 \t Batch 120 \t Training Loss: 48.20273227691651\n",
      "Epoch 12 \t Batch 140 \t Training Loss: 48.2734285899571\n",
      "Epoch 12 \t Batch 160 \t Training Loss: 48.255262756347655\n",
      "Epoch 12 \t Batch 180 \t Training Loss: 48.347877248128256\n",
      "Epoch 12 \t Batch 200 \t Training Loss: 48.47892580032349\n",
      "Epoch 12 \t Batch 220 \t Training Loss: 48.38615131378174\n",
      "Epoch 12 \t Batch 240 \t Training Loss: 48.38018930753072\n",
      "Epoch 12 \t Batch 260 \t Training Loss: 48.30045149876521\n",
      "Epoch 12 \t Batch 280 \t Training Loss: 48.352931662968224\n",
      "Epoch 12 \t Batch 300 \t Training Loss: 48.32779396057129\n",
      "Epoch 12 \t Batch 320 \t Training Loss: 48.33696695566177\n",
      "Epoch 12 \t Batch 340 \t Training Loss: 48.279496361227594\n",
      "Epoch 12 \t Batch 360 \t Training Loss: 48.28491337034438\n",
      "Epoch 12 \t Batch 380 \t Training Loss: 48.31363876744321\n",
      "Epoch 12 \t Batch 400 \t Training Loss: 48.32285262107849\n",
      "Epoch 12 \t Batch 420 \t Training Loss: 48.31862750280471\n",
      "Epoch 12 \t Batch 440 \t Training Loss: 48.31856886256825\n",
      "Epoch 12 \t Batch 460 \t Training Loss: 48.32343938247017\n",
      "Epoch 12 \t Batch 480 \t Training Loss: 48.32159039974213\n",
      "Epoch 12 \t Batch 500 \t Training Loss: 48.371242942810056\n",
      "Epoch 12 \t Batch 520 \t Training Loss: 48.37330322265625\n",
      "Epoch 12 \t Batch 540 \t Training Loss: 48.379098072758424\n",
      "Epoch 12 \t Batch 560 \t Training Loss: 48.305921936035155\n",
      "Epoch 12 \t Batch 580 \t Training Loss: 48.29569821851007\n",
      "Epoch 12 \t Batch 600 \t Training Loss: 48.303632494608564\n",
      "Epoch 12 \t Batch 620 \t Training Loss: 48.344297932040305\n",
      "Epoch 12 \t Batch 640 \t Training Loss: 48.35961292982101\n",
      "Epoch 12 \t Batch 660 \t Training Loss: 48.35231481031938\n",
      "Epoch 12 \t Batch 680 \t Training Loss: 48.31779644349042\n",
      "Epoch 12 \t Batch 700 \t Training Loss: 48.31749839237758\n",
      "Epoch 12 \t Batch 720 \t Training Loss: 48.34404623773363\n",
      "Epoch 12 \t Batch 740 \t Training Loss: 48.32182703275938\n",
      "Epoch 12 \t Batch 760 \t Training Loss: 48.33455664986058\n",
      "Epoch 12 \t Batch 780 \t Training Loss: 48.313795608129254\n",
      "Epoch 12 \t Batch 800 \t Training Loss: 48.335895819664\n",
      "Epoch 12 \t Batch 820 \t Training Loss: 48.34982121630413\n",
      "Epoch 12 \t Batch 840 \t Training Loss: 48.31989501317342\n",
      "Epoch 12 \t Batch 860 \t Training Loss: 48.33729457411655\n",
      "Epoch 12 \t Batch 880 \t Training Loss: 48.353032849051736\n",
      "Epoch 12 \t Batch 900 \t Training Loss: 48.3470187081231\n",
      "Epoch 12 \t Batch 20 \t Validation Loss: 26.970670533180236\n",
      "Epoch 12 \t Batch 40 \t Validation Loss: 27.231450080871582\n",
      "Epoch 12 \t Batch 60 \t Validation Loss: 28.297604179382326\n",
      "Epoch 12 \t Batch 80 \t Validation Loss: 28.336558938026428\n",
      "Epoch 12 \t Batch 100 \t Validation Loss: 28.074933452606203\n",
      "Epoch 12 \t Batch 120 \t Validation Loss: 28.297071901957192\n",
      "Epoch 12 \t Batch 140 \t Validation Loss: 28.22639423097883\n",
      "Epoch 12 \t Batch 160 \t Validation Loss: 29.95544567108154\n",
      "Epoch 12 \t Batch 180 \t Validation Loss: 33.10318770938449\n",
      "Epoch 12 \t Batch 200 \t Validation Loss: 34.33897957801819\n",
      "Epoch 12 \t Batch 220 \t Validation Loss: 35.515806354175915\n",
      "Epoch 12 \t Batch 240 \t Validation Loss: 35.79423620700836\n",
      "Epoch 12 \t Batch 260 \t Validation Loss: 37.784003037672775\n",
      "Epoch 12 \t Batch 280 \t Validation Loss: 38.82766400064741\n",
      "Epoch 12 \t Batch 300 \t Validation Loss: 39.7603725306193\n",
      "Epoch 12 \t Batch 320 \t Validation Loss: 40.142592430114746\n",
      "Epoch 12 \t Batch 340 \t Validation Loss: 40.051243490331316\n",
      "Epoch 12 \t Batch 360 \t Validation Loss: 39.909638338618805\n",
      "Epoch 12 \t Batch 380 \t Validation Loss: 40.13238155465377\n",
      "Epoch 12 \t Batch 400 \t Validation Loss: 39.729745359420775\n",
      "Epoch 12 \t Batch 420 \t Validation Loss: 39.72730033511207\n",
      "Epoch 12 \t Batch 440 \t Validation Loss: 39.44279006177729\n",
      "Epoch 12 \t Batch 460 \t Validation Loss: 39.62077913698943\n",
      "Epoch 12 \t Batch 480 \t Validation Loss: 40.073623808224994\n",
      "Epoch 12 \t Batch 500 \t Validation Loss: 39.76642111587525\n",
      "Epoch 12 \t Batch 520 \t Validation Loss: 39.53016598591438\n",
      "Epoch 12 \t Batch 540 \t Validation Loss: 39.32996249905339\n",
      "Epoch 12 \t Batch 560 \t Validation Loss: 39.22637565646853\n",
      "Epoch 12 \t Batch 580 \t Validation Loss: 39.10469851822689\n",
      "Epoch 12 \t Batch 600 \t Validation Loss: 39.353530265490214\n",
      "Epoch 12 Training Loss: 48.345046793231525 Validation Loss: 39.980365296462914\n",
      "Epoch 12 completed\n",
      "Epoch 13 \t Batch 20 \t Training Loss: 46.21946640014649\n",
      "Epoch 13 \t Batch 40 \t Training Loss: 47.31094398498535\n",
      "Epoch 13 \t Batch 60 \t Training Loss: 47.535285313924156\n",
      "Epoch 13 \t Batch 80 \t Training Loss: 47.36746506690979\n",
      "Epoch 13 \t Batch 100 \t Training Loss: 47.403689041137696\n",
      "Epoch 13 \t Batch 120 \t Training Loss: 47.51047019958496\n",
      "Epoch 13 \t Batch 140 \t Training Loss: 48.01289212363107\n",
      "Epoch 13 \t Batch 160 \t Training Loss: 48.1223783493042\n",
      "Epoch 13 \t Batch 180 \t Training Loss: 48.07596607208252\n",
      "Epoch 13 \t Batch 200 \t Training Loss: 48.09346305847168\n",
      "Epoch 13 \t Batch 220 \t Training Loss: 47.99844582297585\n",
      "Epoch 13 \t Batch 240 \t Training Loss: 48.0722475528717\n",
      "Epoch 13 \t Batch 260 \t Training Loss: 48.14738775400015\n",
      "Epoch 13 \t Batch 280 \t Training Loss: 48.16150323322841\n",
      "Epoch 13 \t Batch 300 \t Training Loss: 48.21230285644531\n",
      "Epoch 13 \t Batch 320 \t Training Loss: 48.20060120820999\n",
      "Epoch 13 \t Batch 340 \t Training Loss: 48.195057667002956\n",
      "Epoch 13 \t Batch 360 \t Training Loss: 48.208145798577206\n",
      "Epoch 13 \t Batch 380 \t Training Loss: 48.222527503967285\n",
      "Epoch 13 \t Batch 400 \t Training Loss: 48.2455837726593\n",
      "Epoch 13 \t Batch 420 \t Training Loss: 48.26007536025274\n",
      "Epoch 13 \t Batch 440 \t Training Loss: 48.2902924971147\n",
      "Epoch 13 \t Batch 460 \t Training Loss: 48.223136794048806\n",
      "Epoch 13 \t Batch 480 \t Training Loss: 48.21311519940694\n",
      "Epoch 13 \t Batch 500 \t Training Loss: 48.240187644958496\n",
      "Epoch 13 \t Batch 520 \t Training Loss: 48.24603501833402\n",
      "Epoch 13 \t Batch 540 \t Training Loss: 48.25332818914343\n",
      "Epoch 13 \t Batch 560 \t Training Loss: 48.21315225873675\n",
      "Epoch 13 \t Batch 580 \t Training Loss: 48.20186433463261\n",
      "Epoch 13 \t Batch 600 \t Training Loss: 48.21206064860026\n",
      "Epoch 13 \t Batch 620 \t Training Loss: 48.24227779757592\n",
      "Epoch 13 \t Batch 640 \t Training Loss: 48.251621222496034\n",
      "Epoch 13 \t Batch 660 \t Training Loss: 48.21776777903239\n",
      "Epoch 13 \t Batch 680 \t Training Loss: 48.21851700614481\n",
      "Epoch 13 \t Batch 700 \t Training Loss: 48.25161378043038\n",
      "Epoch 13 \t Batch 720 \t Training Loss: 48.31435597207811\n",
      "Epoch 13 \t Batch 740 \t Training Loss: 48.316771883577914\n",
      "Epoch 13 \t Batch 760 \t Training Loss: 48.32311341135125\n",
      "Epoch 13 \t Batch 780 \t Training Loss: 48.32192340753017\n",
      "Epoch 13 \t Batch 800 \t Training Loss: 48.30320999145508\n",
      "Epoch 13 \t Batch 820 \t Training Loss: 48.35794540963522\n",
      "Epoch 13 \t Batch 840 \t Training Loss: 48.30311160768781\n",
      "Epoch 13 \t Batch 860 \t Training Loss: 48.265708053943726\n",
      "Epoch 13 \t Batch 880 \t Training Loss: 48.303728528456254\n",
      "Epoch 13 \t Batch 900 \t Training Loss: 48.290607914394805\n",
      "Epoch 13 \t Batch 20 \t Validation Loss: 33.11357860565185\n",
      "Epoch 13 \t Batch 40 \t Validation Loss: 31.49722284078598\n",
      "Epoch 13 \t Batch 60 \t Validation Loss: 33.222849345207216\n",
      "Epoch 13 \t Batch 80 \t Validation Loss: 32.81538534760475\n",
      "Epoch 13 \t Batch 100 \t Validation Loss: 31.630634789466857\n",
      "Epoch 13 \t Batch 120 \t Validation Loss: 31.176890925566354\n",
      "Epoch 13 \t Batch 140 \t Validation Loss: 30.62370914391109\n",
      "Epoch 13 \t Batch 160 \t Validation Loss: 31.957778665423394\n",
      "Epoch 13 \t Batch 180 \t Validation Loss: 34.765694891081914\n",
      "Epoch 13 \t Batch 200 \t Validation Loss: 35.750219161510465\n",
      "Epoch 13 \t Batch 220 \t Validation Loss: 36.7547865932638\n",
      "Epoch 13 \t Batch 240 \t Validation Loss: 36.902441209554674\n",
      "Epoch 13 \t Batch 260 \t Validation Loss: 38.76555343958048\n",
      "Epoch 13 \t Batch 280 \t Validation Loss: 39.67391944783075\n",
      "Epoch 13 \t Batch 300 \t Validation Loss: 40.47018823782603\n",
      "Epoch 13 \t Batch 320 \t Validation Loss: 40.76183352023363\n",
      "Epoch 13 \t Batch 340 \t Validation Loss: 40.58950458554661\n",
      "Epoch 13 \t Batch 360 \t Validation Loss: 40.37283335659239\n",
      "Epoch 13 \t Batch 380 \t Validation Loss: 40.541811309362714\n",
      "Epoch 13 \t Batch 400 \t Validation Loss: 40.08273633360863\n",
      "Epoch 13 \t Batch 420 \t Validation Loss: 40.022330560003006\n",
      "Epoch 13 \t Batch 440 \t Validation Loss: 39.681002685156734\n",
      "Epoch 13 \t Batch 460 \t Validation Loss: 39.83650866280431\n",
      "Epoch 13 \t Batch 480 \t Validation Loss: 40.2444139311711\n",
      "Epoch 13 \t Batch 500 \t Validation Loss: 39.89489272975921\n",
      "Epoch 13 \t Batch 520 \t Validation Loss: 39.63916731522634\n",
      "Epoch 13 \t Batch 540 \t Validation Loss: 39.328960884058915\n",
      "Epoch 13 \t Batch 560 \t Validation Loss: 39.058063339335575\n",
      "Epoch 13 \t Batch 580 \t Validation Loss: 38.79373825254112\n",
      "Epoch 13 \t Batch 600 \t Validation Loss: 38.950683634281155\n",
      "Epoch 13 Training Loss: 48.25405908332916 Validation Loss: 39.556868905370884\n",
      "Epoch 13 completed\n",
      "Epoch 14 \t Batch 20 \t Training Loss: 49.12806396484375\n",
      "Epoch 14 \t Batch 40 \t Training Loss: 48.8475739479065\n",
      "Epoch 14 \t Batch 60 \t Training Loss: 49.19382610321045\n",
      "Epoch 14 \t Batch 80 \t Training Loss: 49.07663364410401\n",
      "Epoch 14 \t Batch 100 \t Training Loss: 48.81641613006592\n",
      "Epoch 14 \t Batch 120 \t Training Loss: 48.652181752522786\n",
      "Epoch 14 \t Batch 140 \t Training Loss: 48.67770772661481\n",
      "Epoch 14 \t Batch 160 \t Training Loss: 48.67365779876709\n",
      "Epoch 14 \t Batch 180 \t Training Loss: 48.66738688151042\n",
      "Epoch 14 \t Batch 200 \t Training Loss: 48.59969863891602\n",
      "Epoch 14 \t Batch 220 \t Training Loss: 48.51259092851119\n",
      "Epoch 14 \t Batch 240 \t Training Loss: 48.4845014890035\n",
      "Epoch 14 \t Batch 260 \t Training Loss: 48.555638944185695\n",
      "Epoch 14 \t Batch 280 \t Training Loss: 48.58914495195661\n",
      "Epoch 14 \t Batch 300 \t Training Loss: 48.44544932047526\n",
      "Epoch 14 \t Batch 320 \t Training Loss: 48.481436014175415\n",
      "Epoch 14 \t Batch 340 \t Training Loss: 48.493642661150766\n",
      "Epoch 14 \t Batch 360 \t Training Loss: 48.492327043745256\n",
      "Epoch 14 \t Batch 380 \t Training Loss: 48.500194629869966\n",
      "Epoch 14 \t Batch 400 \t Training Loss: 48.444566345214845\n",
      "Epoch 14 \t Batch 420 \t Training Loss: 48.41245321546282\n",
      "Epoch 14 \t Batch 440 \t Training Loss: 48.353409793160175\n",
      "Epoch 14 \t Batch 460 \t Training Loss: 48.39460672295612\n",
      "Epoch 14 \t Batch 480 \t Training Loss: 48.43805239995321\n",
      "Epoch 14 \t Batch 500 \t Training Loss: 48.36357911682129\n",
      "Epoch 14 \t Batch 520 \t Training Loss: 48.31597648767325\n",
      "Epoch 14 \t Batch 540 \t Training Loss: 48.324390736332646\n",
      "Epoch 14 \t Batch 560 \t Training Loss: 48.318848317010065\n",
      "Epoch 14 \t Batch 580 \t Training Loss: 48.31652208525559\n",
      "Epoch 14 \t Batch 600 \t Training Loss: 48.326711152394616\n",
      "Epoch 14 \t Batch 620 \t Training Loss: 48.32453233657345\n",
      "Epoch 14 \t Batch 640 \t Training Loss: 48.30210835337639\n",
      "Epoch 14 \t Batch 660 \t Training Loss: 48.281527484547006\n",
      "Epoch 14 \t Batch 680 \t Training Loss: 48.26666467330035\n",
      "Epoch 14 \t Batch 700 \t Training Loss: 48.26440368107387\n",
      "Epoch 14 \t Batch 720 \t Training Loss: 48.26975515153673\n",
      "Epoch 14 \t Batch 740 \t Training Loss: 48.278211598782924\n",
      "Epoch 14 \t Batch 760 \t Training Loss: 48.2438631158126\n",
      "Epoch 14 \t Batch 780 \t Training Loss: 48.21723564343575\n",
      "Epoch 14 \t Batch 800 \t Training Loss: 48.22477619171143\n",
      "Epoch 14 \t Batch 820 \t Training Loss: 48.217255285309584\n",
      "Epoch 14 \t Batch 840 \t Training Loss: 48.23921791258312\n",
      "Epoch 14 \t Batch 860 \t Training Loss: 48.20630329486936\n",
      "Epoch 14 \t Batch 880 \t Training Loss: 48.22855400172147\n",
      "Epoch 14 \t Batch 900 \t Training Loss: 48.2230629518297\n",
      "Epoch 14 \t Batch 20 \t Validation Loss: 34.74527593851089\n",
      "Epoch 14 \t Batch 40 \t Validation Loss: 32.42310333848\n",
      "Epoch 14 \t Batch 60 \t Validation Loss: 34.59714333613714\n",
      "Epoch 14 \t Batch 80 \t Validation Loss: 34.218878236413005\n",
      "Epoch 14 \t Batch 100 \t Validation Loss: 32.48475266695023\n",
      "Epoch 14 \t Batch 120 \t Validation Loss: 31.76689928571383\n",
      "Epoch 14 \t Batch 140 \t Validation Loss: 31.085684069565364\n",
      "Epoch 14 \t Batch 160 \t Validation Loss: 32.5690348520875\n",
      "Epoch 14 \t Batch 180 \t Validation Loss: 35.80067537758085\n",
      "Epoch 14 \t Batch 200 \t Validation Loss: 36.913693071603774\n",
      "Epoch 14 \t Batch 220 \t Validation Loss: 38.07625245722858\n",
      "Epoch 14 \t Batch 240 \t Validation Loss: 38.33544783294201\n",
      "Epoch 14 \t Batch 260 \t Validation Loss: 40.28329808253508\n",
      "Epoch 14 \t Batch 280 \t Validation Loss: 41.18586329136576\n",
      "Epoch 14 \t Batch 300 \t Validation Loss: 42.17766648689906\n",
      "Epoch 14 \t Batch 320 \t Validation Loss: 42.553534627705815\n",
      "Epoch 14 \t Batch 340 \t Validation Loss: 42.3701807029107\n",
      "Epoch 14 \t Batch 360 \t Validation Loss: 42.152507676680884\n",
      "Epoch 14 \t Batch 380 \t Validation Loss: 42.30628695299751\n",
      "Epoch 14 \t Batch 400 \t Validation Loss: 41.80585172712803\n",
      "Epoch 14 \t Batch 420 \t Validation Loss: 41.73563204776673\n",
      "Epoch 14 \t Batch 440 \t Validation Loss: 41.35886852578683\n",
      "Epoch 14 \t Batch 460 \t Validation Loss: 41.47328503701998\n",
      "Epoch 14 \t Batch 480 \t Validation Loss: 41.862518501778446\n",
      "Epoch 14 \t Batch 500 \t Validation Loss: 41.51603812837601\n",
      "Epoch 14 \t Batch 520 \t Validation Loss: 41.22465259157694\n",
      "Epoch 14 \t Batch 540 \t Validation Loss: 40.86750906176037\n",
      "Epoch 14 \t Batch 560 \t Validation Loss: 40.58413635705199\n",
      "Epoch 14 \t Batch 580 \t Validation Loss: 40.297899092065876\n",
      "Epoch 14 \t Batch 600 \t Validation Loss: 40.412172025442125\n",
      "Epoch 14 Training Loss: 48.21416296173728 Validation Loss: 40.964703999943545\n",
      "Epoch 14 completed\n",
      "Epoch 15 \t Batch 20 \t Training Loss: 49.567722702026366\n",
      "Epoch 15 \t Batch 40 \t Training Loss: 48.37867212295532\n",
      "Epoch 15 \t Batch 60 \t Training Loss: 48.28047415415446\n",
      "Epoch 15 \t Batch 80 \t Training Loss: 48.2556173324585\n",
      "Epoch 15 \t Batch 100 \t Training Loss: 47.62822154998779\n",
      "Epoch 15 \t Batch 120 \t Training Loss: 47.856822872161864\n",
      "Epoch 15 \t Batch 140 \t Training Loss: 47.98512682233538\n",
      "Epoch 15 \t Batch 160 \t Training Loss: 47.935285568237305\n",
      "Epoch 15 \t Batch 180 \t Training Loss: 48.18145169152154\n",
      "Epoch 15 \t Batch 200 \t Training Loss: 48.05897327423096\n",
      "Epoch 15 \t Batch 220 \t Training Loss: 48.04212084683505\n",
      "Epoch 15 \t Batch 240 \t Training Loss: 48.001040093104045\n",
      "Epoch 15 \t Batch 260 \t Training Loss: 47.99945942805363\n",
      "Epoch 15 \t Batch 280 \t Training Loss: 47.997113473074776\n",
      "Epoch 15 \t Batch 300 \t Training Loss: 47.93938103993734\n",
      "Epoch 15 \t Batch 320 \t Training Loss: 48.0350382566452\n",
      "Epoch 15 \t Batch 340 \t Training Loss: 48.05061056473676\n",
      "Epoch 15 \t Batch 360 \t Training Loss: 48.0737799750434\n",
      "Epoch 15 \t Batch 380 \t Training Loss: 48.07765817140278\n",
      "Epoch 15 \t Batch 400 \t Training Loss: 48.102010583877565\n",
      "Epoch 15 \t Batch 420 \t Training Loss: 48.06803020295643\n",
      "Epoch 15 \t Batch 440 \t Training Loss: 48.05478247729215\n",
      "Epoch 15 \t Batch 460 \t Training Loss: 48.10477186285931\n",
      "Epoch 15 \t Batch 480 \t Training Loss: 48.10611514250437\n",
      "Epoch 15 \t Batch 500 \t Training Loss: 48.111855667114256\n",
      "Epoch 15 \t Batch 520 \t Training Loss: 48.1530145571782\n",
      "Epoch 15 \t Batch 540 \t Training Loss: 48.18790365148474\n",
      "Epoch 15 \t Batch 560 \t Training Loss: 48.175879165104455\n",
      "Epoch 15 \t Batch 580 \t Training Loss: 48.13949961169013\n",
      "Epoch 15 \t Batch 600 \t Training Loss: 48.20270930608114\n",
      "Epoch 15 \t Batch 620 \t Training Loss: 48.183906106025944\n",
      "Epoch 15 \t Batch 640 \t Training Loss: 48.22122071385384\n",
      "Epoch 15 \t Batch 660 \t Training Loss: 48.23572653568152\n",
      "Epoch 15 \t Batch 680 \t Training Loss: 48.262630423377544\n",
      "Epoch 15 \t Batch 700 \t Training Loss: 48.27391911642892\n",
      "Epoch 15 \t Batch 720 \t Training Loss: 48.28708497683207\n",
      "Epoch 15 \t Batch 740 \t Training Loss: 48.25152449221224\n",
      "Epoch 15 \t Batch 760 \t Training Loss: 48.20661756113956\n",
      "Epoch 15 \t Batch 780 \t Training Loss: 48.202950922648114\n",
      "Epoch 15 \t Batch 800 \t Training Loss: 48.17185482978821\n",
      "Epoch 15 \t Batch 820 \t Training Loss: 48.153181578473344\n",
      "Epoch 15 \t Batch 840 \t Training Loss: 48.16461720239548\n",
      "Epoch 15 \t Batch 860 \t Training Loss: 48.13746686092643\n",
      "Epoch 15 \t Batch 880 \t Training Loss: 48.152076248689134\n",
      "Epoch 15 \t Batch 900 \t Training Loss: 48.11806404537625\n",
      "Epoch 15 \t Batch 20 \t Validation Loss: 29.00015559196472\n",
      "Epoch 15 \t Batch 40 \t Validation Loss: 28.793233394622803\n",
      "Epoch 15 \t Batch 60 \t Validation Loss: 30.047124020258586\n",
      "Epoch 15 \t Batch 80 \t Validation Loss: 29.81772861480713\n",
      "Epoch 15 \t Batch 100 \t Validation Loss: 29.50015506744385\n",
      "Epoch 15 \t Batch 120 \t Validation Loss: 29.5220770517985\n",
      "Epoch 15 \t Batch 140 \t Validation Loss: 29.276654638562885\n",
      "Epoch 15 \t Batch 160 \t Validation Loss: 30.822633802890778\n",
      "Epoch 15 \t Batch 180 \t Validation Loss: 33.792231512069705\n",
      "Epoch 15 \t Batch 200 \t Validation Loss: 34.89657101154327\n",
      "Epoch 15 \t Batch 220 \t Validation Loss: 35.986768579483034\n",
      "Epoch 15 \t Batch 240 \t Validation Loss: 36.204561698436734\n",
      "Epoch 15 \t Batch 260 \t Validation Loss: 38.12756284567026\n",
      "Epoch 15 \t Batch 280 \t Validation Loss: 39.13881392819541\n",
      "Epoch 15 \t Batch 300 \t Validation Loss: 39.992914686203\n",
      "Epoch 15 \t Batch 320 \t Validation Loss: 40.318925860524175\n",
      "Epoch 15 \t Batch 340 \t Validation Loss: 40.19563240443959\n",
      "Epoch 15 \t Batch 360 \t Validation Loss: 40.0292053937912\n",
      "Epoch 15 \t Batch 380 \t Validation Loss: 40.2267221174742\n",
      "Epoch 15 \t Batch 400 \t Validation Loss: 39.806055619716645\n",
      "Epoch 15 \t Batch 420 \t Validation Loss: 39.77284948485238\n",
      "Epoch 15 \t Batch 440 \t Validation Loss: 39.477729149298234\n",
      "Epoch 15 \t Batch 460 \t Validation Loss: 39.655892913237864\n",
      "Epoch 15 \t Batch 480 \t Validation Loss: 40.0774104932944\n",
      "Epoch 15 \t Batch 500 \t Validation Loss: 39.74714164924622\n",
      "Epoch 15 \t Batch 520 \t Validation Loss: 39.5138522203152\n",
      "Epoch 15 \t Batch 540 \t Validation Loss: 39.24492762706898\n",
      "Epoch 15 \t Batch 560 \t Validation Loss: 39.00917868784496\n",
      "Epoch 15 \t Batch 580 \t Validation Loss: 38.78289479716071\n",
      "Epoch 15 \t Batch 600 \t Validation Loss: 38.963116119702654\n",
      "Epoch 15 Training Loss: 48.133748506901696 Validation Loss: 39.56713459244022\n",
      "Epoch 15 completed\n",
      "Epoch 16 \t Batch 20 \t Training Loss: 46.956161689758304\n",
      "Epoch 16 \t Batch 40 \t Training Loss: 47.472545337677005\n",
      "Epoch 16 \t Batch 60 \t Training Loss: 47.662479400634766\n",
      "Epoch 16 \t Batch 80 \t Training Loss: 47.83258757591248\n",
      "Epoch 16 \t Batch 100 \t Training Loss: 48.128370170593264\n",
      "Epoch 16 \t Batch 120 \t Training Loss: 48.073176034291585\n",
      "Epoch 16 \t Batch 140 \t Training Loss: 47.963983808244976\n",
      "Epoch 16 \t Batch 160 \t Training Loss: 47.939283108711244\n",
      "Epoch 16 \t Batch 180 \t Training Loss: 47.89472618103027\n",
      "Epoch 16 \t Batch 200 \t Training Loss: 47.94627435684204\n",
      "Epoch 16 \t Batch 220 \t Training Loss: 48.030376347628504\n",
      "Epoch 16 \t Batch 240 \t Training Loss: 48.004288546244304\n",
      "Epoch 16 \t Batch 260 \t Training Loss: 48.00329369765062\n",
      "Epoch 16 \t Batch 280 \t Training Loss: 48.019414111546105\n",
      "Epoch 16 \t Batch 300 \t Training Loss: 47.898748041788735\n",
      "Epoch 16 \t Batch 320 \t Training Loss: 47.91451245546341\n",
      "Epoch 16 \t Batch 340 \t Training Loss: 48.006170542099895\n",
      "Epoch 16 \t Batch 360 \t Training Loss: 47.99743593004015\n",
      "Epoch 16 \t Batch 380 \t Training Loss: 48.019695603220086\n",
      "Epoch 16 \t Batch 400 \t Training Loss: 48.08252923965454\n",
      "Epoch 16 \t Batch 420 \t Training Loss: 48.08296418871198\n",
      "Epoch 16 \t Batch 440 \t Training Loss: 48.06608320582997\n",
      "Epoch 16 \t Batch 460 \t Training Loss: 48.03636187677798\n",
      "Epoch 16 \t Batch 480 \t Training Loss: 48.06063721179962\n",
      "Epoch 16 \t Batch 500 \t Training Loss: 48.07658071136475\n",
      "Epoch 16 \t Batch 520 \t Training Loss: 48.103779565371\n",
      "Epoch 16 \t Batch 540 \t Training Loss: 48.08771542443169\n",
      "Epoch 16 \t Batch 560 \t Training Loss: 48.00429071017674\n",
      "Epoch 16 \t Batch 580 \t Training Loss: 47.97192461079565\n",
      "Epoch 16 \t Batch 600 \t Training Loss: 47.97309665044149\n",
      "Epoch 16 \t Batch 620 \t Training Loss: 47.956967409195435\n",
      "Epoch 16 \t Batch 640 \t Training Loss: 47.970085912942885\n",
      "Epoch 16 \t Batch 660 \t Training Loss: 47.989313680475405\n",
      "Epoch 16 \t Batch 680 \t Training Loss: 47.991256293128515\n",
      "Epoch 16 \t Batch 700 \t Training Loss: 48.01406232561384\n",
      "Epoch 16 \t Batch 720 \t Training Loss: 48.0273211479187\n",
      "Epoch 16 \t Batch 740 \t Training Loss: 48.02333530735325\n",
      "Epoch 16 \t Batch 760 \t Training Loss: 48.05607764595433\n",
      "Epoch 16 \t Batch 780 \t Training Loss: 48.09374013069348\n",
      "Epoch 16 \t Batch 800 \t Training Loss: 48.06306556224823\n",
      "Epoch 16 \t Batch 820 \t Training Loss: 48.09661161841416\n",
      "Epoch 16 \t Batch 840 \t Training Loss: 48.057367165883385\n",
      "Epoch 16 \t Batch 860 \t Training Loss: 48.04510035403939\n",
      "Epoch 16 \t Batch 880 \t Training Loss: 48.05080387375572\n",
      "Epoch 16 \t Batch 900 \t Training Loss: 48.04431544409858\n",
      "Epoch 16 \t Batch 20 \t Validation Loss: 38.044595670700076\n",
      "Epoch 16 \t Batch 40 \t Validation Loss: 34.29245121479035\n",
      "Epoch 16 \t Batch 60 \t Validation Loss: 37.01564122835795\n",
      "Epoch 16 \t Batch 80 \t Validation Loss: 36.1794992685318\n",
      "Epoch 16 \t Batch 100 \t Validation Loss: 34.2317135810852\n",
      "Epoch 16 \t Batch 120 \t Validation Loss: 33.249246549606326\n",
      "Epoch 16 \t Batch 140 \t Validation Loss: 32.36730946132115\n",
      "Epoch 16 \t Batch 160 \t Validation Loss: 33.500121641159055\n",
      "Epoch 16 \t Batch 180 \t Validation Loss: 36.096522675620186\n",
      "Epoch 16 \t Batch 200 \t Validation Loss: 36.94074792385101\n",
      "Epoch 16 \t Batch 220 \t Validation Loss: 37.7641741882671\n",
      "Epoch 16 \t Batch 240 \t Validation Loss: 37.78509027560552\n",
      "Epoch 16 \t Batch 260 \t Validation Loss: 39.530053384487445\n",
      "Epoch 16 \t Batch 280 \t Validation Loss: 40.337375610215325\n",
      "Epoch 16 \t Batch 300 \t Validation Loss: 41.08582173347473\n",
      "Epoch 16 \t Batch 320 \t Validation Loss: 41.30852364599705\n",
      "Epoch 16 \t Batch 340 \t Validation Loss: 41.12749746827518\n",
      "Epoch 16 \t Batch 360 \t Validation Loss: 40.88846077654097\n",
      "Epoch 16 \t Batch 380 \t Validation Loss: 41.02486675914965\n",
      "Epoch 16 \t Batch 400 \t Validation Loss: 40.572103807926176\n",
      "Epoch 16 \t Batch 420 \t Validation Loss: 40.514593430927825\n",
      "Epoch 16 \t Batch 440 \t Validation Loss: 40.16833325516094\n",
      "Epoch 16 \t Batch 460 \t Validation Loss: 40.3044227330581\n",
      "Epoch 16 \t Batch 480 \t Validation Loss: 40.711956983804704\n",
      "Epoch 16 \t Batch 500 \t Validation Loss: 40.37239779090881\n",
      "Epoch 16 \t Batch 520 \t Validation Loss: 40.09216737930591\n",
      "Epoch 16 \t Batch 540 \t Validation Loss: 39.79144363933139\n",
      "Epoch 16 \t Batch 560 \t Validation Loss: 39.521784818172456\n",
      "Epoch 16 \t Batch 580 \t Validation Loss: 39.246072418936365\n",
      "Epoch 16 \t Batch 600 \t Validation Loss: 39.413150947888695\n",
      "Epoch 16 Training Loss: 48.06540053855372 Validation Loss: 39.99773393203686\n",
      "Epoch 16 completed\n",
      "Epoch 17 \t Batch 20 \t Training Loss: 48.84806327819824\n",
      "Epoch 17 \t Batch 40 \t Training Loss: 48.37236795425415\n",
      "Epoch 17 \t Batch 60 \t Training Loss: 48.623133977254234\n",
      "Epoch 17 \t Batch 80 \t Training Loss: 48.489232015609744\n",
      "Epoch 17 \t Batch 100 \t Training Loss: 48.34056655883789\n",
      "Epoch 17 \t Batch 120 \t Training Loss: 48.14910974502563\n",
      "Epoch 17 \t Batch 140 \t Training Loss: 48.191776738848006\n",
      "Epoch 17 \t Batch 160 \t Training Loss: 48.15766446590423\n",
      "Epoch 17 \t Batch 180 \t Training Loss: 48.177958212958444\n",
      "Epoch 17 \t Batch 200 \t Training Loss: 48.2743540763855\n",
      "Epoch 17 \t Batch 220 \t Training Loss: 48.30880253531716\n",
      "Epoch 17 \t Batch 240 \t Training Loss: 48.191436227162676\n",
      "Epoch 17 \t Batch 260 \t Training Loss: 48.20975268437312\n",
      "Epoch 17 \t Batch 280 \t Training Loss: 48.24056316103254\n",
      "Epoch 17 \t Batch 300 \t Training Loss: 48.25548027038574\n",
      "Epoch 17 \t Batch 320 \t Training Loss: 48.22906723022461\n",
      "Epoch 17 \t Batch 340 \t Training Loss: 48.20170143351835\n",
      "Epoch 17 \t Batch 360 \t Training Loss: 48.19818640814887\n",
      "Epoch 17 \t Batch 380 \t Training Loss: 48.16122994673879\n",
      "Epoch 17 \t Batch 400 \t Training Loss: 48.11747720718384\n",
      "Epoch 17 \t Batch 420 \t Training Loss: 48.08990964435396\n",
      "Epoch 17 \t Batch 440 \t Training Loss: 48.0651345426386\n",
      "Epoch 17 \t Batch 460 \t Training Loss: 48.102797259455144\n",
      "Epoch 17 \t Batch 480 \t Training Loss: 48.07866651217143\n",
      "Epoch 17 \t Batch 500 \t Training Loss: 48.130601524353025\n",
      "Epoch 17 \t Batch 520 \t Training Loss: 48.115621288006125\n",
      "Epoch 17 \t Batch 540 \t Training Loss: 48.130421009770146\n",
      "Epoch 17 \t Batch 560 \t Training Loss: 48.1241291318621\n",
      "Epoch 17 \t Batch 580 \t Training Loss: 48.06807460127206\n",
      "Epoch 17 \t Batch 600 \t Training Loss: 47.99250891367594\n",
      "Epoch 17 \t Batch 620 \t Training Loss: 48.02723098262664\n",
      "Epoch 17 \t Batch 640 \t Training Loss: 48.04732763171196\n",
      "Epoch 17 \t Batch 660 \t Training Loss: 48.03158346233946\n",
      "Epoch 17 \t Batch 680 \t Training Loss: 48.0636957729564\n",
      "Epoch 17 \t Batch 700 \t Training Loss: 48.06720819200788\n",
      "Epoch 17 \t Batch 720 \t Training Loss: 48.05204256375631\n",
      "Epoch 17 \t Batch 740 \t Training Loss: 48.039515330340414\n",
      "Epoch 17 \t Batch 760 \t Training Loss: 48.04756652932418\n",
      "Epoch 17 \t Batch 780 \t Training Loss: 48.00997554583427\n",
      "Epoch 17 \t Batch 800 \t Training Loss: 47.98584686279297\n",
      "Epoch 17 \t Batch 820 \t Training Loss: 47.99523227505568\n",
      "Epoch 17 \t Batch 840 \t Training Loss: 48.00816143126715\n",
      "Epoch 17 \t Batch 860 \t Training Loss: 47.97943029181902\n",
      "Epoch 17 \t Batch 880 \t Training Loss: 47.969845680757004\n",
      "Epoch 17 \t Batch 900 \t Training Loss: 47.982863900926375\n",
      "Epoch 17 \t Batch 20 \t Validation Loss: 45.659096240997314\n",
      "Epoch 17 \t Batch 40 \t Validation Loss: 40.72507946491241\n",
      "Epoch 17 \t Batch 60 \t Validation Loss: 43.735698795318605\n",
      "Epoch 17 \t Batch 80 \t Validation Loss: 42.233445060253146\n",
      "Epoch 17 \t Batch 100 \t Validation Loss: 39.201206369400026\n",
      "Epoch 17 \t Batch 120 \t Validation Loss: 37.61014131704966\n",
      "Epoch 17 \t Batch 140 \t Validation Loss: 36.176160144805905\n",
      "Epoch 17 \t Batch 160 \t Validation Loss: 36.854913640022275\n",
      "Epoch 17 \t Batch 180 \t Validation Loss: 39.25268552038405\n",
      "Epoch 17 \t Batch 200 \t Validation Loss: 39.908020243644714\n",
      "Epoch 17 \t Batch 220 \t Validation Loss: 40.59968652291732\n",
      "Epoch 17 \t Batch 240 \t Validation Loss: 40.44428699016571\n",
      "Epoch 17 \t Batch 260 \t Validation Loss: 42.11565275559059\n",
      "Epoch 17 \t Batch 280 \t Validation Loss: 42.88318358830043\n",
      "Epoch 17 \t Batch 300 \t Validation Loss: 43.509967702229815\n",
      "Epoch 17 \t Batch 320 \t Validation Loss: 43.638155835866925\n",
      "Epoch 17 \t Batch 340 \t Validation Loss: 43.330916028864245\n",
      "Epoch 17 \t Batch 360 \t Validation Loss: 42.971013543340895\n",
      "Epoch 17 \t Batch 380 \t Validation Loss: 43.03597711512917\n",
      "Epoch 17 \t Batch 400 \t Validation Loss: 42.474517352581024\n",
      "Epoch 17 \t Batch 420 \t Validation Loss: 42.32140895071484\n",
      "Epoch 17 \t Batch 440 \t Validation Loss: 41.91276353706013\n",
      "Epoch 17 \t Batch 460 \t Validation Loss: 41.971779655373616\n",
      "Epoch 17 \t Batch 480 \t Validation Loss: 42.310981760422386\n",
      "Epoch 17 \t Batch 500 \t Validation Loss: 41.8960260219574\n",
      "Epoch 17 \t Batch 520 \t Validation Loss: 41.56413031174586\n",
      "Epoch 17 \t Batch 540 \t Validation Loss: 41.23009339968363\n",
      "Epoch 17 \t Batch 560 \t Validation Loss: 40.97710294212614\n",
      "Epoch 17 \t Batch 580 \t Validation Loss: 40.76885578714568\n",
      "Epoch 17 \t Batch 600 \t Validation Loss: 40.90176185766856\n",
      "Epoch 17 Training Loss: 48.002440087537856 Validation Loss: 41.47447146843006\n",
      "Epoch 17 completed\n",
      "Epoch 18 \t Batch 20 \t Training Loss: 47.538040733337404\n",
      "Epoch 18 \t Batch 40 \t Training Loss: 47.635741996765134\n",
      "Epoch 18 \t Batch 60 \t Training Loss: 47.77491404215495\n",
      "Epoch 18 \t Batch 80 \t Training Loss: 47.747884798049924\n",
      "Epoch 18 \t Batch 100 \t Training Loss: 47.55132743835449\n",
      "Epoch 18 \t Batch 120 \t Training Loss: 47.959525680541994\n",
      "Epoch 18 \t Batch 140 \t Training Loss: 48.04998422350202\n",
      "Epoch 18 \t Batch 160 \t Training Loss: 47.953085684776305\n",
      "Epoch 18 \t Batch 180 \t Training Loss: 47.907630602518715\n",
      "Epoch 18 \t Batch 200 \t Training Loss: 47.980460262298585\n",
      "Epoch 18 \t Batch 220 \t Training Loss: 47.75857604633678\n",
      "Epoch 18 \t Batch 240 \t Training Loss: 47.88796072006225\n",
      "Epoch 18 \t Batch 260 \t Training Loss: 48.01517586341271\n",
      "Epoch 18 \t Batch 280 \t Training Loss: 48.0792003767831\n",
      "Epoch 18 \t Batch 300 \t Training Loss: 48.01614278157552\n",
      "Epoch 18 \t Batch 320 \t Training Loss: 48.01154440641403\n",
      "Epoch 18 \t Batch 340 \t Training Loss: 48.01091348984662\n",
      "Epoch 18 \t Batch 360 \t Training Loss: 47.95167433420817\n",
      "Epoch 18 \t Batch 380 \t Training Loss: 47.918568048979104\n",
      "Epoch 18 \t Batch 400 \t Training Loss: 47.855510997772214\n",
      "Epoch 18 \t Batch 420 \t Training Loss: 47.805240921747114\n",
      "Epoch 18 \t Batch 440 \t Training Loss: 47.85318888750943\n",
      "Epoch 18 \t Batch 460 \t Training Loss: 47.82938531792682\n",
      "Epoch 18 \t Batch 480 \t Training Loss: 47.87689276536306\n",
      "Epoch 18 \t Batch 500 \t Training Loss: 47.914429237365724\n",
      "Epoch 18 \t Batch 520 \t Training Loss: 47.90101834077102\n",
      "Epoch 18 \t Batch 540 \t Training Loss: 47.86647783915202\n",
      "Epoch 18 \t Batch 560 \t Training Loss: 47.87831839152745\n",
      "Epoch 18 \t Batch 580 \t Training Loss: 47.84046840010018\n",
      "Epoch 18 \t Batch 600 \t Training Loss: 47.79387179692586\n",
      "Epoch 18 \t Batch 620 \t Training Loss: 47.773617959791615\n",
      "Epoch 18 \t Batch 640 \t Training Loss: 47.77574970722198\n",
      "Epoch 18 \t Batch 660 \t Training Loss: 47.787142140937576\n",
      "Epoch 18 \t Batch 680 \t Training Loss: 47.76986322402954\n",
      "Epoch 18 \t Batch 700 \t Training Loss: 47.84722640991211\n",
      "Epoch 18 \t Batch 720 \t Training Loss: 47.849996137619016\n",
      "Epoch 18 \t Batch 740 \t Training Loss: 47.87955045957823\n",
      "Epoch 18 \t Batch 760 \t Training Loss: 47.885761105386834\n",
      "Epoch 18 \t Batch 780 \t Training Loss: 47.85643203930977\n",
      "Epoch 18 \t Batch 800 \t Training Loss: 47.85598774909973\n",
      "Epoch 18 \t Batch 820 \t Training Loss: 47.891994406537314\n",
      "Epoch 18 \t Batch 840 \t Training Loss: 47.903109891074045\n",
      "Epoch 18 \t Batch 860 \t Training Loss: 47.91465564106786\n",
      "Epoch 18 \t Batch 880 \t Training Loss: 47.91651617396962\n",
      "Epoch 18 \t Batch 900 \t Training Loss: 47.93020271725125\n",
      "Epoch 18 \t Batch 20 \t Validation Loss: 38.85007085800171\n",
      "Epoch 18 \t Batch 40 \t Validation Loss: 35.25607014894486\n",
      "Epoch 18 \t Batch 60 \t Validation Loss: 37.867444173494974\n",
      "Epoch 18 \t Batch 80 \t Validation Loss: 36.8793767273426\n",
      "Epoch 18 \t Batch 100 \t Validation Loss: 34.99675165653229\n",
      "Epoch 18 \t Batch 120 \t Validation Loss: 34.18753705422083\n",
      "Epoch 18 \t Batch 140 \t Validation Loss: 33.32345543248313\n",
      "Epoch 18 \t Batch 160 \t Validation Loss: 34.475414553284644\n",
      "Epoch 18 \t Batch 180 \t Validation Loss: 37.45378288957808\n",
      "Epoch 18 \t Batch 200 \t Validation Loss: 38.48800301790237\n",
      "Epoch 18 \t Batch 220 \t Validation Loss: 39.420003693754026\n",
      "Epoch 18 \t Batch 240 \t Validation Loss: 39.489524382352826\n",
      "Epoch 18 \t Batch 260 \t Validation Loss: 41.35099621002491\n",
      "Epoch 18 \t Batch 280 \t Validation Loss: 42.23557009526662\n",
      "Epoch 18 \t Batch 300 \t Validation Loss: 43.10348434289296\n",
      "Epoch 18 \t Batch 320 \t Validation Loss: 43.35195675939322\n",
      "Epoch 18 \t Batch 340 \t Validation Loss: 43.11135668894824\n",
      "Epoch 18 \t Batch 360 \t Validation Loss: 42.827283820841046\n",
      "Epoch 18 \t Batch 380 \t Validation Loss: 42.92759882650877\n",
      "Epoch 18 \t Batch 400 \t Validation Loss: 42.38435980677605\n",
      "Epoch 18 \t Batch 420 \t Validation Loss: 42.2678893668311\n",
      "Epoch 18 \t Batch 440 \t Validation Loss: 41.83479432517832\n",
      "Epoch 18 \t Batch 460 \t Validation Loss: 41.91363824657772\n",
      "Epoch 18 \t Batch 480 \t Validation Loss: 42.27440232336521\n",
      "Epoch 18 \t Batch 500 \t Validation Loss: 41.90034381389618\n",
      "Epoch 18 \t Batch 520 \t Validation Loss: 41.56563572425109\n",
      "Epoch 18 \t Batch 540 \t Validation Loss: 41.18581104190261\n",
      "Epoch 18 \t Batch 560 \t Validation Loss: 40.885878957169396\n",
      "Epoch 18 \t Batch 580 \t Validation Loss: 40.62886328779418\n",
      "Epoch 18 \t Batch 600 \t Validation Loss: 40.7196201411883\n",
      "Epoch 18 Training Loss: 47.921867071087256 Validation Loss: 41.28645389652871\n",
      "Epoch 18 completed\n",
      "Epoch 19 \t Batch 20 \t Training Loss: 50.08087997436523\n",
      "Epoch 19 \t Batch 40 \t Training Loss: 49.21306457519531\n",
      "Epoch 19 \t Batch 60 \t Training Loss: 48.295437240600585\n",
      "Epoch 19 \t Batch 80 \t Training Loss: 47.957249307632445\n",
      "Epoch 19 \t Batch 100 \t Training Loss: 47.80407245635986\n",
      "Epoch 19 \t Batch 120 \t Training Loss: 47.69382260640462\n",
      "Epoch 19 \t Batch 140 \t Training Loss: 47.6551349912371\n",
      "Epoch 19 \t Batch 160 \t Training Loss: 47.73008587360382\n",
      "Epoch 19 \t Batch 180 \t Training Loss: 47.83006042904324\n",
      "Epoch 19 \t Batch 200 \t Training Loss: 47.76799940109253\n",
      "Epoch 19 \t Batch 220 \t Training Loss: 47.70458157279275\n",
      "Epoch 19 \t Batch 240 \t Training Loss: 47.76278773943583\n",
      "Epoch 19 \t Batch 260 \t Training Loss: 47.81715996082013\n",
      "Epoch 19 \t Batch 280 \t Training Loss: 47.81226197651454\n",
      "Epoch 19 \t Batch 300 \t Training Loss: 47.900634358723956\n",
      "Epoch 19 \t Batch 320 \t Training Loss: 48.00530468225479\n",
      "Epoch 19 \t Batch 340 \t Training Loss: 48.01684453627642\n",
      "Epoch 19 \t Batch 360 \t Training Loss: 47.98472704357571\n",
      "Epoch 19 \t Batch 380 \t Training Loss: 47.98356130499589\n",
      "Epoch 19 \t Batch 400 \t Training Loss: 48.05141669273377\n",
      "Epoch 19 \t Batch 420 \t Training Loss: 48.055913407461986\n",
      "Epoch 19 \t Batch 440 \t Training Loss: 48.03653545379639\n",
      "Epoch 19 \t Batch 460 \t Training Loss: 48.037659520688266\n",
      "Epoch 19 \t Batch 480 \t Training Loss: 47.98200032711029\n",
      "Epoch 19 \t Batch 500 \t Training Loss: 47.972109085083005\n",
      "Epoch 19 \t Batch 520 \t Training Loss: 47.97526863538302\n",
      "Epoch 19 \t Batch 540 \t Training Loss: 47.92199995252821\n",
      "Epoch 19 \t Batch 560 \t Training Loss: 47.91460524286543\n",
      "Epoch 19 \t Batch 580 \t Training Loss: 47.87484603750295\n",
      "Epoch 19 \t Batch 600 \t Training Loss: 47.85859539031983\n",
      "Epoch 19 \t Batch 620 \t Training Loss: 47.873070907592776\n",
      "Epoch 19 \t Batch 640 \t Training Loss: 47.91253347992897\n",
      "Epoch 19 \t Batch 660 \t Training Loss: 47.943439616579\n",
      "Epoch 19 \t Batch 680 \t Training Loss: 47.95219506656422\n",
      "Epoch 19 \t Batch 700 \t Training Loss: 47.95205953870501\n",
      "Epoch 19 \t Batch 720 \t Training Loss: 47.98185733159383\n",
      "Epoch 19 \t Batch 740 \t Training Loss: 47.9559241062886\n",
      "Epoch 19 \t Batch 760 \t Training Loss: 47.919422616456686\n",
      "Epoch 19 \t Batch 780 \t Training Loss: 47.88841750316131\n",
      "Epoch 19 \t Batch 800 \t Training Loss: 47.852045769691465\n",
      "Epoch 19 \t Batch 820 \t Training Loss: 47.825169847069716\n",
      "Epoch 19 \t Batch 840 \t Training Loss: 47.85000591278076\n",
      "Epoch 19 \t Batch 860 \t Training Loss: 47.82598707066026\n",
      "Epoch 19 \t Batch 880 \t Training Loss: 47.88223219784823\n",
      "Epoch 19 \t Batch 900 \t Training Loss: 47.90307190789117\n",
      "Epoch 19 \t Batch 20 \t Validation Loss: 35.008572578430176\n",
      "Epoch 19 \t Batch 40 \t Validation Loss: 32.747264659404756\n",
      "Epoch 19 \t Batch 60 \t Validation Loss: 34.580785091718035\n",
      "Epoch 19 \t Batch 80 \t Validation Loss: 33.867118960618974\n",
      "Epoch 19 \t Batch 100 \t Validation Loss: 32.38494965076447\n",
      "Epoch 19 \t Batch 120 \t Validation Loss: 31.861512545744578\n",
      "Epoch 19 \t Batch 140 \t Validation Loss: 31.253140132767815\n",
      "Epoch 19 \t Batch 160 \t Validation Loss: 32.73418326079845\n",
      "Epoch 19 \t Batch 180 \t Validation Loss: 35.946511477894255\n",
      "Epoch 19 \t Batch 200 \t Validation Loss: 37.23298247098923\n",
      "Epoch 19 \t Batch 220 \t Validation Loss: 38.29975026520816\n",
      "Epoch 19 \t Batch 240 \t Validation Loss: 38.51570335825284\n",
      "Epoch 19 \t Batch 260 \t Validation Loss: 40.49150043084071\n",
      "Epoch 19 \t Batch 280 \t Validation Loss: 41.44911858183997\n",
      "Epoch 19 \t Batch 300 \t Validation Loss: 42.41859082063039\n",
      "Epoch 19 \t Batch 320 \t Validation Loss: 42.738740749657154\n",
      "Epoch 19 \t Batch 340 \t Validation Loss: 42.59302677406984\n",
      "Epoch 19 \t Batch 360 \t Validation Loss: 42.36300158103307\n",
      "Epoch 19 \t Batch 380 \t Validation Loss: 42.538758866410504\n",
      "Epoch 19 \t Batch 400 \t Validation Loss: 42.10713840603828\n",
      "Epoch 19 \t Batch 420 \t Validation Loss: 42.057424687203905\n",
      "Epoch 19 \t Batch 440 \t Validation Loss: 41.75549734397368\n",
      "Epoch 19 \t Batch 460 \t Validation Loss: 41.89950276354085\n",
      "Epoch 19 \t Batch 480 \t Validation Loss: 42.26948591172695\n",
      "Epoch 19 \t Batch 500 \t Validation Loss: 41.92813656330109\n",
      "Epoch 19 \t Batch 520 \t Validation Loss: 41.700308488882506\n",
      "Epoch 19 \t Batch 540 \t Validation Loss: 41.371989385286966\n",
      "Epoch 19 \t Batch 560 \t Validation Loss: 41.131838482618335\n",
      "Epoch 19 \t Batch 580 \t Validation Loss: 40.903501642983535\n",
      "Epoch 19 \t Batch 600 \t Validation Loss: 41.02331529060999\n",
      "Epoch 19 Training Loss: 47.90165920527967 Validation Loss: 41.58703322302211\n",
      "Epoch 19 completed\n",
      "Epoch 20 \t Batch 20 \t Training Loss: 46.35384330749512\n",
      "Epoch 20 \t Batch 40 \t Training Loss: 47.505661201477054\n",
      "Epoch 20 \t Batch 60 \t Training Loss: 47.60559546152751\n",
      "Epoch 20 \t Batch 80 \t Training Loss: 47.667422676086424\n",
      "Epoch 20 \t Batch 100 \t Training Loss: 47.68199405670166\n",
      "Epoch 20 \t Batch 120 \t Training Loss: 47.86215960184733\n",
      "Epoch 20 \t Batch 140 \t Training Loss: 48.055891036987305\n",
      "Epoch 20 \t Batch 160 \t Training Loss: 48.06664781570434\n",
      "Epoch 20 \t Batch 180 \t Training Loss: 47.99211752149794\n",
      "Epoch 20 \t Batch 200 \t Training Loss: 47.85364555358887\n",
      "Epoch 20 \t Batch 220 \t Training Loss: 47.78704749020663\n",
      "Epoch 20 \t Batch 240 \t Training Loss: 47.85086517333984\n",
      "Epoch 20 \t Batch 260 \t Training Loss: 47.98197661179763\n",
      "Epoch 20 \t Batch 280 \t Training Loss: 48.00623795645578\n",
      "Epoch 20 \t Batch 300 \t Training Loss: 47.959702224731444\n",
      "Epoch 20 \t Batch 320 \t Training Loss: 47.82720637321472\n",
      "Epoch 20 \t Batch 340 \t Training Loss: 47.83681999655331\n",
      "Epoch 20 \t Batch 360 \t Training Loss: 47.924175347222224\n",
      "Epoch 20 \t Batch 380 \t Training Loss: 47.90950262170089\n",
      "Epoch 20 \t Batch 400 \t Training Loss: 47.9848320388794\n",
      "Epoch 20 \t Batch 420 \t Training Loss: 47.98081137339274\n",
      "Epoch 20 \t Batch 440 \t Training Loss: 47.99214672608809\n",
      "Epoch 20 \t Batch 460 \t Training Loss: 47.96768945611041\n",
      "Epoch 20 \t Batch 480 \t Training Loss: 47.922980936368305\n",
      "Epoch 20 \t Batch 500 \t Training Loss: 47.98370204925537\n",
      "Epoch 20 \t Batch 520 \t Training Loss: 47.965692138671876\n",
      "Epoch 20 \t Batch 540 \t Training Loss: 47.966239010846174\n",
      "Epoch 20 \t Batch 560 \t Training Loss: 47.91627502441406\n",
      "Epoch 20 \t Batch 580 \t Training Loss: 47.9308341322274\n",
      "Epoch 20 \t Batch 600 \t Training Loss: 47.92503262201945\n",
      "Epoch 20 \t Batch 620 \t Training Loss: 47.937255970124276\n",
      "Epoch 20 \t Batch 640 \t Training Loss: 47.925193190574646\n",
      "Epoch 20 \t Batch 660 \t Training Loss: 47.9090577212247\n",
      "Epoch 20 \t Batch 680 \t Training Loss: 47.90193760815789\n",
      "Epoch 20 \t Batch 700 \t Training Loss: 47.88990941728864\n",
      "Epoch 20 \t Batch 720 \t Training Loss: 47.87374625735813\n",
      "Epoch 20 \t Batch 740 \t Training Loss: 47.90125461784569\n",
      "Epoch 20 \t Batch 760 \t Training Loss: 47.89212452235975\n",
      "Epoch 20 \t Batch 780 \t Training Loss: 47.885845262576375\n",
      "Epoch 20 \t Batch 800 \t Training Loss: 47.91308291912079\n",
      "Epoch 20 \t Batch 820 \t Training Loss: 47.887170465981086\n",
      "Epoch 20 \t Batch 840 \t Training Loss: 47.886313960665746\n",
      "Epoch 20 \t Batch 860 \t Training Loss: 47.889016058278635\n",
      "Epoch 20 \t Batch 880 \t Training Loss: 47.87025185064836\n",
      "Epoch 20 \t Batch 900 \t Training Loss: 47.85052079942491\n",
      "Epoch 20 \t Batch 20 \t Validation Loss: 30.678755378723146\n",
      "Epoch 20 \t Batch 40 \t Validation Loss: 28.91575523018837\n",
      "Epoch 20 \t Batch 60 \t Validation Loss: 30.719178410371146\n",
      "Epoch 20 \t Batch 80 \t Validation Loss: 30.257795694470406\n",
      "Epoch 20 \t Batch 100 \t Validation Loss: 29.475069868564606\n",
      "Epoch 20 \t Batch 120 \t Validation Loss: 29.413842115799586\n",
      "Epoch 20 \t Batch 140 \t Validation Loss: 29.11852537052972\n",
      "Epoch 20 \t Batch 160 \t Validation Loss: 30.65429504066706\n",
      "Epoch 20 \t Batch 180 \t Validation Loss: 33.82060588863161\n",
      "Epoch 20 \t Batch 200 \t Validation Loss: 35.036955295801164\n",
      "Epoch 20 \t Batch 220 \t Validation Loss: 36.1459867011417\n",
      "Epoch 20 \t Batch 240 \t Validation Loss: 36.380062271157904\n",
      "Epoch 20 \t Batch 260 \t Validation Loss: 38.382717324220216\n",
      "Epoch 20 \t Batch 280 \t Validation Loss: 39.397903678246905\n",
      "Epoch 20 \t Batch 300 \t Validation Loss: 40.32481293757757\n",
      "Epoch 20 \t Batch 320 \t Validation Loss: 40.64861967936158\n",
      "Epoch 20 \t Batch 340 \t Validation Loss: 40.497511654040395\n",
      "Epoch 20 \t Batch 360 \t Validation Loss: 40.278353705008826\n",
      "Epoch 20 \t Batch 380 \t Validation Loss: 40.455397734516545\n",
      "Epoch 20 \t Batch 400 \t Validation Loss: 39.97913827240467\n",
      "Epoch 20 \t Batch 420 \t Validation Loss: 39.93115645192918\n",
      "Epoch 20 \t Batch 440 \t Validation Loss: 39.58227830637585\n",
      "Epoch 20 \t Batch 460 \t Validation Loss: 39.711071199437846\n",
      "Epoch 20 \t Batch 480 \t Validation Loss: 40.12685586462418\n",
      "Epoch 20 \t Batch 500 \t Validation Loss: 39.790320826053616\n",
      "Epoch 20 \t Batch 520 \t Validation Loss: 39.51048403198902\n",
      "Epoch 20 \t Batch 540 \t Validation Loss: 39.266490131395834\n",
      "Epoch 20 \t Batch 560 \t Validation Loss: 39.14024021668094\n",
      "Epoch 20 \t Batch 580 \t Validation Loss: 39.044065909961176\n",
      "Epoch 20 \t Batch 600 \t Validation Loss: 39.253174674113595\n",
      "Epoch 20 Training Loss: 47.84516037303463 Validation Loss: 39.931801610565806\n",
      "Epoch 20 completed\n",
      "Epoch 21 \t Batch 20 \t Training Loss: 46.73734188079834\n",
      "Epoch 21 \t Batch 40 \t Training Loss: 47.22581071853638\n",
      "Epoch 21 \t Batch 60 \t Training Loss: 47.24495944976807\n",
      "Epoch 21 \t Batch 80 \t Training Loss: 47.297053241729735\n",
      "Epoch 21 \t Batch 100 \t Training Loss: 47.079452476501466\n",
      "Epoch 21 \t Batch 120 \t Training Loss: 47.349663988749185\n",
      "Epoch 21 \t Batch 140 \t Training Loss: 47.62599370138986\n",
      "Epoch 21 \t Batch 160 \t Training Loss: 47.62791967391968\n",
      "Epoch 21 \t Batch 180 \t Training Loss: 47.62715928819444\n",
      "Epoch 21 \t Batch 200 \t Training Loss: 47.64534698486328\n",
      "Epoch 21 \t Batch 220 \t Training Loss: 47.57746493599632\n",
      "Epoch 21 \t Batch 240 \t Training Loss: 47.63901691436767\n",
      "Epoch 21 \t Batch 260 \t Training Loss: 47.54170728830191\n",
      "Epoch 21 \t Batch 280 \t Training Loss: 47.606516688210625\n",
      "Epoch 21 \t Batch 300 \t Training Loss: 47.65313873291016\n",
      "Epoch 21 \t Batch 320 \t Training Loss: 47.70203444957733\n",
      "Epoch 21 \t Batch 340 \t Training Loss: 47.64557828342213\n",
      "Epoch 21 \t Batch 360 \t Training Loss: 47.574937121073404\n",
      "Epoch 21 \t Batch 380 \t Training Loss: 47.65719726964047\n",
      "Epoch 21 \t Batch 400 \t Training Loss: 47.675398960113526\n",
      "Epoch 21 \t Batch 420 \t Training Loss: 47.66750534602574\n",
      "Epoch 21 \t Batch 440 \t Training Loss: 47.66589780287309\n",
      "Epoch 21 \t Batch 460 \t Training Loss: 47.67758099099864\n",
      "Epoch 21 \t Batch 480 \t Training Loss: 47.69637798468272\n",
      "Epoch 21 \t Batch 500 \t Training Loss: 47.74307041931152\n",
      "Epoch 21 \t Batch 520 \t Training Loss: 47.71988529792199\n",
      "Epoch 21 \t Batch 540 \t Training Loss: 47.79616355895996\n",
      "Epoch 21 \t Batch 560 \t Training Loss: 47.78372760500227\n",
      "Epoch 21 \t Batch 580 \t Training Loss: 47.78884435193292\n",
      "Epoch 21 \t Batch 600 \t Training Loss: 47.76721612294515\n",
      "Epoch 21 \t Batch 620 \t Training Loss: 47.7730891935287\n",
      "Epoch 21 \t Batch 640 \t Training Loss: 47.77249183654785\n",
      "Epoch 21 \t Batch 660 \t Training Loss: 47.76251263473973\n",
      "Epoch 21 \t Batch 680 \t Training Loss: 47.76165228450999\n",
      "Epoch 21 \t Batch 700 \t Training Loss: 47.755084757123676\n",
      "Epoch 21 \t Batch 720 \t Training Loss: 47.752901252110796\n",
      "Epoch 21 \t Batch 740 \t Training Loss: 47.744156729208456\n",
      "Epoch 21 \t Batch 760 \t Training Loss: 47.74582996368408\n",
      "Epoch 21 \t Batch 780 \t Training Loss: 47.717411212432076\n",
      "Epoch 21 \t Batch 800 \t Training Loss: 47.730621724128724\n",
      "Epoch 21 \t Batch 820 \t Training Loss: 47.742260039724954\n",
      "Epoch 21 \t Batch 840 \t Training Loss: 47.74349614097959\n",
      "Epoch 21 \t Batch 860 \t Training Loss: 47.74694421901259\n",
      "Epoch 21 \t Batch 880 \t Training Loss: 47.753505091233684\n",
      "Epoch 21 \t Batch 900 \t Training Loss: 47.7743375354343\n",
      "Epoch 21 \t Batch 20 \t Validation Loss: 34.44403109550476\n",
      "Epoch 21 \t Batch 40 \t Validation Loss: 31.664152610301972\n",
      "Epoch 21 \t Batch 60 \t Validation Loss: 34.065304176012674\n",
      "Epoch 21 \t Batch 80 \t Validation Loss: 33.32303571105003\n",
      "Epoch 21 \t Batch 100 \t Validation Loss: 31.976462836265565\n",
      "Epoch 21 \t Batch 120 \t Validation Loss: 31.417789566516877\n",
      "Epoch 21 \t Batch 140 \t Validation Loss: 30.83538341181619\n",
      "Epoch 21 \t Batch 160 \t Validation Loss: 32.09495944678783\n",
      "Epoch 21 \t Batch 180 \t Validation Loss: 34.978514806429544\n",
      "Epoch 21 \t Batch 200 \t Validation Loss: 35.97763989210129\n",
      "Epoch 21 \t Batch 220 \t Validation Loss: 36.911998920007186\n",
      "Epoch 21 \t Batch 240 \t Validation Loss: 37.0241002937158\n",
      "Epoch 21 \t Batch 260 \t Validation Loss: 38.899823531737695\n",
      "Epoch 21 \t Batch 280 \t Validation Loss: 39.80711555991854\n",
      "Epoch 21 \t Batch 300 \t Validation Loss: 40.61713916619619\n",
      "Epoch 21 \t Batch 320 \t Validation Loss: 40.906771866977216\n",
      "Epoch 21 \t Batch 340 \t Validation Loss: 40.740219367251676\n",
      "Epoch 21 \t Batch 360 \t Validation Loss: 40.50042614009645\n",
      "Epoch 21 \t Batch 380 \t Validation Loss: 40.65064159819954\n",
      "Epoch 21 \t Batch 400 \t Validation Loss: 40.175126477479935\n",
      "Epoch 21 \t Batch 420 \t Validation Loss: 40.10133921873002\n",
      "Epoch 21 \t Batch 440 \t Validation Loss: 39.7362228881229\n",
      "Epoch 21 \t Batch 460 \t Validation Loss: 39.871947343453115\n",
      "Epoch 21 \t Batch 480 \t Validation Loss: 40.28812017540137\n",
      "Epoch 21 \t Batch 500 \t Validation Loss: 39.935795899391174\n",
      "Epoch 21 \t Batch 520 \t Validation Loss: 39.64522372667606\n",
      "Epoch 21 \t Batch 540 \t Validation Loss: 39.375527557620295\n",
      "Epoch 21 \t Batch 560 \t Validation Loss: 39.14357959798404\n",
      "Epoch 21 \t Batch 580 \t Validation Loss: 38.8985624897069\n",
      "Epoch 21 \t Batch 600 \t Validation Loss: 39.08782405455907\n",
      "Epoch 21 Training Loss: 47.77038715657907 Validation Loss: 39.68496970619474\n",
      "Epoch 21 completed\n",
      "Epoch 22 \t Batch 20 \t Training Loss: 49.211815452575685\n",
      "Epoch 22 \t Batch 40 \t Training Loss: 48.17410078048706\n",
      "Epoch 22 \t Batch 60 \t Training Loss: 47.83055305480957\n",
      "Epoch 22 \t Batch 80 \t Training Loss: 47.70562224388122\n",
      "Epoch 22 \t Batch 100 \t Training Loss: 47.95903255462647\n",
      "Epoch 22 \t Batch 120 \t Training Loss: 47.98875176111857\n",
      "Epoch 22 \t Batch 140 \t Training Loss: 47.946038246154785\n",
      "Epoch 22 \t Batch 160 \t Training Loss: 48.05189230442047\n",
      "Epoch 22 \t Batch 180 \t Training Loss: 48.04656380547418\n",
      "Epoch 22 \t Batch 200 \t Training Loss: 47.978694190979006\n",
      "Epoch 22 \t Batch 220 \t Training Loss: 47.91533076546409\n",
      "Epoch 22 \t Batch 240 \t Training Loss: 48.06355535189311\n",
      "Epoch 22 \t Batch 260 \t Training Loss: 48.03387009547307\n",
      "Epoch 22 \t Batch 280 \t Training Loss: 47.98538748877389\n",
      "Epoch 22 \t Batch 300 \t Training Loss: 47.96414395650228\n",
      "Epoch 22 \t Batch 320 \t Training Loss: 47.899619948863986\n",
      "Epoch 22 \t Batch 340 \t Training Loss: 47.90292913773481\n",
      "Epoch 22 \t Batch 360 \t Training Loss: 47.911615986294215\n",
      "Epoch 22 \t Batch 380 \t Training Loss: 47.93931178042763\n",
      "Epoch 22 \t Batch 400 \t Training Loss: 47.95928636550903\n",
      "Epoch 22 \t Batch 420 \t Training Loss: 48.019166192554295\n",
      "Epoch 22 \t Batch 440 \t Training Loss: 48.00271915955977\n",
      "Epoch 22 \t Batch 460 \t Training Loss: 47.9787645837535\n",
      "Epoch 22 \t Batch 480 \t Training Loss: 48.07613859176636\n",
      "Epoch 22 \t Batch 500 \t Training Loss: 47.98154615020752\n",
      "Epoch 22 \t Batch 520 \t Training Loss: 47.959581705240105\n",
      "Epoch 22 \t Batch 540 \t Training Loss: 47.93337169929787\n",
      "Epoch 22 \t Batch 560 \t Training Loss: 47.85778900555202\n",
      "Epoch 22 \t Batch 580 \t Training Loss: 47.832090404115874\n",
      "Epoch 22 \t Batch 600 \t Training Loss: 47.798060773213706\n",
      "Epoch 22 \t Batch 620 \t Training Loss: 47.81221824153777\n",
      "Epoch 22 \t Batch 640 \t Training Loss: 47.805157041549684\n",
      "Epoch 22 \t Batch 660 \t Training Loss: 47.817200643366036\n",
      "Epoch 22 \t Batch 680 \t Training Loss: 47.79582347308888\n",
      "Epoch 22 \t Batch 700 \t Training Loss: 47.72083802904402\n",
      "Epoch 22 \t Batch 720 \t Training Loss: 47.731105428271825\n",
      "Epoch 22 \t Batch 740 \t Training Loss: 47.73977723508268\n",
      "Epoch 22 \t Batch 760 \t Training Loss: 47.79603779441432\n",
      "Epoch 22 \t Batch 780 \t Training Loss: 47.79225632594182\n",
      "Epoch 22 \t Batch 800 \t Training Loss: 47.75200132369995\n",
      "Epoch 22 \t Batch 820 \t Training Loss: 47.746342896252145\n",
      "Epoch 22 \t Batch 840 \t Training Loss: 47.75499171756563\n",
      "Epoch 22 \t Batch 860 \t Training Loss: 47.753963390616484\n",
      "Epoch 22 \t Batch 880 \t Training Loss: 47.7355927250602\n",
      "Epoch 22 \t Batch 900 \t Training Loss: 47.753792457580566\n",
      "Epoch 22 \t Batch 20 \t Validation Loss: 28.209683585166932\n",
      "Epoch 22 \t Batch 40 \t Validation Loss: 26.793672466278075\n",
      "Epoch 22 \t Batch 60 \t Validation Loss: 28.50652413368225\n",
      "Epoch 22 \t Batch 80 \t Validation Loss: 28.035354632139207\n",
      "Epoch 22 \t Batch 100 \t Validation Loss: 27.709213728904725\n",
      "Epoch 22 \t Batch 120 \t Validation Loss: 27.90638142824173\n",
      "Epoch 22 \t Batch 140 \t Validation Loss: 27.820020944731578\n",
      "Epoch 22 \t Batch 160 \t Validation Loss: 29.622947940230368\n",
      "Epoch 22 \t Batch 180 \t Validation Loss: 32.91090018749237\n",
      "Epoch 22 \t Batch 200 \t Validation Loss: 34.319155423641206\n",
      "Epoch 22 \t Batch 220 \t Validation Loss: 35.60371546962045\n",
      "Epoch 22 \t Batch 240 \t Validation Loss: 35.92655774950981\n",
      "Epoch 22 \t Batch 260 \t Validation Loss: 38.033586705647984\n",
      "Epoch 22 \t Batch 280 \t Validation Loss: 39.18855731998171\n",
      "Epoch 22 \t Batch 300 \t Validation Loss: 40.08895193258921\n",
      "Epoch 22 \t Batch 320 \t Validation Loss: 40.495631696283816\n",
      "Epoch 22 \t Batch 340 \t Validation Loss: 40.42296824315015\n",
      "Epoch 22 \t Batch 360 \t Validation Loss: 40.25916062063641\n",
      "Epoch 22 \t Batch 380 \t Validation Loss: 40.53697213248203\n",
      "Epoch 22 \t Batch 400 \t Validation Loss: 40.153916054964064\n",
      "Epoch 22 \t Batch 420 \t Validation Loss: 40.18301017057328\n",
      "Epoch 22 \t Batch 440 \t Validation Loss: 39.93543797081167\n",
      "Epoch 22 \t Batch 460 \t Validation Loss: 40.141264700889586\n",
      "Epoch 22 \t Batch 480 \t Validation Loss: 40.5886270771424\n",
      "Epoch 22 \t Batch 500 \t Validation Loss: 40.286089329719545\n",
      "Epoch 22 \t Batch 520 \t Validation Loss: 40.046125244177304\n",
      "Epoch 22 \t Batch 540 \t Validation Loss: 39.7647059696692\n",
      "Epoch 22 \t Batch 560 \t Validation Loss: 39.49600446990558\n",
      "Epoch 22 \t Batch 580 \t Validation Loss: 39.15293228626251\n",
      "Epoch 22 \t Batch 600 \t Validation Loss: 39.348193906943\n",
      "Epoch 22 Training Loss: 47.776042023037874 Validation Loss: 39.910570538663244\n",
      "Epoch 22 completed\n",
      "Epoch 23 \t Batch 20 \t Training Loss: 48.58965530395508\n",
      "Epoch 23 \t Batch 40 \t Training Loss: 47.9433235168457\n",
      "Epoch 23 \t Batch 60 \t Training Loss: 47.4275546391805\n",
      "Epoch 23 \t Batch 80 \t Training Loss: 47.25471410751343\n",
      "Epoch 23 \t Batch 100 \t Training Loss: 47.679379043579104\n",
      "Epoch 23 \t Batch 120 \t Training Loss: 47.451699352264406\n",
      "Epoch 23 \t Batch 140 \t Training Loss: 47.41371955871582\n",
      "Epoch 23 \t Batch 160 \t Training Loss: 47.42260262966156\n",
      "Epoch 23 \t Batch 180 \t Training Loss: 47.5734096315172\n",
      "Epoch 23 \t Batch 200 \t Training Loss: 47.38938209533691\n",
      "Epoch 23 \t Batch 220 \t Training Loss: 47.32143256447532\n",
      "Epoch 23 \t Batch 240 \t Training Loss: 47.30710406303406\n",
      "Epoch 23 \t Batch 260 \t Training Loss: 47.355721459021936\n",
      "Epoch 23 \t Batch 280 \t Training Loss: 47.44016763142177\n",
      "Epoch 23 \t Batch 300 \t Training Loss: 47.528483123779296\n",
      "Epoch 23 \t Batch 320 \t Training Loss: 47.47615795135498\n",
      "Epoch 23 \t Batch 340 \t Training Loss: 47.5709337683285\n",
      "Epoch 23 \t Batch 360 \t Training Loss: 47.506562084621855\n",
      "Epoch 23 \t Batch 380 \t Training Loss: 47.52475545782792\n",
      "Epoch 23 \t Batch 400 \t Training Loss: 47.58344320297241\n",
      "Epoch 23 \t Batch 420 \t Training Loss: 47.53242102123442\n",
      "Epoch 23 \t Batch 440 \t Training Loss: 47.499901407415216\n",
      "Epoch 23 \t Batch 460 \t Training Loss: 47.53867758875308\n",
      "Epoch 23 \t Batch 480 \t Training Loss: 47.556943917274474\n",
      "Epoch 23 \t Batch 500 \t Training Loss: 47.578440368652345\n",
      "Epoch 23 \t Batch 520 \t Training Loss: 47.54934413616474\n",
      "Epoch 23 \t Batch 540 \t Training Loss: 47.53424599965413\n",
      "Epoch 23 \t Batch 560 \t Training Loss: 47.549000304085865\n",
      "Epoch 23 \t Batch 580 \t Training Loss: 47.498086475503854\n",
      "Epoch 23 \t Batch 600 \t Training Loss: 47.457824115753176\n",
      "Epoch 23 \t Batch 620 \t Training Loss: 47.521486528458134\n",
      "Epoch 23 \t Batch 640 \t Training Loss: 47.495859348773955\n",
      "Epoch 23 \t Batch 660 \t Training Loss: 47.54802829280044\n",
      "Epoch 23 \t Batch 680 \t Training Loss: 47.50878497292014\n",
      "Epoch 23 \t Batch 700 \t Training Loss: 47.529887357439314\n",
      "Epoch 23 \t Batch 720 \t Training Loss: 47.54329916636149\n",
      "Epoch 23 \t Batch 740 \t Training Loss: 47.60688932779673\n",
      "Epoch 23 \t Batch 760 \t Training Loss: 47.632332109150134\n",
      "Epoch 23 \t Batch 780 \t Training Loss: 47.65989485520583\n",
      "Epoch 23 \t Batch 800 \t Training Loss: 47.67942842483521\n",
      "Epoch 23 \t Batch 820 \t Training Loss: 47.71394583539265\n",
      "Epoch 23 \t Batch 840 \t Training Loss: 47.693443175724575\n",
      "Epoch 23 \t Batch 860 \t Training Loss: 47.67949639253838\n",
      "Epoch 23 \t Batch 880 \t Training Loss: 47.709875817732375\n",
      "Epoch 23 \t Batch 900 \t Training Loss: 47.69671099768745\n",
      "Epoch 23 \t Batch 20 \t Validation Loss: 34.62867374420166\n",
      "Epoch 23 \t Batch 40 \t Validation Loss: 32.01776252985\n",
      "Epoch 23 \t Batch 60 \t Validation Loss: 34.058350157737735\n",
      "Epoch 23 \t Batch 80 \t Validation Loss: 33.17963963150978\n",
      "Epoch 23 \t Batch 100 \t Validation Loss: 32.00383991718292\n",
      "Epoch 23 \t Batch 120 \t Validation Loss: 31.634469131628673\n",
      "Epoch 23 \t Batch 140 \t Validation Loss: 31.09800456251417\n",
      "Epoch 23 \t Batch 160 \t Validation Loss: 32.521583595871924\n",
      "Epoch 23 \t Batch 180 \t Validation Loss: 35.68865375783708\n",
      "Epoch 23 \t Batch 200 \t Validation Loss: 36.86574699640274\n",
      "Epoch 23 \t Batch 220 \t Validation Loss: 37.976498579978944\n",
      "Epoch 23 \t Batch 240 \t Validation Loss: 38.173838692903516\n",
      "Epoch 23 \t Batch 260 \t Validation Loss: 40.15060105873988\n",
      "Epoch 23 \t Batch 280 \t Validation Loss: 41.15412212610245\n",
      "Epoch 23 \t Batch 300 \t Validation Loss: 42.049273451169334\n",
      "Epoch 23 \t Batch 320 \t Validation Loss: 42.38093280643225\n",
      "Epoch 23 \t Batch 340 \t Validation Loss: 42.21055684229907\n",
      "Epoch 23 \t Batch 360 \t Validation Loss: 41.979643216398024\n",
      "Epoch 23 \t Batch 380 \t Validation Loss: 42.16383980575361\n",
      "Epoch 23 \t Batch 400 \t Validation Loss: 41.6891349542141\n",
      "Epoch 23 \t Batch 420 \t Validation Loss: 41.62198914913904\n",
      "Epoch 23 \t Batch 440 \t Validation Loss: 41.287587056376715\n",
      "Epoch 23 \t Batch 460 \t Validation Loss: 41.41910490264063\n",
      "Epoch 23 \t Batch 480 \t Validation Loss: 41.80004581908385\n",
      "Epoch 23 \t Batch 500 \t Validation Loss: 41.44460496807098\n",
      "Epoch 23 \t Batch 520 \t Validation Loss: 41.170499338553505\n",
      "Epoch 23 \t Batch 540 \t Validation Loss: 40.85766575689669\n",
      "Epoch 23 \t Batch 560 \t Validation Loss: 40.59725283810071\n",
      "Epoch 23 \t Batch 580 \t Validation Loss: 40.33352895851793\n",
      "Epoch 23 \t Batch 600 \t Validation Loss: 40.47653931538264\n",
      "Epoch 23 Training Loss: 47.69885492688827 Validation Loss: 41.03575133580666\n",
      "Epoch 23 completed\n",
      "Epoch 24 \t Batch 20 \t Training Loss: 46.67488174438476\n",
      "Epoch 24 \t Batch 40 \t Training Loss: 47.28716926574707\n",
      "Epoch 24 \t Batch 60 \t Training Loss: 47.756149482727054\n",
      "Epoch 24 \t Batch 80 \t Training Loss: 47.95195450782776\n",
      "Epoch 24 \t Batch 100 \t Training Loss: 47.767263946533205\n",
      "Epoch 24 \t Batch 120 \t Training Loss: 47.50376453399658\n",
      "Epoch 24 \t Batch 140 \t Training Loss: 47.546910939897806\n",
      "Epoch 24 \t Batch 160 \t Training Loss: 47.68053143024444\n",
      "Epoch 24 \t Batch 180 \t Training Loss: 47.69553305308024\n",
      "Epoch 24 \t Batch 200 \t Training Loss: 47.790733318328854\n",
      "Epoch 24 \t Batch 220 \t Training Loss: 47.82564204822887\n",
      "Epoch 24 \t Batch 240 \t Training Loss: 47.73677287101746\n",
      "Epoch 24 \t Batch 260 \t Training Loss: 47.676269457890434\n",
      "Epoch 24 \t Batch 280 \t Training Loss: 47.580244241441996\n",
      "Epoch 24 \t Batch 300 \t Training Loss: 47.74571001688639\n",
      "Epoch 24 \t Batch 320 \t Training Loss: 47.82261174917221\n",
      "Epoch 24 \t Batch 340 \t Training Loss: 47.840318848105035\n",
      "Epoch 24 \t Batch 360 \t Training Loss: 47.79641969468859\n",
      "Epoch 24 \t Batch 380 \t Training Loss: 47.835523083335474\n",
      "Epoch 24 \t Batch 400 \t Training Loss: 47.86236234664917\n",
      "Epoch 24 \t Batch 420 \t Training Loss: 47.83533983684721\n",
      "Epoch 24 \t Batch 440 \t Training Loss: 47.853645012595436\n",
      "Epoch 24 \t Batch 460 \t Training Loss: 47.81554692309836\n",
      "Epoch 24 \t Batch 480 \t Training Loss: 47.81702162424723\n",
      "Epoch 24 \t Batch 500 \t Training Loss: 47.849593299865724\n",
      "Epoch 24 \t Batch 520 \t Training Loss: 47.76456694236168\n",
      "Epoch 24 \t Batch 540 \t Training Loss: 47.78217785446732\n",
      "Epoch 24 \t Batch 560 \t Training Loss: 47.81447293417794\n",
      "Epoch 24 \t Batch 580 \t Training Loss: 47.80454832932045\n",
      "Epoch 24 \t Batch 600 \t Training Loss: 47.76570149739583\n",
      "Epoch 24 \t Batch 620 \t Training Loss: 47.78175338622062\n",
      "Epoch 24 \t Batch 640 \t Training Loss: 47.75082063674927\n",
      "Epoch 24 \t Batch 660 \t Training Loss: 47.75792577338941\n",
      "Epoch 24 \t Batch 680 \t Training Loss: 47.75378020791447\n",
      "Epoch 24 \t Batch 700 \t Training Loss: 47.72785285949707\n",
      "Epoch 24 \t Batch 720 \t Training Loss: 47.71778245502048\n",
      "Epoch 24 \t Batch 740 \t Training Loss: 47.73759254764866\n",
      "Epoch 24 \t Batch 760 \t Training Loss: 47.6934087502329\n",
      "Epoch 24 \t Batch 780 \t Training Loss: 47.68900391994379\n",
      "Epoch 24 \t Batch 800 \t Training Loss: 47.662337646484374\n",
      "Epoch 24 \t Batch 820 \t Training Loss: 47.664288516160916\n",
      "Epoch 24 \t Batch 840 \t Training Loss: 47.64825245993478\n",
      "Epoch 24 \t Batch 860 \t Training Loss: 47.65929272673851\n",
      "Epoch 24 \t Batch 880 \t Training Loss: 47.67702241377397\n",
      "Epoch 24 \t Batch 900 \t Training Loss: 47.64043375227186\n",
      "Epoch 24 \t Batch 20 \t Validation Loss: 41.642144846916196\n",
      "Epoch 24 \t Batch 40 \t Validation Loss: 37.49595543146133\n",
      "Epoch 24 \t Batch 60 \t Validation Loss: 40.667210173606875\n",
      "Epoch 24 \t Batch 80 \t Validation Loss: 39.41321036219597\n",
      "Epoch 24 \t Batch 100 \t Validation Loss: 36.933488421440124\n",
      "Epoch 24 \t Batch 120 \t Validation Loss: 35.632313986619316\n",
      "Epoch 24 \t Batch 140 \t Validation Loss: 34.45317069802965\n",
      "Epoch 24 \t Batch 160 \t Validation Loss: 35.14813647568226\n",
      "Epoch 24 \t Batch 180 \t Validation Loss: 37.27353537612491\n",
      "Epoch 24 \t Batch 200 \t Validation Loss: 37.929210126399994\n",
      "Epoch 24 \t Batch 220 \t Validation Loss: 38.48439892638813\n",
      "Epoch 24 \t Batch 240 \t Validation Loss: 38.28516747355461\n",
      "Epoch 24 \t Batch 260 \t Validation Loss: 39.87441500700437\n",
      "Epoch 24 \t Batch 280 \t Validation Loss: 40.60943226303373\n",
      "Epoch 24 \t Batch 300 \t Validation Loss: 41.165854767163594\n",
      "Epoch 24 \t Batch 320 \t Validation Loss: 41.28707835227251\n",
      "Epoch 24 \t Batch 340 \t Validation Loss: 41.076268214337965\n",
      "Epoch 24 \t Batch 360 \t Validation Loss: 40.76689860158496\n",
      "Epoch 24 \t Batch 380 \t Validation Loss: 40.884021308547574\n",
      "Epoch 24 \t Batch 400 \t Validation Loss: 40.44121928334236\n",
      "Epoch 24 \t Batch 420 \t Validation Loss: 40.38819324516115\n",
      "Epoch 24 \t Batch 440 \t Validation Loss: 40.07120318954641\n",
      "Epoch 24 \t Batch 460 \t Validation Loss: 40.19652746138365\n",
      "Epoch 24 \t Batch 480 \t Validation Loss: 40.57943242092927\n",
      "Epoch 24 \t Batch 500 \t Validation Loss: 40.21235909366607\n",
      "Epoch 24 \t Batch 520 \t Validation Loss: 39.93915918882077\n",
      "Epoch 24 \t Batch 540 \t Validation Loss: 39.67207896709442\n",
      "Epoch 24 \t Batch 560 \t Validation Loss: 39.470771442992344\n",
      "Epoch 24 \t Batch 580 \t Validation Loss: 39.28322159421855\n",
      "Epoch 24 \t Batch 600 \t Validation Loss: 39.46607054789861\n",
      "Epoch 24 Training Loss: 47.661897661381744 Validation Loss: 40.10502888088102\n",
      "Epoch 24 completed\n",
      "Epoch 25 \t Batch 20 \t Training Loss: 46.388207817077635\n",
      "Epoch 25 \t Batch 40 \t Training Loss: 47.306355476379395\n",
      "Epoch 25 \t Batch 60 \t Training Loss: 47.40406443277995\n",
      "Epoch 25 \t Batch 80 \t Training Loss: 47.719753170013426\n",
      "Epoch 25 \t Batch 100 \t Training Loss: 47.60713657379151\n",
      "Epoch 25 \t Batch 120 \t Training Loss: 47.50373598734538\n",
      "Epoch 25 \t Batch 140 \t Training Loss: 47.534697014944896\n",
      "Epoch 25 \t Batch 160 \t Training Loss: 47.54137694835663\n",
      "Epoch 25 \t Batch 180 \t Training Loss: 47.550289916992185\n",
      "Epoch 25 \t Batch 200 \t Training Loss: 47.54998519897461\n",
      "Epoch 25 \t Batch 220 \t Training Loss: 47.52575007351962\n",
      "Epoch 25 \t Batch 240 \t Training Loss: 47.5816081682841\n",
      "Epoch 25 \t Batch 260 \t Training Loss: 47.690322259756236\n",
      "Epoch 25 \t Batch 280 \t Training Loss: 47.76143828800746\n",
      "Epoch 25 \t Batch 300 \t Training Loss: 47.78312254587809\n",
      "Epoch 25 \t Batch 320 \t Training Loss: 47.74468041658402\n",
      "Epoch 25 \t Batch 340 \t Training Loss: 47.725938740898584\n",
      "Epoch 25 \t Batch 360 \t Training Loss: 47.7124264187283\n",
      "Epoch 25 \t Batch 380 \t Training Loss: 47.62130518461529\n",
      "Epoch 25 \t Batch 400 \t Training Loss: 47.62461332321167\n",
      "Epoch 25 \t Batch 420 \t Training Loss: 47.63262047540574\n",
      "Epoch 25 \t Batch 440 \t Training Loss: 47.63213852102106\n",
      "Epoch 25 \t Batch 460 \t Training Loss: 47.61157627934995\n",
      "Epoch 25 \t Batch 480 \t Training Loss: 47.58374732335408\n",
      "Epoch 25 \t Batch 500 \t Training Loss: 47.55984991455078\n",
      "Epoch 25 \t Batch 520 \t Training Loss: 47.51814732184777\n",
      "Epoch 25 \t Batch 540 \t Training Loss: 47.49077163978859\n",
      "Epoch 25 \t Batch 560 \t Training Loss: 47.41964314324515\n",
      "Epoch 25 \t Batch 580 \t Training Loss: 47.371204099984006\n",
      "Epoch 25 \t Batch 600 \t Training Loss: 47.43073382059733\n",
      "Epoch 25 \t Batch 620 \t Training Loss: 47.4402625114687\n",
      "Epoch 25 \t Batch 640 \t Training Loss: 47.44372133612633\n",
      "Epoch 25 \t Batch 660 \t Training Loss: 47.40783475818056\n",
      "Epoch 25 \t Batch 680 \t Training Loss: 47.41175087199492\n",
      "Epoch 25 \t Batch 700 \t Training Loss: 47.47407143184117\n",
      "Epoch 25 \t Batch 720 \t Training Loss: 47.49027370346917\n",
      "Epoch 25 \t Batch 740 \t Training Loss: 47.51523667928335\n",
      "Epoch 25 \t Batch 760 \t Training Loss: 47.554609665117766\n",
      "Epoch 25 \t Batch 780 \t Training Loss: 47.53132255749825\n",
      "Epoch 25 \t Batch 800 \t Training Loss: 47.5318410205841\n",
      "Epoch 25 \t Batch 820 \t Training Loss: 47.56337549628281\n",
      "Epoch 25 \t Batch 840 \t Training Loss: 47.5528871581668\n",
      "Epoch 25 \t Batch 860 \t Training Loss: 47.56919655467188\n",
      "Epoch 25 \t Batch 880 \t Training Loss: 47.6192062811418\n",
      "Epoch 25 \t Batch 900 \t Training Loss: 47.63610462612576\n",
      "Epoch 25 \t Batch 20 \t Validation Loss: 42.024655199050905\n",
      "Epoch 25 \t Batch 40 \t Validation Loss: 37.501837241649625\n",
      "Epoch 25 \t Batch 60 \t Validation Loss: 40.8585521141688\n",
      "Epoch 25 \t Batch 80 \t Validation Loss: 39.231305128335954\n",
      "Epoch 25 \t Batch 100 \t Validation Loss: 36.77644091129303\n",
      "Epoch 25 \t Batch 120 \t Validation Loss: 35.383663682142895\n",
      "Epoch 25 \t Batch 140 \t Validation Loss: 34.24555641923632\n",
      "Epoch 25 \t Batch 160 \t Validation Loss: 35.23797691762447\n",
      "Epoch 25 \t Batch 180 \t Validation Loss: 37.89193142520057\n",
      "Epoch 25 \t Batch 200 \t Validation Loss: 38.76887651205063\n",
      "Epoch 25 \t Batch 220 \t Validation Loss: 39.561000631072304\n",
      "Epoch 25 \t Batch 240 \t Validation Loss: 39.51949754357338\n",
      "Epoch 25 \t Batch 260 \t Validation Loss: 41.29294863664187\n",
      "Epoch 25 \t Batch 280 \t Validation Loss: 42.110907529081615\n",
      "Epoch 25 \t Batch 300 \t Validation Loss: 42.84450480937958\n",
      "Epoch 25 \t Batch 320 \t Validation Loss: 43.02386788874865\n",
      "Epoch 25 \t Batch 340 \t Validation Loss: 42.757350440586315\n",
      "Epoch 25 \t Batch 360 \t Validation Loss: 42.454264001051584\n",
      "Epoch 25 \t Batch 380 \t Validation Loss: 42.57458120270779\n",
      "Epoch 25 \t Batch 400 \t Validation Loss: 42.06699182152748\n",
      "Epoch 25 \t Batch 420 \t Validation Loss: 41.95446133045923\n",
      "Epoch 25 \t Batch 440 \t Validation Loss: 41.580904398181225\n",
      "Epoch 25 \t Batch 460 \t Validation Loss: 41.68162805412127\n",
      "Epoch 25 \t Batch 480 \t Validation Loss: 42.03247528970242\n",
      "Epoch 25 \t Batch 500 \t Validation Loss: 41.64218374347687\n",
      "Epoch 25 \t Batch 520 \t Validation Loss: 41.389102107744954\n",
      "Epoch 25 \t Batch 540 \t Validation Loss: 41.11324803829193\n",
      "Epoch 25 \t Batch 560 \t Validation Loss: 40.93578327127865\n",
      "Epoch 25 \t Batch 580 \t Validation Loss: 40.85752157753912\n",
      "Epoch 25 \t Batch 600 \t Validation Loss: 41.031189680894215\n",
      "Epoch 25 Training Loss: 47.611858471919305 Validation Loss: 41.66608423149431\n",
      "Epoch 25 completed\n",
      "Epoch 26 \t Batch 20 \t Training Loss: 46.341492080688475\n",
      "Epoch 26 \t Batch 40 \t Training Loss: 46.749876976013184\n",
      "Epoch 26 \t Batch 60 \t Training Loss: 47.1753298441569\n",
      "Epoch 26 \t Batch 80 \t Training Loss: 46.9210018157959\n",
      "Epoch 26 \t Batch 100 \t Training Loss: 47.11272243499756\n",
      "Epoch 26 \t Batch 120 \t Training Loss: 47.24171593983968\n",
      "Epoch 26 \t Batch 140 \t Training Loss: 47.17390471867153\n",
      "Epoch 26 \t Batch 160 \t Training Loss: 47.142278337478636\n",
      "Epoch 26 \t Batch 180 \t Training Loss: 47.24309868282742\n",
      "Epoch 26 \t Batch 200 \t Training Loss: 47.341584568023684\n",
      "Epoch 26 \t Batch 220 \t Training Loss: 47.452085529674186\n",
      "Epoch 26 \t Batch 240 \t Training Loss: 47.493929688135786\n",
      "Epoch 26 \t Batch 260 \t Training Loss: 47.47158474555383\n",
      "Epoch 26 \t Batch 280 \t Training Loss: 47.5066933631897\n",
      "Epoch 26 \t Batch 300 \t Training Loss: 47.582072995503744\n",
      "Epoch 26 \t Batch 320 \t Training Loss: 47.536336195468905\n",
      "Epoch 26 \t Batch 340 \t Training Loss: 47.57255433026482\n",
      "Epoch 26 \t Batch 360 \t Training Loss: 47.52251275380453\n",
      "Epoch 26 \t Batch 380 \t Training Loss: 47.433309735749894\n",
      "Epoch 26 \t Batch 400 \t Training Loss: 47.41104377746582\n",
      "Epoch 26 \t Batch 420 \t Training Loss: 47.3982064020066\n",
      "Epoch 26 \t Batch 440 \t Training Loss: 47.35067657990889\n",
      "Epoch 26 \t Batch 460 \t Training Loss: 47.38642296998397\n",
      "Epoch 26 \t Batch 480 \t Training Loss: 47.40135691165924\n",
      "Epoch 26 \t Batch 500 \t Training Loss: 47.409588531494144\n",
      "Epoch 26 \t Batch 520 \t Training Loss: 47.48326088098379\n",
      "Epoch 26 \t Batch 540 \t Training Loss: 47.46640578375922\n",
      "Epoch 26 \t Batch 560 \t Training Loss: 47.46878782681056\n",
      "Epoch 26 \t Batch 580 \t Training Loss: 47.47283734288709\n",
      "Epoch 26 \t Batch 600 \t Training Loss: 47.56975121815999\n",
      "Epoch 26 \t Batch 620 \t Training Loss: 47.527179071980136\n",
      "Epoch 26 \t Batch 640 \t Training Loss: 47.5442634165287\n",
      "Epoch 26 \t Batch 660 \t Training Loss: 47.51099677230373\n",
      "Epoch 26 \t Batch 680 \t Training Loss: 47.53872678981108\n",
      "Epoch 26 \t Batch 700 \t Training Loss: 47.55761109488351\n",
      "Epoch 26 \t Batch 720 \t Training Loss: 47.5823350376553\n",
      "Epoch 26 \t Batch 740 \t Training Loss: 47.568279890111974\n",
      "Epoch 26 \t Batch 760 \t Training Loss: 47.56293432336105\n",
      "Epoch 26 \t Batch 780 \t Training Loss: 47.58811334463266\n",
      "Epoch 26 \t Batch 800 \t Training Loss: 47.584734716415404\n",
      "Epoch 26 \t Batch 820 \t Training Loss: 47.601912437997214\n",
      "Epoch 26 \t Batch 840 \t Training Loss: 47.59887476421538\n",
      "Epoch 26 \t Batch 860 \t Training Loss: 47.61615864066191\n",
      "Epoch 26 \t Batch 880 \t Training Loss: 47.62832467772744\n",
      "Epoch 26 \t Batch 900 \t Training Loss: 47.60917177412245\n",
      "Epoch 26 \t Batch 20 \t Validation Loss: 35.24203336238861\n",
      "Epoch 26 \t Batch 40 \t Validation Loss: 32.561922532320025\n",
      "Epoch 26 \t Batch 60 \t Validation Loss: 34.93037476936976\n",
      "Epoch 26 \t Batch 80 \t Validation Loss: 34.255753728747365\n",
      "Epoch 26 \t Batch 100 \t Validation Loss: 32.5574691939354\n",
      "Epoch 26 \t Batch 120 \t Validation Loss: 31.847749509414037\n",
      "Epoch 26 \t Batch 140 \t Validation Loss: 31.218861312525615\n",
      "Epoch 26 \t Batch 160 \t Validation Loss: 32.594135873019695\n",
      "Epoch 26 \t Batch 180 \t Validation Loss: 35.444080446826085\n",
      "Epoch 26 \t Batch 200 \t Validation Loss: 36.51031134486198\n",
      "Epoch 26 \t Batch 220 \t Validation Loss: 37.48697757612575\n",
      "Epoch 26 \t Batch 240 \t Validation Loss: 37.61823845207691\n",
      "Epoch 26 \t Batch 260 \t Validation Loss: 39.49630706035174\n",
      "Epoch 26 \t Batch 280 \t Validation Loss: 40.40634442993573\n",
      "Epoch 26 \t Batch 300 \t Validation Loss: 41.22967910369237\n",
      "Epoch 26 \t Batch 320 \t Validation Loss: 41.53601238951087\n",
      "Epoch 26 \t Batch 340 \t Validation Loss: 41.38257187324412\n",
      "Epoch 26 \t Batch 360 \t Validation Loss: 41.182970623837576\n",
      "Epoch 26 \t Batch 380 \t Validation Loss: 41.35977581864909\n",
      "Epoch 26 \t Batch 400 \t Validation Loss: 40.912222828269\n",
      "Epoch 26 \t Batch 420 \t Validation Loss: 40.86034410056614\n",
      "Epoch 26 \t Batch 440 \t Validation Loss: 40.56432484659282\n",
      "Epoch 26 \t Batch 460 \t Validation Loss: 40.72533062592797\n",
      "Epoch 26 \t Batch 480 \t Validation Loss: 41.131297722955544\n",
      "Epoch 26 \t Batch 500 \t Validation Loss: 40.78644729566574\n",
      "Epoch 26 \t Batch 520 \t Validation Loss: 40.57186790016981\n",
      "Epoch 26 \t Batch 540 \t Validation Loss: 40.31904612426405\n",
      "Epoch 26 \t Batch 560 \t Validation Loss: 40.13688894808293\n",
      "Epoch 26 \t Batch 580 \t Validation Loss: 40.01612951138924\n",
      "Epoch 26 \t Batch 600 \t Validation Loss: 40.203831880489986\n",
      "Epoch 26 Training Loss: 47.586272906321085 Validation Loss: 40.83335799791596\n",
      "Epoch 26 completed\n",
      "Epoch 27 \t Batch 20 \t Training Loss: 48.436892700195315\n",
      "Epoch 27 \t Batch 40 \t Training Loss: 47.709919166564944\n",
      "Epoch 27 \t Batch 60 \t Training Loss: 47.703891563415525\n",
      "Epoch 27 \t Batch 80 \t Training Loss: 47.5881085395813\n",
      "Epoch 27 \t Batch 100 \t Training Loss: 47.38200889587402\n",
      "Epoch 27 \t Batch 120 \t Training Loss: 47.341500504811606\n",
      "Epoch 27 \t Batch 140 \t Training Loss: 47.49991406032017\n",
      "Epoch 27 \t Batch 160 \t Training Loss: 47.3735276222229\n",
      "Epoch 27 \t Batch 180 \t Training Loss: 47.58720249599881\n",
      "Epoch 27 \t Batch 200 \t Training Loss: 47.486479358673094\n",
      "Epoch 27 \t Batch 220 \t Training Loss: 47.43533970225941\n",
      "Epoch 27 \t Batch 240 \t Training Loss: 47.3978408018748\n",
      "Epoch 27 \t Batch 260 \t Training Loss: 47.3173349233774\n",
      "Epoch 27 \t Batch 280 \t Training Loss: 47.34396685191563\n",
      "Epoch 27 \t Batch 300 \t Training Loss: 47.330023040771486\n",
      "Epoch 27 \t Batch 320 \t Training Loss: 47.347925055027005\n",
      "Epoch 27 \t Batch 340 \t Training Loss: 47.27283463197596\n",
      "Epoch 27 \t Batch 360 \t Training Loss: 47.24355546103583\n",
      "Epoch 27 \t Batch 380 \t Training Loss: 47.293327672857984\n",
      "Epoch 27 \t Batch 400 \t Training Loss: 47.26141194343567\n",
      "Epoch 27 \t Batch 420 \t Training Loss: 47.32291983649844\n",
      "Epoch 27 \t Batch 440 \t Training Loss: 47.33014951185746\n",
      "Epoch 27 \t Batch 460 \t Training Loss: 47.305153307707414\n",
      "Epoch 27 \t Batch 480 \t Training Loss: 47.23681613604228\n",
      "Epoch 27 \t Batch 500 \t Training Loss: 47.220014595031735\n",
      "Epoch 27 \t Batch 520 \t Training Loss: 47.28063496076144\n",
      "Epoch 27 \t Batch 540 \t Training Loss: 47.27628002166748\n",
      "Epoch 27 \t Batch 560 \t Training Loss: 47.26119976724897\n",
      "Epoch 27 \t Batch 580 \t Training Loss: 47.322667549396385\n",
      "Epoch 27 \t Batch 600 \t Training Loss: 47.31958240509033\n",
      "Epoch 27 \t Batch 620 \t Training Loss: 47.38241630677254\n",
      "Epoch 27 \t Batch 640 \t Training Loss: 47.42400156855583\n",
      "Epoch 27 \t Batch 660 \t Training Loss: 47.42013367739591\n",
      "Epoch 27 \t Batch 680 \t Training Loss: 47.43430080974803\n",
      "Epoch 27 \t Batch 700 \t Training Loss: 47.45450138092041\n",
      "Epoch 27 \t Batch 720 \t Training Loss: 47.46710733307732\n",
      "Epoch 27 \t Batch 740 \t Training Loss: 47.50647151534622\n",
      "Epoch 27 \t Batch 760 \t Training Loss: 47.486067350287186\n",
      "Epoch 27 \t Batch 780 \t Training Loss: 47.397281680962976\n",
      "Epoch 27 \t Batch 800 \t Training Loss: 47.39888718128204\n",
      "Epoch 27 \t Batch 820 \t Training Loss: 47.41361001177532\n",
      "Epoch 27 \t Batch 840 \t Training Loss: 47.47270726703462\n",
      "Epoch 27 \t Batch 860 \t Training Loss: 47.52143945028615\n",
      "Epoch 27 \t Batch 880 \t Training Loss: 47.49481116208163\n",
      "Epoch 27 \t Batch 900 \t Training Loss: 47.52381803300646\n",
      "Epoch 27 \t Batch 20 \t Validation Loss: 42.953519105911255\n",
      "Epoch 27 \t Batch 40 \t Validation Loss: 37.43647336959839\n",
      "Epoch 27 \t Batch 60 \t Validation Loss: 40.8934566338857\n",
      "Epoch 27 \t Batch 80 \t Validation Loss: 39.37498360872269\n",
      "Epoch 27 \t Batch 100 \t Validation Loss: 36.97434630393982\n",
      "Epoch 27 \t Batch 120 \t Validation Loss: 35.80704605579376\n",
      "Epoch 27 \t Batch 140 \t Validation Loss: 34.623047372273035\n",
      "Epoch 27 \t Batch 160 \t Validation Loss: 35.51434229016304\n",
      "Epoch 27 \t Batch 180 \t Validation Loss: 38.16908571985033\n",
      "Epoch 27 \t Batch 200 \t Validation Loss: 38.88371224880218\n",
      "Epoch 27 \t Batch 220 \t Validation Loss: 39.702676829424774\n",
      "Epoch 27 \t Batch 240 \t Validation Loss: 39.681217737992604\n",
      "Epoch 27 \t Batch 260 \t Validation Loss: 41.43661200450017\n",
      "Epoch 27 \t Batch 280 \t Validation Loss: 42.25913567883628\n",
      "Epoch 27 \t Batch 300 \t Validation Loss: 42.97211059888204\n",
      "Epoch 27 \t Batch 320 \t Validation Loss: 43.16251192986965\n",
      "Epoch 27 \t Batch 340 \t Validation Loss: 42.887967421026794\n",
      "Epoch 27 \t Batch 360 \t Validation Loss: 42.541378696759544\n",
      "Epoch 27 \t Batch 380 \t Validation Loss: 42.60416023103814\n",
      "Epoch 27 \t Batch 400 \t Validation Loss: 42.0072975897789\n",
      "Epoch 27 \t Batch 420 \t Validation Loss: 41.87767603510902\n",
      "Epoch 27 \t Batch 440 \t Validation Loss: 41.435432288863446\n",
      "Epoch 27 \t Batch 460 \t Validation Loss: 41.51443195135697\n",
      "Epoch 27 \t Batch 480 \t Validation Loss: 41.84826874534289\n",
      "Epoch 27 \t Batch 500 \t Validation Loss: 41.45750102424621\n",
      "Epoch 27 \t Batch 520 \t Validation Loss: 41.06108476565434\n",
      "Epoch 27 \t Batch 540 \t Validation Loss: 40.70904366528546\n",
      "Epoch 27 \t Batch 560 \t Validation Loss: 40.399023618016926\n",
      "Epoch 27 \t Batch 580 \t Validation Loss: 39.96377949056954\n",
      "Epoch 27 \t Batch 600 \t Validation Loss: 40.13622369925181\n",
      "Epoch 27 Training Loss: 47.53269817472934 Validation Loss: 40.65904210294996\n",
      "Epoch 27 completed\n",
      "Epoch 28 \t Batch 20 \t Training Loss: 47.385654258728025\n",
      "Epoch 28 \t Batch 40 \t Training Loss: 47.805277252197264\n",
      "Epoch 28 \t Batch 60 \t Training Loss: 47.91029987335205\n",
      "Epoch 28 \t Batch 80 \t Training Loss: 47.49099235534668\n",
      "Epoch 28 \t Batch 100 \t Training Loss: 47.48392837524414\n",
      "Epoch 28 \t Batch 120 \t Training Loss: 47.39367151260376\n",
      "Epoch 28 \t Batch 140 \t Training Loss: 47.525260761805946\n",
      "Epoch 28 \t Batch 160 \t Training Loss: 47.53153393268585\n",
      "Epoch 28 \t Batch 180 \t Training Loss: 47.46938917371962\n",
      "Epoch 28 \t Batch 200 \t Training Loss: 47.449838886260984\n",
      "Epoch 28 \t Batch 220 \t Training Loss: 47.469626634771174\n",
      "Epoch 28 \t Batch 240 \t Training Loss: 47.534552876154585\n",
      "Epoch 28 \t Batch 260 \t Training Loss: 47.516251652057356\n",
      "Epoch 28 \t Batch 280 \t Training Loss: 47.490120329175674\n",
      "Epoch 28 \t Batch 300 \t Training Loss: 47.40467138926188\n",
      "Epoch 28 \t Batch 320 \t Training Loss: 47.36453720331192\n",
      "Epoch 28 \t Batch 340 \t Training Loss: 47.40169646319221\n",
      "Epoch 28 \t Batch 360 \t Training Loss: 47.347504160139295\n",
      "Epoch 28 \t Batch 380 \t Training Loss: 47.35400635568719\n",
      "Epoch 28 \t Batch 400 \t Training Loss: 47.35938943862915\n",
      "Epoch 28 \t Batch 420 \t Training Loss: 47.3588336127145\n",
      "Epoch 28 \t Batch 440 \t Training Loss: 47.35613912235607\n",
      "Epoch 28 \t Batch 460 \t Training Loss: 47.39887467259946\n",
      "Epoch 28 \t Batch 480 \t Training Loss: 47.36744959354401\n",
      "Epoch 28 \t Batch 500 \t Training Loss: 47.46229703521728\n",
      "Epoch 28 \t Batch 520 \t Training Loss: 47.514563604501575\n",
      "Epoch 28 \t Batch 540 \t Training Loss: 47.4929178237915\n",
      "Epoch 28 \t Batch 560 \t Training Loss: 47.53841227803912\n",
      "Epoch 28 \t Batch 580 \t Training Loss: 47.5257334347429\n",
      "Epoch 28 \t Batch 600 \t Training Loss: 47.48809783299764\n",
      "Epoch 28 \t Batch 620 \t Training Loss: 47.49889020612163\n",
      "Epoch 28 \t Batch 640 \t Training Loss: 47.50307845473289\n",
      "Epoch 28 \t Batch 660 \t Training Loss: 47.51470828778816\n",
      "Epoch 28 \t Batch 680 \t Training Loss: 47.50481429941514\n",
      "Epoch 28 \t Batch 700 \t Training Loss: 47.489770976475306\n",
      "Epoch 28 \t Batch 720 \t Training Loss: 47.47600659794278\n",
      "Epoch 28 \t Batch 740 \t Training Loss: 47.45573156717661\n",
      "Epoch 28 \t Batch 760 \t Training Loss: 47.46471781479685\n",
      "Epoch 28 \t Batch 780 \t Training Loss: 47.49411964416504\n",
      "Epoch 28 \t Batch 800 \t Training Loss: 47.50047066688538\n",
      "Epoch 28 \t Batch 820 \t Training Loss: 47.49287084253823\n",
      "Epoch 28 \t Batch 840 \t Training Loss: 47.487219329107376\n",
      "Epoch 28 \t Batch 860 \t Training Loss: 47.494918752271076\n",
      "Epoch 28 \t Batch 880 \t Training Loss: 47.46208675991405\n",
      "Epoch 28 \t Batch 900 \t Training Loss: 47.46314701504178\n",
      "Epoch 28 \t Batch 20 \t Validation Loss: 36.4716561794281\n",
      "Epoch 28 \t Batch 40 \t Validation Loss: 33.129441905021665\n",
      "Epoch 28 \t Batch 60 \t Validation Loss: 35.651973470052084\n",
      "Epoch 28 \t Batch 80 \t Validation Loss: 34.640761613845825\n",
      "Epoch 28 \t Batch 100 \t Validation Loss: 33.11797069549561\n",
      "Epoch 28 \t Batch 120 \t Validation Loss: 32.46308053334554\n",
      "Epoch 28 \t Batch 140 \t Validation Loss: 31.74336722237723\n",
      "Epoch 28 \t Batch 160 \t Validation Loss: 32.96153095960617\n",
      "Epoch 28 \t Batch 180 \t Validation Loss: 35.801567792892456\n",
      "Epoch 28 \t Batch 200 \t Validation Loss: 36.847511887550354\n",
      "Epoch 28 \t Batch 220 \t Validation Loss: 37.771596843546085\n",
      "Epoch 28 \t Batch 240 \t Validation Loss: 37.8537667075793\n",
      "Epoch 28 \t Batch 260 \t Validation Loss: 39.71876993912917\n",
      "Epoch 28 \t Batch 280 \t Validation Loss: 40.632254774229864\n",
      "Epoch 28 \t Batch 300 \t Validation Loss: 41.39963041623434\n",
      "Epoch 28 \t Batch 320 \t Validation Loss: 41.63959036171436\n",
      "Epoch 28 \t Batch 340 \t Validation Loss: 41.4458769938525\n",
      "Epoch 28 \t Batch 360 \t Validation Loss: 41.17903253502316\n",
      "Epoch 28 \t Batch 380 \t Validation Loss: 41.32133049462971\n",
      "Epoch 28 \t Batch 400 \t Validation Loss: 40.848267600536346\n",
      "Epoch 28 \t Batch 420 \t Validation Loss: 40.766184409459434\n",
      "Epoch 28 \t Batch 440 \t Validation Loss: 40.413487150452355\n",
      "Epoch 28 \t Batch 460 \t Validation Loss: 40.54753416102866\n",
      "Epoch 28 \t Batch 480 \t Validation Loss: 40.930843283732735\n",
      "Epoch 28 \t Batch 500 \t Validation Loss: 40.573308122634884\n",
      "Epoch 28 \t Batch 520 \t Validation Loss: 40.29086720576653\n",
      "Epoch 28 \t Batch 540 \t Validation Loss: 40.00696649374785\n",
      "Epoch 28 \t Batch 560 \t Validation Loss: 39.749992297376906\n",
      "Epoch 28 \t Batch 580 \t Validation Loss: 39.4634960782939\n",
      "Epoch 28 \t Batch 600 \t Validation Loss: 39.631637399991355\n",
      "Epoch 28 Training Loss: 47.492343132992474 Validation Loss: 40.2057212095756\n",
      "Epoch 28 completed\n",
      "Epoch 29 \t Batch 20 \t Training Loss: 46.172465324401855\n",
      "Epoch 29 \t Batch 40 \t Training Loss: 46.95691547393799\n",
      "Epoch 29 \t Batch 60 \t Training Loss: 47.08551413218181\n",
      "Epoch 29 \t Batch 80 \t Training Loss: 47.2482430934906\n",
      "Epoch 29 \t Batch 100 \t Training Loss: 47.405761337280275\n",
      "Epoch 29 \t Batch 120 \t Training Loss: 47.248041502634685\n",
      "Epoch 29 \t Batch 140 \t Training Loss: 47.090409769330705\n",
      "Epoch 29 \t Batch 160 \t Training Loss: 47.093545126914975\n",
      "Epoch 29 \t Batch 180 \t Training Loss: 47.02934210035536\n",
      "Epoch 29 \t Batch 200 \t Training Loss: 47.12313024520874\n",
      "Epoch 29 \t Batch 220 \t Training Loss: 47.15361924604936\n",
      "Epoch 29 \t Batch 240 \t Training Loss: 47.20359578132629\n",
      "Epoch 29 \t Batch 260 \t Training Loss: 47.24767514742338\n",
      "Epoch 29 \t Batch 280 \t Training Loss: 47.191865757533485\n",
      "Epoch 29 \t Batch 300 \t Training Loss: 47.34768772125244\n",
      "Epoch 29 \t Batch 320 \t Training Loss: 47.3956329703331\n",
      "Epoch 29 \t Batch 340 \t Training Loss: 47.51362323760986\n",
      "Epoch 29 \t Batch 360 \t Training Loss: 47.540869098239476\n",
      "Epoch 29 \t Batch 380 \t Training Loss: 47.54684662066008\n",
      "Epoch 29 \t Batch 400 \t Training Loss: 47.51763560295105\n",
      "Epoch 29 \t Batch 420 \t Training Loss: 47.47545421237037\n",
      "Epoch 29 \t Batch 440 \t Training Loss: 47.458397908644244\n",
      "Epoch 29 \t Batch 460 \t Training Loss: 47.43887950233791\n",
      "Epoch 29 \t Batch 480 \t Training Loss: 47.48163944085439\n",
      "Epoch 29 \t Batch 500 \t Training Loss: 47.51849046325684\n",
      "Epoch 29 \t Batch 520 \t Training Loss: 47.46035870772142\n",
      "Epoch 29 \t Batch 540 \t Training Loss: 47.49420775660762\n",
      "Epoch 29 \t Batch 560 \t Training Loss: 47.5530960559845\n",
      "Epoch 29 \t Batch 580 \t Training Loss: 47.590648749779014\n",
      "Epoch 29 \t Batch 600 \t Training Loss: 47.57500258127848\n",
      "Epoch 29 \t Batch 620 \t Training Loss: 47.5825252532959\n",
      "Epoch 29 \t Batch 640 \t Training Loss: 47.540112578868865\n",
      "Epoch 29 \t Batch 660 \t Training Loss: 47.54926465930361\n",
      "Epoch 29 \t Batch 680 \t Training Loss: 47.523065443599926\n",
      "Epoch 29 \t Batch 700 \t Training Loss: 47.53512771606445\n",
      "Epoch 29 \t Batch 720 \t Training Loss: 47.51351900100708\n",
      "Epoch 29 \t Batch 740 \t Training Loss: 47.51420285508439\n",
      "Epoch 29 \t Batch 760 \t Training Loss: 47.477702366678336\n",
      "Epoch 29 \t Batch 780 \t Training Loss: 47.519629977299616\n",
      "Epoch 29 \t Batch 800 \t Training Loss: 47.51943007946014\n",
      "Epoch 29 \t Batch 820 \t Training Loss: 47.4685927972561\n",
      "Epoch 29 \t Batch 840 \t Training Loss: 47.46782590321132\n",
      "Epoch 29 \t Batch 860 \t Training Loss: 47.505680150763936\n",
      "Epoch 29 \t Batch 880 \t Training Loss: 47.48192210197449\n",
      "Epoch 29 \t Batch 900 \t Training Loss: 47.46590472327338\n",
      "Epoch 29 \t Batch 20 \t Validation Loss: 52.13177146911621\n",
      "Epoch 29 \t Batch 40 \t Validation Loss: 45.29233996868133\n",
      "Epoch 29 \t Batch 60 \t Validation Loss: 49.372730843226115\n",
      "Epoch 29 \t Batch 80 \t Validation Loss: 47.21472009420395\n",
      "Epoch 29 \t Batch 100 \t Validation Loss: 43.360463533401486\n",
      "Epoch 29 \t Batch 120 \t Validation Loss: 41.184888037045795\n",
      "Epoch 29 \t Batch 140 \t Validation Loss: 39.30747506959098\n",
      "Epoch 29 \t Batch 160 \t Validation Loss: 39.63471266627312\n",
      "Epoch 29 \t Batch 180 \t Validation Loss: 41.88752688831753\n",
      "Epoch 29 \t Batch 200 \t Validation Loss: 42.50637423038483\n",
      "Epoch 29 \t Batch 220 \t Validation Loss: 43.00586953163147\n",
      "Epoch 29 \t Batch 240 \t Validation Loss: 42.72186383008957\n",
      "Epoch 29 \t Batch 260 \t Validation Loss: 44.23964040462787\n",
      "Epoch 29 \t Batch 280 \t Validation Loss: 44.852839837755475\n",
      "Epoch 29 \t Batch 300 \t Validation Loss: 45.42731688181559\n",
      "Epoch 29 \t Batch 320 \t Validation Loss: 45.48578447699547\n",
      "Epoch 29 \t Batch 340 \t Validation Loss: 45.121343584621656\n",
      "Epoch 29 \t Batch 360 \t Validation Loss: 44.6529051568773\n",
      "Epoch 29 \t Batch 380 \t Validation Loss: 44.626120818288705\n",
      "Epoch 29 \t Batch 400 \t Validation Loss: 44.03964578151703\n",
      "Epoch 29 \t Batch 420 \t Validation Loss: 43.896433153606594\n",
      "Epoch 29 \t Batch 440 \t Validation Loss: 43.470224311135034\n",
      "Epoch 29 \t Batch 460 \t Validation Loss: 43.49814629762069\n",
      "Epoch 29 \t Batch 480 \t Validation Loss: 43.814400760332745\n",
      "Epoch 29 \t Batch 500 \t Validation Loss: 43.3801820602417\n",
      "Epoch 29 \t Batch 520 \t Validation Loss: 42.98164327328022\n",
      "Epoch 29 \t Batch 540 \t Validation Loss: 42.52086334581728\n",
      "Epoch 29 \t Batch 560 \t Validation Loss: 42.114113865579874\n",
      "Epoch 29 \t Batch 580 \t Validation Loss: 41.653759101341514\n",
      "Epoch 29 \t Batch 600 \t Validation Loss: 41.707348359425865\n",
      "Epoch 29 Training Loss: 47.469587794047015 Validation Loss: 42.19298336722634\n",
      "Epoch 29 completed\n",
      "Epoch 30 \t Batch 20 \t Training Loss: 47.53890609741211\n",
      "Epoch 30 \t Batch 40 \t Training Loss: 46.987098693847656\n",
      "Epoch 30 \t Batch 60 \t Training Loss: 47.36090641021728\n",
      "Epoch 30 \t Batch 80 \t Training Loss: 47.33160195350647\n",
      "Epoch 30 \t Batch 100 \t Training Loss: 47.402949562072756\n",
      "Epoch 30 \t Batch 120 \t Training Loss: 47.302496846516924\n",
      "Epoch 30 \t Batch 140 \t Training Loss: 47.36265425000872\n",
      "Epoch 30 \t Batch 160 \t Training Loss: 47.417774891853334\n",
      "Epoch 30 \t Batch 180 \t Training Loss: 47.47771519554986\n",
      "Epoch 30 \t Batch 200 \t Training Loss: 47.32404890060425\n",
      "Epoch 30 \t Batch 220 \t Training Loss: 47.503034661032935\n",
      "Epoch 30 \t Batch 240 \t Training Loss: 47.5015434106191\n",
      "Epoch 30 \t Batch 260 \t Training Loss: 47.61419420975905\n",
      "Epoch 30 \t Batch 280 \t Training Loss: 47.56950907026018\n",
      "Epoch 30 \t Batch 300 \t Training Loss: 47.599231783548994\n",
      "Epoch 30 \t Batch 320 \t Training Loss: 47.6021388053894\n",
      "Epoch 30 \t Batch 340 \t Training Loss: 47.72421100840849\n",
      "Epoch 30 \t Batch 360 \t Training Loss: 47.78616480297512\n",
      "Epoch 30 \t Batch 380 \t Training Loss: 47.76017831501208\n",
      "Epoch 30 \t Batch 400 \t Training Loss: 47.794037895202635\n",
      "Epoch 30 \t Batch 420 \t Training Loss: 47.783270735967726\n",
      "Epoch 30 \t Batch 440 \t Training Loss: 47.7215632872148\n",
      "Epoch 30 \t Batch 460 \t Training Loss: 47.67394084101138\n",
      "Epoch 30 \t Batch 480 \t Training Loss: 47.60262324810028\n",
      "Epoch 30 \t Batch 500 \t Training Loss: 47.613392288208004\n",
      "Epoch 30 \t Batch 520 \t Training Loss: 47.61487211080698\n",
      "Epoch 30 \t Batch 540 \t Training Loss: 47.634078096460414\n",
      "Epoch 30 \t Batch 560 \t Training Loss: 47.63148471287319\n",
      "Epoch 30 \t Batch 580 \t Training Loss: 47.58930519367087\n",
      "Epoch 30 \t Batch 600 \t Training Loss: 47.607358570098874\n",
      "Epoch 30 \t Batch 620 \t Training Loss: 47.61819309111564\n",
      "Epoch 30 \t Batch 640 \t Training Loss: 47.60934266448021\n",
      "Epoch 30 \t Batch 660 \t Training Loss: 47.59055974555738\n",
      "Epoch 30 \t Batch 680 \t Training Loss: 47.585034583596624\n",
      "Epoch 30 \t Batch 700 \t Training Loss: 47.59817352294922\n",
      "Epoch 30 \t Batch 720 \t Training Loss: 47.57202251222399\n",
      "Epoch 30 \t Batch 740 \t Training Loss: 47.56707431690113\n",
      "Epoch 30 \t Batch 760 \t Training Loss: 47.55013356961702\n",
      "Epoch 30 \t Batch 780 \t Training Loss: 47.49922933823023\n",
      "Epoch 30 \t Batch 800 \t Training Loss: 47.445732426643374\n",
      "Epoch 30 \t Batch 820 \t Training Loss: 47.3852107071295\n",
      "Epoch 30 \t Batch 840 \t Training Loss: 47.408860215686616\n",
      "Epoch 30 \t Batch 860 \t Training Loss: 47.369747161865234\n",
      "Epoch 30 \t Batch 880 \t Training Loss: 47.377896360917525\n",
      "Epoch 30 \t Batch 900 \t Training Loss: 47.399539879692924\n",
      "Epoch 30 \t Batch 20 \t Validation Loss: 45.10299372673035\n",
      "Epoch 30 \t Batch 40 \t Validation Loss: 39.905899381637575\n",
      "Epoch 30 \t Batch 60 \t Validation Loss: 43.22573574384054\n",
      "Epoch 30 \t Batch 80 \t Validation Loss: 41.55734884738922\n",
      "Epoch 30 \t Batch 100 \t Validation Loss: 38.84787630081177\n",
      "Epoch 30 \t Batch 120 \t Validation Loss: 37.39303919474284\n",
      "Epoch 30 \t Batch 140 \t Validation Loss: 36.07087666647775\n",
      "Epoch 30 \t Batch 160 \t Validation Loss: 36.765894293785095\n",
      "Epoch 30 \t Batch 180 \t Validation Loss: 39.24265620443556\n",
      "Epoch 30 \t Batch 200 \t Validation Loss: 39.95883468151092\n",
      "Epoch 30 \t Batch 220 \t Validation Loss: 40.65913690653714\n",
      "Epoch 30 \t Batch 240 \t Validation Loss: 40.55113418896993\n",
      "Epoch 30 \t Batch 260 \t Validation Loss: 42.24046098268949\n",
      "Epoch 30 \t Batch 280 \t Validation Loss: 43.00331386838641\n",
      "Epoch 30 \t Batch 300 \t Validation Loss: 43.66169944127401\n",
      "Epoch 30 \t Batch 320 \t Validation Loss: 43.81770648956299\n",
      "Epoch 30 \t Batch 340 \t Validation Loss: 43.53191141240737\n",
      "Epoch 30 \t Batch 360 \t Validation Loss: 43.176358143488564\n",
      "Epoch 30 \t Batch 380 \t Validation Loss: 43.235548686981204\n",
      "Epoch 30 \t Batch 400 \t Validation Loss: 42.683453593254086\n",
      "Epoch 30 \t Batch 420 \t Validation Loss: 42.54863397961571\n",
      "Epoch 30 \t Batch 440 \t Validation Loss: 42.145881071957675\n",
      "Epoch 30 \t Batch 460 \t Validation Loss: 42.20766876884129\n",
      "Epoch 30 \t Batch 480 \t Validation Loss: 42.53083255290985\n",
      "Epoch 30 \t Batch 500 \t Validation Loss: 42.12502978897095\n",
      "Epoch 30 \t Batch 520 \t Validation Loss: 41.79752579285548\n",
      "Epoch 30 \t Batch 540 \t Validation Loss: 41.413088966298986\n",
      "Epoch 30 \t Batch 560 \t Validation Loss: 41.09024163143975\n",
      "Epoch 30 \t Batch 580 \t Validation Loss: 40.759300236866395\n",
      "Epoch 30 \t Batch 600 \t Validation Loss: 40.855125133196516\n",
      "Epoch 30 Training Loss: 47.41668558068728 Validation Loss: 41.42255523452511\n",
      "Epoch 30 completed\n",
      "Epoch 31 \t Batch 20 \t Training Loss: 47.003529930114745\n",
      "Epoch 31 \t Batch 40 \t Training Loss: 46.385319519042966\n",
      "Epoch 31 \t Batch 60 \t Training Loss: 46.85954895019531\n",
      "Epoch 31 \t Batch 80 \t Training Loss: 46.97038569450378\n",
      "Epoch 31 \t Batch 100 \t Training Loss: 46.98809185028076\n",
      "Epoch 31 \t Batch 120 \t Training Loss: 47.24770263036092\n",
      "Epoch 31 \t Batch 140 \t Training Loss: 47.355061994280135\n",
      "Epoch 31 \t Batch 160 \t Training Loss: 47.388125324249266\n",
      "Epoch 31 \t Batch 180 \t Training Loss: 47.392439397176105\n",
      "Epoch 31 \t Batch 200 \t Training Loss: 47.46589340209961\n",
      "Epoch 31 \t Batch 220 \t Training Loss: 47.44928134571422\n",
      "Epoch 31 \t Batch 240 \t Training Loss: 47.38423312505086\n",
      "Epoch 31 \t Batch 260 \t Training Loss: 47.37641740945669\n",
      "Epoch 31 \t Batch 280 \t Training Loss: 47.28686116082328\n",
      "Epoch 31 \t Batch 300 \t Training Loss: 47.37626080830892\n",
      "Epoch 31 \t Batch 320 \t Training Loss: 47.37398014068604\n",
      "Epoch 31 \t Batch 340 \t Training Loss: 47.44407077116125\n",
      "Epoch 31 \t Batch 360 \t Training Loss: 47.37634092966716\n",
      "Epoch 31 \t Batch 380 \t Training Loss: 47.466084811561984\n",
      "Epoch 31 \t Batch 400 \t Training Loss: 47.49217287063598\n",
      "Epoch 31 \t Batch 420 \t Training Loss: 47.49625679197766\n",
      "Epoch 31 \t Batch 440 \t Training Loss: 47.537532685019755\n",
      "Epoch 31 \t Batch 460 \t Training Loss: 47.53581440967062\n",
      "Epoch 31 \t Batch 480 \t Training Loss: 47.57310358683268\n",
      "Epoch 31 \t Batch 500 \t Training Loss: 47.49953062438965\n",
      "Epoch 31 \t Batch 520 \t Training Loss: 47.46777031971858\n",
      "Epoch 31 \t Batch 540 \t Training Loss: 47.48138185430456\n",
      "Epoch 31 \t Batch 560 \t Training Loss: 47.522585657664706\n",
      "Epoch 31 \t Batch 580 \t Training Loss: 47.49301615090206\n",
      "Epoch 31 \t Batch 600 \t Training Loss: 47.50795429865519\n",
      "Epoch 31 \t Batch 620 \t Training Loss: 47.51769885401572\n",
      "Epoch 31 \t Batch 640 \t Training Loss: 47.459559237957\n",
      "Epoch 31 \t Batch 660 \t Training Loss: 47.43952952298251\n",
      "Epoch 31 \t Batch 680 \t Training Loss: 47.386394814883964\n",
      "Epoch 31 \t Batch 700 \t Training Loss: 47.422394262041365\n",
      "Epoch 31 \t Batch 720 \t Training Loss: 47.39121634695265\n",
      "Epoch 31 \t Batch 740 \t Training Loss: 47.35028321034199\n",
      "Epoch 31 \t Batch 760 \t Training Loss: 47.3490167768378\n",
      "Epoch 31 \t Batch 780 \t Training Loss: 47.408073259011296\n",
      "Epoch 31 \t Batch 800 \t Training Loss: 47.4059417963028\n",
      "Epoch 31 \t Batch 820 \t Training Loss: 47.39839967402016\n",
      "Epoch 31 \t Batch 840 \t Training Loss: 47.37069600877308\n",
      "Epoch 31 \t Batch 860 \t Training Loss: 47.37789453462113\n",
      "Epoch 31 \t Batch 880 \t Training Loss: 47.38044577945362\n",
      "Epoch 31 \t Batch 900 \t Training Loss: 47.38283185746935\n",
      "Epoch 31 \t Batch 20 \t Validation Loss: 46.78546762466431\n",
      "Epoch 31 \t Batch 40 \t Validation Loss: 40.9063334107399\n",
      "Epoch 31 \t Batch 60 \t Validation Loss: 44.52690405050914\n",
      "Epoch 31 \t Batch 80 \t Validation Loss: 42.78933498263359\n",
      "Epoch 31 \t Batch 100 \t Validation Loss: 39.768036532402036\n",
      "Epoch 31 \t Batch 120 \t Validation Loss: 38.154532221953076\n",
      "Epoch 31 \t Batch 140 \t Validation Loss: 36.70561798300062\n",
      "Epoch 31 \t Batch 160 \t Validation Loss: 37.28734709918499\n",
      "Epoch 31 \t Batch 180 \t Validation Loss: 39.67872109678056\n",
      "Epoch 31 \t Batch 200 \t Validation Loss: 40.29260312795639\n",
      "Epoch 31 \t Batch 220 \t Validation Loss: 40.955954081361945\n",
      "Epoch 31 \t Batch 240 \t Validation Loss: 40.7950014770031\n",
      "Epoch 31 \t Batch 260 \t Validation Loss: 42.45552124426915\n",
      "Epoch 31 \t Batch 280 \t Validation Loss: 43.163202011585234\n",
      "Epoch 31 \t Batch 300 \t Validation Loss: 43.76461891015371\n",
      "Epoch 31 \t Batch 320 \t Validation Loss: 43.870958988368514\n",
      "Epoch 31 \t Batch 340 \t Validation Loss: 43.5220866133185\n",
      "Epoch 31 \t Batch 360 \t Validation Loss: 43.133700833055705\n",
      "Epoch 31 \t Batch 380 \t Validation Loss: 43.15212673513513\n",
      "Epoch 31 \t Batch 400 \t Validation Loss: 42.51397345662117\n",
      "Epoch 31 \t Batch 420 \t Validation Loss: 42.33147117637453\n",
      "Epoch 31 \t Batch 440 \t Validation Loss: 41.83618744178252\n",
      "Epoch 31 \t Batch 460 \t Validation Loss: 41.88280462700388\n",
      "Epoch 31 \t Batch 480 \t Validation Loss: 42.19508561392625\n",
      "Epoch 31 \t Batch 500 \t Validation Loss: 41.76941497135162\n",
      "Epoch 31 \t Batch 520 \t Validation Loss: 41.371721318134895\n",
      "Epoch 31 \t Batch 540 \t Validation Loss: 41.006299187518934\n",
      "Epoch 31 \t Batch 560 \t Validation Loss: 40.663185289076395\n",
      "Epoch 31 \t Batch 580 \t Validation Loss: 40.28924879781131\n",
      "Epoch 31 \t Batch 600 \t Validation Loss: 40.41245807568232\n",
      "Epoch 31 Training Loss: 47.381191282781934 Validation Loss: 40.97083028183355\n",
      "Epoch 31 completed\n",
      "Epoch 32 \t Batch 20 \t Training Loss: 48.32394180297852\n",
      "Epoch 32 \t Batch 40 \t Training Loss: 47.83990659713745\n",
      "Epoch 32 \t Batch 60 \t Training Loss: 47.723771222432454\n",
      "Epoch 32 \t Batch 80 \t Training Loss: 47.451177740097044\n",
      "Epoch 32 \t Batch 100 \t Training Loss: 47.274513778686526\n",
      "Epoch 32 \t Batch 120 \t Training Loss: 47.08591661453247\n",
      "Epoch 32 \t Batch 140 \t Training Loss: 47.09145229884556\n",
      "Epoch 32 \t Batch 160 \t Training Loss: 46.894565510749814\n",
      "Epoch 32 \t Batch 180 \t Training Loss: 46.76120925479465\n",
      "Epoch 32 \t Batch 200 \t Training Loss: 46.896573848724366\n",
      "Epoch 32 \t Batch 220 \t Training Loss: 47.035758556019175\n",
      "Epoch 32 \t Batch 240 \t Training Loss: 47.10027556419372\n",
      "Epoch 32 \t Batch 260 \t Training Loss: 47.09242289616511\n",
      "Epoch 32 \t Batch 280 \t Training Loss: 47.178994587489534\n",
      "Epoch 32 \t Batch 300 \t Training Loss: 47.20582413991292\n",
      "Epoch 32 \t Batch 320 \t Training Loss: 47.244271636009216\n",
      "Epoch 32 \t Batch 340 \t Training Loss: 47.31817864810719\n",
      "Epoch 32 \t Batch 360 \t Training Loss: 47.292096583048504\n",
      "Epoch 32 \t Batch 380 \t Training Loss: 47.29337415193257\n",
      "Epoch 32 \t Batch 400 \t Training Loss: 47.28150591850281\n",
      "Epoch 32 \t Batch 420 \t Training Loss: 47.25095174880255\n",
      "Epoch 32 \t Batch 440 \t Training Loss: 47.23212542967363\n",
      "Epoch 32 \t Batch 460 \t Training Loss: 47.17093257074771\n",
      "Epoch 32 \t Batch 480 \t Training Loss: 47.155073809623715\n",
      "Epoch 32 \t Batch 500 \t Training Loss: 47.19989459991455\n",
      "Epoch 32 \t Batch 520 \t Training Loss: 47.232611061976506\n",
      "Epoch 32 \t Batch 540 \t Training Loss: 47.253953008298524\n",
      "Epoch 32 \t Batch 560 \t Training Loss: 47.23782758712768\n",
      "Epoch 32 \t Batch 580 \t Training Loss: 47.244508887981546\n",
      "Epoch 32 \t Batch 600 \t Training Loss: 47.25232648849487\n",
      "Epoch 32 \t Batch 620 \t Training Loss: 47.247382563929406\n",
      "Epoch 32 \t Batch 640 \t Training Loss: 47.24403262734413\n",
      "Epoch 32 \t Batch 660 \t Training Loss: 47.25409012996789\n",
      "Epoch 32 \t Batch 680 \t Training Loss: 47.31537665198831\n",
      "Epoch 32 \t Batch 700 \t Training Loss: 47.3234663772583\n",
      "Epoch 32 \t Batch 720 \t Training Loss: 47.321148766411675\n",
      "Epoch 32 \t Batch 740 \t Training Loss: 47.32449352161304\n",
      "Epoch 32 \t Batch 760 \t Training Loss: 47.34932559163947\n",
      "Epoch 32 \t Batch 780 \t Training Loss: 47.39406100542117\n",
      "Epoch 32 \t Batch 800 \t Training Loss: 47.36915627479553\n",
      "Epoch 32 \t Batch 820 \t Training Loss: 47.37097830888702\n",
      "Epoch 32 \t Batch 840 \t Training Loss: 47.34348750795637\n",
      "Epoch 32 \t Batch 860 \t Training Loss: 47.387158708794175\n",
      "Epoch 32 \t Batch 880 \t Training Loss: 47.36226568655534\n",
      "Epoch 32 \t Batch 900 \t Training Loss: 47.37488074408637\n",
      "Epoch 32 \t Batch 20 \t Validation Loss: 43.36058311462402\n",
      "Epoch 32 \t Batch 40 \t Validation Loss: 38.274153399467465\n",
      "Epoch 32 \t Batch 60 \t Validation Loss: 41.68931709925334\n",
      "Epoch 32 \t Batch 80 \t Validation Loss: 39.865853011608124\n",
      "Epoch 32 \t Batch 100 \t Validation Loss: 37.41024935722351\n",
      "Epoch 32 \t Batch 120 \t Validation Loss: 36.154848567644756\n",
      "Epoch 32 \t Batch 140 \t Validation Loss: 34.96815466880798\n",
      "Epoch 32 \t Batch 160 \t Validation Loss: 35.86486732363701\n",
      "Epoch 32 \t Batch 180 \t Validation Loss: 38.562052445941504\n",
      "Epoch 32 \t Batch 200 \t Validation Loss: 39.42437568187714\n",
      "Epoch 32 \t Batch 220 \t Validation Loss: 40.24620698148554\n",
      "Epoch 32 \t Batch 240 \t Validation Loss: 40.20475013256073\n",
      "Epoch 32 \t Batch 260 \t Validation Loss: 41.98812113908621\n",
      "Epoch 32 \t Batch 280 \t Validation Loss: 42.830918649264746\n",
      "Epoch 32 \t Batch 300 \t Validation Loss: 43.561259749730425\n",
      "Epoch 32 \t Batch 320 \t Validation Loss: 43.75553607642651\n",
      "Epoch 32 \t Batch 340 \t Validation Loss: 43.47867175551022\n",
      "Epoch 32 \t Batch 360 \t Validation Loss: 43.12972844176822\n",
      "Epoch 32 \t Batch 380 \t Validation Loss: 43.2121822583048\n",
      "Epoch 32 \t Batch 400 \t Validation Loss: 42.64884498357773\n",
      "Epoch 32 \t Batch 420 \t Validation Loss: 42.52906420344398\n",
      "Epoch 32 \t Batch 440 \t Validation Loss: 42.11960545236414\n",
      "Epoch 32 \t Batch 460 \t Validation Loss: 42.17307890809101\n",
      "Epoch 32 \t Batch 480 \t Validation Loss: 42.51048946976662\n",
      "Epoch 32 \t Batch 500 \t Validation Loss: 42.11888719749451\n",
      "Epoch 32 \t Batch 520 \t Validation Loss: 41.77247633750622\n",
      "Epoch 32 \t Batch 540 \t Validation Loss: 41.39043039569148\n",
      "Epoch 32 \t Batch 560 \t Validation Loss: 41.1069741334234\n",
      "Epoch 32 \t Batch 580 \t Validation Loss: 40.863445957775774\n",
      "Epoch 32 \t Batch 600 \t Validation Loss: 40.96732270081838\n",
      "Epoch 32 Training Loss: 47.359954675905314 Validation Loss: 41.53178838166323\n",
      "Epoch 32 completed\n",
      "Epoch 33 \t Batch 20 \t Training Loss: 47.36794891357422\n",
      "Epoch 33 \t Batch 40 \t Training Loss: 46.894979763031\n",
      "Epoch 33 \t Batch 60 \t Training Loss: 46.79427871704102\n",
      "Epoch 33 \t Batch 80 \t Training Loss: 46.934668684005736\n",
      "Epoch 33 \t Batch 100 \t Training Loss: 47.180651054382324\n",
      "Epoch 33 \t Batch 120 \t Training Loss: 47.31015625\n",
      "Epoch 33 \t Batch 140 \t Training Loss: 47.4635443006243\n",
      "Epoch 33 \t Batch 160 \t Training Loss: 47.47557446956635\n",
      "Epoch 33 \t Batch 180 \t Training Loss: 47.652271334330244\n",
      "Epoch 33 \t Batch 200 \t Training Loss: 47.54058679580689\n",
      "Epoch 33 \t Batch 220 \t Training Loss: 47.53552820032293\n",
      "Epoch 33 \t Batch 240 \t Training Loss: 47.54277105331421\n",
      "Epoch 33 \t Batch 260 \t Training Loss: 47.53298392662635\n",
      "Epoch 33 \t Batch 280 \t Training Loss: 47.543861634390694\n",
      "Epoch 33 \t Batch 300 \t Training Loss: 47.63237433115641\n",
      "Epoch 33 \t Batch 320 \t Training Loss: 47.68279742002487\n",
      "Epoch 33 \t Batch 340 \t Training Loss: 47.65948000515208\n",
      "Epoch 33 \t Batch 360 \t Training Loss: 47.643790054321286\n",
      "Epoch 33 \t Batch 380 \t Training Loss: 47.66534364599931\n",
      "Epoch 33 \t Batch 400 \t Training Loss: 47.71048691749573\n",
      "Epoch 33 \t Batch 420 \t Training Loss: 47.726871990022204\n",
      "Epoch 33 \t Batch 440 \t Training Loss: 47.66780987652865\n",
      "Epoch 33 \t Batch 460 \t Training Loss: 47.64809982465661\n",
      "Epoch 33 \t Batch 480 \t Training Loss: 47.60218050479889\n",
      "Epoch 33 \t Batch 500 \t Training Loss: 47.66664967346191\n",
      "Epoch 33 \t Batch 520 \t Training Loss: 47.652402305603026\n",
      "Epoch 33 \t Batch 540 \t Training Loss: 47.589983763518156\n",
      "Epoch 33 \t Batch 560 \t Training Loss: 47.59522624697004\n",
      "Epoch 33 \t Batch 580 \t Training Loss: 47.56005658116834\n",
      "Epoch 33 \t Batch 600 \t Training Loss: 47.553680884043374\n",
      "Epoch 33 \t Batch 620 \t Training Loss: 47.559586974113216\n",
      "Epoch 33 \t Batch 640 \t Training Loss: 47.49195038676262\n",
      "Epoch 33 \t Batch 660 \t Training Loss: 47.50728461525657\n",
      "Epoch 33 \t Batch 680 \t Training Loss: 47.4318309952231\n",
      "Epoch 33 \t Batch 700 \t Training Loss: 47.42887572697231\n",
      "Epoch 33 \t Batch 720 \t Training Loss: 47.419374550713435\n",
      "Epoch 33 \t Batch 740 \t Training Loss: 47.41864708049877\n",
      "Epoch 33 \t Batch 760 \t Training Loss: 47.38910390954268\n",
      "Epoch 33 \t Batch 780 \t Training Loss: 47.376149759537135\n",
      "Epoch 33 \t Batch 800 \t Training Loss: 47.31926368713379\n",
      "Epoch 33 \t Batch 820 \t Training Loss: 47.317693268380516\n",
      "Epoch 33 \t Batch 840 \t Training Loss: 47.3217019308181\n",
      "Epoch 33 \t Batch 860 \t Training Loss: 47.312570385600246\n",
      "Epoch 33 \t Batch 880 \t Training Loss: 47.34504315636375\n",
      "Epoch 33 \t Batch 900 \t Training Loss: 47.32398377312554\n",
      "Epoch 33 \t Batch 20 \t Validation Loss: 52.01608467102051\n",
      "Epoch 33 \t Batch 40 \t Validation Loss: 44.34525101184845\n",
      "Epoch 33 \t Batch 60 \t Validation Loss: 48.64026505152385\n",
      "Epoch 33 \t Batch 80 \t Validation Loss: 46.263259851932524\n",
      "Epoch 33 \t Batch 100 \t Validation Loss: 42.54573899269104\n",
      "Epoch 33 \t Batch 120 \t Validation Loss: 40.486443098386125\n",
      "Epoch 33 \t Batch 140 \t Validation Loss: 38.68593039512634\n",
      "Epoch 33 \t Batch 160 \t Validation Loss: 39.06665695309639\n",
      "Epoch 33 \t Batch 180 \t Validation Loss: 41.42359975179036\n",
      "Epoch 33 \t Batch 200 \t Validation Loss: 41.92436782836914\n",
      "Epoch 33 \t Batch 220 \t Validation Loss: 42.49600316827947\n",
      "Epoch 33 \t Batch 240 \t Validation Loss: 42.26145667632421\n",
      "Epoch 33 \t Batch 260 \t Validation Loss: 43.86617926817674\n",
      "Epoch 33 \t Batch 280 \t Validation Loss: 44.53083633014134\n",
      "Epoch 33 \t Batch 300 \t Validation Loss: 45.1866923459371\n",
      "Epoch 33 \t Batch 320 \t Validation Loss: 45.27784705758095\n",
      "Epoch 33 \t Batch 340 \t Validation Loss: 44.894563904930564\n",
      "Epoch 33 \t Batch 360 \t Validation Loss: 44.46594347159068\n",
      "Epoch 33 \t Batch 380 \t Validation Loss: 44.4330025396849\n",
      "Epoch 33 \t Batch 400 \t Validation Loss: 43.74239080190659\n",
      "Epoch 33 \t Batch 420 \t Validation Loss: 43.52764413016183\n",
      "Epoch 33 \t Batch 440 \t Validation Loss: 42.99547367095947\n",
      "Epoch 33 \t Batch 460 \t Validation Loss: 42.97711285300877\n",
      "Epoch 33 \t Batch 480 \t Validation Loss: 43.26478530963262\n",
      "Epoch 33 \t Batch 500 \t Validation Loss: 42.82467673110962\n",
      "Epoch 33 \t Batch 520 \t Validation Loss: 42.3980770954719\n",
      "Epoch 33 \t Batch 540 \t Validation Loss: 42.0017530158714\n",
      "Epoch 33 \t Batch 560 \t Validation Loss: 41.6330997977938\n",
      "Epoch 33 \t Batch 580 \t Validation Loss: 41.254959099868245\n",
      "Epoch 33 \t Batch 600 \t Validation Loss: 41.349901100794476\n",
      "Epoch 33 Training Loss: 47.32992229232871 Validation Loss: 41.901552540915354\n",
      "Epoch 33 completed\n",
      "Epoch 34 \t Batch 20 \t Training Loss: 47.88802642822266\n",
      "Epoch 34 \t Batch 40 \t Training Loss: 47.39364004135132\n",
      "Epoch 34 \t Batch 60 \t Training Loss: 46.594468434651695\n",
      "Epoch 34 \t Batch 80 \t Training Loss: 46.93937339782715\n",
      "Epoch 34 \t Batch 100 \t Training Loss: 47.27153301239014\n",
      "Epoch 34 \t Batch 120 \t Training Loss: 47.06909669240316\n",
      "Epoch 34 \t Batch 140 \t Training Loss: 47.04705350058419\n",
      "Epoch 34 \t Batch 160 \t Training Loss: 47.01101093292236\n",
      "Epoch 34 \t Batch 180 \t Training Loss: 46.883809110853406\n",
      "Epoch 34 \t Batch 200 \t Training Loss: 46.92446594238281\n",
      "Epoch 34 \t Batch 220 \t Training Loss: 46.87667694091797\n",
      "Epoch 34 \t Batch 240 \t Training Loss: 46.74527678489685\n",
      "Epoch 34 \t Batch 260 \t Training Loss: 46.74942798614502\n",
      "Epoch 34 \t Batch 280 \t Training Loss: 46.843942110879084\n",
      "Epoch 34 \t Batch 300 \t Training Loss: 46.95266220092773\n",
      "Epoch 34 \t Batch 320 \t Training Loss: 47.036105406284335\n",
      "Epoch 34 \t Batch 340 \t Training Loss: 47.21223151263069\n",
      "Epoch 34 \t Batch 360 \t Training Loss: 47.28931920793321\n",
      "Epoch 34 \t Batch 380 \t Training Loss: 47.289463796113665\n",
      "Epoch 34 \t Batch 400 \t Training Loss: 47.276140451431274\n",
      "Epoch 34 \t Batch 420 \t Training Loss: 47.27731821877616\n",
      "Epoch 34 \t Batch 440 \t Training Loss: 47.283841306513004\n",
      "Epoch 34 \t Batch 460 \t Training Loss: 47.2533412684565\n",
      "Epoch 34 \t Batch 480 \t Training Loss: 47.259101303418475\n",
      "Epoch 34 \t Batch 500 \t Training Loss: 47.31525073242187\n",
      "Epoch 34 \t Batch 520 \t Training Loss: 47.30717022235577\n",
      "Epoch 34 \t Batch 540 \t Training Loss: 47.32275206954391\n",
      "Epoch 34 \t Batch 560 \t Training Loss: 47.24059401239668\n",
      "Epoch 34 \t Batch 580 \t Training Loss: 47.220304548329324\n",
      "Epoch 34 \t Batch 600 \t Training Loss: 47.22142557144165\n",
      "Epoch 34 \t Batch 620 \t Training Loss: 47.21147697202621\n",
      "Epoch 34 \t Batch 640 \t Training Loss: 47.244147044420245\n",
      "Epoch 34 \t Batch 660 \t Training Loss: 47.24875075600364\n",
      "Epoch 34 \t Batch 680 \t Training Loss: 47.2521440113292\n",
      "Epoch 34 \t Batch 700 \t Training Loss: 47.28998858860561\n",
      "Epoch 34 \t Batch 720 \t Training Loss: 47.34883916113112\n",
      "Epoch 34 \t Batch 740 \t Training Loss: 47.34828964955098\n",
      "Epoch 34 \t Batch 760 \t Training Loss: 47.33296523345144\n",
      "Epoch 34 \t Batch 780 \t Training Loss: 47.30403606708233\n",
      "Epoch 34 \t Batch 800 \t Training Loss: 47.3079486989975\n",
      "Epoch 34 \t Batch 820 \t Training Loss: 47.30625822485947\n",
      "Epoch 34 \t Batch 840 \t Training Loss: 47.305710075015114\n",
      "Epoch 34 \t Batch 860 \t Training Loss: 47.31433978413427\n",
      "Epoch 34 \t Batch 880 \t Training Loss: 47.29662017822265\n",
      "Epoch 34 \t Batch 900 \t Training Loss: 47.28712086571588\n",
      "Epoch 34 \t Batch 20 \t Validation Loss: 46.82428936958313\n",
      "Epoch 34 \t Batch 40 \t Validation Loss: 40.93921890258789\n",
      "Epoch 34 \t Batch 60 \t Validation Loss: 44.43027540842692\n",
      "Epoch 34 \t Batch 80 \t Validation Loss: 42.616906750202176\n",
      "Epoch 34 \t Batch 100 \t Validation Loss: 39.380221586227414\n",
      "Epoch 34 \t Batch 120 \t Validation Loss: 37.61830208301544\n",
      "Epoch 34 \t Batch 140 \t Validation Loss: 36.15714161736624\n",
      "Epoch 34 \t Batch 160 \t Validation Loss: 36.901710629463196\n",
      "Epoch 34 \t Batch 180 \t Validation Loss: 39.409213087293836\n",
      "Epoch 34 \t Batch 200 \t Validation Loss: 40.201178874969486\n",
      "Epoch 34 \t Batch 220 \t Validation Loss: 40.89772726405751\n",
      "Epoch 34 \t Batch 240 \t Validation Loss: 40.78482083479563\n",
      "Epoch 34 \t Batch 260 \t Validation Loss: 42.48827623587388\n",
      "Epoch 34 \t Batch 280 \t Validation Loss: 43.22656371252877\n",
      "Epoch 34 \t Batch 300 \t Validation Loss: 43.92552032470703\n",
      "Epoch 34 \t Batch 320 \t Validation Loss: 44.06229850053787\n",
      "Epoch 34 \t Batch 340 \t Validation Loss: 43.76889607485603\n",
      "Epoch 34 \t Batch 360 \t Validation Loss: 43.40853815078735\n",
      "Epoch 34 \t Batch 380 \t Validation Loss: 43.461557363208975\n",
      "Epoch 34 \t Batch 400 \t Validation Loss: 42.8922474861145\n",
      "Epoch 34 \t Batch 420 \t Validation Loss: 42.75767435346331\n",
      "Epoch 34 \t Batch 440 \t Validation Loss: 42.34754563244906\n",
      "Epoch 34 \t Batch 460 \t Validation Loss: 42.41768329039864\n",
      "Epoch 34 \t Batch 480 \t Validation Loss: 42.72793989181518\n",
      "Epoch 34 \t Batch 500 \t Validation Loss: 42.32911545181275\n",
      "Epoch 34 \t Batch 520 \t Validation Loss: 42.027508640289305\n",
      "Epoch 34 \t Batch 540 \t Validation Loss: 41.63767226183856\n",
      "Epoch 34 \t Batch 560 \t Validation Loss: 41.33850376605987\n",
      "Epoch 34 \t Batch 580 \t Validation Loss: 41.09972300365053\n",
      "Epoch 34 \t Batch 600 \t Validation Loss: 41.19292881329854\n",
      "Epoch 34 Training Loss: 47.29859906844511 Validation Loss: 41.839183733060764\n",
      "Epoch 34 completed\n",
      "Epoch 35 \t Batch 20 \t Training Loss: 46.85169925689697\n",
      "Epoch 35 \t Batch 40 \t Training Loss: 46.62340087890625\n",
      "Epoch 35 \t Batch 60 \t Training Loss: 46.90493469238281\n",
      "Epoch 35 \t Batch 80 \t Training Loss: 46.78584656715393\n",
      "Epoch 35 \t Batch 100 \t Training Loss: 47.32938003540039\n",
      "Epoch 35 \t Batch 120 \t Training Loss: 47.201652908325194\n",
      "Epoch 35 \t Batch 140 \t Training Loss: 47.38586169651577\n",
      "Epoch 35 \t Batch 160 \t Training Loss: 47.30601007938385\n",
      "Epoch 35 \t Batch 180 \t Training Loss: 47.30429825252957\n",
      "Epoch 35 \t Batch 200 \t Training Loss: 47.11531469345093\n",
      "Epoch 35 \t Batch 220 \t Training Loss: 47.03374665000222\n",
      "Epoch 35 \t Batch 240 \t Training Loss: 47.116865921020505\n",
      "Epoch 35 \t Batch 260 \t Training Loss: 47.084594579843376\n",
      "Epoch 35 \t Batch 280 \t Training Loss: 47.100984518868586\n",
      "Epoch 35 \t Batch 300 \t Training Loss: 47.04356204986572\n",
      "Epoch 35 \t Batch 320 \t Training Loss: 47.08029261827469\n",
      "Epoch 35 \t Batch 340 \t Training Loss: 47.069874101526594\n",
      "Epoch 35 \t Batch 360 \t Training Loss: 47.10479553010729\n",
      "Epoch 35 \t Batch 380 \t Training Loss: 47.219491316142836\n",
      "Epoch 35 \t Batch 400 \t Training Loss: 47.3196026134491\n",
      "Epoch 35 \t Batch 420 \t Training Loss: 47.37247297196161\n",
      "Epoch 35 \t Batch 440 \t Training Loss: 47.35271600376476\n",
      "Epoch 35 \t Batch 460 \t Training Loss: 47.304715819980785\n",
      "Epoch 35 \t Batch 480 \t Training Loss: 47.37257470289866\n",
      "Epoch 35 \t Batch 500 \t Training Loss: 47.35112882232666\n",
      "Epoch 35 \t Batch 520 \t Training Loss: 47.34255796579214\n",
      "Epoch 35 \t Batch 540 \t Training Loss: 47.38001059779415\n",
      "Epoch 35 \t Batch 560 \t Training Loss: 47.41126937866211\n",
      "Epoch 35 \t Batch 580 \t Training Loss: 47.43874591958934\n",
      "Epoch 35 \t Batch 600 \t Training Loss: 47.40630656560262\n",
      "Epoch 35 \t Batch 620 \t Training Loss: 47.44390891905754\n",
      "Epoch 35 \t Batch 640 \t Training Loss: 47.444681173563005\n",
      "Epoch 35 \t Batch 660 \t Training Loss: 47.467220560709634\n",
      "Epoch 35 \t Batch 680 \t Training Loss: 47.457740536858054\n",
      "Epoch 35 \t Batch 700 \t Training Loss: 47.42640924181257\n",
      "Epoch 35 \t Batch 720 \t Training Loss: 47.43255738682217\n",
      "Epoch 35 \t Batch 740 \t Training Loss: 47.40387726861077\n",
      "Epoch 35 \t Batch 760 \t Training Loss: 47.380550710778486\n",
      "Epoch 35 \t Batch 780 \t Training Loss: 47.35174561035939\n",
      "Epoch 35 \t Batch 800 \t Training Loss: 47.3133769607544\n",
      "Epoch 35 \t Batch 820 \t Training Loss: 47.33079413902469\n",
      "Epoch 35 \t Batch 840 \t Training Loss: 47.30096496400379\n",
      "Epoch 35 \t Batch 860 \t Training Loss: 47.303020694643955\n",
      "Epoch 35 \t Batch 880 \t Training Loss: 47.30257269685919\n",
      "Epoch 35 \t Batch 900 \t Training Loss: 47.29539474487305\n",
      "Epoch 35 \t Batch 20 \t Validation Loss: 41.95877959728241\n",
      "Epoch 35 \t Batch 40 \t Validation Loss: 36.80176004171371\n",
      "Epoch 35 \t Batch 60 \t Validation Loss: 40.37360322475433\n",
      "Epoch 35 \t Batch 80 \t Validation Loss: 38.61483646631241\n",
      "Epoch 35 \t Batch 100 \t Validation Loss: 36.15875847816467\n",
      "Epoch 35 \t Batch 120 \t Validation Loss: 34.877164920171104\n",
      "Epoch 35 \t Batch 140 \t Validation Loss: 33.79027125494821\n",
      "Epoch 35 \t Batch 160 \t Validation Loss: 34.70289355516434\n",
      "Epoch 35 \t Batch 180 \t Validation Loss: 37.160583554373844\n",
      "Epoch 35 \t Batch 200 \t Validation Loss: 37.98396056652069\n",
      "Epoch 35 \t Batch 220 \t Validation Loss: 38.73400079987266\n",
      "Epoch 35 \t Batch 240 \t Validation Loss: 38.66026781797409\n",
      "Epoch 35 \t Batch 260 \t Validation Loss: 40.3999042731065\n",
      "Epoch 35 \t Batch 280 \t Validation Loss: 41.20111971582685\n",
      "Epoch 35 \t Batch 300 \t Validation Loss: 41.842804158528644\n",
      "Epoch 35 \t Batch 320 \t Validation Loss: 42.003364169597624\n",
      "Epoch 35 \t Batch 340 \t Validation Loss: 41.78153902502621\n",
      "Epoch 35 \t Batch 360 \t Validation Loss: 41.4585998111301\n",
      "Epoch 35 \t Batch 380 \t Validation Loss: 41.565615664030375\n",
      "Epoch 35 \t Batch 400 \t Validation Loss: 41.04802865028381\n",
      "Epoch 35 \t Batch 420 \t Validation Loss: 40.96963990983509\n",
      "Epoch 35 \t Batch 440 \t Validation Loss: 40.58837110779502\n",
      "Epoch 35 \t Batch 460 \t Validation Loss: 40.686771405261496\n",
      "Epoch 35 \t Batch 480 \t Validation Loss: 41.066279554367064\n",
      "Epoch 35 \t Batch 500 \t Validation Loss: 40.70063093185425\n",
      "Epoch 35 \t Batch 520 \t Validation Loss: 40.380868385388304\n",
      "Epoch 35 \t Batch 540 \t Validation Loss: 40.10601474797284\n",
      "Epoch 35 \t Batch 560 \t Validation Loss: 39.9344645346914\n",
      "Epoch 35 \t Batch 580 \t Validation Loss: 39.81273869317153\n",
      "Epoch 35 \t Batch 600 \t Validation Loss: 40.01129535833994\n",
      "Epoch 35 Training Loss: 47.282113498533825 Validation Loss: 40.64618849599516\n",
      "Epoch 35 completed\n",
      "Epoch 36 \t Batch 20 \t Training Loss: 47.18916492462158\n",
      "Epoch 36 \t Batch 40 \t Training Loss: 47.108248233795166\n",
      "Epoch 36 \t Batch 60 \t Training Loss: 46.90522397359212\n",
      "Epoch 36 \t Batch 80 \t Training Loss: 47.12287917137146\n",
      "Epoch 36 \t Batch 100 \t Training Loss: 46.93506031036377\n",
      "Epoch 36 \t Batch 120 \t Training Loss: 47.21044692993164\n",
      "Epoch 36 \t Batch 140 \t Training Loss: 47.2266342163086\n",
      "Epoch 36 \t Batch 160 \t Training Loss: 47.1567144870758\n",
      "Epoch 36 \t Batch 180 \t Training Loss: 47.198731380038794\n",
      "Epoch 36 \t Batch 200 \t Training Loss: 47.23598213195801\n",
      "Epoch 36 \t Batch 220 \t Training Loss: 47.31601050983776\n",
      "Epoch 36 \t Batch 240 \t Training Loss: 47.31114594141642\n",
      "Epoch 36 \t Batch 260 \t Training Loss: 47.33716204716609\n",
      "Epoch 36 \t Batch 280 \t Training Loss: 47.26989022663661\n",
      "Epoch 36 \t Batch 300 \t Training Loss: 47.24513394673665\n",
      "Epoch 36 \t Batch 320 \t Training Loss: 47.17394320964813\n",
      "Epoch 36 \t Batch 340 \t Training Loss: 47.22729148864746\n",
      "Epoch 36 \t Batch 360 \t Training Loss: 47.256921460893416\n",
      "Epoch 36 \t Batch 380 \t Training Loss: 47.20366192867881\n",
      "Epoch 36 \t Batch 400 \t Training Loss: 47.22046710014343\n",
      "Epoch 36 \t Batch 420 \t Training Loss: 47.172268967401415\n",
      "Epoch 36 \t Batch 440 \t Training Loss: 47.18050901239569\n",
      "Epoch 36 \t Batch 460 \t Training Loss: 47.231630010190216\n",
      "Epoch 36 \t Batch 480 \t Training Loss: 47.23300693035126\n",
      "Epoch 36 \t Batch 500 \t Training Loss: 47.284244003295896\n",
      "Epoch 36 \t Batch 520 \t Training Loss: 47.254818080021785\n",
      "Epoch 36 \t Batch 540 \t Training Loss: 47.251692870811176\n",
      "Epoch 36 \t Batch 560 \t Training Loss: 47.2574531691415\n",
      "Epoch 36 \t Batch 580 \t Training Loss: 47.24410617433745\n",
      "Epoch 36 \t Batch 600 \t Training Loss: 47.219339561462405\n",
      "Epoch 36 \t Batch 620 \t Training Loss: 47.20451789363738\n",
      "Epoch 36 \t Batch 640 \t Training Loss: 47.18203198313713\n",
      "Epoch 36 \t Batch 660 \t Training Loss: 47.16012224139589\n",
      "Epoch 36 \t Batch 680 \t Training Loss: 47.13672866821289\n",
      "Epoch 36 \t Batch 700 \t Training Loss: 47.14977383204869\n",
      "Epoch 36 \t Batch 720 \t Training Loss: 47.17622105280558\n",
      "Epoch 36 \t Batch 740 \t Training Loss: 47.199809461026575\n",
      "Epoch 36 \t Batch 760 \t Training Loss: 47.2136666749653\n",
      "Epoch 36 \t Batch 780 \t Training Loss: 47.22695971268874\n",
      "Epoch 36 \t Batch 800 \t Training Loss: 47.22918395042419\n",
      "Epoch 36 \t Batch 820 \t Training Loss: 47.262370211903644\n",
      "Epoch 36 \t Batch 840 \t Training Loss: 47.2489506312779\n",
      "Epoch 36 \t Batch 860 \t Training Loss: 47.24071167790613\n",
      "Epoch 36 \t Batch 880 \t Training Loss: 47.234475248510186\n",
      "Epoch 36 \t Batch 900 \t Training Loss: 47.23581748538547\n",
      "Epoch 36 \t Batch 20 \t Validation Loss: 43.00397729873657\n",
      "Epoch 36 \t Batch 40 \t Validation Loss: 37.62305284738541\n",
      "Epoch 36 \t Batch 60 \t Validation Loss: 41.116470265388486\n",
      "Epoch 36 \t Batch 80 \t Validation Loss: 39.53396202325821\n",
      "Epoch 36 \t Batch 100 \t Validation Loss: 36.994454565048215\n",
      "Epoch 36 \t Batch 120 \t Validation Loss: 35.67975423336029\n",
      "Epoch 36 \t Batch 140 \t Validation Loss: 34.49648537635803\n",
      "Epoch 36 \t Batch 160 \t Validation Loss: 35.24491856694222\n",
      "Epoch 36 \t Batch 180 \t Validation Loss: 37.64451162020365\n",
      "Epoch 36 \t Batch 200 \t Validation Loss: 38.377784523963925\n",
      "Epoch 36 \t Batch 220 \t Validation Loss: 39.05081981745633\n",
      "Epoch 36 \t Batch 240 \t Validation Loss: 38.942320839564005\n",
      "Epoch 36 \t Batch 260 \t Validation Loss: 40.633903562105615\n",
      "Epoch 36 \t Batch 280 \t Validation Loss: 41.37123436587198\n",
      "Epoch 36 \t Batch 300 \t Validation Loss: 42.0076656182607\n",
      "Epoch 36 \t Batch 320 \t Validation Loss: 42.15792334973812\n",
      "Epoch 36 \t Batch 340 \t Validation Loss: 41.905431071449726\n",
      "Epoch 36 \t Batch 360 \t Validation Loss: 41.546161686049565\n",
      "Epoch 36 \t Batch 380 \t Validation Loss: 41.617131220666984\n",
      "Epoch 36 \t Batch 400 \t Validation Loss: 41.08581619501114\n",
      "Epoch 36 \t Batch 420 \t Validation Loss: 40.980135615666704\n",
      "Epoch 36 \t Batch 440 \t Validation Loss: 40.60600229826841\n",
      "Epoch 36 \t Batch 460 \t Validation Loss: 40.70578745759052\n",
      "Epoch 36 \t Batch 480 \t Validation Loss: 41.06677514910698\n",
      "Epoch 36 \t Batch 500 \t Validation Loss: 40.676265336990355\n",
      "Epoch 36 \t Batch 520 \t Validation Loss: 40.33610539252941\n",
      "Epoch 36 \t Batch 540 \t Validation Loss: 40.004323696207116\n",
      "Epoch 36 \t Batch 560 \t Validation Loss: 39.721468007564546\n",
      "Epoch 36 \t Batch 580 \t Validation Loss: 39.37994173148583\n",
      "Epoch 36 \t Batch 600 \t Validation Loss: 39.53356441020966\n",
      "Epoch 36 Training Loss: 47.23739675191133 Validation Loss: 40.08779310096394\n",
      "Epoch 36 completed\n",
      "Epoch 37 \t Batch 20 \t Training Loss: 46.828335762023926\n",
      "Epoch 37 \t Batch 40 \t Training Loss: 47.06373805999756\n",
      "Epoch 37 \t Batch 60 \t Training Loss: 47.202422841389975\n",
      "Epoch 37 \t Batch 80 \t Training Loss: 47.157576370239255\n",
      "Epoch 37 \t Batch 100 \t Training Loss: 46.91132850646973\n",
      "Epoch 37 \t Batch 120 \t Training Loss: 46.797380956013996\n",
      "Epoch 37 \t Batch 140 \t Training Loss: 46.87399984087263\n",
      "Epoch 37 \t Batch 160 \t Training Loss: 46.841321206092836\n",
      "Epoch 37 \t Batch 180 \t Training Loss: 46.816952323913576\n",
      "Epoch 37 \t Batch 200 \t Training Loss: 46.86062286376953\n",
      "Epoch 37 \t Batch 220 \t Training Loss: 47.015550335970794\n",
      "Epoch 37 \t Batch 240 \t Training Loss: 47.00259335835775\n",
      "Epoch 37 \t Batch 260 \t Training Loss: 46.983698375408466\n",
      "Epoch 37 \t Batch 280 \t Training Loss: 47.05125488553728\n",
      "Epoch 37 \t Batch 300 \t Training Loss: 47.10947934468587\n",
      "Epoch 37 \t Batch 320 \t Training Loss: 47.11032981872559\n",
      "Epoch 37 \t Batch 340 \t Training Loss: 47.14391912572524\n",
      "Epoch 37 \t Batch 360 \t Training Loss: 47.14605272081163\n",
      "Epoch 37 \t Batch 380 \t Training Loss: 47.081368185344495\n",
      "Epoch 37 \t Batch 400 \t Training Loss: 47.10207248687744\n",
      "Epoch 37 \t Batch 420 \t Training Loss: 47.03151914505732\n",
      "Epoch 37 \t Batch 440 \t Training Loss: 47.10799416628751\n",
      "Epoch 37 \t Batch 460 \t Training Loss: 47.187910502889885\n",
      "Epoch 37 \t Batch 480 \t Training Loss: 47.1977592309316\n",
      "Epoch 37 \t Batch 500 \t Training Loss: 47.24696102142334\n",
      "Epoch 37 \t Batch 520 \t Training Loss: 47.26361657656156\n",
      "Epoch 37 \t Batch 540 \t Training Loss: 47.26793704562717\n",
      "Epoch 37 \t Batch 560 \t Training Loss: 47.28278544289725\n",
      "Epoch 37 \t Batch 580 \t Training Loss: 47.30193539323478\n",
      "Epoch 37 \t Batch 600 \t Training Loss: 47.314727268218995\n",
      "Epoch 37 \t Batch 620 \t Training Loss: 47.25674245895878\n",
      "Epoch 37 \t Batch 640 \t Training Loss: 47.23480541110039\n",
      "Epoch 37 \t Batch 660 \t Training Loss: 47.23831870917118\n",
      "Epoch 37 \t Batch 680 \t Training Loss: 47.20730155496036\n",
      "Epoch 37 \t Batch 700 \t Training Loss: 47.21703002929687\n",
      "Epoch 37 \t Batch 720 \t Training Loss: 47.219638390011255\n",
      "Epoch 37 \t Batch 740 \t Training Loss: 47.228773008810506\n",
      "Epoch 37 \t Batch 760 \t Training Loss: 47.22045411059731\n",
      "Epoch 37 \t Batch 780 \t Training Loss: 47.25472840040158\n",
      "Epoch 37 \t Batch 800 \t Training Loss: 47.272107486724856\n",
      "Epoch 37 \t Batch 820 \t Training Loss: 47.26525102010587\n",
      "Epoch 37 \t Batch 840 \t Training Loss: 47.26560639426822\n",
      "Epoch 37 \t Batch 860 \t Training Loss: 47.23043640491574\n",
      "Epoch 37 \t Batch 880 \t Training Loss: 47.23897888443687\n",
      "Epoch 37 \t Batch 900 \t Training Loss: 47.251774910820856\n",
      "Epoch 37 \t Batch 20 \t Validation Loss: 45.83093099594116\n",
      "Epoch 37 \t Batch 40 \t Validation Loss: 40.33178102970123\n",
      "Epoch 37 \t Batch 60 \t Validation Loss: 43.624952189127605\n",
      "Epoch 37 \t Batch 80 \t Validation Loss: 41.817648994922635\n",
      "Epoch 37 \t Batch 100 \t Validation Loss: 39.08388823509216\n",
      "Epoch 37 \t Batch 120 \t Validation Loss: 37.632900786399844\n",
      "Epoch 37 \t Batch 140 \t Validation Loss: 36.30074959482465\n",
      "Epoch 37 \t Batch 160 \t Validation Loss: 37.11297919154167\n",
      "Epoch 37 \t Batch 180 \t Validation Loss: 39.84112795723809\n",
      "Epoch 37 \t Batch 200 \t Validation Loss: 40.75121671676636\n",
      "Epoch 37 \t Batch 220 \t Validation Loss: 41.56063855778087\n",
      "Epoch 37 \t Batch 240 \t Validation Loss: 41.49503485361735\n",
      "Epoch 37 \t Batch 260 \t Validation Loss: 43.26102335269635\n",
      "Epoch 37 \t Batch 280 \t Validation Loss: 44.09297399180276\n",
      "Epoch 37 \t Batch 300 \t Validation Loss: 44.82660201072693\n",
      "Epoch 37 \t Batch 320 \t Validation Loss: 44.985274919867514\n",
      "Epoch 37 \t Batch 340 \t Validation Loss: 44.684514413160436\n",
      "Epoch 37 \t Batch 360 \t Validation Loss: 44.29764901532067\n",
      "Epoch 37 \t Batch 380 \t Validation Loss: 44.340139562205266\n",
      "Epoch 37 \t Batch 400 \t Validation Loss: 43.75739800691605\n",
      "Epoch 37 \t Batch 420 \t Validation Loss: 43.64648996761867\n",
      "Epoch 37 \t Batch 440 \t Validation Loss: 43.20488913492723\n",
      "Epoch 37 \t Batch 460 \t Validation Loss: 43.251693876929906\n",
      "Epoch 37 \t Batch 480 \t Validation Loss: 43.5836977104346\n",
      "Epoch 37 \t Batch 500 \t Validation Loss: 43.19054081916809\n",
      "Epoch 37 \t Batch 520 \t Validation Loss: 42.79743782373575\n",
      "Epoch 37 \t Batch 540 \t Validation Loss: 42.33215890990363\n",
      "Epoch 37 \t Batch 560 \t Validation Loss: 41.927422082424165\n",
      "Epoch 37 \t Batch 580 \t Validation Loss: 41.50923455172572\n",
      "Epoch 37 \t Batch 600 \t Validation Loss: 41.548411854108174\n",
      "Epoch 37 Training Loss: 47.21205213207715 Validation Loss: 42.0489168801865\n",
      "Epoch 37 completed\n",
      "Epoch 38 \t Batch 20 \t Training Loss: 46.87114791870117\n",
      "Epoch 38 \t Batch 40 \t Training Loss: 46.45952787399292\n",
      "Epoch 38 \t Batch 60 \t Training Loss: 46.58817615509033\n",
      "Epoch 38 \t Batch 80 \t Training Loss: 46.33955340385437\n",
      "Epoch 38 \t Batch 100 \t Training Loss: 46.80952495574951\n",
      "Epoch 38 \t Batch 120 \t Training Loss: 46.78812478383382\n",
      "Epoch 38 \t Batch 140 \t Training Loss: 46.90751628875732\n",
      "Epoch 38 \t Batch 160 \t Training Loss: 46.92648406028748\n",
      "Epoch 38 \t Batch 180 \t Training Loss: 47.04358954959446\n",
      "Epoch 38 \t Batch 200 \t Training Loss: 46.931577892303466\n",
      "Epoch 38 \t Batch 220 \t Training Loss: 46.926519549976696\n",
      "Epoch 38 \t Batch 240 \t Training Loss: 46.965569241841635\n",
      "Epoch 38 \t Batch 260 \t Training Loss: 47.024357296870306\n",
      "Epoch 38 \t Batch 280 \t Training Loss: 47.157334613800046\n",
      "Epoch 38 \t Batch 300 \t Training Loss: 47.190666694641116\n",
      "Epoch 38 \t Batch 320 \t Training Loss: 47.27558127641678\n",
      "Epoch 38 \t Batch 340 \t Training Loss: 47.29472859326531\n",
      "Epoch 38 \t Batch 360 \t Training Loss: 47.300823010338675\n",
      "Epoch 38 \t Batch 380 \t Training Loss: 47.29873250660143\n",
      "Epoch 38 \t Batch 400 \t Training Loss: 47.31283874511719\n",
      "Epoch 38 \t Batch 420 \t Training Loss: 47.31889870961507\n",
      "Epoch 38 \t Batch 440 \t Training Loss: 47.32716877677224\n",
      "Epoch 38 \t Batch 460 \t Training Loss: 47.31569003229556\n",
      "Epoch 38 \t Batch 480 \t Training Loss: 47.358481057484944\n",
      "Epoch 38 \t Batch 500 \t Training Loss: 47.26618778991699\n",
      "Epoch 38 \t Batch 520 \t Training Loss: 47.251398636744575\n",
      "Epoch 38 \t Batch 540 \t Training Loss: 47.24068230523004\n",
      "Epoch 38 \t Batch 560 \t Training Loss: 47.232222012111116\n",
      "Epoch 38 \t Batch 580 \t Training Loss: 47.204397793473866\n",
      "Epoch 38 \t Batch 600 \t Training Loss: 47.1320556640625\n",
      "Epoch 38 \t Batch 620 \t Training Loss: 47.16592420762585\n",
      "Epoch 38 \t Batch 640 \t Training Loss: 47.14686197638512\n",
      "Epoch 38 \t Batch 660 \t Training Loss: 47.135720351248075\n",
      "Epoch 38 \t Batch 680 \t Training Loss: 47.16726433810066\n",
      "Epoch 38 \t Batch 700 \t Training Loss: 47.19310867854527\n",
      "Epoch 38 \t Batch 720 \t Training Loss: 47.20638057390849\n",
      "Epoch 38 \t Batch 740 \t Training Loss: 47.15789205190298\n",
      "Epoch 38 \t Batch 760 \t Training Loss: 47.168787419168574\n",
      "Epoch 38 \t Batch 780 \t Training Loss: 47.16142280285175\n",
      "Epoch 38 \t Batch 800 \t Training Loss: 47.19588768959046\n",
      "Epoch 38 \t Batch 820 \t Training Loss: 47.19760164865633\n",
      "Epoch 38 \t Batch 840 \t Training Loss: 47.17424651554653\n",
      "Epoch 38 \t Batch 860 \t Training Loss: 47.16593611074048\n",
      "Epoch 38 \t Batch 880 \t Training Loss: 47.147004426609385\n",
      "Epoch 38 \t Batch 900 \t Training Loss: 47.16605000813802\n",
      "Epoch 38 \t Batch 20 \t Validation Loss: 46.49473323822021\n",
      "Epoch 38 \t Batch 40 \t Validation Loss: 40.328016901016234\n",
      "Epoch 38 \t Batch 60 \t Validation Loss: 44.19242550532023\n",
      "Epoch 38 \t Batch 80 \t Validation Loss: 42.16065689921379\n",
      "Epoch 38 \t Batch 100 \t Validation Loss: 39.05925607204437\n",
      "Epoch 38 \t Batch 120 \t Validation Loss: 37.384281822045644\n",
      "Epoch 38 \t Batch 140 \t Validation Loss: 35.976825261116026\n",
      "Epoch 38 \t Batch 160 \t Validation Loss: 36.71471033990383\n",
      "Epoch 38 \t Batch 180 \t Validation Loss: 39.181296192275155\n",
      "Epoch 38 \t Batch 200 \t Validation Loss: 39.92184635400772\n",
      "Epoch 38 \t Batch 220 \t Validation Loss: 40.646008268269625\n",
      "Epoch 38 \t Batch 240 \t Validation Loss: 40.51775596340497\n",
      "Epoch 38 \t Batch 260 \t Validation Loss: 42.223977692310626\n",
      "Epoch 38 \t Batch 280 \t Validation Loss: 42.995699203014375\n",
      "Epoch 38 \t Batch 300 \t Validation Loss: 43.63054795742035\n",
      "Epoch 38 \t Batch 320 \t Validation Loss: 43.781285448372365\n",
      "Epoch 38 \t Batch 340 \t Validation Loss: 43.4748034547357\n",
      "Epoch 38 \t Batch 360 \t Validation Loss: 43.11814612415102\n",
      "Epoch 38 \t Batch 380 \t Validation Loss: 43.17701151621969\n",
      "Epoch 38 \t Batch 400 \t Validation Loss: 42.624864279031755\n",
      "Epoch 38 \t Batch 420 \t Validation Loss: 42.48514352185386\n",
      "Epoch 38 \t Batch 440 \t Validation Loss: 42.08164484826001\n",
      "Epoch 38 \t Batch 460 \t Validation Loss: 42.150044732508455\n",
      "Epoch 38 \t Batch 480 \t Validation Loss: 42.48027904133002\n",
      "Epoch 38 \t Batch 500 \t Validation Loss: 42.07088964939118\n",
      "Epoch 38 \t Batch 520 \t Validation Loss: 41.7386069893837\n",
      "Epoch 38 \t Batch 540 \t Validation Loss: 41.34118847405469\n",
      "Epoch 38 \t Batch 560 \t Validation Loss: 41.01674819077764\n",
      "Epoch 38 \t Batch 580 \t Validation Loss: 40.6890047837948\n",
      "Epoch 38 \t Batch 600 \t Validation Loss: 40.773777644634244\n",
      "Epoch 38 Training Loss: 47.168149043325506 Validation Loss: 41.33126804813162\n",
      "Epoch 38 completed\n",
      "Epoch 39 \t Batch 20 \t Training Loss: 47.3604772567749\n",
      "Epoch 39 \t Batch 40 \t Training Loss: 47.6700927734375\n",
      "Epoch 39 \t Batch 60 \t Training Loss: 47.15919488271077\n",
      "Epoch 39 \t Batch 80 \t Training Loss: 46.94969415664673\n",
      "Epoch 39 \t Batch 100 \t Training Loss: 46.90275497436524\n",
      "Epoch 39 \t Batch 120 \t Training Loss: 46.80479176839193\n",
      "Epoch 39 \t Batch 140 \t Training Loss: 46.847672843933104\n",
      "Epoch 39 \t Batch 160 \t Training Loss: 46.89809880256653\n",
      "Epoch 39 \t Batch 180 \t Training Loss: 46.789049805535214\n",
      "Epoch 39 \t Batch 200 \t Training Loss: 46.708163299560546\n",
      "Epoch 39 \t Batch 220 \t Training Loss: 46.73718164617365\n",
      "Epoch 39 \t Batch 240 \t Training Loss: 46.86969304084778\n",
      "Epoch 39 \t Batch 260 \t Training Loss: 46.94560001079853\n",
      "Epoch 39 \t Batch 280 \t Training Loss: 46.954390375954766\n",
      "Epoch 39 \t Batch 300 \t Training Loss: 46.9411979675293\n",
      "Epoch 39 \t Batch 320 \t Training Loss: 46.95023114681244\n",
      "Epoch 39 \t Batch 340 \t Training Loss: 47.03216419219971\n",
      "Epoch 39 \t Batch 360 \t Training Loss: 47.03694324493408\n",
      "Epoch 39 \t Batch 380 \t Training Loss: 47.034530378642835\n",
      "Epoch 39 \t Batch 400 \t Training Loss: 47.03660538673401\n",
      "Epoch 39 \t Batch 420 \t Training Loss: 47.02641490754627\n",
      "Epoch 39 \t Batch 440 \t Training Loss: 47.06226236169989\n",
      "Epoch 39 \t Batch 460 \t Training Loss: 47.052882990629776\n",
      "Epoch 39 \t Batch 480 \t Training Loss: 47.033698995908104\n",
      "Epoch 39 \t Batch 500 \t Training Loss: 47.041743965148925\n",
      "Epoch 39 \t Batch 520 \t Training Loss: 46.964333431537334\n",
      "Epoch 39 \t Batch 540 \t Training Loss: 47.022785907321506\n",
      "Epoch 39 \t Batch 560 \t Training Loss: 47.0067047732217\n",
      "Epoch 39 \t Batch 580 \t Training Loss: 47.008639644754346\n",
      "Epoch 39 \t Batch 600 \t Training Loss: 47.034616247812906\n",
      "Epoch 39 \t Batch 620 \t Training Loss: 46.985736957673105\n",
      "Epoch 39 \t Batch 640 \t Training Loss: 47.00031704902649\n",
      "Epoch 39 \t Batch 660 \t Training Loss: 47.00968900738341\n",
      "Epoch 39 \t Batch 680 \t Training Loss: 46.99765432582182\n",
      "Epoch 39 \t Batch 700 \t Training Loss: 47.040617261614116\n",
      "Epoch 39 \t Batch 720 \t Training Loss: 47.081266185972424\n",
      "Epoch 39 \t Batch 740 \t Training Loss: 47.0516383918556\n",
      "Epoch 39 \t Batch 760 \t Training Loss: 47.090747853329304\n",
      "Epoch 39 \t Batch 780 \t Training Loss: 47.09306293389736\n",
      "Epoch 39 \t Batch 800 \t Training Loss: 47.074712281227114\n",
      "Epoch 39 \t Batch 820 \t Training Loss: 47.07718744045351\n",
      "Epoch 39 \t Batch 840 \t Training Loss: 47.0478774933588\n",
      "Epoch 39 \t Batch 860 \t Training Loss: 47.07012168529422\n",
      "Epoch 39 \t Batch 880 \t Training Loss: 47.1158232905648\n",
      "Epoch 39 \t Batch 900 \t Training Loss: 47.1664645046658\n",
      "Epoch 39 \t Batch 20 \t Validation Loss: 48.01982834339142\n",
      "Epoch 39 \t Batch 40 \t Validation Loss: 40.856585681438446\n",
      "Epoch 39 \t Batch 60 \t Validation Loss: 45.12718454202016\n",
      "Epoch 39 \t Batch 80 \t Validation Loss: 43.13733531832695\n",
      "Epoch 39 \t Batch 100 \t Validation Loss: 39.821885256767274\n",
      "Epoch 39 \t Batch 120 \t Validation Loss: 38.074929384390515\n",
      "Epoch 39 \t Batch 140 \t Validation Loss: 36.58810946941376\n",
      "Epoch 39 \t Batch 160 \t Validation Loss: 37.277628222107886\n",
      "Epoch 39 \t Batch 180 \t Validation Loss: 39.90540722476111\n",
      "Epoch 39 \t Batch 200 \t Validation Loss: 40.63307016134262\n",
      "Epoch 39 \t Batch 220 \t Validation Loss: 41.37745054201646\n",
      "Epoch 39 \t Batch 240 \t Validation Loss: 41.27112240990003\n",
      "Epoch 39 \t Batch 260 \t Validation Loss: 42.985035479985754\n",
      "Epoch 39 \t Batch 280 \t Validation Loss: 43.71929673297065\n",
      "Epoch 39 \t Batch 300 \t Validation Loss: 44.44024608453115\n",
      "Epoch 39 \t Batch 320 \t Validation Loss: 44.598321203887465\n",
      "Epoch 39 \t Batch 340 \t Validation Loss: 44.25746642701766\n",
      "Epoch 39 \t Batch 360 \t Validation Loss: 43.87148723999659\n",
      "Epoch 39 \t Batch 380 \t Validation Loss: 43.893370948339765\n",
      "Epoch 39 \t Batch 400 \t Validation Loss: 43.254838455915454\n",
      "Epoch 39 \t Batch 420 \t Validation Loss: 43.08270636399587\n",
      "Epoch 39 \t Batch 440 \t Validation Loss: 42.59380350004543\n",
      "Epoch 39 \t Batch 460 \t Validation Loss: 42.63250346287437\n",
      "Epoch 39 \t Batch 480 \t Validation Loss: 42.94542636374633\n",
      "Epoch 39 \t Batch 500 \t Validation Loss: 42.5291417760849\n",
      "Epoch 39 \t Batch 520 \t Validation Loss: 42.13224714811032\n",
      "Epoch 39 \t Batch 540 \t Validation Loss: 41.73302196661631\n",
      "Epoch 39 \t Batch 560 \t Validation Loss: 41.362989225557875\n",
      "Epoch 39 \t Batch 580 \t Validation Loss: 40.93486457446526\n",
      "Epoch 39 \t Batch 600 \t Validation Loss: 41.04002291440964\n",
      "Epoch 39 Training Loss: 47.151556728450394 Validation Loss: 41.5652264534653\n",
      "Epoch 39 completed\n",
      "Epoch 40 \t Batch 20 \t Training Loss: 46.45948085784912\n",
      "Epoch 40 \t Batch 40 \t Training Loss: 46.33417015075683\n",
      "Epoch 40 \t Batch 60 \t Training Loss: 46.62637920379639\n",
      "Epoch 40 \t Batch 80 \t Training Loss: 46.62581729888916\n",
      "Epoch 40 \t Batch 100 \t Training Loss: 46.80464347839356\n",
      "Epoch 40 \t Batch 120 \t Training Loss: 46.683551184336345\n",
      "Epoch 40 \t Batch 140 \t Training Loss: 46.64772581372942\n",
      "Epoch 40 \t Batch 160 \t Training Loss: 46.72350633144379\n",
      "Epoch 40 \t Batch 180 \t Training Loss: 46.778225368923614\n",
      "Epoch 40 \t Batch 200 \t Training Loss: 46.74398946762085\n",
      "Epoch 40 \t Batch 220 \t Training Loss: 46.84084678996693\n",
      "Epoch 40 \t Batch 240 \t Training Loss: 46.8918228785197\n",
      "Epoch 40 \t Batch 260 \t Training Loss: 46.953423382685735\n",
      "Epoch 40 \t Batch 280 \t Training Loss: 47.08538096291678\n",
      "Epoch 40 \t Batch 300 \t Training Loss: 47.046628112792966\n",
      "Epoch 40 \t Batch 320 \t Training Loss: 47.07985701560974\n",
      "Epoch 40 \t Batch 340 \t Training Loss: 47.03461428249584\n",
      "Epoch 40 \t Batch 360 \t Training Loss: 47.11167197757297\n",
      "Epoch 40 \t Batch 380 \t Training Loss: 47.09558591340718\n",
      "Epoch 40 \t Batch 400 \t Training Loss: 47.09150349617004\n",
      "Epoch 40 \t Batch 420 \t Training Loss: 47.11207378932408\n",
      "Epoch 40 \t Batch 440 \t Training Loss: 47.07530105764216\n",
      "Epoch 40 \t Batch 460 \t Training Loss: 47.13977986211362\n",
      "Epoch 40 \t Batch 480 \t Training Loss: 47.03074916203817\n",
      "Epoch 40 \t Batch 500 \t Training Loss: 47.05697924804687\n",
      "Epoch 40 \t Batch 520 \t Training Loss: 47.050085830688474\n",
      "Epoch 40 \t Batch 540 \t Training Loss: 46.99363995304814\n",
      "Epoch 40 \t Batch 560 \t Training Loss: 47.01757217815944\n",
      "Epoch 40 \t Batch 580 \t Training Loss: 46.981241607666014\n",
      "Epoch 40 \t Batch 600 \t Training Loss: 47.003354187011716\n",
      "Epoch 40 \t Batch 620 \t Training Loss: 47.00485936441729\n",
      "Epoch 40 \t Batch 640 \t Training Loss: 47.00693750977516\n",
      "Epoch 40 \t Batch 660 \t Training Loss: 47.00991223653158\n",
      "Epoch 40 \t Batch 680 \t Training Loss: 47.02413026585298\n",
      "Epoch 40 \t Batch 700 \t Training Loss: 47.02193504333496\n",
      "Epoch 40 \t Batch 720 \t Training Loss: 47.00663906733195\n",
      "Epoch 40 \t Batch 740 \t Training Loss: 47.03681787026895\n",
      "Epoch 40 \t Batch 760 \t Training Loss: 47.044981068059016\n",
      "Epoch 40 \t Batch 780 \t Training Loss: 47.05044014759553\n",
      "Epoch 40 \t Batch 800 \t Training Loss: 47.09465653896332\n",
      "Epoch 40 \t Batch 820 \t Training Loss: 47.137200769564004\n",
      "Epoch 40 \t Batch 840 \t Training Loss: 47.13811872573126\n",
      "Epoch 40 \t Batch 860 \t Training Loss: 47.14862120428751\n",
      "Epoch 40 \t Batch 880 \t Training Loss: 47.13704773729498\n",
      "Epoch 40 \t Batch 900 \t Training Loss: 47.13095161861843\n",
      "Epoch 40 \t Batch 20 \t Validation Loss: 47.670674848556516\n",
      "Epoch 40 \t Batch 40 \t Validation Loss: 41.988654947280885\n",
      "Epoch 40 \t Batch 60 \t Validation Loss: 45.38628859519959\n",
      "Epoch 40 \t Batch 80 \t Validation Loss: 43.51470649242401\n",
      "Epoch 40 \t Batch 100 \t Validation Loss: 40.27176233291626\n",
      "Epoch 40 \t Batch 120 \t Validation Loss: 38.48400750160217\n",
      "Epoch 40 \t Batch 140 \t Validation Loss: 36.98199954714094\n",
      "Epoch 40 \t Batch 160 \t Validation Loss: 37.63240437507629\n",
      "Epoch 40 \t Batch 180 \t Validation Loss: 40.02440142101712\n",
      "Epoch 40 \t Batch 200 \t Validation Loss: 40.67609962463379\n",
      "Epoch 40 \t Batch 220 \t Validation Loss: 41.3314884445884\n",
      "Epoch 40 \t Batch 240 \t Validation Loss: 41.16844730377197\n",
      "Epoch 40 \t Batch 260 \t Validation Loss: 42.802439088087816\n",
      "Epoch 40 \t Batch 280 \t Validation Loss: 43.481654848371235\n",
      "Epoch 40 \t Batch 300 \t Validation Loss: 44.1355251121521\n",
      "Epoch 40 \t Batch 320 \t Validation Loss: 44.29332824349403\n",
      "Epoch 40 \t Batch 340 \t Validation Loss: 43.979306866140924\n",
      "Epoch 40 \t Batch 360 \t Validation Loss: 43.619407108094954\n",
      "Epoch 40 \t Batch 380 \t Validation Loss: 43.67491641295584\n",
      "Epoch 40 \t Batch 400 \t Validation Loss: 43.14389178276062\n",
      "Epoch 40 \t Batch 420 \t Validation Loss: 42.99386488142468\n",
      "Epoch 40 \t Batch 440 \t Validation Loss: 42.64770302772522\n",
      "Epoch 40 \t Batch 460 \t Validation Loss: 42.75072614006374\n",
      "Epoch 40 \t Batch 480 \t Validation Loss: 43.07099570830663\n",
      "Epoch 40 \t Batch 500 \t Validation Loss: 42.65069005966186\n",
      "Epoch 40 \t Batch 520 \t Validation Loss: 42.38003307489248\n",
      "Epoch 40 \t Batch 540 \t Validation Loss: 42.008277847148754\n",
      "Epoch 40 \t Batch 560 \t Validation Loss: 41.707067356790816\n",
      "Epoch 40 \t Batch 580 \t Validation Loss: 41.41766730670271\n",
      "Epoch 40 \t Batch 600 \t Validation Loss: 41.51728133837382\n",
      "Epoch 40 Training Loss: 47.12802640276361 Validation Loss: 42.068755478053895\n",
      "Epoch 40 completed\n",
      "Epoch 41 \t Batch 20 \t Training Loss: 46.79375057220459\n",
      "Epoch 41 \t Batch 40 \t Training Loss: 47.02543182373047\n",
      "Epoch 41 \t Batch 60 \t Training Loss: 47.5515443166097\n",
      "Epoch 41 \t Batch 80 \t Training Loss: 47.56343913078308\n",
      "Epoch 41 \t Batch 100 \t Training Loss: 47.39938362121582\n",
      "Epoch 41 \t Batch 120 \t Training Loss: 47.49719886779785\n",
      "Epoch 41 \t Batch 140 \t Training Loss: 47.51413756779262\n",
      "Epoch 41 \t Batch 160 \t Training Loss: 47.33661615848541\n",
      "Epoch 41 \t Batch 180 \t Training Loss: 47.42911425696479\n",
      "Epoch 41 \t Batch 200 \t Training Loss: 47.369437084198\n",
      "Epoch 41 \t Batch 220 \t Training Loss: 47.307418424432925\n",
      "Epoch 41 \t Batch 240 \t Training Loss: 47.352357387542725\n",
      "Epoch 41 \t Batch 260 \t Training Loss: 47.35866587712214\n",
      "Epoch 41 \t Batch 280 \t Training Loss: 47.307171317509244\n",
      "Epoch 41 \t Batch 300 \t Training Loss: 47.2537483215332\n",
      "Epoch 41 \t Batch 320 \t Training Loss: 47.22655519247055\n",
      "Epoch 41 \t Batch 340 \t Training Loss: 47.20466630599078\n",
      "Epoch 41 \t Batch 360 \t Training Loss: 47.22945179409451\n",
      "Epoch 41 \t Batch 380 \t Training Loss: 47.14574518705669\n",
      "Epoch 41 \t Batch 400 \t Training Loss: 47.181165037155154\n",
      "Epoch 41 \t Batch 420 \t Training Loss: 47.20306825183687\n",
      "Epoch 41 \t Batch 440 \t Training Loss: 47.21176238493486\n",
      "Epoch 41 \t Batch 460 \t Training Loss: 47.196256960993225\n",
      "Epoch 41 \t Batch 480 \t Training Loss: 47.14935480753581\n",
      "Epoch 41 \t Batch 500 \t Training Loss: 47.20620070648194\n",
      "Epoch 41 \t Batch 520 \t Training Loss: 47.18149946652926\n",
      "Epoch 41 \t Batch 540 \t Training Loss: 47.1239512902719\n",
      "Epoch 41 \t Batch 560 \t Training Loss: 47.12161688804626\n",
      "Epoch 41 \t Batch 580 \t Training Loss: 47.153352158645106\n",
      "Epoch 41 \t Batch 600 \t Training Loss: 47.172970091501874\n",
      "Epoch 41 \t Batch 620 \t Training Loss: 47.169107800145305\n",
      "Epoch 41 \t Batch 640 \t Training Loss: 47.17813184261322\n",
      "Epoch 41 \t Batch 660 \t Training Loss: 47.161697098703094\n",
      "Epoch 41 \t Batch 680 \t Training Loss: 47.12369551939123\n",
      "Epoch 41 \t Batch 700 \t Training Loss: 47.119773314339774\n",
      "Epoch 41 \t Batch 720 \t Training Loss: 47.09513308207194\n",
      "Epoch 41 \t Batch 740 \t Training Loss: 47.10229155050742\n",
      "Epoch 41 \t Batch 760 \t Training Loss: 47.09084789878444\n",
      "Epoch 41 \t Batch 780 \t Training Loss: 47.115223248799644\n",
      "Epoch 41 \t Batch 800 \t Training Loss: 47.11290819168091\n",
      "Epoch 41 \t Batch 820 \t Training Loss: 47.098877971928296\n",
      "Epoch 41 \t Batch 840 \t Training Loss: 47.12055809384301\n",
      "Epoch 41 \t Batch 860 \t Training Loss: 47.14126135803932\n",
      "Epoch 41 \t Batch 880 \t Training Loss: 47.107962586662985\n",
      "Epoch 41 \t Batch 900 \t Training Loss: 47.11044819725884\n",
      "Epoch 41 \t Batch 20 \t Validation Loss: 43.504919624328615\n",
      "Epoch 41 \t Batch 40 \t Validation Loss: 38.355437386035916\n",
      "Epoch 41 \t Batch 60 \t Validation Loss: 42.045282340049745\n",
      "Epoch 41 \t Batch 80 \t Validation Loss: 40.11715714335442\n",
      "Epoch 41 \t Batch 100 \t Validation Loss: 37.48510419368744\n",
      "Epoch 41 \t Batch 120 \t Validation Loss: 36.04739127556483\n",
      "Epoch 41 \t Batch 140 \t Validation Loss: 34.829174930708746\n",
      "Epoch 41 \t Batch 160 \t Validation Loss: 35.71388200819492\n",
      "Epoch 41 \t Batch 180 \t Validation Loss: 38.37172780301836\n",
      "Epoch 41 \t Batch 200 \t Validation Loss: 39.147565410137176\n",
      "Epoch 41 \t Batch 220 \t Validation Loss: 40.02573284452612\n",
      "Epoch 41 \t Batch 240 \t Validation Loss: 40.03152407606443\n",
      "Epoch 41 \t Batch 260 \t Validation Loss: 41.81814729800591\n",
      "Epoch 41 \t Batch 280 \t Validation Loss: 42.61967225245067\n",
      "Epoch 41 \t Batch 300 \t Validation Loss: 43.3068109591802\n",
      "Epoch 41 \t Batch 320 \t Validation Loss: 43.5078089132905\n",
      "Epoch 41 \t Batch 340 \t Validation Loss: 43.21451961994171\n",
      "Epoch 41 \t Batch 360 \t Validation Loss: 42.869416227605605\n",
      "Epoch 41 \t Batch 380 \t Validation Loss: 42.925759788563376\n",
      "Epoch 41 \t Batch 400 \t Validation Loss: 42.31554185509682\n",
      "Epoch 41 \t Batch 420 \t Validation Loss: 42.14976879187993\n",
      "Epoch 41 \t Batch 440 \t Validation Loss: 41.697518863461234\n",
      "Epoch 41 \t Batch 460 \t Validation Loss: 41.753519144265546\n",
      "Epoch 41 \t Batch 480 \t Validation Loss: 42.08534945944945\n",
      "Epoch 41 \t Batch 500 \t Validation Loss: 41.675915553092956\n",
      "Epoch 41 \t Batch 520 \t Validation Loss: 41.29887504302538\n",
      "Epoch 41 \t Batch 540 \t Validation Loss: 40.92306877153891\n",
      "Epoch 41 \t Batch 560 \t Validation Loss: 40.57105530415262\n",
      "Epoch 41 \t Batch 580 \t Validation Loss: 40.13136194574422\n",
      "Epoch 41 \t Batch 600 \t Validation Loss: 40.262016294002535\n",
      "Epoch 41 Training Loss: 47.118486294179604 Validation Loss: 40.79216217452829\n",
      "Epoch 41 completed\n",
      "Epoch 42 \t Batch 20 \t Training Loss: 46.77453880310058\n",
      "Epoch 42 \t Batch 40 \t Training Loss: 46.349251651763915\n",
      "Epoch 42 \t Batch 60 \t Training Loss: 46.72892837524414\n",
      "Epoch 42 \t Batch 80 \t Training Loss: 46.7022271156311\n",
      "Epoch 42 \t Batch 100 \t Training Loss: 46.29261692047119\n",
      "Epoch 42 \t Batch 120 \t Training Loss: 46.38567056655884\n",
      "Epoch 42 \t Batch 140 \t Training Loss: 46.51422369820731\n",
      "Epoch 42 \t Batch 160 \t Training Loss: 46.632940602302554\n",
      "Epoch 42 \t Batch 180 \t Training Loss: 46.48556368086073\n",
      "Epoch 42 \t Batch 200 \t Training Loss: 46.45503662109375\n",
      "Epoch 42 \t Batch 220 \t Training Loss: 46.47902200872248\n",
      "Epoch 42 \t Batch 240 \t Training Loss: 46.59661741256714\n",
      "Epoch 42 \t Batch 260 \t Training Loss: 46.635780202425444\n",
      "Epoch 42 \t Batch 280 \t Training Loss: 46.68577972139631\n",
      "Epoch 42 \t Batch 300 \t Training Loss: 46.72950912475586\n",
      "Epoch 42 \t Batch 320 \t Training Loss: 46.72111834287644\n",
      "Epoch 42 \t Batch 340 \t Training Loss: 46.85665812772863\n",
      "Epoch 42 \t Batch 360 \t Training Loss: 46.885886107550725\n",
      "Epoch 42 \t Batch 380 \t Training Loss: 46.938152493928605\n",
      "Epoch 42 \t Batch 400 \t Training Loss: 46.9492366027832\n",
      "Epoch 42 \t Batch 420 \t Training Loss: 46.91226157233829\n",
      "Epoch 42 \t Batch 440 \t Training Loss: 46.88270599191839\n",
      "Epoch 42 \t Batch 460 \t Training Loss: 46.94267481099004\n",
      "Epoch 42 \t Batch 480 \t Training Loss: 46.94463412761688\n",
      "Epoch 42 \t Batch 500 \t Training Loss: 47.00161653900147\n",
      "Epoch 42 \t Batch 520 \t Training Loss: 47.00320103718684\n",
      "Epoch 42 \t Batch 540 \t Training Loss: 46.94397241097909\n",
      "Epoch 42 \t Batch 560 \t Training Loss: 46.91181748935154\n",
      "Epoch 42 \t Batch 580 \t Training Loss: 46.96081465359392\n",
      "Epoch 42 \t Batch 600 \t Training Loss: 46.92571870803833\n",
      "Epoch 42 \t Batch 620 \t Training Loss: 46.923754962798085\n",
      "Epoch 42 \t Batch 640 \t Training Loss: 46.96462062597275\n",
      "Epoch 42 \t Batch 660 \t Training Loss: 46.97344821583141\n",
      "Epoch 42 \t Batch 680 \t Training Loss: 46.976390187880575\n",
      "Epoch 42 \t Batch 700 \t Training Loss: 46.91871640886579\n",
      "Epoch 42 \t Batch 720 \t Training Loss: 46.93021380636427\n",
      "Epoch 42 \t Batch 740 \t Training Loss: 46.95956223462079\n",
      "Epoch 42 \t Batch 760 \t Training Loss: 47.006282309481975\n",
      "Epoch 42 \t Batch 780 \t Training Loss: 47.02691462590144\n",
      "Epoch 42 \t Batch 800 \t Training Loss: 47.01376009941101\n",
      "Epoch 42 \t Batch 820 \t Training Loss: 46.99973474828209\n",
      "Epoch 42 \t Batch 840 \t Training Loss: 46.99408369064331\n",
      "Epoch 42 \t Batch 860 \t Training Loss: 47.0301134198211\n",
      "Epoch 42 \t Batch 880 \t Training Loss: 47.05207920074463\n",
      "Epoch 42 \t Batch 900 \t Training Loss: 47.069774462382\n",
      "Epoch 42 \t Batch 20 \t Validation Loss: 33.105312967300414\n",
      "Epoch 42 \t Batch 40 \t Validation Loss: 30.381956100463867\n",
      "Epoch 42 \t Batch 60 \t Validation Loss: 32.978991413116454\n",
      "Epoch 42 \t Batch 80 \t Validation Loss: 31.929786670207978\n",
      "Epoch 42 \t Batch 100 \t Validation Loss: 30.91229645729065\n",
      "Epoch 42 \t Batch 120 \t Validation Loss: 30.59313348134359\n",
      "Epoch 42 \t Batch 140 \t Validation Loss: 30.12752342224121\n",
      "Epoch 42 \t Batch 160 \t Validation Loss: 31.604206120967866\n",
      "Epoch 42 \t Batch 180 \t Validation Loss: 34.66417649057176\n",
      "Epoch 42 \t Batch 200 \t Validation Loss: 35.84486300945282\n",
      "Epoch 42 \t Batch 220 \t Validation Loss: 36.95031279217113\n",
      "Epoch 42 \t Batch 240 \t Validation Loss: 37.151054692268374\n",
      "Epoch 42 \t Batch 260 \t Validation Loss: 39.130046363977286\n",
      "Epoch 42 \t Batch 280 \t Validation Loss: 40.1168842622212\n",
      "Epoch 42 \t Batch 300 \t Validation Loss: 40.96077220598857\n",
      "Epoch 42 \t Batch 320 \t Validation Loss: 41.292263039946555\n",
      "Epoch 42 \t Batch 340 \t Validation Loss: 41.16057621170493\n",
      "Epoch 42 \t Batch 360 \t Validation Loss: 40.92996788289812\n",
      "Epoch 42 \t Batch 380 \t Validation Loss: 41.09056070478339\n",
      "Epoch 42 \t Batch 400 \t Validation Loss: 40.60095138311386\n",
      "Epoch 42 \t Batch 420 \t Validation Loss: 40.56186629931132\n",
      "Epoch 42 \t Batch 440 \t Validation Loss: 40.18870392929424\n",
      "Epoch 42 \t Batch 460 \t Validation Loss: 40.316549746886544\n",
      "Epoch 42 \t Batch 480 \t Validation Loss: 40.75072345932325\n",
      "Epoch 42 \t Batch 500 \t Validation Loss: 40.447365480422974\n",
      "Epoch 42 \t Batch 520 \t Validation Loss: 40.114841118225684\n",
      "Epoch 42 \t Batch 540 \t Validation Loss: 39.75810457335578\n",
      "Epoch 42 \t Batch 560 \t Validation Loss: 39.44439297573907\n",
      "Epoch 42 \t Batch 580 \t Validation Loss: 39.12109521504106\n",
      "Epoch 42 \t Batch 600 \t Validation Loss: 39.25287351131439\n",
      "Epoch 42 Training Loss: 47.07765002235072 Validation Loss: 39.82822448402256\n",
      "Epoch 42 completed\n",
      "Epoch 43 \t Batch 20 \t Training Loss: 47.63328838348389\n",
      "Epoch 43 \t Batch 40 \t Training Loss: 47.56195278167725\n",
      "Epoch 43 \t Batch 60 \t Training Loss: 47.802323468526204\n",
      "Epoch 43 \t Batch 80 \t Training Loss: 47.4383508682251\n",
      "Epoch 43 \t Batch 100 \t Training Loss: 47.00630947113037\n",
      "Epoch 43 \t Batch 120 \t Training Loss: 47.1284966468811\n",
      "Epoch 43 \t Batch 140 \t Training Loss: 47.04092848641532\n",
      "Epoch 43 \t Batch 160 \t Training Loss: 47.081346249580385\n",
      "Epoch 43 \t Batch 180 \t Training Loss: 47.06564812130398\n",
      "Epoch 43 \t Batch 200 \t Training Loss: 47.090988883972166\n",
      "Epoch 43 \t Batch 220 \t Training Loss: 46.90963013388894\n",
      "Epoch 43 \t Batch 240 \t Training Loss: 46.9648876508077\n",
      "Epoch 43 \t Batch 260 \t Training Loss: 47.091077965956465\n",
      "Epoch 43 \t Batch 280 \t Training Loss: 47.1294340133667\n",
      "Epoch 43 \t Batch 300 \t Training Loss: 47.0998091506958\n",
      "Epoch 43 \t Batch 320 \t Training Loss: 47.154274213314054\n",
      "Epoch 43 \t Batch 340 \t Training Loss: 47.12332173515769\n",
      "Epoch 43 \t Batch 360 \t Training Loss: 46.94577573140462\n",
      "Epoch 43 \t Batch 380 \t Training Loss: 46.95674777783846\n",
      "Epoch 43 \t Batch 400 \t Training Loss: 47.006884775161744\n",
      "Epoch 43 \t Batch 420 \t Training Loss: 46.92828536260696\n",
      "Epoch 43 \t Batch 440 \t Training Loss: 46.99514542492953\n",
      "Epoch 43 \t Batch 460 \t Training Loss: 46.95270583940589\n",
      "Epoch 43 \t Batch 480 \t Training Loss: 46.981016620000204\n",
      "Epoch 43 \t Batch 500 \t Training Loss: 46.99315044403076\n",
      "Epoch 43 \t Batch 520 \t Training Loss: 46.997665038475624\n",
      "Epoch 43 \t Batch 540 \t Training Loss: 46.97998069480614\n",
      "Epoch 43 \t Batch 560 \t Training Loss: 46.991598633357455\n",
      "Epoch 43 \t Batch 580 \t Training Loss: 47.017709659707954\n",
      "Epoch 43 \t Batch 600 \t Training Loss: 46.99618701299032\n",
      "Epoch 43 \t Batch 620 \t Training Loss: 47.020122940309584\n",
      "Epoch 43 \t Batch 640 \t Training Loss: 47.033500474691394\n",
      "Epoch 43 \t Batch 660 \t Training Loss: 47.03295071919759\n",
      "Epoch 43 \t Batch 680 \t Training Loss: 47.01754195269417\n",
      "Epoch 43 \t Batch 700 \t Training Loss: 46.99400705610003\n",
      "Epoch 43 \t Batch 720 \t Training Loss: 47.000552309883965\n",
      "Epoch 43 \t Batch 740 \t Training Loss: 46.995336702707654\n",
      "Epoch 43 \t Batch 760 \t Training Loss: 46.98094944201018\n",
      "Epoch 43 \t Batch 780 \t Training Loss: 46.987605539957684\n",
      "Epoch 43 \t Batch 800 \t Training Loss: 47.023781309127806\n",
      "Epoch 43 \t Batch 820 \t Training Loss: 47.081866255039124\n",
      "Epoch 43 \t Batch 840 \t Training Loss: 47.07566412517003\n",
      "Epoch 43 \t Batch 860 \t Training Loss: 47.05662468311399\n",
      "Epoch 43 \t Batch 880 \t Training Loss: 47.05255841341886\n",
      "Epoch 43 \t Batch 900 \t Training Loss: 47.042950490315754\n",
      "Epoch 43 \t Batch 20 \t Validation Loss: 41.86716237068176\n",
      "Epoch 43 \t Batch 40 \t Validation Loss: 37.22821354866028\n",
      "Epoch 43 \t Batch 60 \t Validation Loss: 40.388163868586226\n",
      "Epoch 43 \t Batch 80 \t Validation Loss: 38.8793759226799\n",
      "Epoch 43 \t Batch 100 \t Validation Loss: 36.732110509872435\n",
      "Epoch 43 \t Batch 120 \t Validation Loss: 35.68075530529022\n",
      "Epoch 43 \t Batch 140 \t Validation Loss: 34.620304455075946\n",
      "Epoch 43 \t Batch 160 \t Validation Loss: 35.51103828549385\n",
      "Epoch 43 \t Batch 180 \t Validation Loss: 38.16246044900682\n",
      "Epoch 43 \t Batch 200 \t Validation Loss: 38.916585631370545\n",
      "Epoch 43 \t Batch 220 \t Validation Loss: 39.73454529588873\n",
      "Epoch 43 \t Batch 240 \t Validation Loss: 39.7039408882459\n",
      "Epoch 43 \t Batch 260 \t Validation Loss: 41.43189351742084\n",
      "Epoch 43 \t Batch 280 \t Validation Loss: 42.21085587569645\n",
      "Epoch 43 \t Batch 300 \t Validation Loss: 42.9156893825531\n",
      "Epoch 43 \t Batch 320 \t Validation Loss: 43.115571305155754\n",
      "Epoch 43 \t Batch 340 \t Validation Loss: 42.840357104469746\n",
      "Epoch 43 \t Batch 360 \t Validation Loss: 42.51855689419641\n",
      "Epoch 43 \t Batch 380 \t Validation Loss: 42.577730688295865\n",
      "Epoch 43 \t Batch 400 \t Validation Loss: 42.01170746088028\n",
      "Epoch 43 \t Batch 420 \t Validation Loss: 41.88376184418088\n",
      "Epoch 43 \t Batch 440 \t Validation Loss: 41.48427793546156\n",
      "Epoch 43 \t Batch 460 \t Validation Loss: 41.57964245132778\n",
      "Epoch 43 \t Batch 480 \t Validation Loss: 41.9227386812369\n",
      "Epoch 43 \t Batch 500 \t Validation Loss: 41.52449751091003\n",
      "Epoch 43 \t Batch 520 \t Validation Loss: 41.17420305288755\n",
      "Epoch 43 \t Batch 540 \t Validation Loss: 40.827563273465195\n",
      "Epoch 43 \t Batch 560 \t Validation Loss: 40.49796993902751\n",
      "Epoch 43 \t Batch 580 \t Validation Loss: 40.09249536086773\n",
      "Epoch 43 \t Batch 600 \t Validation Loss: 40.25154022057851\n",
      "Epoch 43 Training Loss: 47.04053209444169 Validation Loss: 40.78412820921316\n",
      "Epoch 43 completed\n",
      "Epoch 44 \t Batch 20 \t Training Loss: 46.11144027709961\n",
      "Epoch 44 \t Batch 40 \t Training Loss: 46.53276529312134\n",
      "Epoch 44 \t Batch 60 \t Training Loss: 46.518972651163736\n",
      "Epoch 44 \t Batch 80 \t Training Loss: 46.477552556991576\n",
      "Epoch 44 \t Batch 100 \t Training Loss: 46.66844383239746\n",
      "Epoch 44 \t Batch 120 \t Training Loss: 46.54821542104085\n",
      "Epoch 44 \t Batch 140 \t Training Loss: 46.5031523840768\n",
      "Epoch 44 \t Batch 160 \t Training Loss: 46.58285388946533\n",
      "Epoch 44 \t Batch 180 \t Training Loss: 46.53527204725477\n",
      "Epoch 44 \t Batch 200 \t Training Loss: 46.59835386276245\n",
      "Epoch 44 \t Batch 220 \t Training Loss: 46.68354884060946\n",
      "Epoch 44 \t Batch 240 \t Training Loss: 46.70314612388611\n",
      "Epoch 44 \t Batch 260 \t Training Loss: 46.691110288179836\n",
      "Epoch 44 \t Batch 280 \t Training Loss: 46.710443196977884\n",
      "Epoch 44 \t Batch 300 \t Training Loss: 46.793354008992516\n",
      "Epoch 44 \t Batch 320 \t Training Loss: 46.87059134244919\n",
      "Epoch 44 \t Batch 340 \t Training Loss: 46.88416819852941\n",
      "Epoch 44 \t Batch 360 \t Training Loss: 46.87518654929267\n",
      "Epoch 44 \t Batch 380 \t Training Loss: 46.859969420182075\n",
      "Epoch 44 \t Batch 400 \t Training Loss: 46.8877398109436\n",
      "Epoch 44 \t Batch 420 \t Training Loss: 46.90906759897868\n",
      "Epoch 44 \t Batch 440 \t Training Loss: 46.97861436497082\n",
      "Epoch 44 \t Batch 460 \t Training Loss: 46.9703158337137\n",
      "Epoch 44 \t Batch 480 \t Training Loss: 46.96437860329946\n",
      "Epoch 44 \t Batch 500 \t Training Loss: 46.953407905578615\n",
      "Epoch 44 \t Batch 520 \t Training Loss: 46.94701939362746\n",
      "Epoch 44 \t Batch 540 \t Training Loss: 46.935472361246745\n",
      "Epoch 44 \t Batch 560 \t Training Loss: 46.917923770632065\n",
      "Epoch 44 \t Batch 580 \t Training Loss: 46.95477246909306\n",
      "Epoch 44 \t Batch 600 \t Training Loss: 46.92633726755778\n",
      "Epoch 44 \t Batch 620 \t Training Loss: 46.94416339012884\n",
      "Epoch 44 \t Batch 640 \t Training Loss: 46.97767528295517\n",
      "Epoch 44 \t Batch 660 \t Training Loss: 46.99892774639708\n",
      "Epoch 44 \t Batch 680 \t Training Loss: 47.00736908071181\n",
      "Epoch 44 \t Batch 700 \t Training Loss: 47.028465363638745\n",
      "Epoch 44 \t Batch 720 \t Training Loss: 47.013679122924806\n",
      "Epoch 44 \t Batch 740 \t Training Loss: 46.98063512235075\n",
      "Epoch 44 \t Batch 760 \t Training Loss: 46.99194985439903\n",
      "Epoch 44 \t Batch 780 \t Training Loss: 46.997542811662726\n",
      "Epoch 44 \t Batch 800 \t Training Loss: 47.040237631797794\n",
      "Epoch 44 \t Batch 820 \t Training Loss: 47.06789206062875\n",
      "Epoch 44 \t Batch 840 \t Training Loss: 47.05769840876261\n",
      "Epoch 44 \t Batch 860 \t Training Loss: 47.06839818289114\n",
      "Epoch 44 \t Batch 880 \t Training Loss: 47.05713804851879\n",
      "Epoch 44 \t Batch 900 \t Training Loss: 47.03473573472765\n",
      "Epoch 44 \t Batch 20 \t Validation Loss: 34.89423370361328\n",
      "Epoch 44 \t Batch 40 \t Validation Loss: 31.379240500926972\n",
      "Epoch 44 \t Batch 60 \t Validation Loss: 34.506115317344666\n",
      "Epoch 44 \t Batch 80 \t Validation Loss: 33.20193011760712\n",
      "Epoch 44 \t Batch 100 \t Validation Loss: 32.01494749069214\n",
      "Epoch 44 \t Batch 120 \t Validation Loss: 31.650392882029216\n",
      "Epoch 44 \t Batch 140 \t Validation Loss: 31.13561233792986\n",
      "Epoch 44 \t Batch 160 \t Validation Loss: 32.449583888053894\n",
      "Epoch 44 \t Batch 180 \t Validation Loss: 35.44789847267999\n",
      "Epoch 44 \t Batch 200 \t Validation Loss: 36.553023195266725\n",
      "Epoch 44 \t Batch 220 \t Validation Loss: 37.61780272397128\n",
      "Epoch 44 \t Batch 240 \t Validation Loss: 37.749031428496046\n",
      "Epoch 44 \t Batch 260 \t Validation Loss: 39.72173498960642\n",
      "Epoch 44 \t Batch 280 \t Validation Loss: 40.71875815050942\n",
      "Epoch 44 \t Batch 300 \t Validation Loss: 41.510200770696\n",
      "Epoch 44 \t Batch 320 \t Validation Loss: 41.77758843004703\n",
      "Epoch 44 \t Batch 340 \t Validation Loss: 41.58874883371241\n",
      "Epoch 44 \t Batch 360 \t Validation Loss: 41.323316889339026\n",
      "Epoch 44 \t Batch 380 \t Validation Loss: 41.48309002173574\n",
      "Epoch 44 \t Batch 400 \t Validation Loss: 40.96993916273117\n",
      "Epoch 44 \t Batch 420 \t Validation Loss: 40.879211160114835\n",
      "Epoch 44 \t Batch 440 \t Validation Loss: 40.51932483803142\n",
      "Epoch 44 \t Batch 460 \t Validation Loss: 40.62153528254965\n",
      "Epoch 44 \t Batch 480 \t Validation Loss: 41.00922583142916\n",
      "Epoch 44 \t Batch 500 \t Validation Loss: 40.64813614463806\n",
      "Epoch 44 \t Batch 520 \t Validation Loss: 40.34837293991676\n",
      "Epoch 44 \t Batch 540 \t Validation Loss: 40.03319240146213\n",
      "Epoch 44 \t Batch 560 \t Validation Loss: 39.76812360627311\n",
      "Epoch 44 \t Batch 580 \t Validation Loss: 39.507172637150205\n",
      "Epoch 44 \t Batch 600 \t Validation Loss: 39.661572551727296\n",
      "Epoch 44 Training Loss: 47.041211730391275 Validation Loss: 40.2560810516407\n",
      "Epoch 44 completed\n",
      "Epoch 45 \t Batch 20 \t Training Loss: 46.958790397644044\n",
      "Epoch 45 \t Batch 40 \t Training Loss: 46.831692123413085\n",
      "Epoch 45 \t Batch 60 \t Training Loss: 47.820987637837725\n",
      "Epoch 45 \t Batch 80 \t Training Loss: 47.569432163238524\n",
      "Epoch 45 \t Batch 100 \t Training Loss: 47.35012794494629\n",
      "Epoch 45 \t Batch 120 \t Training Loss: 47.51643597284953\n",
      "Epoch 45 \t Batch 140 \t Training Loss: 47.2876829964774\n",
      "Epoch 45 \t Batch 160 \t Training Loss: 47.301865577697754\n",
      "Epoch 45 \t Batch 180 \t Training Loss: 47.322320535447865\n",
      "Epoch 45 \t Batch 200 \t Training Loss: 47.23888917922974\n",
      "Epoch 45 \t Batch 220 \t Training Loss: 47.241853159124204\n",
      "Epoch 45 \t Batch 240 \t Training Loss: 47.12098166147868\n",
      "Epoch 45 \t Batch 260 \t Training Loss: 47.214719200134276\n",
      "Epoch 45 \t Batch 280 \t Training Loss: 47.14310562951224\n",
      "Epoch 45 \t Batch 300 \t Training Loss: 47.113821818033855\n",
      "Epoch 45 \t Batch 320 \t Training Loss: 47.065827429294586\n",
      "Epoch 45 \t Batch 340 \t Training Loss: 47.07132277769201\n",
      "Epoch 45 \t Batch 360 \t Training Loss: 47.10791123708089\n",
      "Epoch 45 \t Batch 380 \t Training Loss: 47.130768394470216\n",
      "Epoch 45 \t Batch 400 \t Training Loss: 47.18047564506531\n",
      "Epoch 45 \t Batch 420 \t Training Loss: 47.17331051599412\n",
      "Epoch 45 \t Batch 440 \t Training Loss: 47.13217365091497\n",
      "Epoch 45 \t Batch 460 \t Training Loss: 47.16695619666058\n",
      "Epoch 45 \t Batch 480 \t Training Loss: 47.13657750288645\n",
      "Epoch 45 \t Batch 500 \t Training Loss: 47.14345078277588\n",
      "Epoch 45 \t Batch 520 \t Training Loss: 47.14941270534809\n",
      "Epoch 45 \t Batch 540 \t Training Loss: 47.160190914295335\n",
      "Epoch 45 \t Batch 560 \t Training Loss: 47.165250532967704\n",
      "Epoch 45 \t Batch 580 \t Training Loss: 47.19209099473625\n",
      "Epoch 45 \t Batch 600 \t Training Loss: 47.15454284032186\n",
      "Epoch 45 \t Batch 620 \t Training Loss: 47.09215126652872\n",
      "Epoch 45 \t Batch 640 \t Training Loss: 47.078359466791156\n",
      "Epoch 45 \t Batch 660 \t Training Loss: 47.062259610493975\n",
      "Epoch 45 \t Batch 680 \t Training Loss: 47.05727563745835\n",
      "Epoch 45 \t Batch 700 \t Training Loss: 47.08107143947056\n",
      "Epoch 45 \t Batch 720 \t Training Loss: 47.077335288789534\n",
      "Epoch 45 \t Batch 740 \t Training Loss: 47.07047106253134\n",
      "Epoch 45 \t Batch 760 \t Training Loss: 47.08379757027877\n",
      "Epoch 45 \t Batch 780 \t Training Loss: 47.105659069159096\n",
      "Epoch 45 \t Batch 800 \t Training Loss: 47.12814192771911\n",
      "Epoch 45 \t Batch 820 \t Training Loss: 47.120398074824635\n",
      "Epoch 45 \t Batch 840 \t Training Loss: 47.11097917102632\n",
      "Epoch 45 \t Batch 860 \t Training Loss: 47.03204629144003\n",
      "Epoch 45 \t Batch 880 \t Training Loss: 47.03494633761319\n",
      "Epoch 45 \t Batch 900 \t Training Loss: 47.023391842312286\n",
      "Epoch 45 \t Batch 20 \t Validation Loss: 38.71229915618896\n",
      "Epoch 45 \t Batch 40 \t Validation Loss: 34.7550446510315\n",
      "Epoch 45 \t Batch 60 \t Validation Loss: 37.679088815053305\n",
      "Epoch 45 \t Batch 80 \t Validation Loss: 36.7445585846901\n",
      "Epoch 45 \t Batch 100 \t Validation Loss: 34.748278322219846\n",
      "Epoch 45 \t Batch 120 \t Validation Loss: 33.80466438134511\n",
      "Epoch 45 \t Batch 140 \t Validation Loss: 32.92253071240017\n",
      "Epoch 45 \t Batch 160 \t Validation Loss: 34.07443589568138\n",
      "Epoch 45 \t Batch 180 \t Validation Loss: 36.871651622984146\n",
      "Epoch 45 \t Batch 200 \t Validation Loss: 37.844493689537046\n",
      "Epoch 45 \t Batch 220 \t Validation Loss: 38.77776448076422\n",
      "Epoch 45 \t Batch 240 \t Validation Loss: 38.811162090301515\n",
      "Epoch 45 \t Batch 260 \t Validation Loss: 40.68269148973318\n",
      "Epoch 45 \t Batch 280 \t Validation Loss: 41.58268060343606\n",
      "Epoch 45 \t Batch 300 \t Validation Loss: 42.331351397832236\n",
      "Epoch 45 \t Batch 320 \t Validation Loss: 42.570446053147315\n",
      "Epoch 45 \t Batch 340 \t Validation Loss: 42.32835793495178\n",
      "Epoch 45 \t Batch 360 \t Validation Loss: 42.04567203256819\n",
      "Epoch 45 \t Batch 380 \t Validation Loss: 42.16488265740244\n",
      "Epoch 45 \t Batch 400 \t Validation Loss: 41.62945816755295\n",
      "Epoch 45 \t Batch 420 \t Validation Loss: 41.52921831721351\n",
      "Epoch 45 \t Batch 440 \t Validation Loss: 41.13600282452323\n",
      "Epoch 45 \t Batch 460 \t Validation Loss: 41.232674240029375\n",
      "Epoch 45 \t Batch 480 \t Validation Loss: 41.609749096632\n",
      "Epoch 45 \t Batch 500 \t Validation Loss: 41.23512775993347\n",
      "Epoch 45 \t Batch 520 \t Validation Loss: 40.91540408684657\n",
      "Epoch 45 \t Batch 540 \t Validation Loss: 40.55952113646048\n",
      "Epoch 45 \t Batch 560 \t Validation Loss: 40.24744957004275\n",
      "Epoch 45 \t Batch 580 \t Validation Loss: 39.938332233757805\n",
      "Epoch 45 \t Batch 600 \t Validation Loss: 40.059094592730204\n",
      "Epoch 45 Training Loss: 47.005888557226086 Validation Loss: 40.62891315330159\n",
      "Epoch 45 completed\n",
      "Epoch 46 \t Batch 20 \t Training Loss: 46.24898204803467\n",
      "Epoch 46 \t Batch 40 \t Training Loss: 46.136936569213866\n",
      "Epoch 46 \t Batch 60 \t Training Loss: 46.39395694732666\n",
      "Epoch 46 \t Batch 80 \t Training Loss: 46.41625032424927\n",
      "Epoch 46 \t Batch 100 \t Training Loss: 46.57807174682617\n",
      "Epoch 46 \t Batch 120 \t Training Loss: 46.43119449615479\n",
      "Epoch 46 \t Batch 140 \t Training Loss: 46.7019709450858\n",
      "Epoch 46 \t Batch 160 \t Training Loss: 46.86053564548492\n",
      "Epoch 46 \t Batch 180 \t Training Loss: 46.85646406809489\n",
      "Epoch 46 \t Batch 200 \t Training Loss: 46.80295734405517\n",
      "Epoch 46 \t Batch 220 \t Training Loss: 46.76510330546986\n",
      "Epoch 46 \t Batch 240 \t Training Loss: 46.6528724193573\n",
      "Epoch 46 \t Batch 260 \t Training Loss: 46.758754656865044\n",
      "Epoch 46 \t Batch 280 \t Training Loss: 46.83477310453142\n",
      "Epoch 46 \t Batch 300 \t Training Loss: 46.776491228739424\n",
      "Epoch 46 \t Batch 320 \t Training Loss: 46.776576900482176\n",
      "Epoch 46 \t Batch 340 \t Training Loss: 46.83991746341481\n",
      "Epoch 46 \t Batch 360 \t Training Loss: 46.837165154351126\n",
      "Epoch 46 \t Batch 380 \t Training Loss: 46.83061777416029\n",
      "Epoch 46 \t Batch 400 \t Training Loss: 46.83478118896485\n",
      "Epoch 46 \t Batch 420 \t Training Loss: 46.773966425941104\n",
      "Epoch 46 \t Batch 440 \t Training Loss: 46.795969208804046\n",
      "Epoch 46 \t Batch 460 \t Training Loss: 46.82011532161547\n",
      "Epoch 46 \t Batch 480 \t Training Loss: 46.802619417508446\n",
      "Epoch 46 \t Batch 500 \t Training Loss: 46.86020807647705\n",
      "Epoch 46 \t Batch 520 \t Training Loss: 46.8319028707651\n",
      "Epoch 46 \t Batch 540 \t Training Loss: 46.883085780673554\n",
      "Epoch 46 \t Batch 560 \t Training Loss: 46.86303575379508\n",
      "Epoch 46 \t Batch 580 \t Training Loss: 46.92860824321878\n",
      "Epoch 46 \t Batch 600 \t Training Loss: 46.931366475423175\n",
      "Epoch 46 \t Batch 620 \t Training Loss: 46.932849859422255\n",
      "Epoch 46 \t Batch 640 \t Training Loss: 46.9683555662632\n",
      "Epoch 46 \t Batch 660 \t Training Loss: 46.93855865362919\n",
      "Epoch 46 \t Batch 680 \t Training Loss: 46.95596907559563\n",
      "Epoch 46 \t Batch 700 \t Training Loss: 46.94499896458217\n",
      "Epoch 46 \t Batch 720 \t Training Loss: 46.97018390231662\n",
      "Epoch 46 \t Batch 740 \t Training Loss: 46.99624045603984\n",
      "Epoch 46 \t Batch 760 \t Training Loss: 46.95769882704082\n",
      "Epoch 46 \t Batch 780 \t Training Loss: 46.97135205391126\n",
      "Epoch 46 \t Batch 800 \t Training Loss: 46.965474700927736\n",
      "Epoch 46 \t Batch 820 \t Training Loss: 46.90648210106826\n",
      "Epoch 46 \t Batch 840 \t Training Loss: 46.94235883894421\n",
      "Epoch 46 \t Batch 860 \t Training Loss: 46.95185837856559\n",
      "Epoch 46 \t Batch 880 \t Training Loss: 46.98538163358515\n",
      "Epoch 46 \t Batch 900 \t Training Loss: 46.99017150031196\n",
      "Epoch 46 \t Batch 20 \t Validation Loss: 36.64731764793396\n",
      "Epoch 46 \t Batch 40 \t Validation Loss: 32.857139229774475\n",
      "Epoch 46 \t Batch 60 \t Validation Loss: 35.88455066680908\n",
      "Epoch 46 \t Batch 80 \t Validation Loss: 34.70933686494827\n",
      "Epoch 46 \t Batch 100 \t Validation Loss: 33.3436599445343\n",
      "Epoch 46 \t Batch 120 \t Validation Loss: 32.77495205402374\n",
      "Epoch 46 \t Batch 140 \t Validation Loss: 32.118817349842615\n",
      "Epoch 46 \t Batch 160 \t Validation Loss: 33.40987830758095\n",
      "Epoch 46 \t Batch 180 \t Validation Loss: 36.44218826293945\n",
      "Epoch 46 \t Batch 200 \t Validation Loss: 37.56455818176269\n",
      "Epoch 46 \t Batch 220 \t Validation Loss: 38.666917168010364\n",
      "Epoch 46 \t Batch 240 \t Validation Loss: 38.808202687899275\n",
      "Epoch 46 \t Batch 260 \t Validation Loss: 40.759514401509215\n",
      "Epoch 46 \t Batch 280 \t Validation Loss: 41.73042621612549\n",
      "Epoch 46 \t Batch 300 \t Validation Loss: 42.56617135365804\n",
      "Epoch 46 \t Batch 320 \t Validation Loss: 42.84885622262955\n",
      "Epoch 46 \t Batch 340 \t Validation Loss: 42.62479468513938\n",
      "Epoch 46 \t Batch 360 \t Validation Loss: 42.33683557510376\n",
      "Epoch 46 \t Batch 380 \t Validation Loss: 42.486761986581904\n",
      "Epoch 46 \t Batch 400 \t Validation Loss: 41.95741054534912\n",
      "Epoch 46 \t Batch 420 \t Validation Loss: 41.87310990833101\n",
      "Epoch 46 \t Batch 440 \t Validation Loss: 41.50678880864923\n",
      "Epoch 46 \t Batch 460 \t Validation Loss: 41.61123891498732\n",
      "Epoch 46 \t Batch 480 \t Validation Loss: 41.96362242301305\n",
      "Epoch 46 \t Batch 500 \t Validation Loss: 41.5882939414978\n",
      "Epoch 46 \t Batch 520 \t Validation Loss: 41.29445982162769\n",
      "Epoch 46 \t Batch 540 \t Validation Loss: 40.924454309322215\n",
      "Epoch 46 \t Batch 560 \t Validation Loss: 40.63377054589135\n",
      "Epoch 46 \t Batch 580 \t Validation Loss: 40.36695155604132\n",
      "Epoch 46 \t Batch 600 \t Validation Loss: 40.47642580509186\n",
      "Epoch 46 Training Loss: 46.985642947963434 Validation Loss: 41.051891504944145\n",
      "Epoch 46 completed\n",
      "Epoch 47 \t Batch 20 \t Training Loss: 45.79248943328857\n",
      "Epoch 47 \t Batch 40 \t Training Loss: 46.89760246276855\n",
      "Epoch 47 \t Batch 60 \t Training Loss: 46.63430334726969\n",
      "Epoch 47 \t Batch 80 \t Training Loss: 46.41264090538025\n",
      "Epoch 47 \t Batch 100 \t Training Loss: 46.49248622894287\n",
      "Epoch 47 \t Batch 120 \t Training Loss: 46.57098671595256\n",
      "Epoch 47 \t Batch 140 \t Training Loss: 46.54779870169504\n",
      "Epoch 47 \t Batch 160 \t Training Loss: 46.35008010864258\n",
      "Epoch 47 \t Batch 180 \t Training Loss: 46.305828454759386\n",
      "Epoch 47 \t Batch 200 \t Training Loss: 46.387665252685544\n",
      "Epoch 47 \t Batch 220 \t Training Loss: 46.485517484491524\n",
      "Epoch 47 \t Batch 240 \t Training Loss: 46.44148850440979\n",
      "Epoch 47 \t Batch 260 \t Training Loss: 46.60912234966572\n",
      "Epoch 47 \t Batch 280 \t Training Loss: 46.50544082096645\n",
      "Epoch 47 \t Batch 300 \t Training Loss: 46.549915618896485\n",
      "Epoch 47 \t Batch 320 \t Training Loss: 46.65661185979843\n",
      "Epoch 47 \t Batch 340 \t Training Loss: 46.61782347735237\n",
      "Epoch 47 \t Batch 360 \t Training Loss: 46.57770751317342\n",
      "Epoch 47 \t Batch 380 \t Training Loss: 46.52103540520919\n",
      "Epoch 47 \t Batch 400 \t Training Loss: 46.53596049308777\n",
      "Epoch 47 \t Batch 420 \t Training Loss: 46.65959207443964\n",
      "Epoch 47 \t Batch 440 \t Training Loss: 46.69591486670754\n",
      "Epoch 47 \t Batch 460 \t Training Loss: 46.735755298448645\n",
      "Epoch 47 \t Batch 480 \t Training Loss: 46.706470878918964\n",
      "Epoch 47 \t Batch 500 \t Training Loss: 46.67878788757324\n",
      "Epoch 47 \t Batch 520 \t Training Loss: 46.64334485714252\n",
      "Epoch 47 \t Batch 540 \t Training Loss: 46.67063884735107\n",
      "Epoch 47 \t Batch 560 \t Training Loss: 46.73280826296125\n",
      "Epoch 47 \t Batch 580 \t Training Loss: 46.7404300558156\n",
      "Epoch 47 \t Batch 600 \t Training Loss: 46.78217113494873\n",
      "Epoch 47 \t Batch 620 \t Training Loss: 46.76995024527273\n",
      "Epoch 47 \t Batch 640 \t Training Loss: 46.78089296221733\n",
      "Epoch 47 \t Batch 660 \t Training Loss: 46.78419819456158\n",
      "Epoch 47 \t Batch 680 \t Training Loss: 46.79938986161176\n",
      "Epoch 47 \t Batch 700 \t Training Loss: 46.83430656978062\n",
      "Epoch 47 \t Batch 720 \t Training Loss: 46.834710110558404\n",
      "Epoch 47 \t Batch 740 \t Training Loss: 46.87163455550735\n",
      "Epoch 47 \t Batch 760 \t Training Loss: 46.904094339671886\n",
      "Epoch 47 \t Batch 780 \t Training Loss: 46.91720379071358\n",
      "Epoch 47 \t Batch 800 \t Training Loss: 46.888381342887875\n",
      "Epoch 47 \t Batch 820 \t Training Loss: 46.90190485977545\n",
      "Epoch 47 \t Batch 840 \t Training Loss: 46.91957946050735\n",
      "Epoch 47 \t Batch 860 \t Training Loss: 46.95272702505422\n",
      "Epoch 47 \t Batch 880 \t Training Loss: 46.96344971656799\n",
      "Epoch 47 \t Batch 900 \t Training Loss: 46.978538326687286\n",
      "Epoch 47 \t Batch 20 \t Validation Loss: 34.35508222579956\n",
      "Epoch 47 \t Batch 40 \t Validation Loss: 30.60908341407776\n",
      "Epoch 47 \t Batch 60 \t Validation Loss: 33.521613661448164\n",
      "Epoch 47 \t Batch 80 \t Validation Loss: 32.47636799216271\n",
      "Epoch 47 \t Batch 100 \t Validation Loss: 31.343085341453552\n",
      "Epoch 47 \t Batch 120 \t Validation Loss: 31.048724551995594\n",
      "Epoch 47 \t Batch 140 \t Validation Loss: 30.5702800989151\n",
      "Epoch 47 \t Batch 160 \t Validation Loss: 31.90637476146221\n",
      "Epoch 47 \t Batch 180 \t Validation Loss: 34.78696883254581\n",
      "Epoch 47 \t Batch 200 \t Validation Loss: 35.83155367612839\n",
      "Epoch 47 \t Batch 220 \t Validation Loss: 36.820913043889135\n",
      "Epoch 47 \t Batch 240 \t Validation Loss: 36.93155053257942\n",
      "Epoch 47 \t Batch 260 \t Validation Loss: 38.83342214180873\n",
      "Epoch 47 \t Batch 280 \t Validation Loss: 39.77234998600824\n",
      "Epoch 47 \t Batch 300 \t Validation Loss: 40.58571647802989\n",
      "Epoch 47 \t Batch 320 \t Validation Loss: 40.85280351191759\n",
      "Epoch 47 \t Batch 340 \t Validation Loss: 40.67115511192995\n",
      "Epoch 47 \t Batch 360 \t Validation Loss: 40.412934850321875\n",
      "Epoch 47 \t Batch 380 \t Validation Loss: 40.55579253874327\n",
      "Epoch 47 \t Batch 400 \t Validation Loss: 40.049967461824416\n",
      "Epoch 47 \t Batch 420 \t Validation Loss: 39.98963653814225\n",
      "Epoch 47 \t Batch 440 \t Validation Loss: 39.6165167407556\n",
      "Epoch 47 \t Batch 460 \t Validation Loss: 39.74812724382981\n",
      "Epoch 47 \t Batch 480 \t Validation Loss: 40.147601547837255\n",
      "Epoch 47 \t Batch 500 \t Validation Loss: 39.81373751354217\n",
      "Epoch 47 \t Batch 520 \t Validation Loss: 39.48205065268736\n",
      "Epoch 47 \t Batch 540 \t Validation Loss: 39.146194379417985\n",
      "Epoch 47 \t Batch 560 \t Validation Loss: 38.84033351881163\n",
      "Epoch 47 \t Batch 580 \t Validation Loss: 38.463208536444036\n",
      "Epoch 47 \t Batch 600 \t Validation Loss: 38.62251401185989\n",
      "Epoch 47 Training Loss: 46.95826689575595 Validation Loss: 39.1862767663869\n",
      "Epoch 47 completed\n",
      "Epoch 48 \t Batch 20 \t Training Loss: 46.66794700622559\n",
      "Epoch 48 \t Batch 40 \t Training Loss: 46.73292427062988\n",
      "Epoch 48 \t Batch 60 \t Training Loss: 46.843800671895345\n",
      "Epoch 48 \t Batch 80 \t Training Loss: 47.25318975448609\n",
      "Epoch 48 \t Batch 100 \t Training Loss: 46.9523397064209\n",
      "Epoch 48 \t Batch 120 \t Training Loss: 46.91275698343913\n",
      "Epoch 48 \t Batch 140 \t Training Loss: 46.79688143048968\n",
      "Epoch 48 \t Batch 160 \t Training Loss: 46.74865720272064\n",
      "Epoch 48 \t Batch 180 \t Training Loss: 46.60431685977512\n",
      "Epoch 48 \t Batch 200 \t Training Loss: 46.71237892150879\n",
      "Epoch 48 \t Batch 220 \t Training Loss: 46.58353103290904\n",
      "Epoch 48 \t Batch 240 \t Training Loss: 46.540015920003256\n",
      "Epoch 48 \t Batch 260 \t Training Loss: 46.668515909635104\n",
      "Epoch 48 \t Batch 280 \t Training Loss: 46.60022318703788\n",
      "Epoch 48 \t Batch 300 \t Training Loss: 46.59803732554118\n",
      "Epoch 48 \t Batch 320 \t Training Loss: 46.62677417993545\n",
      "Epoch 48 \t Batch 340 \t Training Loss: 46.695409079159006\n",
      "Epoch 48 \t Batch 360 \t Training Loss: 46.65021592246161\n",
      "Epoch 48 \t Batch 380 \t Training Loss: 46.66917059045089\n",
      "Epoch 48 \t Batch 400 \t Training Loss: 46.711409425735475\n",
      "Epoch 48 \t Batch 420 \t Training Loss: 46.67829284667969\n",
      "Epoch 48 \t Batch 440 \t Training Loss: 46.74073480259288\n",
      "Epoch 48 \t Batch 460 \t Training Loss: 46.72706590735394\n",
      "Epoch 48 \t Batch 480 \t Training Loss: 46.72273655732473\n",
      "Epoch 48 \t Batch 500 \t Training Loss: 46.68086402130127\n",
      "Epoch 48 \t Batch 520 \t Training Loss: 46.72688039632944\n",
      "Epoch 48 \t Batch 540 \t Training Loss: 46.76522298035798\n",
      "Epoch 48 \t Batch 560 \t Training Loss: 46.74725698062352\n",
      "Epoch 48 \t Batch 580 \t Training Loss: 46.75604664375042\n",
      "Epoch 48 \t Batch 600 \t Training Loss: 46.76408062616984\n",
      "Epoch 48 \t Batch 620 \t Training Loss: 46.78030256455944\n",
      "Epoch 48 \t Batch 640 \t Training Loss: 46.77219192385674\n",
      "Epoch 48 \t Batch 660 \t Training Loss: 46.75392287861217\n",
      "Epoch 48 \t Batch 680 \t Training Loss: 46.724537361369414\n",
      "Epoch 48 \t Batch 700 \t Training Loss: 46.76598778315953\n",
      "Epoch 48 \t Batch 720 \t Training Loss: 46.763948832617864\n",
      "Epoch 48 \t Batch 740 \t Training Loss: 46.81323778307116\n",
      "Epoch 48 \t Batch 760 \t Training Loss: 46.830665528146845\n",
      "Epoch 48 \t Batch 780 \t Training Loss: 46.87317717136481\n",
      "Epoch 48 \t Batch 800 \t Training Loss: 46.89446420669556\n",
      "Epoch 48 \t Batch 820 \t Training Loss: 46.894349070293146\n",
      "Epoch 48 \t Batch 840 \t Training Loss: 46.929213246845066\n",
      "Epoch 48 \t Batch 860 \t Training Loss: 46.906401847129644\n",
      "Epoch 48 \t Batch 880 \t Training Loss: 46.92748821865428\n",
      "Epoch 48 \t Batch 900 \t Training Loss: 46.919890615675186\n",
      "Epoch 48 \t Batch 20 \t Validation Loss: 32.90079140663147\n",
      "Epoch 48 \t Batch 40 \t Validation Loss: 30.399879276752472\n",
      "Epoch 48 \t Batch 60 \t Validation Loss: 33.04049344857534\n",
      "Epoch 48 \t Batch 80 \t Validation Loss: 32.01014044880867\n",
      "Epoch 48 \t Batch 100 \t Validation Loss: 31.080950989723206\n",
      "Epoch 48 \t Batch 120 \t Validation Loss: 30.772134673595428\n",
      "Epoch 48 \t Batch 140 \t Validation Loss: 30.35561901160649\n",
      "Epoch 48 \t Batch 160 \t Validation Loss: 31.945262303948404\n",
      "Epoch 48 \t Batch 180 \t Validation Loss: 35.313709463013545\n",
      "Epoch 48 \t Batch 200 \t Validation Loss: 36.63690905809403\n",
      "Epoch 48 \t Batch 220 \t Validation Loss: 37.83384521007538\n",
      "Epoch 48 \t Batch 240 \t Validation Loss: 38.10368466973305\n",
      "Epoch 48 \t Batch 260 \t Validation Loss: 40.15264657277327\n",
      "Epoch 48 \t Batch 280 \t Validation Loss: 41.16021489245551\n",
      "Epoch 48 \t Batch 300 \t Validation Loss: 42.16980176766713\n",
      "Epoch 48 \t Batch 320 \t Validation Loss: 42.531012861430646\n",
      "Epoch 48 \t Batch 340 \t Validation Loss: 42.3622869533651\n",
      "Epoch 48 \t Batch 360 \t Validation Loss: 42.139581573009494\n",
      "Epoch 48 \t Batch 380 \t Validation Loss: 42.287182922112315\n",
      "Epoch 48 \t Batch 400 \t Validation Loss: 41.76368278861046\n",
      "Epoch 48 \t Batch 420 \t Validation Loss: 41.68600166298094\n",
      "Epoch 48 \t Batch 440 \t Validation Loss: 41.28456874218854\n",
      "Epoch 48 \t Batch 460 \t Validation Loss: 41.39837412108546\n",
      "Epoch 48 \t Batch 480 \t Validation Loss: 41.809118810296056\n",
      "Epoch 48 \t Batch 500 \t Validation Loss: 41.4810849943161\n",
      "Epoch 48 \t Batch 520 \t Validation Loss: 41.15570753445992\n",
      "Epoch 48 \t Batch 540 \t Validation Loss: 40.78098196365215\n",
      "Epoch 48 \t Batch 560 \t Validation Loss: 40.44710607443537\n",
      "Epoch 48 \t Batch 580 \t Validation Loss: 40.123574081782635\n",
      "Epoch 48 \t Batch 600 \t Validation Loss: 40.2218513862292\n",
      "Epoch 48 Training Loss: 46.940438685526374 Validation Loss: 40.76730835515183\n",
      "Epoch 48 completed\n",
      "Epoch 49 \t Batch 20 \t Training Loss: 45.77004413604736\n",
      "Epoch 49 \t Batch 40 \t Training Loss: 45.79628458023071\n",
      "Epoch 49 \t Batch 60 \t Training Loss: 46.13377145131429\n",
      "Epoch 49 \t Batch 80 \t Training Loss: 46.35363416671753\n",
      "Epoch 49 \t Batch 100 \t Training Loss: 46.41012462615967\n",
      "Epoch 49 \t Batch 120 \t Training Loss: 46.29583028157552\n",
      "Epoch 49 \t Batch 140 \t Training Loss: 46.35192497798375\n",
      "Epoch 49 \t Batch 160 \t Training Loss: 46.45024304389953\n",
      "Epoch 49 \t Batch 180 \t Training Loss: 46.3723464541965\n",
      "Epoch 49 \t Batch 200 \t Training Loss: 46.33089874267578\n",
      "Epoch 49 \t Batch 220 \t Training Loss: 46.30316873030229\n",
      "Epoch 49 \t Batch 240 \t Training Loss: 46.38629962603251\n",
      "Epoch 49 \t Batch 260 \t Training Loss: 46.485120729299695\n",
      "Epoch 49 \t Batch 280 \t Training Loss: 46.48755534035819\n",
      "Epoch 49 \t Batch 300 \t Training Loss: 46.600434697469076\n",
      "Epoch 49 \t Batch 320 \t Training Loss: 46.62378506660461\n",
      "Epoch 49 \t Batch 340 \t Training Loss: 46.74348826688879\n",
      "Epoch 49 \t Batch 360 \t Training Loss: 46.746400345696344\n",
      "Epoch 49 \t Batch 380 \t Training Loss: 46.71801179584704\n",
      "Epoch 49 \t Batch 400 \t Training Loss: 46.65835399627686\n",
      "Epoch 49 \t Batch 420 \t Training Loss: 46.67580877031599\n",
      "Epoch 49 \t Batch 440 \t Training Loss: 46.61327740929344\n",
      "Epoch 49 \t Batch 460 \t Training Loss: 46.60759561787481\n",
      "Epoch 49 \t Batch 480 \t Training Loss: 46.65845574537913\n",
      "Epoch 49 \t Batch 500 \t Training Loss: 46.664850555419925\n",
      "Epoch 49 \t Batch 520 \t Training Loss: 46.71400489807129\n",
      "Epoch 49 \t Batch 540 \t Training Loss: 46.757643424140085\n",
      "Epoch 49 \t Batch 560 \t Training Loss: 46.82251203400748\n",
      "Epoch 49 \t Batch 580 \t Training Loss: 46.885244533933445\n",
      "Epoch 49 \t Batch 600 \t Training Loss: 46.88595335006714\n",
      "Epoch 49 \t Batch 620 \t Training Loss: 46.858531785780386\n",
      "Epoch 49 \t Batch 640 \t Training Loss: 46.8258136510849\n",
      "Epoch 49 \t Batch 660 \t Training Loss: 46.853837984258476\n",
      "Epoch 49 \t Batch 680 \t Training Loss: 46.856414222717284\n",
      "Epoch 49 \t Batch 700 \t Training Loss: 46.82207786015102\n",
      "Epoch 49 \t Batch 720 \t Training Loss: 46.82595896191067\n",
      "Epoch 49 \t Batch 740 \t Training Loss: 46.83799157013764\n",
      "Epoch 49 \t Batch 760 \t Training Loss: 46.86333167427465\n",
      "Epoch 49 \t Batch 780 \t Training Loss: 46.92172399667593\n",
      "Epoch 49 \t Batch 800 \t Training Loss: 46.91846696853638\n",
      "Epoch 49 \t Batch 820 \t Training Loss: 46.90314245456602\n",
      "Epoch 49 \t Batch 840 \t Training Loss: 46.91481120699928\n",
      "Epoch 49 \t Batch 860 \t Training Loss: 46.92594061119612\n",
      "Epoch 49 \t Batch 880 \t Training Loss: 46.896401370655404\n",
      "Epoch 49 \t Batch 900 \t Training Loss: 46.89544631110297\n",
      "Epoch 49 \t Batch 20 \t Validation Loss: 46.94971742630005\n",
      "Epoch 49 \t Batch 40 \t Validation Loss: 41.02856318950653\n",
      "Epoch 49 \t Batch 60 \t Validation Loss: 44.840181573232016\n",
      "Epoch 49 \t Batch 80 \t Validation Loss: 42.945350205898286\n",
      "Epoch 49 \t Batch 100 \t Validation Loss: 39.86889603614807\n",
      "Epoch 49 \t Batch 120 \t Validation Loss: 38.20355635484059\n",
      "Epoch 49 \t Batch 140 \t Validation Loss: 36.74072394371033\n",
      "Epoch 49 \t Batch 160 \t Validation Loss: 37.39358589053154\n",
      "Epoch 49 \t Batch 180 \t Validation Loss: 39.900530348883734\n",
      "Epoch 49 \t Batch 200 \t Validation Loss: 40.528863487243655\n",
      "Epoch 49 \t Batch 220 \t Validation Loss: 41.25264305634932\n",
      "Epoch 49 \t Batch 240 \t Validation Loss: 41.111706189314525\n",
      "Epoch 49 \t Batch 260 \t Validation Loss: 42.80016989341149\n",
      "Epoch 49 \t Batch 280 \t Validation Loss: 43.53868736880166\n",
      "Epoch 49 \t Batch 300 \t Validation Loss: 44.2175356388092\n",
      "Epoch 49 \t Batch 320 \t Validation Loss: 44.352986910939215\n",
      "Epoch 49 \t Batch 340 \t Validation Loss: 43.992618367251225\n",
      "Epoch 49 \t Batch 360 \t Validation Loss: 43.609681741396585\n",
      "Epoch 49 \t Batch 380 \t Validation Loss: 43.64023524083589\n",
      "Epoch 49 \t Batch 400 \t Validation Loss: 43.000433123111726\n",
      "Epoch 49 \t Batch 420 \t Validation Loss: 42.831876625333514\n",
      "Epoch 49 \t Batch 440 \t Validation Loss: 42.34772712750868\n",
      "Epoch 49 \t Batch 460 \t Validation Loss: 42.37912060488825\n",
      "Epoch 49 \t Batch 480 \t Validation Loss: 42.689383906126025\n",
      "Epoch 49 \t Batch 500 \t Validation Loss: 42.27519198417664\n",
      "Epoch 49 \t Batch 520 \t Validation Loss: 41.910077757101796\n",
      "Epoch 49 \t Batch 540 \t Validation Loss: 41.52117403348287\n",
      "Epoch 49 \t Batch 560 \t Validation Loss: 41.22983954463686\n",
      "Epoch 49 \t Batch 580 \t Validation Loss: 40.9908537190536\n",
      "Epoch 49 \t Batch 600 \t Validation Loss: 41.09469031174978\n",
      "Epoch 49 Training Loss: 46.89548973324775 Validation Loss: 41.653274023687686\n",
      "Epoch 49 completed\n",
      "Epoch 50 \t Batch 20 \t Training Loss: 45.44220657348633\n",
      "Epoch 50 \t Batch 40 \t Training Loss: 46.31945714950562\n",
      "Epoch 50 \t Batch 60 \t Training Loss: 47.068590545654295\n",
      "Epoch 50 \t Batch 80 \t Training Loss: 47.29754905700683\n",
      "Epoch 50 \t Batch 100 \t Training Loss: 47.1255260848999\n",
      "Epoch 50 \t Batch 120 \t Training Loss: 47.08976834615071\n",
      "Epoch 50 \t Batch 140 \t Training Loss: 47.19545732225691\n",
      "Epoch 50 \t Batch 160 \t Training Loss: 47.27121660709381\n",
      "Epoch 50 \t Batch 180 \t Training Loss: 47.05543535020616\n",
      "Epoch 50 \t Batch 200 \t Training Loss: 47.22572259902954\n",
      "Epoch 50 \t Batch 220 \t Training Loss: 47.18799885836515\n",
      "Epoch 50 \t Batch 240 \t Training Loss: 47.21175376574198\n",
      "Epoch 50 \t Batch 260 \t Training Loss: 47.10866043384259\n",
      "Epoch 50 \t Batch 280 \t Training Loss: 47.13488488878522\n",
      "Epoch 50 \t Batch 300 \t Training Loss: 47.210438893636066\n",
      "Epoch 50 \t Batch 320 \t Training Loss: 47.2223837852478\n",
      "Epoch 50 \t Batch 340 \t Training Loss: 47.25050350637997\n",
      "Epoch 50 \t Batch 360 \t Training Loss: 47.15268740124173\n",
      "Epoch 50 \t Batch 380 \t Training Loss: 47.130612222771894\n",
      "Epoch 50 \t Batch 400 \t Training Loss: 47.13075572013855\n",
      "Epoch 50 \t Batch 420 \t Training Loss: 47.106344804309664\n",
      "Epoch 50 \t Batch 440 \t Training Loss: 47.16119503541426\n",
      "Epoch 50 \t Batch 460 \t Training Loss: 47.174374787703805\n",
      "Epoch 50 \t Batch 480 \t Training Loss: 47.17078134218852\n",
      "Epoch 50 \t Batch 500 \t Training Loss: 47.12990726470947\n",
      "Epoch 50 \t Batch 520 \t Training Loss: 47.10336531125582\n",
      "Epoch 50 \t Batch 540 \t Training Loss: 47.101792625144675\n",
      "Epoch 50 \t Batch 560 \t Training Loss: 47.07896706036159\n",
      "Epoch 50 \t Batch 580 \t Training Loss: 47.091707847858295\n",
      "Epoch 50 \t Batch 600 \t Training Loss: 47.079840666453045\n",
      "Epoch 50 \t Batch 620 \t Training Loss: 47.05177803654824\n",
      "Epoch 50 \t Batch 640 \t Training Loss: 47.05437548756599\n",
      "Epoch 50 \t Batch 660 \t Training Loss: 47.103804495840365\n",
      "Epoch 50 \t Batch 680 \t Training Loss: 47.10179477018468\n",
      "Epoch 50 \t Batch 700 \t Training Loss: 47.11051374707903\n",
      "Epoch 50 \t Batch 720 \t Training Loss: 47.1136678536733\n",
      "Epoch 50 \t Batch 740 \t Training Loss: 47.088744555292905\n",
      "Epoch 50 \t Batch 760 \t Training Loss: 47.08432803906892\n",
      "Epoch 50 \t Batch 780 \t Training Loss: 47.02846883138021\n",
      "Epoch 50 \t Batch 800 \t Training Loss: 47.0258899641037\n",
      "Epoch 50 \t Batch 820 \t Training Loss: 47.01445174798733\n",
      "Epoch 50 \t Batch 840 \t Training Loss: 47.02004289627075\n",
      "Epoch 50 \t Batch 860 \t Training Loss: 46.98799497471299\n",
      "Epoch 50 \t Batch 880 \t Training Loss: 46.95796659209511\n",
      "Epoch 50 \t Batch 900 \t Training Loss: 46.90719554901123\n",
      "Epoch 50 \t Batch 20 \t Validation Loss: 39.778604459762576\n",
      "Epoch 50 \t Batch 40 \t Validation Loss: 34.98247480392456\n",
      "Epoch 50 \t Batch 60 \t Validation Loss: 38.66603318850199\n",
      "Epoch 50 \t Batch 80 \t Validation Loss: 37.04177978634834\n",
      "Epoch 50 \t Batch 100 \t Validation Loss: 34.97761858463287\n",
      "Epoch 50 \t Batch 120 \t Validation Loss: 34.03382029930751\n",
      "Epoch 50 \t Batch 140 \t Validation Loss: 33.126321046692986\n",
      "Epoch 50 \t Batch 160 \t Validation Loss: 34.25150220692158\n",
      "Epoch 50 \t Batch 180 \t Validation Loss: 37.217760353618196\n",
      "Epoch 50 \t Batch 200 \t Validation Loss: 38.22087478876114\n",
      "Epoch 50 \t Batch 220 \t Validation Loss: 39.218639514663\n",
      "Epoch 50 \t Batch 240 \t Validation Loss: 39.304723209142686\n",
      "Epoch 50 \t Batch 260 \t Validation Loss: 41.2051223883262\n",
      "Epoch 50 \t Batch 280 \t Validation Loss: 42.0912050809179\n",
      "Epoch 50 \t Batch 300 \t Validation Loss: 42.89773997147878\n",
      "Epoch 50 \t Batch 320 \t Validation Loss: 43.15423097759485\n",
      "Epoch 50 \t Batch 340 \t Validation Loss: 42.907518994106965\n",
      "Epoch 50 \t Batch 360 \t Validation Loss: 42.5936088124911\n",
      "Epoch 50 \t Batch 380 \t Validation Loss: 42.67357287030471\n",
      "Epoch 50 \t Batch 400 \t Validation Loss: 42.082084974050524\n",
      "Epoch 50 \t Batch 420 \t Validation Loss: 41.94444844268617\n",
      "Epoch 50 \t Batch 440 \t Validation Loss: 41.523848901011725\n",
      "Epoch 50 \t Batch 460 \t Validation Loss: 41.59043361311374\n",
      "Epoch 50 \t Batch 480 \t Validation Loss: 41.93860427836577\n",
      "Epoch 50 \t Batch 500 \t Validation Loss: 41.55439271831512\n",
      "Epoch 50 \t Batch 520 \t Validation Loss: 41.19818297991386\n",
      "Epoch 50 \t Batch 540 \t Validation Loss: 40.8197625345654\n",
      "Epoch 50 \t Batch 560 \t Validation Loss: 40.48017939925194\n",
      "Epoch 50 \t Batch 580 \t Validation Loss: 40.10618980753011\n",
      "Epoch 50 \t Batch 600 \t Validation Loss: 40.2216828195254\n",
      "Epoch 50 Training Loss: 46.89964429632788 Validation Loss: 40.77642418889256\n",
      "Epoch 50 completed\n",
      "Epoch 51 \t Batch 20 \t Training Loss: 47.22006416320801\n",
      "Epoch 51 \t Batch 40 \t Training Loss: 47.250460147857666\n",
      "Epoch 51 \t Batch 60 \t Training Loss: 46.805715815226236\n",
      "Epoch 51 \t Batch 80 \t Training Loss: 46.72858157157898\n",
      "Epoch 51 \t Batch 100 \t Training Loss: 46.84692459106445\n",
      "Epoch 51 \t Batch 120 \t Training Loss: 46.844372208913164\n",
      "Epoch 51 \t Batch 140 \t Training Loss: 46.862014634268625\n",
      "Epoch 51 \t Batch 160 \t Training Loss: 46.84254794120788\n",
      "Epoch 51 \t Batch 180 \t Training Loss: 46.658726395501034\n",
      "Epoch 51 \t Batch 200 \t Training Loss: 46.56478656768799\n",
      "Epoch 51 \t Batch 220 \t Training Loss: 46.681653040105644\n",
      "Epoch 51 \t Batch 240 \t Training Loss: 46.619100379943845\n",
      "Epoch 51 \t Batch 260 \t Training Loss: 46.62927055358887\n",
      "Epoch 51 \t Batch 280 \t Training Loss: 46.573793928963795\n",
      "Epoch 51 \t Batch 300 \t Training Loss: 46.64883753458659\n",
      "Epoch 51 \t Batch 320 \t Training Loss: 46.69810940027237\n",
      "Epoch 51 \t Batch 340 \t Training Loss: 46.72804665284998\n",
      "Epoch 51 \t Batch 360 \t Training Loss: 46.74729569753011\n",
      "Epoch 51 \t Batch 380 \t Training Loss: 46.792341151990385\n",
      "Epoch 51 \t Batch 400 \t Training Loss: 46.76555603027344\n",
      "Epoch 51 \t Batch 420 \t Training Loss: 46.7790924344744\n",
      "Epoch 51 \t Batch 440 \t Training Loss: 46.75353042429144\n",
      "Epoch 51 \t Batch 460 \t Training Loss: 46.77726472771686\n",
      "Epoch 51 \t Batch 480 \t Training Loss: 46.84081288178762\n",
      "Epoch 51 \t Batch 500 \t Training Loss: 46.898819580078126\n",
      "Epoch 51 \t Batch 520 \t Training Loss: 46.92934008378249\n",
      "Epoch 51 \t Batch 540 \t Training Loss: 46.907061661614314\n",
      "Epoch 51 \t Batch 560 \t Training Loss: 46.845576313563754\n",
      "Epoch 51 \t Batch 580 \t Training Loss: 46.810539705999965\n",
      "Epoch 51 \t Batch 600 \t Training Loss: 46.840166371663415\n",
      "Epoch 51 \t Batch 620 \t Training Loss: 46.83169109590592\n",
      "Epoch 51 \t Batch 640 \t Training Loss: 46.82327606081962\n",
      "Epoch 51 \t Batch 660 \t Training Loss: 46.814062216787626\n",
      "Epoch 51 \t Batch 680 \t Training Loss: 46.83505044824937\n",
      "Epoch 51 \t Batch 700 \t Training Loss: 46.854244428362165\n",
      "Epoch 51 \t Batch 720 \t Training Loss: 46.82339220046997\n",
      "Epoch 51 \t Batch 740 \t Training Loss: 46.84267105411839\n",
      "Epoch 51 \t Batch 760 \t Training Loss: 46.84181724347566\n",
      "Epoch 51 \t Batch 780 \t Training Loss: 46.871573360149675\n",
      "Epoch 51 \t Batch 800 \t Training Loss: 46.8702773809433\n",
      "Epoch 51 \t Batch 820 \t Training Loss: 46.85008802646544\n",
      "Epoch 51 \t Batch 840 \t Training Loss: 46.82852140154157\n",
      "Epoch 51 \t Batch 860 \t Training Loss: 46.83549077677172\n",
      "Epoch 51 \t Batch 880 \t Training Loss: 46.863427730040115\n",
      "Epoch 51 \t Batch 900 \t Training Loss: 46.8684710099962\n",
      "Epoch 51 \t Batch 20 \t Validation Loss: 39.408224487304686\n",
      "Epoch 51 \t Batch 40 \t Validation Loss: 34.56950845718384\n",
      "Epoch 51 \t Batch 60 \t Validation Loss: 38.08583092689514\n",
      "Epoch 51 \t Batch 80 \t Validation Loss: 36.68695383071899\n",
      "Epoch 51 \t Batch 100 \t Validation Loss: 35.13339567184448\n",
      "Epoch 51 \t Batch 120 \t Validation Loss: 34.505224720637\n",
      "Epoch 51 \t Batch 140 \t Validation Loss: 33.691341577257425\n",
      "Epoch 51 \t Batch 160 \t Validation Loss: 34.71650640964508\n",
      "Epoch 51 \t Batch 180 \t Validation Loss: 37.63839462068346\n",
      "Epoch 51 \t Batch 200 \t Validation Loss: 38.55237865447998\n",
      "Epoch 51 \t Batch 220 \t Validation Loss: 39.46606052572077\n",
      "Epoch 51 \t Batch 240 \t Validation Loss: 39.49887600739797\n",
      "Epoch 51 \t Batch 260 \t Validation Loss: 41.346630708987895\n",
      "Epoch 51 \t Batch 280 \t Validation Loss: 42.2045398235321\n",
      "Epoch 51 \t Batch 300 \t Validation Loss: 43.01417032877604\n",
      "Epoch 51 \t Batch 320 \t Validation Loss: 43.25674766898155\n",
      "Epoch 51 \t Batch 340 \t Validation Loss: 42.99383077060475\n",
      "Epoch 51 \t Batch 360 \t Validation Loss: 42.65154065026177\n",
      "Epoch 51 \t Batch 380 \t Validation Loss: 42.72130051161113\n",
      "Epoch 51 \t Batch 400 \t Validation Loss: 42.13398706436157\n",
      "Epoch 51 \t Batch 420 \t Validation Loss: 41.99587216831389\n",
      "Epoch 51 \t Batch 440 \t Validation Loss: 41.5700321522626\n",
      "Epoch 51 \t Batch 460 \t Validation Loss: 41.628724286867225\n",
      "Epoch 51 \t Batch 480 \t Validation Loss: 41.965958923101425\n",
      "Epoch 51 \t Batch 500 \t Validation Loss: 41.576241563797\n",
      "Epoch 51 \t Batch 520 \t Validation Loss: 41.19380997327658\n",
      "Epoch 51 \t Batch 540 \t Validation Loss: 40.80894640286763\n",
      "Epoch 51 \t Batch 560 \t Validation Loss: 40.46329127209527\n",
      "Epoch 51 \t Batch 580 \t Validation Loss: 40.063748085087745\n",
      "Epoch 51 \t Batch 600 \t Validation Loss: 40.187173857688904\n",
      "Epoch 51 Training Loss: 46.88401546499071 Validation Loss: 40.730949607762426\n",
      "Epoch 51 completed\n",
      "Epoch 52 \t Batch 20 \t Training Loss: 48.67042198181152\n",
      "Epoch 52 \t Batch 40 \t Training Loss: 47.449896812438965\n",
      "Epoch 52 \t Batch 60 \t Training Loss: 47.47621148427327\n",
      "Epoch 52 \t Batch 80 \t Training Loss: 47.765770292282106\n",
      "Epoch 52 \t Batch 100 \t Training Loss: 47.53472743988037\n",
      "Epoch 52 \t Batch 120 \t Training Loss: 47.41005048751831\n",
      "Epoch 52 \t Batch 140 \t Training Loss: 47.21720311301095\n",
      "Epoch 52 \t Batch 160 \t Training Loss: 47.30021140575409\n",
      "Epoch 52 \t Batch 180 \t Training Loss: 47.20370368957519\n",
      "Epoch 52 \t Batch 200 \t Training Loss: 47.27358215332031\n",
      "Epoch 52 \t Batch 220 \t Training Loss: 47.19930360967463\n",
      "Epoch 52 \t Batch 240 \t Training Loss: 47.10128595034281\n",
      "Epoch 52 \t Batch 260 \t Training Loss: 47.17519663297213\n",
      "Epoch 52 \t Batch 280 \t Training Loss: 47.1136385508946\n",
      "Epoch 52 \t Batch 300 \t Training Loss: 47.043254369099934\n",
      "Epoch 52 \t Batch 320 \t Training Loss: 47.06523195505142\n",
      "Epoch 52 \t Batch 340 \t Training Loss: 47.03770022672765\n",
      "Epoch 52 \t Batch 360 \t Training Loss: 46.954346625010174\n",
      "Epoch 52 \t Batch 380 \t Training Loss: 46.95640437477513\n",
      "Epoch 52 \t Batch 400 \t Training Loss: 46.981848316192625\n",
      "Epoch 52 \t Batch 420 \t Training Loss: 46.95082576388405\n",
      "Epoch 52 \t Batch 440 \t Training Loss: 46.955944928255946\n",
      "Epoch 52 \t Batch 460 \t Training Loss: 46.95719051361084\n",
      "Epoch 52 \t Batch 480 \t Training Loss: 46.944547565778095\n",
      "Epoch 52 \t Batch 500 \t Training Loss: 46.93369666290283\n",
      "Epoch 52 \t Batch 520 \t Training Loss: 46.96890423848079\n",
      "Epoch 52 \t Batch 540 \t Training Loss: 46.97954593234592\n",
      "Epoch 52 \t Batch 560 \t Training Loss: 46.946902241025654\n",
      "Epoch 52 \t Batch 580 \t Training Loss: 46.9636050717584\n",
      "Epoch 52 \t Batch 600 \t Training Loss: 46.969001585642495\n",
      "Epoch 52 \t Batch 620 \t Training Loss: 46.97829632912913\n",
      "Epoch 52 \t Batch 640 \t Training Loss: 46.972160547971725\n",
      "Epoch 52 \t Batch 660 \t Training Loss: 47.007974023529975\n",
      "Epoch 52 \t Batch 680 \t Training Loss: 46.9667672774371\n",
      "Epoch 52 \t Batch 700 \t Training Loss: 46.96728029523577\n",
      "Epoch 52 \t Batch 720 \t Training Loss: 46.951646386252506\n",
      "Epoch 52 \t Batch 740 \t Training Loss: 46.938364858885066\n",
      "Epoch 52 \t Batch 760 \t Training Loss: 46.92466577228747\n",
      "Epoch 52 \t Batch 780 \t Training Loss: 46.891458345070866\n",
      "Epoch 52 \t Batch 800 \t Training Loss: 46.86912757396698\n",
      "Epoch 52 \t Batch 820 \t Training Loss: 46.89694737690251\n",
      "Epoch 52 \t Batch 840 \t Training Loss: 46.8715652238755\n",
      "Epoch 52 \t Batch 860 \t Training Loss: 46.85442776790885\n",
      "Epoch 52 \t Batch 880 \t Training Loss: 46.87520891102878\n",
      "Epoch 52 \t Batch 900 \t Training Loss: 46.85722534179688\n",
      "Epoch 52 \t Batch 20 \t Validation Loss: 42.782825803756715\n",
      "Epoch 52 \t Batch 40 \t Validation Loss: 37.87745523452759\n",
      "Epoch 52 \t Batch 60 \t Validation Loss: 41.2474130153656\n",
      "Epoch 52 \t Batch 80 \t Validation Loss: 39.7072958946228\n",
      "Epoch 52 \t Batch 100 \t Validation Loss: 37.79504581451416\n",
      "Epoch 52 \t Batch 120 \t Validation Loss: 36.87954797744751\n",
      "Epoch 52 \t Batch 140 \t Validation Loss: 35.80241606576102\n",
      "Epoch 52 \t Batch 160 \t Validation Loss: 36.578264796733855\n",
      "Epoch 52 \t Batch 180 \t Validation Loss: 39.17481019761827\n",
      "Epoch 52 \t Batch 200 \t Validation Loss: 39.90953455924988\n",
      "Epoch 52 \t Batch 220 \t Validation Loss: 40.61830943714489\n",
      "Epoch 52 \t Batch 240 \t Validation Loss: 40.520232355594636\n",
      "Epoch 52 \t Batch 260 \t Validation Loss: 42.208048886519215\n",
      "Epoch 52 \t Batch 280 \t Validation Loss: 42.94072717598507\n",
      "Epoch 52 \t Batch 300 \t Validation Loss: 43.67966056505839\n",
      "Epoch 52 \t Batch 320 \t Validation Loss: 43.8618053227663\n",
      "Epoch 52 \t Batch 340 \t Validation Loss: 43.55503943106707\n",
      "Epoch 52 \t Batch 360 \t Validation Loss: 43.19290301005046\n",
      "Epoch 52 \t Batch 380 \t Validation Loss: 43.22667841158415\n",
      "Epoch 52 \t Batch 400 \t Validation Loss: 42.652839250564575\n",
      "Epoch 52 \t Batch 420 \t Validation Loss: 42.50893820580982\n",
      "Epoch 52 \t Batch 440 \t Validation Loss: 42.10231289213354\n",
      "Epoch 52 \t Batch 460 \t Validation Loss: 42.15943952850674\n",
      "Epoch 52 \t Batch 480 \t Validation Loss: 42.505969780683515\n",
      "Epoch 52 \t Batch 500 \t Validation Loss: 42.1023852558136\n",
      "Epoch 52 \t Batch 520 \t Validation Loss: 41.76311342349419\n",
      "Epoch 52 \t Batch 540 \t Validation Loss: 41.38884980413649\n",
      "Epoch 52 \t Batch 560 \t Validation Loss: 41.06819120304925\n",
      "Epoch 52 \t Batch 580 \t Validation Loss: 40.73887904430258\n",
      "Epoch 52 \t Batch 600 \t Validation Loss: 40.83947186628978\n",
      "Epoch 52 Training Loss: 46.86634285863379 Validation Loss: 41.39569129417469\n",
      "Epoch 52 completed\n",
      "Epoch 53 \t Batch 20 \t Training Loss: 48.06999111175537\n",
      "Epoch 53 \t Batch 40 \t Training Loss: 47.34707889556885\n",
      "Epoch 53 \t Batch 60 \t Training Loss: 47.03295923868815\n",
      "Epoch 53 \t Batch 80 \t Training Loss: 46.89112133979798\n",
      "Epoch 53 \t Batch 100 \t Training Loss: 46.71520935058594\n",
      "Epoch 53 \t Batch 120 \t Training Loss: 46.82380437850952\n",
      "Epoch 53 \t Batch 140 \t Training Loss: 46.851887048993795\n",
      "Epoch 53 \t Batch 160 \t Training Loss: 46.79341621398926\n",
      "Epoch 53 \t Batch 180 \t Training Loss: 47.04514045715332\n",
      "Epoch 53 \t Batch 200 \t Training Loss: 47.12919807434082\n",
      "Epoch 53 \t Batch 220 \t Training Loss: 47.30513381958008\n",
      "Epoch 53 \t Batch 240 \t Training Loss: 47.39452527364095\n",
      "Epoch 53 \t Batch 260 \t Training Loss: 47.477467815692606\n",
      "Epoch 53 \t Batch 280 \t Training Loss: 47.318283380780905\n",
      "Epoch 53 \t Batch 300 \t Training Loss: 47.20750591278076\n",
      "Epoch 53 \t Batch 320 \t Training Loss: 47.14974533319473\n",
      "Epoch 53 \t Batch 340 \t Training Loss: 47.186849111669204\n",
      "Epoch 53 \t Batch 360 \t Training Loss: 47.14775703218248\n",
      "Epoch 53 \t Batch 380 \t Training Loss: 47.217882517764444\n",
      "Epoch 53 \t Batch 400 \t Training Loss: 47.1416704082489\n",
      "Epoch 53 \t Batch 420 \t Training Loss: 47.16758140382313\n",
      "Epoch 53 \t Batch 440 \t Training Loss: 47.09265457500111\n",
      "Epoch 53 \t Batch 460 \t Training Loss: 47.07314686153246\n",
      "Epoch 53 \t Batch 480 \t Training Loss: 47.03713043530782\n",
      "Epoch 53 \t Batch 500 \t Training Loss: 47.01530139923096\n",
      "Epoch 53 \t Batch 520 \t Training Loss: 46.946203840695894\n",
      "Epoch 53 \t Batch 540 \t Training Loss: 46.9527816136678\n",
      "Epoch 53 \t Batch 560 \t Training Loss: 46.868490253176006\n",
      "Epoch 53 \t Batch 580 \t Training Loss: 46.8579054273408\n",
      "Epoch 53 \t Batch 600 \t Training Loss: 46.83692787806193\n",
      "Epoch 53 \t Batch 620 \t Training Loss: 46.850028622534964\n",
      "Epoch 53 \t Batch 640 \t Training Loss: 46.8669518828392\n",
      "Epoch 53 \t Batch 660 \t Training Loss: 46.887653558904475\n",
      "Epoch 53 \t Batch 680 \t Training Loss: 46.852590600182026\n",
      "Epoch 53 \t Batch 700 \t Training Loss: 46.87770133427211\n",
      "Epoch 53 \t Batch 720 \t Training Loss: 46.83095037672255\n",
      "Epoch 53 \t Batch 740 \t Training Loss: 46.854569507289575\n",
      "Epoch 53 \t Batch 760 \t Training Loss: 46.81292434993543\n",
      "Epoch 53 \t Batch 780 \t Training Loss: 46.84500247759697\n",
      "Epoch 53 \t Batch 800 \t Training Loss: 46.85050061702728\n",
      "Epoch 53 \t Batch 820 \t Training Loss: 46.81019171272836\n",
      "Epoch 53 \t Batch 840 \t Training Loss: 46.825873715536936\n",
      "Epoch 53 \t Batch 860 \t Training Loss: 46.77534109603527\n",
      "Epoch 53 \t Batch 880 \t Training Loss: 46.76401533647017\n",
      "Epoch 53 \t Batch 900 \t Training Loss: 46.76429463704427\n",
      "Epoch 53 \t Batch 20 \t Validation Loss: 31.883457136154174\n",
      "Epoch 53 \t Batch 40 \t Validation Loss: 29.52195143699646\n",
      "Epoch 53 \t Batch 60 \t Validation Loss: 32.05997996330261\n",
      "Epoch 53 \t Batch 80 \t Validation Loss: 31.30831218957901\n",
      "Epoch 53 \t Batch 100 \t Validation Loss: 30.66194968223572\n",
      "Epoch 53 \t Batch 120 \t Validation Loss: 30.57558376789093\n",
      "Epoch 53 \t Batch 140 \t Validation Loss: 30.287700060435704\n",
      "Epoch 53 \t Batch 160 \t Validation Loss: 31.822650796175004\n",
      "Epoch 53 \t Batch 180 \t Validation Loss: 35.069428480996024\n",
      "Epoch 53 \t Batch 200 \t Validation Loss: 36.40771237850189\n",
      "Epoch 53 \t Batch 220 \t Validation Loss: 37.621065499565816\n",
      "Epoch 53 \t Batch 240 \t Validation Loss: 37.878983688354495\n",
      "Epoch 53 \t Batch 260 \t Validation Loss: 39.93827199569115\n",
      "Epoch 53 \t Batch 280 \t Validation Loss: 41.003547184807914\n",
      "Epoch 53 \t Batch 300 \t Validation Loss: 41.87163888295492\n",
      "Epoch 53 \t Batch 320 \t Validation Loss: 42.1944538474083\n",
      "Epoch 53 \t Batch 340 \t Validation Loss: 42.03631142447976\n",
      "Epoch 53 \t Batch 360 \t Validation Loss: 41.78625748422411\n",
      "Epoch 53 \t Batch 380 \t Validation Loss: 41.953027153015135\n",
      "Epoch 53 \t Batch 400 \t Validation Loss: 41.460756459236144\n",
      "Epoch 53 \t Batch 420 \t Validation Loss: 41.42094672975086\n",
      "Epoch 53 \t Batch 440 \t Validation Loss: 41.03334172855724\n",
      "Epoch 53 \t Batch 460 \t Validation Loss: 41.13366869221563\n",
      "Epoch 53 \t Batch 480 \t Validation Loss: 41.527547152837116\n",
      "Epoch 53 \t Batch 500 \t Validation Loss: 41.194797580719\n",
      "Epoch 53 \t Batch 520 \t Validation Loss: 40.868855065565846\n",
      "Epoch 53 \t Batch 540 \t Validation Loss: 40.49835660369308\n",
      "Epoch 53 \t Batch 560 \t Validation Loss: 40.2085002217974\n",
      "Epoch 53 \t Batch 580 \t Validation Loss: 39.90542504540805\n",
      "Epoch 53 \t Batch 600 \t Validation Loss: 40.029409894943235\n",
      "Epoch 53 Training Loss: 46.81414141805767 Validation Loss: 40.650073983452536\n",
      "Epoch 53 completed\n",
      "Epoch 54 \t Batch 20 \t Training Loss: 47.05529308319092\n",
      "Epoch 54 \t Batch 40 \t Training Loss: 46.43936710357666\n",
      "Epoch 54 \t Batch 60 \t Training Loss: 46.34057057698568\n",
      "Epoch 54 \t Batch 80 \t Training Loss: 46.45158224105835\n",
      "Epoch 54 \t Batch 100 \t Training Loss: 46.50509990692139\n",
      "Epoch 54 \t Batch 120 \t Training Loss: 46.45971110661824\n",
      "Epoch 54 \t Batch 140 \t Training Loss: 46.6491089957101\n",
      "Epoch 54 \t Batch 160 \t Training Loss: 46.715157341957095\n",
      "Epoch 54 \t Batch 180 \t Training Loss: 46.6308155907525\n",
      "Epoch 54 \t Batch 200 \t Training Loss: 46.595702934265134\n",
      "Epoch 54 \t Batch 220 \t Training Loss: 46.63330560164018\n",
      "Epoch 54 \t Batch 240 \t Training Loss: 46.603745714823404\n",
      "Epoch 54 \t Batch 260 \t Training Loss: 46.67782833392803\n",
      "Epoch 54 \t Batch 280 \t Training Loss: 46.643261664254325\n",
      "Epoch 54 \t Batch 300 \t Training Loss: 46.64811626434326\n",
      "Epoch 54 \t Batch 320 \t Training Loss: 46.56096177101135\n",
      "Epoch 54 \t Batch 340 \t Training Loss: 46.538065124960504\n",
      "Epoch 54 \t Batch 360 \t Training Loss: 46.50187089708116\n",
      "Epoch 54 \t Batch 380 \t Training Loss: 46.55870499861868\n",
      "Epoch 54 \t Batch 400 \t Training Loss: 46.52132532119751\n",
      "Epoch 54 \t Batch 420 \t Training Loss: 46.62582969665527\n",
      "Epoch 54 \t Batch 440 \t Training Loss: 46.65712623596191\n",
      "Epoch 54 \t Batch 460 \t Training Loss: 46.69241493059241\n",
      "Epoch 54 \t Batch 480 \t Training Loss: 46.67311027844747\n",
      "Epoch 54 \t Batch 500 \t Training Loss: 46.67467739105225\n",
      "Epoch 54 \t Batch 520 \t Training Loss: 46.66023844939012\n",
      "Epoch 54 \t Batch 540 \t Training Loss: 46.649067899915906\n",
      "Epoch 54 \t Batch 560 \t Training Loss: 46.65441012382507\n",
      "Epoch 54 \t Batch 580 \t Training Loss: 46.68748058450633\n",
      "Epoch 54 \t Batch 600 \t Training Loss: 46.658177032470704\n",
      "Epoch 54 \t Batch 620 \t Training Loss: 46.65528360797513\n",
      "Epoch 54 \t Batch 640 \t Training Loss: 46.67206981182098\n",
      "Epoch 54 \t Batch 660 \t Training Loss: 46.71353451699922\n",
      "Epoch 54 \t Batch 680 \t Training Loss: 46.739388695885154\n",
      "Epoch 54 \t Batch 700 \t Training Loss: 46.725818208966935\n",
      "Epoch 54 \t Batch 720 \t Training Loss: 46.69544497066074\n",
      "Epoch 54 \t Batch 740 \t Training Loss: 46.71166348328462\n",
      "Epoch 54 \t Batch 760 \t Training Loss: 46.79347705339131\n",
      "Epoch 54 \t Batch 780 \t Training Loss: 46.79146106426533\n",
      "Epoch 54 \t Batch 800 \t Training Loss: 46.84913712024689\n",
      "Epoch 54 \t Batch 820 \t Training Loss: 46.830165742083295\n",
      "Epoch 54 \t Batch 840 \t Training Loss: 46.816422930217925\n",
      "Epoch 54 \t Batch 860 \t Training Loss: 46.84026786892913\n",
      "Epoch 54 \t Batch 880 \t Training Loss: 46.82126777822321\n",
      "Epoch 54 \t Batch 900 \t Training Loss: 46.81083915710449\n",
      "Epoch 54 \t Batch 20 \t Validation Loss: 39.27541599273682\n",
      "Epoch 54 \t Batch 40 \t Validation Loss: 34.665235710144046\n",
      "Epoch 54 \t Batch 60 \t Validation Loss: 38.293312311172485\n",
      "Epoch 54 \t Batch 80 \t Validation Loss: 36.7277753829956\n",
      "Epoch 54 \t Batch 100 \t Validation Loss: 35.02522565841675\n",
      "Epoch 54 \t Batch 120 \t Validation Loss: 34.27449752489726\n",
      "Epoch 54 \t Batch 140 \t Validation Loss: 33.45247629710606\n",
      "Epoch 54 \t Batch 160 \t Validation Loss: 34.38749737739563\n",
      "Epoch 54 \t Batch 180 \t Validation Loss: 36.99168727133009\n",
      "Epoch 54 \t Batch 200 \t Validation Loss: 37.87242063045502\n",
      "Epoch 54 \t Batch 220 \t Validation Loss: 38.70437241901051\n",
      "Epoch 54 \t Batch 240 \t Validation Loss: 38.6736839056015\n",
      "Epoch 54 \t Batch 260 \t Validation Loss: 40.47955942887526\n",
      "Epoch 54 \t Batch 280 \t Validation Loss: 41.347622421809604\n",
      "Epoch 54 \t Batch 300 \t Validation Loss: 42.0056128247579\n",
      "Epoch 54 \t Batch 320 \t Validation Loss: 42.1815329015255\n",
      "Epoch 54 \t Batch 340 \t Validation Loss: 41.9331646863152\n",
      "Epoch 54 \t Batch 360 \t Validation Loss: 41.59806345568763\n",
      "Epoch 54 \t Batch 380 \t Validation Loss: 41.699120539113096\n",
      "Epoch 54 \t Batch 400 \t Validation Loss: 41.16979553461075\n",
      "Epoch 54 \t Batch 420 \t Validation Loss: 41.06118871825082\n",
      "Epoch 54 \t Batch 440 \t Validation Loss: 40.664071796157145\n",
      "Epoch 54 \t Batch 460 \t Validation Loss: 40.74655304162398\n",
      "Epoch 54 \t Batch 480 \t Validation Loss: 41.10857094724973\n",
      "Epoch 54 \t Batch 500 \t Validation Loss: 40.72176062583923\n",
      "Epoch 54 \t Batch 520 \t Validation Loss: 40.40474381263439\n",
      "Epoch 54 \t Batch 540 \t Validation Loss: 40.07552347359834\n",
      "Epoch 54 \t Batch 560 \t Validation Loss: 39.819198650973185\n",
      "Epoch 54 \t Batch 580 \t Validation Loss: 39.57344573941724\n",
      "Epoch 54 \t Batch 600 \t Validation Loss: 39.71957809607188\n",
      "Epoch 54 Training Loss: 46.81660411506079 Validation Loss: 40.32299788431688\n",
      "Epoch 54 completed\n",
      "Epoch 55 \t Batch 20 \t Training Loss: 46.93604717254639\n",
      "Epoch 55 \t Batch 40 \t Training Loss: 46.36659383773804\n",
      "Epoch 55 \t Batch 60 \t Training Loss: 46.500061734517416\n",
      "Epoch 55 \t Batch 80 \t Training Loss: 46.17012329101563\n",
      "Epoch 55 \t Batch 100 \t Training Loss: 46.21069351196289\n",
      "Epoch 55 \t Batch 120 \t Training Loss: 46.158669757843015\n",
      "Epoch 55 \t Batch 140 \t Training Loss: 46.17652181897844\n",
      "Epoch 55 \t Batch 160 \t Training Loss: 46.41528558731079\n",
      "Epoch 55 \t Batch 180 \t Training Loss: 46.30929130978055\n",
      "Epoch 55 \t Batch 200 \t Training Loss: 46.396029472351074\n",
      "Epoch 55 \t Batch 220 \t Training Loss: 46.44702992872758\n",
      "Epoch 55 \t Batch 240 \t Training Loss: 46.44824816385905\n",
      "Epoch 55 \t Batch 260 \t Training Loss: 46.48711343911978\n",
      "Epoch 55 \t Batch 280 \t Training Loss: 46.561798749651224\n",
      "Epoch 55 \t Batch 300 \t Training Loss: 46.430785471598305\n",
      "Epoch 55 \t Batch 320 \t Training Loss: 46.41841107606888\n",
      "Epoch 55 \t Batch 340 \t Training Loss: 46.51003592996036\n",
      "Epoch 55 \t Batch 360 \t Training Loss: 46.48990960650974\n",
      "Epoch 55 \t Batch 380 \t Training Loss: 46.50841636657715\n",
      "Epoch 55 \t Batch 400 \t Training Loss: 46.488936557769776\n",
      "Epoch 55 \t Batch 420 \t Training Loss: 46.519071796962194\n",
      "Epoch 55 \t Batch 440 \t Training Loss: 46.58072558316317\n",
      "Epoch 55 \t Batch 460 \t Training Loss: 46.649148783476456\n",
      "Epoch 55 \t Batch 480 \t Training Loss: 46.68790060679118\n",
      "Epoch 55 \t Batch 500 \t Training Loss: 46.6989384765625\n",
      "Epoch 55 \t Batch 520 \t Training Loss: 46.76453656416673\n",
      "Epoch 55 \t Batch 540 \t Training Loss: 46.715974715903954\n",
      "Epoch 55 \t Batch 560 \t Training Loss: 46.71500651495797\n",
      "Epoch 55 \t Batch 580 \t Training Loss: 46.729829479085986\n",
      "Epoch 55 \t Batch 600 \t Training Loss: 46.75234432220459\n",
      "Epoch 55 \t Batch 620 \t Training Loss: 46.75075633756576\n",
      "Epoch 55 \t Batch 640 \t Training Loss: 46.76327615380287\n",
      "Epoch 55 \t Batch 660 \t Training Loss: 46.761534841132885\n",
      "Epoch 55 \t Batch 680 \t Training Loss: 46.77307852015776\n",
      "Epoch 55 \t Batch 700 \t Training Loss: 46.76821826934815\n",
      "Epoch 55 \t Batch 720 \t Training Loss: 46.734530994627214\n",
      "Epoch 55 \t Batch 740 \t Training Loss: 46.727070963060534\n",
      "Epoch 55 \t Batch 760 \t Training Loss: 46.71930927477385\n",
      "Epoch 55 \t Batch 780 \t Training Loss: 46.73137035369873\n",
      "Epoch 55 \t Batch 800 \t Training Loss: 46.74341864109039\n",
      "Epoch 55 \t Batch 820 \t Training Loss: 46.743340855109984\n",
      "Epoch 55 \t Batch 840 \t Training Loss: 46.75940791084653\n",
      "Epoch 55 \t Batch 860 \t Training Loss: 46.75467961777088\n",
      "Epoch 55 \t Batch 880 \t Training Loss: 46.77854375405745\n",
      "Epoch 55 \t Batch 900 \t Training Loss: 46.801321296691896\n",
      "Epoch 55 \t Batch 20 \t Validation Loss: 36.54424090385437\n",
      "Epoch 55 \t Batch 40 \t Validation Loss: 32.82669100761414\n",
      "Epoch 55 \t Batch 60 \t Validation Loss: 35.820934502283734\n",
      "Epoch 55 \t Batch 80 \t Validation Loss: 34.60636984109878\n",
      "Epoch 55 \t Batch 100 \t Validation Loss: 32.91044005393982\n",
      "Epoch 55 \t Batch 120 \t Validation Loss: 32.145241634051004\n",
      "Epoch 55 \t Batch 140 \t Validation Loss: 31.418102025985718\n",
      "Epoch 55 \t Batch 160 \t Validation Loss: 32.72194481492043\n",
      "Epoch 55 \t Batch 180 \t Validation Loss: 35.77180790901184\n",
      "Epoch 55 \t Batch 200 \t Validation Loss: 36.87758331775665\n",
      "Epoch 55 \t Batch 220 \t Validation Loss: 37.907194115898825\n",
      "Epoch 55 \t Batch 240 \t Validation Loss: 38.07331670920054\n",
      "Epoch 55 \t Batch 260 \t Validation Loss: 39.97546027990488\n",
      "Epoch 55 \t Batch 280 \t Validation Loss: 40.8910138130188\n",
      "Epoch 55 \t Batch 300 \t Validation Loss: 41.787888558705646\n",
      "Epoch 55 \t Batch 320 \t Validation Loss: 42.10349434018135\n",
      "Epoch 55 \t Batch 340 \t Validation Loss: 41.912882232666014\n",
      "Epoch 55 \t Batch 360 \t Validation Loss: 41.641435872183905\n",
      "Epoch 55 \t Batch 380 \t Validation Loss: 41.758521767666466\n",
      "Epoch 55 \t Batch 400 \t Validation Loss: 41.24716866970062\n",
      "Epoch 55 \t Batch 420 \t Validation Loss: 41.170928114936466\n",
      "Epoch 55 \t Batch 440 \t Validation Loss: 40.810924395647916\n",
      "Epoch 55 \t Batch 460 \t Validation Loss: 40.91015107528023\n",
      "Epoch 55 \t Batch 480 \t Validation Loss: 41.298289155960084\n",
      "Epoch 55 \t Batch 500 \t Validation Loss: 40.93886240386963\n",
      "Epoch 55 \t Batch 520 \t Validation Loss: 40.62643391169035\n",
      "Epoch 55 \t Batch 540 \t Validation Loss: 40.24910517092104\n",
      "Epoch 55 \t Batch 560 \t Validation Loss: 39.94272219453539\n",
      "Epoch 55 \t Batch 580 \t Validation Loss: 39.59319953589604\n",
      "Epoch 55 \t Batch 600 \t Validation Loss: 39.708456910451254\n",
      "Epoch 55 Training Loss: 46.80836482656431 Validation Loss: 40.27133060740186\n",
      "Epoch 55 completed\n",
      "Epoch 56 \t Batch 20 \t Training Loss: 45.936496353149415\n",
      "Epoch 56 \t Batch 40 \t Training Loss: 45.50662927627563\n",
      "Epoch 56 \t Batch 60 \t Training Loss: 46.33577270507813\n",
      "Epoch 56 \t Batch 80 \t Training Loss: 46.74605450630188\n",
      "Epoch 56 \t Batch 100 \t Training Loss: 46.809210510253905\n",
      "Epoch 56 \t Batch 120 \t Training Loss: 46.90954504013062\n",
      "Epoch 56 \t Batch 140 \t Training Loss: 46.84950182778495\n",
      "Epoch 56 \t Batch 160 \t Training Loss: 46.89957735538483\n",
      "Epoch 56 \t Batch 180 \t Training Loss: 46.79097654554579\n",
      "Epoch 56 \t Batch 200 \t Training Loss: 46.841627559661866\n",
      "Epoch 56 \t Batch 220 \t Training Loss: 46.755388138510966\n",
      "Epoch 56 \t Batch 240 \t Training Loss: 47.022765429814655\n",
      "Epoch 56 \t Batch 260 \t Training Loss: 46.95755651914156\n",
      "Epoch 56 \t Batch 280 \t Training Loss: 46.96615345818656\n",
      "Epoch 56 \t Batch 300 \t Training Loss: 47.00411083221436\n",
      "Epoch 56 \t Batch 320 \t Training Loss: 46.92715982198715\n",
      "Epoch 56 \t Batch 340 \t Training Loss: 46.94026651943431\n",
      "Epoch 56 \t Batch 360 \t Training Loss: 46.914066802130804\n",
      "Epoch 56 \t Batch 380 \t Training Loss: 46.85830002834923\n",
      "Epoch 56 \t Batch 400 \t Training Loss: 46.768257274627686\n",
      "Epoch 56 \t Batch 420 \t Training Loss: 46.75621591295515\n",
      "Epoch 56 \t Batch 440 \t Training Loss: 46.77648241736672\n",
      "Epoch 56 \t Batch 460 \t Training Loss: 46.74029000323752\n",
      "Epoch 56 \t Batch 480 \t Training Loss: 46.71308228969574\n",
      "Epoch 56 \t Batch 500 \t Training Loss: 46.71443306732178\n",
      "Epoch 56 \t Batch 520 \t Training Loss: 46.752768230438235\n",
      "Epoch 56 \t Batch 540 \t Training Loss: 46.772189458211265\n",
      "Epoch 56 \t Batch 560 \t Training Loss: 46.77844880649022\n",
      "Epoch 56 \t Batch 580 \t Training Loss: 46.83743627482447\n",
      "Epoch 56 \t Batch 600 \t Training Loss: 46.793213272094725\n",
      "Epoch 56 \t Batch 620 \t Training Loss: 46.70638432656565\n",
      "Epoch 56 \t Batch 640 \t Training Loss: 46.686301082372665\n",
      "Epoch 56 \t Batch 660 \t Training Loss: 46.703261306069116\n",
      "Epoch 56 \t Batch 680 \t Training Loss: 46.665844333873075\n",
      "Epoch 56 \t Batch 700 \t Training Loss: 46.695830339704244\n",
      "Epoch 56 \t Batch 720 \t Training Loss: 46.72248250113593\n",
      "Epoch 56 \t Batch 740 \t Training Loss: 46.704768474681956\n",
      "Epoch 56 \t Batch 760 \t Training Loss: 46.70882901643452\n",
      "Epoch 56 \t Batch 780 \t Training Loss: 46.76331699566963\n",
      "Epoch 56 \t Batch 800 \t Training Loss: 46.77716498851776\n",
      "Epoch 56 \t Batch 820 \t Training Loss: 46.79450315615026\n",
      "Epoch 56 \t Batch 840 \t Training Loss: 46.77812480472383\n",
      "Epoch 56 \t Batch 860 \t Training Loss: 46.76390961048215\n",
      "Epoch 56 \t Batch 880 \t Training Loss: 46.740457374399355\n",
      "Epoch 56 \t Batch 900 \t Training Loss: 46.757709647284614\n",
      "Epoch 56 \t Batch 20 \t Validation Loss: 39.77349848747254\n",
      "Epoch 56 \t Batch 40 \t Validation Loss: 36.05172026157379\n",
      "Epoch 56 \t Batch 60 \t Validation Loss: 39.26692423820496\n",
      "Epoch 56 \t Batch 80 \t Validation Loss: 37.86597539186478\n",
      "Epoch 56 \t Batch 100 \t Validation Loss: 36.217855806350705\n",
      "Epoch 56 \t Batch 120 \t Validation Loss: 35.42486296494802\n",
      "Epoch 56 \t Batch 140 \t Validation Loss: 34.51928005899702\n",
      "Epoch 56 \t Batch 160 \t Validation Loss: 35.53213465809822\n",
      "Epoch 56 \t Batch 180 \t Validation Loss: 38.45202972094218\n",
      "Epoch 56 \t Batch 200 \t Validation Loss: 39.367716145515445\n",
      "Epoch 56 \t Batch 220 \t Validation Loss: 40.329495867815886\n",
      "Epoch 56 \t Batch 240 \t Validation Loss: 40.38010716835658\n",
      "Epoch 56 \t Batch 260 \t Validation Loss: 42.24716112430279\n",
      "Epoch 56 \t Batch 280 \t Validation Loss: 43.10726834705898\n",
      "Epoch 56 \t Batch 300 \t Validation Loss: 43.909535261789955\n",
      "Epoch 56 \t Batch 320 \t Validation Loss: 44.14291108250618\n",
      "Epoch 56 \t Batch 340 \t Validation Loss: 43.83898100572474\n",
      "Epoch 56 \t Batch 360 \t Validation Loss: 43.501856830384995\n",
      "Epoch 56 \t Batch 380 \t Validation Loss: 43.55201047094245\n",
      "Epoch 56 \t Batch 400 \t Validation Loss: 42.910645160675045\n",
      "Epoch 56 \t Batch 420 \t Validation Loss: 42.735779081072124\n",
      "Epoch 56 \t Batch 440 \t Validation Loss: 42.238444089889526\n",
      "Epoch 56 \t Batch 460 \t Validation Loss: 42.2674763513648\n",
      "Epoch 56 \t Batch 480 \t Validation Loss: 42.5898592710495\n",
      "Epoch 56 \t Batch 500 \t Validation Loss: 42.18609304046631\n",
      "Epoch 56 \t Batch 520 \t Validation Loss: 41.8034732561845\n",
      "Epoch 56 \t Batch 540 \t Validation Loss: 41.412018613462095\n",
      "Epoch 56 \t Batch 560 \t Validation Loss: 41.10023348331451\n",
      "Epoch 56 \t Batch 580 \t Validation Loss: 40.79427143951942\n",
      "Epoch 56 \t Batch 600 \t Validation Loss: 40.89599035898844\n",
      "Epoch 56 Training Loss: 46.76960074238585 Validation Loss: 41.45882869386054\n",
      "Epoch 56 completed\n",
      "Epoch 57 \t Batch 20 \t Training Loss: 46.66206092834473\n",
      "Epoch 57 \t Batch 40 \t Training Loss: 46.89884796142578\n",
      "Epoch 57 \t Batch 60 \t Training Loss: 46.232669258117674\n",
      "Epoch 57 \t Batch 80 \t Training Loss: 46.20816130638123\n",
      "Epoch 57 \t Batch 100 \t Training Loss: 46.07655818939209\n",
      "Epoch 57 \t Batch 120 \t Training Loss: 46.09534975687663\n",
      "Epoch 57 \t Batch 140 \t Training Loss: 46.05726958683559\n",
      "Epoch 57 \t Batch 160 \t Training Loss: 46.305929613113406\n",
      "Epoch 57 \t Batch 180 \t Training Loss: 46.26239840189616\n",
      "Epoch 57 \t Batch 200 \t Training Loss: 46.349177551269534\n",
      "Epoch 57 \t Batch 220 \t Training Loss: 46.50751849087802\n",
      "Epoch 57 \t Batch 240 \t Training Loss: 46.54137293497721\n",
      "Epoch 57 \t Batch 260 \t Training Loss: 46.59723680936373\n",
      "Epoch 57 \t Batch 280 \t Training Loss: 46.56916127886091\n",
      "Epoch 57 \t Batch 300 \t Training Loss: 46.59440547943115\n",
      "Epoch 57 \t Batch 320 \t Training Loss: 46.56793406009674\n",
      "Epoch 57 \t Batch 340 \t Training Loss: 46.61706106522504\n",
      "Epoch 57 \t Batch 360 \t Training Loss: 46.58172660403781\n",
      "Epoch 57 \t Batch 380 \t Training Loss: 46.66792100605212\n",
      "Epoch 57 \t Batch 400 \t Training Loss: 46.67509449005127\n",
      "Epoch 57 \t Batch 420 \t Training Loss: 46.637310255141486\n",
      "Epoch 57 \t Batch 440 \t Training Loss: 46.61470694975419\n",
      "Epoch 57 \t Batch 460 \t Training Loss: 46.620843331710155\n",
      "Epoch 57 \t Batch 480 \t Training Loss: 46.70555181503296\n",
      "Epoch 57 \t Batch 500 \t Training Loss: 46.73507018280029\n",
      "Epoch 57 \t Batch 520 \t Training Loss: 46.77309545370249\n",
      "Epoch 57 \t Batch 540 \t Training Loss: 46.741514389603225\n",
      "Epoch 57 \t Batch 560 \t Training Loss: 46.715482003348214\n",
      "Epoch 57 \t Batch 580 \t Training Loss: 46.71409981168549\n",
      "Epoch 57 \t Batch 600 \t Training Loss: 46.70705467859904\n",
      "Epoch 57 \t Batch 620 \t Training Loss: 46.68336917508033\n",
      "Epoch 57 \t Batch 640 \t Training Loss: 46.667193102836606\n",
      "Epoch 57 \t Batch 660 \t Training Loss: 46.67245693784771\n",
      "Epoch 57 \t Batch 680 \t Training Loss: 46.67656415490543\n",
      "Epoch 57 \t Batch 700 \t Training Loss: 46.64614519391741\n",
      "Epoch 57 \t Batch 720 \t Training Loss: 46.645740832222835\n",
      "Epoch 57 \t Batch 740 \t Training Loss: 46.66533755740604\n",
      "Epoch 57 \t Batch 760 \t Training Loss: 46.66147211476376\n",
      "Epoch 57 \t Batch 780 \t Training Loss: 46.73542996920072\n",
      "Epoch 57 \t Batch 800 \t Training Loss: 46.73021615505218\n",
      "Epoch 57 \t Batch 820 \t Training Loss: 46.74853515625\n",
      "Epoch 57 \t Batch 840 \t Training Loss: 46.77507757459368\n",
      "Epoch 57 \t Batch 860 \t Training Loss: 46.76247861108114\n",
      "Epoch 57 \t Batch 880 \t Training Loss: 46.755114351619376\n",
      "Epoch 57 \t Batch 900 \t Training Loss: 46.76479451497396\n",
      "Epoch 57 \t Batch 20 \t Validation Loss: 34.990407609939574\n",
      "Epoch 57 \t Batch 40 \t Validation Loss: 31.75455775260925\n",
      "Epoch 57 \t Batch 60 \t Validation Loss: 34.54681628545125\n",
      "Epoch 57 \t Batch 80 \t Validation Loss: 33.50590000152588\n",
      "Epoch 57 \t Batch 100 \t Validation Loss: 32.46202571868896\n",
      "Epoch 57 \t Batch 120 \t Validation Loss: 32.0446803410848\n",
      "Epoch 57 \t Batch 140 \t Validation Loss: 31.457119383130756\n",
      "Epoch 57 \t Batch 160 \t Validation Loss: 32.65772496461868\n",
      "Epoch 57 \t Batch 180 \t Validation Loss: 35.40842310587565\n",
      "Epoch 57 \t Batch 200 \t Validation Loss: 36.45547344207764\n",
      "Epoch 57 \t Batch 220 \t Validation Loss: 37.38166046142578\n",
      "Epoch 57 \t Batch 240 \t Validation Loss: 37.45623090267181\n",
      "Epoch 57 \t Batch 260 \t Validation Loss: 39.29014612344595\n",
      "Epoch 57 \t Batch 280 \t Validation Loss: 40.188200909750805\n",
      "Epoch 57 \t Batch 300 \t Validation Loss: 40.91667584101359\n",
      "Epoch 57 \t Batch 320 \t Validation Loss: 41.18086135387421\n",
      "Epoch 57 \t Batch 340 \t Validation Loss: 41.03893709743724\n",
      "Epoch 57 \t Batch 360 \t Validation Loss: 40.775733115937975\n",
      "Epoch 57 \t Batch 380 \t Validation Loss: 40.94584830434699\n",
      "Epoch 57 \t Batch 400 \t Validation Loss: 40.527100434303286\n",
      "Epoch 57 \t Batch 420 \t Validation Loss: 40.511769576299756\n",
      "Epoch 57 \t Batch 440 \t Validation Loss: 40.24712190628052\n",
      "Epoch 57 \t Batch 460 \t Validation Loss: 40.40551983791849\n",
      "Epoch 57 \t Batch 480 \t Validation Loss: 40.8075554450353\n",
      "Epoch 57 \t Batch 500 \t Validation Loss: 40.474206993103024\n",
      "Epoch 57 \t Batch 520 \t Validation Loss: 40.22631316918593\n",
      "Epoch 57 \t Batch 540 \t Validation Loss: 39.92518809989647\n",
      "Epoch 57 \t Batch 560 \t Validation Loss: 39.7008709362575\n",
      "Epoch 57 \t Batch 580 \t Validation Loss: 39.457895351278374\n",
      "Epoch 57 \t Batch 600 \t Validation Loss: 39.63139173825582\n",
      "Epoch 57 Training Loss: 46.73876462247925 Validation Loss: 40.23388872518168\n",
      "Epoch 57 completed\n",
      "Epoch 58 \t Batch 20 \t Training Loss: 46.698171615600586\n",
      "Epoch 58 \t Batch 40 \t Training Loss: 45.98454446792603\n",
      "Epoch 58 \t Batch 60 \t Training Loss: 46.01074562072754\n",
      "Epoch 58 \t Batch 80 \t Training Loss: 46.15678648948669\n",
      "Epoch 58 \t Batch 100 \t Training Loss: 46.083989219665526\n",
      "Epoch 58 \t Batch 120 \t Training Loss: 46.10122521718343\n",
      "Epoch 58 \t Batch 140 \t Training Loss: 46.232849965776715\n",
      "Epoch 58 \t Batch 160 \t Training Loss: 46.39191961288452\n",
      "Epoch 58 \t Batch 180 \t Training Loss: 46.420978503757055\n",
      "Epoch 58 \t Batch 200 \t Training Loss: 46.50699842453003\n",
      "Epoch 58 \t Batch 220 \t Training Loss: 46.32914364554665\n",
      "Epoch 58 \t Batch 240 \t Training Loss: 46.442346270879106\n",
      "Epoch 58 \t Batch 260 \t Training Loss: 46.63433337578407\n",
      "Epoch 58 \t Batch 280 \t Training Loss: 46.51434278488159\n",
      "Epoch 58 \t Batch 300 \t Training Loss: 46.50480083465576\n",
      "Epoch 58 \t Batch 320 \t Training Loss: 46.44003192186356\n",
      "Epoch 58 \t Batch 340 \t Training Loss: 46.4355011435116\n",
      "Epoch 58 \t Batch 360 \t Training Loss: 46.38809774186876\n",
      "Epoch 58 \t Batch 380 \t Training Loss: 46.3549541774549\n",
      "Epoch 58 \t Batch 400 \t Training Loss: 46.32407263755798\n",
      "Epoch 58 \t Batch 420 \t Training Loss: 46.44462822505406\n",
      "Epoch 58 \t Batch 440 \t Training Loss: 46.511915961178865\n",
      "Epoch 58 \t Batch 460 \t Training Loss: 46.484385797251825\n",
      "Epoch 58 \t Batch 480 \t Training Loss: 46.493472798665366\n",
      "Epoch 58 \t Batch 500 \t Training Loss: 46.440090911865234\n",
      "Epoch 58 \t Batch 520 \t Training Loss: 46.46277388792772\n",
      "Epoch 58 \t Batch 540 \t Training Loss: 46.51808259752062\n",
      "Epoch 58 \t Batch 560 \t Training Loss: 46.538940920148576\n",
      "Epoch 58 \t Batch 580 \t Training Loss: 46.51294465558282\n",
      "Epoch 58 \t Batch 600 \t Training Loss: 46.6091881052653\n",
      "Epoch 58 \t Batch 620 \t Training Loss: 46.61345504637688\n",
      "Epoch 58 \t Batch 640 \t Training Loss: 46.61701033711434\n",
      "Epoch 58 \t Batch 660 \t Training Loss: 46.564449685992614\n",
      "Epoch 58 \t Batch 680 \t Training Loss: 46.572800411897546\n",
      "Epoch 58 \t Batch 700 \t Training Loss: 46.58757231031145\n",
      "Epoch 58 \t Batch 720 \t Training Loss: 46.61912038061354\n",
      "Epoch 58 \t Batch 740 \t Training Loss: 46.65023115518931\n",
      "Epoch 58 \t Batch 760 \t Training Loss: 46.6766548909639\n",
      "Epoch 58 \t Batch 780 \t Training Loss: 46.71564692961864\n",
      "Epoch 58 \t Batch 800 \t Training Loss: 46.68149592876434\n",
      "Epoch 58 \t Batch 820 \t Training Loss: 46.71602785063953\n",
      "Epoch 58 \t Batch 840 \t Training Loss: 46.70273287182763\n",
      "Epoch 58 \t Batch 860 \t Training Loss: 46.71070916819018\n",
      "Epoch 58 \t Batch 880 \t Training Loss: 46.722409257021816\n",
      "Epoch 58 \t Batch 900 \t Training Loss: 46.713489252726234\n",
      "Epoch 58 \t Batch 20 \t Validation Loss: 43.98802709579468\n",
      "Epoch 58 \t Batch 40 \t Validation Loss: 38.63255364894867\n",
      "Epoch 58 \t Batch 60 \t Validation Loss: 42.34416443506877\n",
      "Epoch 58 \t Batch 80 \t Validation Loss: 40.526591670513156\n",
      "Epoch 58 \t Batch 100 \t Validation Loss: 38.02216593742371\n",
      "Epoch 58 \t Batch 120 \t Validation Loss: 36.69229360421499\n",
      "Epoch 58 \t Batch 140 \t Validation Loss: 35.50159980910165\n",
      "Epoch 58 \t Batch 160 \t Validation Loss: 36.369574373960496\n",
      "Epoch 58 \t Batch 180 \t Validation Loss: 39.05917343033685\n",
      "Epoch 58 \t Batch 200 \t Validation Loss: 39.96844133853912\n",
      "Epoch 58 \t Batch 220 \t Validation Loss: 40.767466167970134\n",
      "Epoch 58 \t Batch 240 \t Validation Loss: 40.76626224120458\n",
      "Epoch 58 \t Batch 260 \t Validation Loss: 42.54433346895071\n",
      "Epoch 58 \t Batch 280 \t Validation Loss: 43.382736509186884\n",
      "Epoch 58 \t Batch 300 \t Validation Loss: 44.11878644625346\n",
      "Epoch 58 \t Batch 320 \t Validation Loss: 44.31697268784046\n",
      "Epoch 58 \t Batch 340 \t Validation Loss: 44.05308016328251\n",
      "Epoch 58 \t Batch 360 \t Validation Loss: 43.69481831391652\n",
      "Epoch 58 \t Batch 380 \t Validation Loss: 43.76027141621238\n",
      "Epoch 58 \t Batch 400 \t Validation Loss: 43.225455939769745\n",
      "Epoch 58 \t Batch 420 \t Validation Loss: 43.108827397936864\n",
      "Epoch 58 \t Batch 440 \t Validation Loss: 42.745496556975624\n",
      "Epoch 58 \t Batch 460 \t Validation Loss: 42.835668288106504\n",
      "Epoch 58 \t Batch 480 \t Validation Loss: 43.14839695096016\n",
      "Epoch 58 \t Batch 500 \t Validation Loss: 42.75698231697083\n",
      "Epoch 58 \t Batch 520 \t Validation Loss: 42.469464747722334\n",
      "Epoch 58 \t Batch 540 \t Validation Loss: 42.05579710536533\n",
      "Epoch 58 \t Batch 560 \t Validation Loss: 41.71517624003547\n",
      "Epoch 58 \t Batch 580 \t Validation Loss: 41.3437366567809\n",
      "Epoch 58 \t Batch 600 \t Validation Loss: 41.425540801684065\n",
      "Epoch 58 Training Loss: 46.720993441349805 Validation Loss: 41.94548698059924\n",
      "Epoch 58 completed\n",
      "Epoch 59 \t Batch 20 \t Training Loss: 45.173915481567384\n",
      "Epoch 59 \t Batch 40 \t Training Loss: 45.83845586776734\n",
      "Epoch 59 \t Batch 60 \t Training Loss: 46.147443008422854\n",
      "Epoch 59 \t Batch 80 \t Training Loss: 46.17288455963135\n",
      "Epoch 59 \t Batch 100 \t Training Loss: 46.30298393249512\n",
      "Epoch 59 \t Batch 120 \t Training Loss: 46.60099376042684\n",
      "Epoch 59 \t Batch 140 \t Training Loss: 46.453635052272254\n",
      "Epoch 59 \t Batch 160 \t Training Loss: 46.54274985790253\n",
      "Epoch 59 \t Batch 180 \t Training Loss: 46.52493065728082\n",
      "Epoch 59 \t Batch 200 \t Training Loss: 46.41592493057251\n",
      "Epoch 59 \t Batch 220 \t Training Loss: 46.3987456408414\n",
      "Epoch 59 \t Batch 240 \t Training Loss: 46.46459016799927\n",
      "Epoch 59 \t Batch 260 \t Training Loss: 46.52373999082125\n",
      "Epoch 59 \t Batch 280 \t Training Loss: 46.60516777038574\n",
      "Epoch 59 \t Batch 300 \t Training Loss: 46.50728842417399\n",
      "Epoch 59 \t Batch 320 \t Training Loss: 46.465309870243075\n",
      "Epoch 59 \t Batch 340 \t Training Loss: 46.560103248147406\n",
      "Epoch 59 \t Batch 360 \t Training Loss: 46.538817702399356\n",
      "Epoch 59 \t Batch 380 \t Training Loss: 46.61800462823165\n",
      "Epoch 59 \t Batch 400 \t Training Loss: 46.546414346694945\n",
      "Epoch 59 \t Batch 420 \t Training Loss: 46.556962394714354\n",
      "Epoch 59 \t Batch 440 \t Training Loss: 46.61777147813277\n",
      "Epoch 59 \t Batch 460 \t Training Loss: 46.60159788546355\n",
      "Epoch 59 \t Batch 480 \t Training Loss: 46.616602571805316\n",
      "Epoch 59 \t Batch 500 \t Training Loss: 46.64038131713867\n",
      "Epoch 59 \t Batch 520 \t Training Loss: 46.62185995395367\n",
      "Epoch 59 \t Batch 540 \t Training Loss: 46.68396573243318\n",
      "Epoch 59 \t Batch 560 \t Training Loss: 46.65725027493068\n",
      "Epoch 59 \t Batch 580 \t Training Loss: 46.64761539985393\n",
      "Epoch 59 \t Batch 600 \t Training Loss: 46.65351868311564\n",
      "Epoch 59 \t Batch 620 \t Training Loss: 46.66414536506899\n",
      "Epoch 59 \t Batch 640 \t Training Loss: 46.643537992239\n",
      "Epoch 59 \t Batch 660 \t Training Loss: 46.609704283512\n",
      "Epoch 59 \t Batch 680 \t Training Loss: 46.64353764477898\n",
      "Epoch 59 \t Batch 700 \t Training Loss: 46.63866800580706\n",
      "Epoch 59 \t Batch 720 \t Training Loss: 46.63648131688436\n",
      "Epoch 59 \t Batch 740 \t Training Loss: 46.64276656073493\n",
      "Epoch 59 \t Batch 760 \t Training Loss: 46.67527080837049\n",
      "Epoch 59 \t Batch 780 \t Training Loss: 46.674717213557315\n",
      "Epoch 59 \t Batch 800 \t Training Loss: 46.64856084823609\n",
      "Epoch 59 \t Batch 820 \t Training Loss: 46.67424989560755\n",
      "Epoch 59 \t Batch 840 \t Training Loss: 46.64733038402739\n",
      "Epoch 59 \t Batch 860 \t Training Loss: 46.657930569316065\n",
      "Epoch 59 \t Batch 880 \t Training Loss: 46.67534483562816\n",
      "Epoch 59 \t Batch 900 \t Training Loss: 46.69599418216281\n",
      "Epoch 59 \t Batch 20 \t Validation Loss: 33.9203519821167\n",
      "Epoch 59 \t Batch 40 \t Validation Loss: 30.43268666267395\n",
      "Epoch 59 \t Batch 60 \t Validation Loss: 33.36348632176717\n",
      "Epoch 59 \t Batch 80 \t Validation Loss: 32.38915754556656\n",
      "Epoch 59 \t Batch 100 \t Validation Loss: 31.345818853378297\n",
      "Epoch 59 \t Batch 120 \t Validation Loss: 31.096854813893636\n",
      "Epoch 59 \t Batch 140 \t Validation Loss: 30.690471499306813\n",
      "Epoch 59 \t Batch 160 \t Validation Loss: 32.090489780902864\n",
      "Epoch 59 \t Batch 180 \t Validation Loss: 35.05302668677436\n",
      "Epoch 59 \t Batch 200 \t Validation Loss: 36.109825963974\n",
      "Epoch 59 \t Batch 220 \t Validation Loss: 37.13509197235108\n",
      "Epoch 59 \t Batch 240 \t Validation Loss: 37.29912472565969\n",
      "Epoch 59 \t Batch 260 \t Validation Loss: 39.22057097141559\n",
      "Epoch 59 \t Batch 280 \t Validation Loss: 40.1897881099156\n",
      "Epoch 59 \t Batch 300 \t Validation Loss: 41.018734251658124\n",
      "Epoch 59 \t Batch 320 \t Validation Loss: 41.29301218986511\n",
      "Epoch 59 \t Batch 340 \t Validation Loss: 41.10239537743961\n",
      "Epoch 59 \t Batch 360 \t Validation Loss: 40.88694726096259\n",
      "Epoch 59 \t Batch 380 \t Validation Loss: 41.04297551606831\n",
      "Epoch 59 \t Batch 400 \t Validation Loss: 40.5532958650589\n",
      "Epoch 59 \t Batch 420 \t Validation Loss: 40.46570864177885\n",
      "Epoch 59 \t Batch 440 \t Validation Loss: 40.1020470012318\n",
      "Epoch 59 \t Batch 460 \t Validation Loss: 40.21785530836686\n",
      "Epoch 59 \t Batch 480 \t Validation Loss: 40.61127471923828\n",
      "Epoch 59 \t Batch 500 \t Validation Loss: 40.25076831817627\n",
      "Epoch 59 \t Batch 520 \t Validation Loss: 39.97940908578726\n",
      "Epoch 59 \t Batch 540 \t Validation Loss: 39.6806710490474\n",
      "Epoch 59 \t Batch 560 \t Validation Loss: 39.4496299130576\n",
      "Epoch 59 \t Batch 580 \t Validation Loss: 39.26745394673841\n",
      "Epoch 59 \t Batch 600 \t Validation Loss: 39.418712867101036\n",
      "Epoch 59 Training Loss: 46.717985575435726 Validation Loss: 40.072356549176305\n",
      "Epoch 59 completed\n",
      "Epoch 60 \t Batch 20 \t Training Loss: 45.838200378417966\n",
      "Epoch 60 \t Batch 40 \t Training Loss: 45.715487766265866\n",
      "Epoch 60 \t Batch 60 \t Training Loss: 45.70840733846028\n",
      "Epoch 60 \t Batch 80 \t Training Loss: 46.11918387413025\n",
      "Epoch 60 \t Batch 100 \t Training Loss: 46.69090557098389\n",
      "Epoch 60 \t Batch 120 \t Training Loss: 46.71443033218384\n",
      "Epoch 60 \t Batch 140 \t Training Loss: 46.77737167903355\n",
      "Epoch 60 \t Batch 160 \t Training Loss: 46.838797545433046\n",
      "Epoch 60 \t Batch 180 \t Training Loss: 46.78608578575982\n",
      "Epoch 60 \t Batch 200 \t Training Loss: 46.75126417160034\n",
      "Epoch 60 \t Batch 220 \t Training Loss: 46.61114734302868\n",
      "Epoch 60 \t Batch 240 \t Training Loss: 46.58711241086324\n",
      "Epoch 60 \t Batch 260 \t Training Loss: 46.6358146080604\n",
      "Epoch 60 \t Batch 280 \t Training Loss: 46.579855169568745\n",
      "Epoch 60 \t Batch 300 \t Training Loss: 46.55525780995687\n",
      "Epoch 60 \t Batch 320 \t Training Loss: 46.52307481765747\n",
      "Epoch 60 \t Batch 340 \t Training Loss: 46.54384901383344\n",
      "Epoch 60 \t Batch 360 \t Training Loss: 46.52141319910685\n",
      "Epoch 60 \t Batch 380 \t Training Loss: 46.60045919920269\n",
      "Epoch 60 \t Batch 400 \t Training Loss: 46.65778344154358\n",
      "Epoch 60 \t Batch 420 \t Training Loss: 46.607974388485864\n",
      "Epoch 60 \t Batch 440 \t Training Loss: 46.61047845320268\n",
      "Epoch 60 \t Batch 460 \t Training Loss: 46.568905639648435\n",
      "Epoch 60 \t Batch 480 \t Training Loss: 46.56506091753642\n",
      "Epoch 60 \t Batch 500 \t Training Loss: 46.55619680023193\n",
      "Epoch 60 \t Batch 520 \t Training Loss: 46.572445759406456\n",
      "Epoch 60 \t Batch 540 \t Training Loss: 46.581498025964805\n",
      "Epoch 60 \t Batch 560 \t Training Loss: 46.54193378857204\n",
      "Epoch 60 \t Batch 580 \t Training Loss: 46.596564634915055\n",
      "Epoch 60 \t Batch 600 \t Training Loss: 46.61249121348063\n",
      "Epoch 60 \t Batch 620 \t Training Loss: 46.64871748032108\n",
      "Epoch 60 \t Batch 640 \t Training Loss: 46.63715291619301\n",
      "Epoch 60 \t Batch 660 \t Training Loss: 46.61537780183734\n",
      "Epoch 60 \t Batch 680 \t Training Loss: 46.60222462485818\n",
      "Epoch 60 \t Batch 700 \t Training Loss: 46.60760259355818\n",
      "Epoch 60 \t Batch 720 \t Training Loss: 46.576353814866806\n",
      "Epoch 60 \t Batch 740 \t Training Loss: 46.583514486776814\n",
      "Epoch 60 \t Batch 760 \t Training Loss: 46.61136723568565\n",
      "Epoch 60 \t Batch 780 \t Training Loss: 46.63757055233686\n",
      "Epoch 60 \t Batch 800 \t Training Loss: 46.622587814331055\n",
      "Epoch 60 \t Batch 820 \t Training Loss: 46.63907209722007\n",
      "Epoch 60 \t Batch 840 \t Training Loss: 46.65164738609677\n",
      "Epoch 60 \t Batch 860 \t Training Loss: 46.659224909405374\n",
      "Epoch 60 \t Batch 880 \t Training Loss: 46.7069478338415\n",
      "Epoch 60 \t Batch 900 \t Training Loss: 46.694268239339195\n",
      "Epoch 60 \t Batch 20 \t Validation Loss: 42.54934282302857\n",
      "Epoch 60 \t Batch 40 \t Validation Loss: 37.772861218452455\n",
      "Epoch 60 \t Batch 60 \t Validation Loss: 41.02171880404155\n",
      "Epoch 60 \t Batch 80 \t Validation Loss: 39.53134400844574\n",
      "Epoch 60 \t Batch 100 \t Validation Loss: 37.5109729385376\n",
      "Epoch 60 \t Batch 120 \t Validation Loss: 36.501516310373944\n",
      "Epoch 60 \t Batch 140 \t Validation Loss: 35.39452757154192\n",
      "Epoch 60 \t Batch 160 \t Validation Loss: 36.166650485992434\n",
      "Epoch 60 \t Batch 180 \t Validation Loss: 38.85657426516215\n",
      "Epoch 60 \t Batch 200 \t Validation Loss: 39.6878825044632\n",
      "Epoch 60 \t Batch 220 \t Validation Loss: 40.50414123101668\n",
      "Epoch 60 \t Batch 240 \t Validation Loss: 40.44359792470932\n",
      "Epoch 60 \t Batch 260 \t Validation Loss: 42.25188443477337\n",
      "Epoch 60 \t Batch 280 \t Validation Loss: 43.08522839546204\n",
      "Epoch 60 \t Batch 300 \t Validation Loss: 43.78648148854574\n",
      "Epoch 60 \t Batch 320 \t Validation Loss: 43.962748593091966\n",
      "Epoch 60 \t Batch 340 \t Validation Loss: 43.641539169760314\n",
      "Epoch 60 \t Batch 360 \t Validation Loss: 43.25184322198232\n",
      "Epoch 60 \t Batch 380 \t Validation Loss: 43.312205497842086\n",
      "Epoch 60 \t Batch 400 \t Validation Loss: 42.69320744276047\n",
      "Epoch 60 \t Batch 420 \t Validation Loss: 42.519440321695235\n",
      "Epoch 60 \t Batch 440 \t Validation Loss: 42.05958730740981\n",
      "Epoch 60 \t Batch 460 \t Validation Loss: 42.10800386719082\n",
      "Epoch 60 \t Batch 480 \t Validation Loss: 42.419434024890265\n",
      "Epoch 60 \t Batch 500 \t Validation Loss: 41.997170679092406\n",
      "Epoch 60 \t Batch 520 \t Validation Loss: 41.631611334360564\n",
      "Epoch 60 \t Batch 540 \t Validation Loss: 41.241542454119084\n",
      "Epoch 60 \t Batch 560 \t Validation Loss: 40.8851734314646\n",
      "Epoch 60 \t Batch 580 \t Validation Loss: 40.48829977594573\n",
      "Epoch 60 \t Batch 600 \t Validation Loss: 40.60164645671844\n",
      "Epoch 60 Training Loss: 46.70243547162937 Validation Loss: 41.15653490865385\n",
      "Epoch 60 completed\n",
      "Epoch 61 \t Batch 20 \t Training Loss: 46.90747413635254\n",
      "Epoch 61 \t Batch 40 \t Training Loss: 46.52174921035767\n",
      "Epoch 61 \t Batch 60 \t Training Loss: 47.54971110026042\n",
      "Epoch 61 \t Batch 80 \t Training Loss: 47.292014503479\n",
      "Epoch 61 \t Batch 100 \t Training Loss: 47.12933193206787\n",
      "Epoch 61 \t Batch 120 \t Training Loss: 47.246193949381514\n",
      "Epoch 61 \t Batch 140 \t Training Loss: 46.883663286481585\n",
      "Epoch 61 \t Batch 160 \t Training Loss: 46.80765745639801\n",
      "Epoch 61 \t Batch 180 \t Training Loss: 46.796756977505154\n",
      "Epoch 61 \t Batch 200 \t Training Loss: 46.83904371261597\n",
      "Epoch 61 \t Batch 220 \t Training Loss: 46.898433633284135\n",
      "Epoch 61 \t Batch 240 \t Training Loss: 46.90532193183899\n",
      "Epoch 61 \t Batch 260 \t Training Loss: 46.85428306873028\n",
      "Epoch 61 \t Batch 280 \t Training Loss: 46.80146195547921\n",
      "Epoch 61 \t Batch 300 \t Training Loss: 46.858769226074216\n",
      "Epoch 61 \t Batch 320 \t Training Loss: 46.89694514274597\n",
      "Epoch 61 \t Batch 340 \t Training Loss: 46.930545380536245\n",
      "Epoch 61 \t Batch 360 \t Training Loss: 46.909781053331166\n",
      "Epoch 61 \t Batch 380 \t Training Loss: 46.962693234493855\n",
      "Epoch 61 \t Batch 400 \t Training Loss: 46.9348641872406\n",
      "Epoch 61 \t Batch 420 \t Training Loss: 46.880880056108744\n",
      "Epoch 61 \t Batch 440 \t Training Loss: 46.908123909343374\n",
      "Epoch 61 \t Batch 460 \t Training Loss: 46.86562324192213\n",
      "Epoch 61 \t Batch 480 \t Training Loss: 46.86552947362264\n",
      "Epoch 61 \t Batch 500 \t Training Loss: 46.9356125869751\n",
      "Epoch 61 \t Batch 520 \t Training Loss: 46.9317282309899\n",
      "Epoch 61 \t Batch 540 \t Training Loss: 46.96724325109411\n",
      "Epoch 61 \t Batch 560 \t Training Loss: 46.92300637790135\n",
      "Epoch 61 \t Batch 580 \t Training Loss: 46.912241902844656\n",
      "Epoch 61 \t Batch 600 \t Training Loss: 46.87347007751465\n",
      "Epoch 61 \t Batch 620 \t Training Loss: 46.846706581115725\n",
      "Epoch 61 \t Batch 640 \t Training Loss: 46.836376428604126\n",
      "Epoch 61 \t Batch 660 \t Training Loss: 46.82714973796498\n",
      "Epoch 61 \t Batch 680 \t Training Loss: 46.84761988134945\n",
      "Epoch 61 \t Batch 700 \t Training Loss: 46.81423917497907\n",
      "Epoch 61 \t Batch 720 \t Training Loss: 46.799741903940834\n",
      "Epoch 61 \t Batch 740 \t Training Loss: 46.765977983216985\n",
      "Epoch 61 \t Batch 760 \t Training Loss: 46.7433429768211\n",
      "Epoch 61 \t Batch 780 \t Training Loss: 46.739035596603\n",
      "Epoch 61 \t Batch 800 \t Training Loss: 46.74355173110962\n",
      "Epoch 61 \t Batch 820 \t Training Loss: 46.74645816058647\n",
      "Epoch 61 \t Batch 840 \t Training Loss: 46.69773958297003\n",
      "Epoch 61 \t Batch 860 \t Training Loss: 46.68729464952336\n",
      "Epoch 61 \t Batch 880 \t Training Loss: 46.67145808393305\n",
      "Epoch 61 \t Batch 900 \t Training Loss: 46.68051830291748\n",
      "Epoch 61 \t Batch 20 \t Validation Loss: 40.867685651779176\n",
      "Epoch 61 \t Batch 40 \t Validation Loss: 36.20813822746277\n",
      "Epoch 61 \t Batch 60 \t Validation Loss: 39.48855805397034\n",
      "Epoch 61 \t Batch 80 \t Validation Loss: 38.0763338804245\n",
      "Epoch 61 \t Batch 100 \t Validation Loss: 36.340590171813965\n",
      "Epoch 61 \t Batch 120 \t Validation Loss: 35.52072483698527\n",
      "Epoch 61 \t Batch 140 \t Validation Loss: 34.57914097649711\n",
      "Epoch 61 \t Batch 160 \t Validation Loss: 35.494790005683896\n",
      "Epoch 61 \t Batch 180 \t Validation Loss: 38.19980388217502\n",
      "Epoch 61 \t Batch 200 \t Validation Loss: 39.056318073272706\n",
      "Epoch 61 \t Batch 220 \t Validation Loss: 39.913994589718904\n",
      "Epoch 61 \t Batch 240 \t Validation Loss: 39.89366702636083\n",
      "Epoch 61 \t Batch 260 \t Validation Loss: 41.69768043297988\n",
      "Epoch 61 \t Batch 280 \t Validation Loss: 42.53101351261139\n",
      "Epoch 61 \t Batch 300 \t Validation Loss: 43.254912805557254\n",
      "Epoch 61 \t Batch 320 \t Validation Loss: 43.44887562096119\n",
      "Epoch 61 \t Batch 340 \t Validation Loss: 43.172190158507405\n",
      "Epoch 61 \t Batch 360 \t Validation Loss: 42.83296255800459\n",
      "Epoch 61 \t Batch 380 \t Validation Loss: 42.9133322088342\n",
      "Epoch 61 \t Batch 400 \t Validation Loss: 42.33841009855271\n",
      "Epoch 61 \t Batch 420 \t Validation Loss: 42.18379332905724\n",
      "Epoch 61 \t Batch 440 \t Validation Loss: 41.75882178870114\n",
      "Epoch 61 \t Batch 460 \t Validation Loss: 41.8037798155909\n",
      "Epoch 61 \t Batch 480 \t Validation Loss: 42.13649507164955\n",
      "Epoch 61 \t Batch 500 \t Validation Loss: 41.73584983253479\n",
      "Epoch 61 \t Batch 520 \t Validation Loss: 41.406195418651286\n",
      "Epoch 61 \t Batch 540 \t Validation Loss: 41.0247320510723\n",
      "Epoch 61 \t Batch 560 \t Validation Loss: 40.715524637699126\n",
      "Epoch 61 \t Batch 580 \t Validation Loss: 40.44275617763914\n",
      "Epoch 61 \t Batch 600 \t Validation Loss: 40.549673180580136\n",
      "Epoch 61 Training Loss: 46.67310123589203 Validation Loss: 41.19182510964282\n",
      "Epoch 61 completed\n",
      "Epoch 62 \t Batch 20 \t Training Loss: 45.910672950744626\n",
      "Epoch 62 \t Batch 40 \t Training Loss: 47.1495759010315\n",
      "Epoch 62 \t Batch 60 \t Training Loss: 46.6934933980306\n",
      "Epoch 62 \t Batch 80 \t Training Loss: 47.09263434410095\n",
      "Epoch 62 \t Batch 100 \t Training Loss: 47.351952171325685\n",
      "Epoch 62 \t Batch 120 \t Training Loss: 47.17747964859009\n",
      "Epoch 62 \t Batch 140 \t Training Loss: 47.05062070574079\n",
      "Epoch 62 \t Batch 160 \t Training Loss: 47.121698427200315\n",
      "Epoch 62 \t Batch 180 \t Training Loss: 46.94818422529433\n",
      "Epoch 62 \t Batch 200 \t Training Loss: 46.901858444213865\n",
      "Epoch 62 \t Batch 220 \t Training Loss: 46.74196659434926\n",
      "Epoch 62 \t Batch 240 \t Training Loss: 46.74318167368571\n",
      "Epoch 62 \t Batch 260 \t Training Loss: 46.810228186387285\n",
      "Epoch 62 \t Batch 280 \t Training Loss: 46.856393923078265\n",
      "Epoch 62 \t Batch 300 \t Training Loss: 46.82906233469645\n",
      "Epoch 62 \t Batch 320 \t Training Loss: 46.79461711645126\n",
      "Epoch 62 \t Batch 340 \t Training Loss: 46.82685769025017\n",
      "Epoch 62 \t Batch 360 \t Training Loss: 46.83475034501817\n",
      "Epoch 62 \t Batch 380 \t Training Loss: 46.779965762088175\n",
      "Epoch 62 \t Batch 400 \t Training Loss: 46.70925100326538\n",
      "Epoch 62 \t Batch 420 \t Training Loss: 46.76466153462728\n",
      "Epoch 62 \t Batch 440 \t Training Loss: 46.75905688892711\n",
      "Epoch 62 \t Batch 460 \t Training Loss: 46.702577002152154\n",
      "Epoch 62 \t Batch 480 \t Training Loss: 46.69269479115804\n",
      "Epoch 62 \t Batch 500 \t Training Loss: 46.7196837387085\n",
      "Epoch 62 \t Batch 520 \t Training Loss: 46.68624891134409\n",
      "Epoch 62 \t Batch 540 \t Training Loss: 46.68801341586643\n",
      "Epoch 62 \t Batch 560 \t Training Loss: 46.61102561950683\n",
      "Epoch 62 \t Batch 580 \t Training Loss: 46.571784111549114\n",
      "Epoch 62 \t Batch 600 \t Training Loss: 46.613867359161375\n",
      "Epoch 62 \t Batch 620 \t Training Loss: 46.58593319308373\n",
      "Epoch 62 \t Batch 640 \t Training Loss: 46.61915488839149\n",
      "Epoch 62 \t Batch 660 \t Training Loss: 46.60056016517408\n",
      "Epoch 62 \t Batch 680 \t Training Loss: 46.549386506922104\n",
      "Epoch 62 \t Batch 700 \t Training Loss: 46.611002834865026\n",
      "Epoch 62 \t Batch 720 \t Training Loss: 46.62094218995836\n",
      "Epoch 62 \t Batch 740 \t Training Loss: 46.58617450611011\n",
      "Epoch 62 \t Batch 760 \t Training Loss: 46.585817392248856\n",
      "Epoch 62 \t Batch 780 \t Training Loss: 46.56180144089919\n",
      "Epoch 62 \t Batch 800 \t Training Loss: 46.569378566741946\n",
      "Epoch 62 \t Batch 820 \t Training Loss: 46.579846847348094\n",
      "Epoch 62 \t Batch 840 \t Training Loss: 46.60123751504081\n",
      "Epoch 62 \t Batch 860 \t Training Loss: 46.58302845622218\n",
      "Epoch 62 \t Batch 880 \t Training Loss: 46.5677733594721\n",
      "Epoch 62 \t Batch 900 \t Training Loss: 46.57188323126899\n",
      "Epoch 62 \t Batch 20 \t Validation Loss: 32.65563611984253\n",
      "Epoch 62 \t Batch 40 \t Validation Loss: 30.54405987262726\n",
      "Epoch 62 \t Batch 60 \t Validation Loss: 32.77982029914856\n",
      "Epoch 62 \t Batch 80 \t Validation Loss: 32.1432865858078\n",
      "Epoch 62 \t Batch 100 \t Validation Loss: 31.25204298019409\n",
      "Epoch 62 \t Batch 120 \t Validation Loss: 30.99261326789856\n",
      "Epoch 62 \t Batch 140 \t Validation Loss: 30.55879810878209\n",
      "Epoch 62 \t Batch 160 \t Validation Loss: 31.872116327285767\n",
      "Epoch 62 \t Batch 180 \t Validation Loss: 34.88952233526442\n",
      "Epoch 62 \t Batch 200 \t Validation Loss: 35.99693072795868\n",
      "Epoch 62 \t Batch 220 \t Validation Loss: 37.06175082380121\n",
      "Epoch 62 \t Batch 240 \t Validation Loss: 37.24499767621358\n",
      "Epoch 62 \t Batch 260 \t Validation Loss: 39.16817807050852\n",
      "Epoch 62 \t Batch 280 \t Validation Loss: 40.10347383022308\n",
      "Epoch 62 \t Batch 300 \t Validation Loss: 40.907850160598755\n",
      "Epoch 62 \t Batch 320 \t Validation Loss: 41.19139802753925\n",
      "Epoch 62 \t Batch 340 \t Validation Loss: 41.001089446684894\n",
      "Epoch 62 \t Batch 360 \t Validation Loss: 40.72599755658044\n",
      "Epoch 62 \t Batch 380 \t Validation Loss: 40.836755458932174\n",
      "Epoch 62 \t Batch 400 \t Validation Loss: 40.29558531999588\n",
      "Epoch 62 \t Batch 420 \t Validation Loss: 40.19793661889576\n",
      "Epoch 62 \t Batch 440 \t Validation Loss: 39.79250765930522\n",
      "Epoch 62 \t Batch 460 \t Validation Loss: 39.87829822250035\n",
      "Epoch 62 \t Batch 480 \t Validation Loss: 40.26393122871717\n",
      "Epoch 62 \t Batch 500 \t Validation Loss: 39.902719011306765\n",
      "Epoch 62 \t Batch 520 \t Validation Loss: 39.562577748298644\n",
      "Epoch 62 \t Batch 540 \t Validation Loss: 39.22918655430829\n",
      "Epoch 62 \t Batch 560 \t Validation Loss: 38.97702133485249\n",
      "Epoch 62 \t Batch 580 \t Validation Loss: 38.7158998144084\n",
      "Epoch 62 \t Batch 600 \t Validation Loss: 38.873631847699485\n",
      "Epoch 62 Training Loss: 46.62888215126362 Validation Loss: 39.48577425541816\n",
      "Epoch 62 completed\n",
      "Epoch 63 \t Batch 20 \t Training Loss: 44.59395351409912\n",
      "Epoch 63 \t Batch 40 \t Training Loss: 45.92914447784424\n",
      "Epoch 63 \t Batch 60 \t Training Loss: 46.142303212483725\n",
      "Epoch 63 \t Batch 80 \t Training Loss: 46.3168671131134\n",
      "Epoch 63 \t Batch 100 \t Training Loss: 46.225117149353025\n",
      "Epoch 63 \t Batch 120 \t Training Loss: 46.24696038564046\n",
      "Epoch 63 \t Batch 140 \t Training Loss: 46.3075783593314\n",
      "Epoch 63 \t Batch 160 \t Training Loss: 46.28405191898346\n",
      "Epoch 63 \t Batch 180 \t Training Loss: 46.209516292148166\n",
      "Epoch 63 \t Batch 200 \t Training Loss: 46.177247276306154\n",
      "Epoch 63 \t Batch 220 \t Training Loss: 46.22352268912575\n",
      "Epoch 63 \t Batch 240 \t Training Loss: 46.29563670158386\n",
      "Epoch 63 \t Batch 260 \t Training Loss: 46.4330129476694\n",
      "Epoch 63 \t Batch 280 \t Training Loss: 46.44231824874878\n",
      "Epoch 63 \t Batch 300 \t Training Loss: 46.424493115743005\n",
      "Epoch 63 \t Batch 320 \t Training Loss: 46.44980561733246\n",
      "Epoch 63 \t Batch 340 \t Training Loss: 46.40128697788014\n",
      "Epoch 63 \t Batch 360 \t Training Loss: 46.49574170642429\n",
      "Epoch 63 \t Batch 380 \t Training Loss: 46.49055519104004\n",
      "Epoch 63 \t Batch 400 \t Training Loss: 46.52990005493164\n",
      "Epoch 63 \t Batch 420 \t Training Loss: 46.530509049551824\n",
      "Epoch 63 \t Batch 440 \t Training Loss: 46.48974277322942\n",
      "Epoch 63 \t Batch 460 \t Training Loss: 46.46523130665655\n",
      "Epoch 63 \t Batch 480 \t Training Loss: 46.50471822420756\n",
      "Epoch 63 \t Batch 500 \t Training Loss: 46.49006327819824\n",
      "Epoch 63 \t Batch 520 \t Training Loss: 46.510198343717136\n",
      "Epoch 63 \t Batch 540 \t Training Loss: 46.52048083411323\n",
      "Epoch 63 \t Batch 560 \t Training Loss: 46.49183759689331\n",
      "Epoch 63 \t Batch 580 \t Training Loss: 46.515369046967606\n",
      "Epoch 63 \t Batch 600 \t Training Loss: 46.4925990041097\n",
      "Epoch 63 \t Batch 620 \t Training Loss: 46.5144100681428\n",
      "Epoch 63 \t Batch 640 \t Training Loss: 46.490268754959104\n",
      "Epoch 63 \t Batch 660 \t Training Loss: 46.488953948743415\n",
      "Epoch 63 \t Batch 680 \t Training Loss: 46.53429816189934\n",
      "Epoch 63 \t Batch 700 \t Training Loss: 46.53427613939558\n",
      "Epoch 63 \t Batch 720 \t Training Loss: 46.55838442378574\n",
      "Epoch 63 \t Batch 740 \t Training Loss: 46.549042320251466\n",
      "Epoch 63 \t Batch 760 \t Training Loss: 46.54314105385228\n",
      "Epoch 63 \t Batch 780 \t Training Loss: 46.547800401540904\n",
      "Epoch 63 \t Batch 800 \t Training Loss: 46.5599493598938\n",
      "Epoch 63 \t Batch 820 \t Training Loss: 46.55827118012963\n",
      "Epoch 63 \t Batch 840 \t Training Loss: 46.60678063347226\n",
      "Epoch 63 \t Batch 860 \t Training Loss: 46.635489259764206\n",
      "Epoch 63 \t Batch 880 \t Training Loss: 46.66293095675382\n",
      "Epoch 63 \t Batch 900 \t Training Loss: 46.66411186642117\n",
      "Epoch 63 \t Batch 20 \t Validation Loss: 34.012525987625125\n",
      "Epoch 63 \t Batch 40 \t Validation Loss: 31.08942346572876\n",
      "Epoch 63 \t Batch 60 \t Validation Loss: 33.8081067721049\n",
      "Epoch 63 \t Batch 80 \t Validation Loss: 32.816473031044005\n",
      "Epoch 63 \t Batch 100 \t Validation Loss: 31.68897274017334\n",
      "Epoch 63 \t Batch 120 \t Validation Loss: 31.26869687239329\n",
      "Epoch 63 \t Batch 140 \t Validation Loss: 30.77733407020569\n",
      "Epoch 63 \t Batch 160 \t Validation Loss: 32.18141914010048\n",
      "Epoch 63 \t Batch 180 \t Validation Loss: 35.38236538039313\n",
      "Epoch 63 \t Batch 200 \t Validation Loss: 36.56081073284149\n",
      "Epoch 63 \t Batch 220 \t Validation Loss: 37.715685866095804\n",
      "Epoch 63 \t Batch 240 \t Validation Loss: 37.94615531365077\n",
      "Epoch 63 \t Batch 260 \t Validation Loss: 39.978062277573805\n",
      "Epoch 63 \t Batch 280 \t Validation Loss: 40.99342470509665\n",
      "Epoch 63 \t Batch 300 \t Validation Loss: 41.890156520207725\n",
      "Epoch 63 \t Batch 320 \t Validation Loss: 42.224764582514766\n",
      "Epoch 63 \t Batch 340 \t Validation Loss: 42.01924663711996\n",
      "Epoch 63 \t Batch 360 \t Validation Loss: 41.75095596048567\n",
      "Epoch 63 \t Batch 380 \t Validation Loss: 41.901543890802486\n",
      "Epoch 63 \t Batch 400 \t Validation Loss: 41.356370718479155\n",
      "Epoch 63 \t Batch 420 \t Validation Loss: 41.23614026251293\n",
      "Epoch 63 \t Batch 440 \t Validation Loss: 40.83915398554368\n",
      "Epoch 63 \t Batch 460 \t Validation Loss: 40.92799343648164\n",
      "Epoch 63 \t Batch 480 \t Validation Loss: 41.295920715729395\n",
      "Epoch 63 \t Batch 500 \t Validation Loss: 40.91693675804138\n",
      "Epoch 63 \t Batch 520 \t Validation Loss: 40.617544073324936\n",
      "Epoch 63 \t Batch 540 \t Validation Loss: 40.26306411778485\n",
      "Epoch 63 \t Batch 560 \t Validation Loss: 39.96881846530097\n",
      "Epoch 63 \t Batch 580 \t Validation Loss: 39.69423006485248\n",
      "Epoch 63 \t Batch 600 \t Validation Loss: 39.8171208747228\n",
      "Epoch 63 Training Loss: 46.649594201906424 Validation Loss: 40.39518829909238\n",
      "Epoch 63 completed\n",
      "Epoch 64 \t Batch 20 \t Training Loss: 46.75972919464111\n",
      "Epoch 64 \t Batch 40 \t Training Loss: 46.89749889373779\n",
      "Epoch 64 \t Batch 60 \t Training Loss: 46.67863896687825\n",
      "Epoch 64 \t Batch 80 \t Training Loss: 47.137057590484616\n",
      "Epoch 64 \t Batch 100 \t Training Loss: 46.837365036010745\n",
      "Epoch 64 \t Batch 120 \t Training Loss: 46.76927426656087\n",
      "Epoch 64 \t Batch 140 \t Training Loss: 46.70771465301514\n",
      "Epoch 64 \t Batch 160 \t Training Loss: 46.64180099964142\n",
      "Epoch 64 \t Batch 180 \t Training Loss: 46.504653973049585\n",
      "Epoch 64 \t Batch 200 \t Training Loss: 46.510493431091305\n",
      "Epoch 64 \t Batch 220 \t Training Loss: 46.48309967734597\n",
      "Epoch 64 \t Batch 240 \t Training Loss: 46.452204100290935\n",
      "Epoch 64 \t Batch 260 \t Training Loss: 46.451738122793344\n",
      "Epoch 64 \t Batch 280 \t Training Loss: 46.36655564989363\n",
      "Epoch 64 \t Batch 300 \t Training Loss: 46.31312941233317\n",
      "Epoch 64 \t Batch 320 \t Training Loss: 46.32590626478195\n",
      "Epoch 64 \t Batch 340 \t Training Loss: 46.38447868122774\n",
      "Epoch 64 \t Batch 360 \t Training Loss: 46.43499098883735\n",
      "Epoch 64 \t Batch 380 \t Training Loss: 46.36157244632118\n",
      "Epoch 64 \t Batch 400 \t Training Loss: 46.36764119148254\n",
      "Epoch 64 \t Batch 420 \t Training Loss: 46.39199565705799\n",
      "Epoch 64 \t Batch 440 \t Training Loss: 46.37241547324441\n",
      "Epoch 64 \t Batch 460 \t Training Loss: 46.38027832197106\n",
      "Epoch 64 \t Batch 480 \t Training Loss: 46.40472577412923\n",
      "Epoch 64 \t Batch 500 \t Training Loss: 46.468152435302734\n",
      "Epoch 64 \t Batch 520 \t Training Loss: 46.46615850742047\n",
      "Epoch 64 \t Batch 540 \t Training Loss: 46.45445882302744\n",
      "Epoch 64 \t Batch 560 \t Training Loss: 46.50981253896441\n",
      "Epoch 64 \t Batch 580 \t Training Loss: 46.51679414551833\n",
      "Epoch 64 \t Batch 600 \t Training Loss: 46.55042542775472\n",
      "Epoch 64 \t Batch 620 \t Training Loss: 46.5344328788019\n",
      "Epoch 64 \t Batch 640 \t Training Loss: 46.52741530537605\n",
      "Epoch 64 \t Batch 660 \t Training Loss: 46.51715607498631\n",
      "Epoch 64 \t Batch 680 \t Training Loss: 46.50882988537059\n",
      "Epoch 64 \t Batch 700 \t Training Loss: 46.565721239362446\n",
      "Epoch 64 \t Batch 720 \t Training Loss: 46.58115866449144\n",
      "Epoch 64 \t Batch 740 \t Training Loss: 46.58254708470525\n",
      "Epoch 64 \t Batch 760 \t Training Loss: 46.609737190447355\n",
      "Epoch 64 \t Batch 780 \t Training Loss: 46.63245589427459\n",
      "Epoch 64 \t Batch 800 \t Training Loss: 46.61533976078034\n",
      "Epoch 64 \t Batch 820 \t Training Loss: 46.6213366717827\n",
      "Epoch 64 \t Batch 840 \t Training Loss: 46.624793797447566\n",
      "Epoch 64 \t Batch 860 \t Training Loss: 46.61923129059548\n",
      "Epoch 64 \t Batch 880 \t Training Loss: 46.59572750438343\n",
      "Epoch 64 \t Batch 900 \t Training Loss: 46.56722835116916\n",
      "Epoch 64 \t Batch 20 \t Validation Loss: 29.197014570236206\n",
      "Epoch 64 \t Batch 40 \t Validation Loss: 27.100546312332153\n",
      "Epoch 64 \t Batch 60 \t Validation Loss: 29.146072308222454\n",
      "Epoch 64 \t Batch 80 \t Validation Loss: 28.732822680473326\n",
      "Epoch 64 \t Batch 100 \t Validation Loss: 28.638565406799316\n",
      "Epoch 64 \t Batch 120 \t Validation Loss: 28.922624349594116\n",
      "Epoch 64 \t Batch 140 \t Validation Loss: 28.82724802834647\n",
      "Epoch 64 \t Batch 160 \t Validation Loss: 30.32588208913803\n",
      "Epoch 64 \t Batch 180 \t Validation Loss: 33.13799370659722\n",
      "Epoch 64 \t Batch 200 \t Validation Loss: 34.31344783782959\n",
      "Epoch 64 \t Batch 220 \t Validation Loss: 35.35242463892156\n",
      "Epoch 64 \t Batch 240 \t Validation Loss: 35.51669183969498\n",
      "Epoch 64 \t Batch 260 \t Validation Loss: 37.44078620397128\n",
      "Epoch 64 \t Batch 280 \t Validation Loss: 38.42279588154384\n",
      "Epoch 64 \t Batch 300 \t Validation Loss: 39.16142984390259\n",
      "Epoch 64 \t Batch 320 \t Validation Loss: 39.4531896173954\n",
      "Epoch 64 \t Batch 340 \t Validation Loss: 39.36226882373585\n",
      "Epoch 64 \t Batch 360 \t Validation Loss: 39.16814121935103\n",
      "Epoch 64 \t Batch 380 \t Validation Loss: 39.38890332924692\n",
      "Epoch 64 \t Batch 400 \t Validation Loss: 38.997660148143765\n",
      "Epoch 64 \t Batch 420 \t Validation Loss: 38.98829354558672\n",
      "Epoch 64 \t Batch 440 \t Validation Loss: 38.71181685057553\n",
      "Epoch 64 \t Batch 460 \t Validation Loss: 38.89528881363247\n",
      "Epoch 64 \t Batch 480 \t Validation Loss: 39.34052983323733\n",
      "Epoch 64 \t Batch 500 \t Validation Loss: 39.034832654953\n",
      "Epoch 64 \t Batch 520 \t Validation Loss: 38.80126566886902\n",
      "Epoch 64 \t Batch 540 \t Validation Loss: 38.540406732205994\n",
      "Epoch 64 \t Batch 560 \t Validation Loss: 38.33228646005903\n",
      "Epoch 64 \t Batch 580 \t Validation Loss: 38.15001800471339\n",
      "Epoch 64 \t Batch 600 \t Validation Loss: 38.34502568244934\n",
      "Epoch 64 Training Loss: 46.611850056289455 Validation Loss: 38.97474650903182\n",
      "Epoch 64 completed\n",
      "Epoch 65 \t Batch 20 \t Training Loss: 45.8311092376709\n",
      "Epoch 65 \t Batch 40 \t Training Loss: 45.87450590133667\n",
      "Epoch 65 \t Batch 60 \t Training Loss: 46.60215085347493\n",
      "Epoch 65 \t Batch 80 \t Training Loss: 46.326845502853395\n",
      "Epoch 65 \t Batch 100 \t Training Loss: 46.3909871673584\n",
      "Epoch 65 \t Batch 120 \t Training Loss: 46.36279420852661\n",
      "Epoch 65 \t Batch 140 \t Training Loss: 46.654679134913856\n",
      "Epoch 65 \t Batch 160 \t Training Loss: 46.5532065153122\n",
      "Epoch 65 \t Batch 180 \t Training Loss: 46.592597007751465\n",
      "Epoch 65 \t Batch 200 \t Training Loss: 46.608726787567136\n",
      "Epoch 65 \t Batch 220 \t Training Loss: 46.56524640863592\n",
      "Epoch 65 \t Batch 240 \t Training Loss: 46.55733165740967\n",
      "Epoch 65 \t Batch 260 \t Training Loss: 46.50500563108004\n",
      "Epoch 65 \t Batch 280 \t Training Loss: 46.47428748267038\n",
      "Epoch 65 \t Batch 300 \t Training Loss: 46.522829081217445\n",
      "Epoch 65 \t Batch 320 \t Training Loss: 46.53309512138367\n",
      "Epoch 65 \t Batch 340 \t Training Loss: 46.64166052201215\n",
      "Epoch 65 \t Batch 360 \t Training Loss: 46.62878057691786\n",
      "Epoch 65 \t Batch 380 \t Training Loss: 46.73441232380114\n",
      "Epoch 65 \t Batch 400 \t Training Loss: 46.7931791973114\n",
      "Epoch 65 \t Batch 420 \t Training Loss: 46.75229182470412\n",
      "Epoch 65 \t Batch 440 \t Training Loss: 46.66961878863248\n",
      "Epoch 65 \t Batch 460 \t Training Loss: 46.687644941910456\n",
      "Epoch 65 \t Batch 480 \t Training Loss: 46.6948130051295\n",
      "Epoch 65 \t Batch 500 \t Training Loss: 46.64318428039551\n",
      "Epoch 65 \t Batch 520 \t Training Loss: 46.6768995651832\n",
      "Epoch 65 \t Batch 540 \t Training Loss: 46.735373143796565\n",
      "Epoch 65 \t Batch 560 \t Training Loss: 46.69984034129551\n",
      "Epoch 65 \t Batch 580 \t Training Loss: 46.644441709847285\n",
      "Epoch 65 \t Batch 600 \t Training Loss: 46.66977001190185\n",
      "Epoch 65 \t Batch 620 \t Training Loss: 46.69360635818974\n",
      "Epoch 65 \t Batch 640 \t Training Loss: 46.6990742623806\n",
      "Epoch 65 \t Batch 660 \t Training Loss: 46.70515628583504\n",
      "Epoch 65 \t Batch 680 \t Training Loss: 46.651572670656094\n",
      "Epoch 65 \t Batch 700 \t Training Loss: 46.6742932510376\n",
      "Epoch 65 \t Batch 720 \t Training Loss: 46.64360678460863\n",
      "Epoch 65 \t Batch 740 \t Training Loss: 46.633811863048656\n",
      "Epoch 65 \t Batch 760 \t Training Loss: 46.65897949118363\n",
      "Epoch 65 \t Batch 780 \t Training Loss: 46.63811486562093\n",
      "Epoch 65 \t Batch 800 \t Training Loss: 46.60992824554443\n",
      "Epoch 65 \t Batch 820 \t Training Loss: 46.59212191977152\n",
      "Epoch 65 \t Batch 840 \t Training Loss: 46.59784577233451\n",
      "Epoch 65 \t Batch 860 \t Training Loss: 46.56842368813448\n",
      "Epoch 65 \t Batch 880 \t Training Loss: 46.57175995219838\n",
      "Epoch 65 \t Batch 900 \t Training Loss: 46.605441924201116\n",
      "Epoch 65 \t Batch 20 \t Validation Loss: 26.879823923110962\n",
      "Epoch 65 \t Batch 40 \t Validation Loss: 25.755992031097414\n",
      "Epoch 65 \t Batch 60 \t Validation Loss: 27.47262628873189\n",
      "Epoch 65 \t Batch 80 \t Validation Loss: 27.12950345277786\n",
      "Epoch 65 \t Batch 100 \t Validation Loss: 27.174378957748413\n",
      "Epoch 65 \t Batch 120 \t Validation Loss: 27.63059340318044\n",
      "Epoch 65 \t Batch 140 \t Validation Loss: 27.640947457722255\n",
      "Epoch 65 \t Batch 160 \t Validation Loss: 29.415120702981948\n",
      "Epoch 65 \t Batch 180 \t Validation Loss: 32.782808017730716\n",
      "Epoch 65 \t Batch 200 \t Validation Loss: 34.16635429382324\n",
      "Epoch 65 \t Batch 220 \t Validation Loss: 35.42279423800382\n",
      "Epoch 65 \t Batch 240 \t Validation Loss: 35.78288079102834\n",
      "Epoch 65 \t Batch 260 \t Validation Loss: 37.84404774078956\n",
      "Epoch 65 \t Batch 280 \t Validation Loss: 38.90711558205741\n",
      "Epoch 65 \t Batch 300 \t Validation Loss: 39.889048144022624\n",
      "Epoch 65 \t Batch 320 \t Validation Loss: 40.29656240940094\n",
      "Epoch 65 \t Batch 340 \t Validation Loss: 40.21037925832412\n",
      "Epoch 65 \t Batch 360 \t Validation Loss: 40.026854281955295\n",
      "Epoch 65 \t Batch 380 \t Validation Loss: 40.22400027827213\n",
      "Epoch 65 \t Batch 400 \t Validation Loss: 39.78572229385376\n",
      "Epoch 65 \t Batch 420 \t Validation Loss: 39.79067448661441\n",
      "Epoch 65 \t Batch 440 \t Validation Loss: 39.47021702853116\n",
      "Epoch 65 \t Batch 460 \t Validation Loss: 39.64701285984205\n",
      "Epoch 65 \t Batch 480 \t Validation Loss: 40.09411336978277\n",
      "Epoch 65 \t Batch 500 \t Validation Loss: 39.808606353759764\n",
      "Epoch 65 \t Batch 520 \t Validation Loss: 39.52390951743493\n",
      "Epoch 65 \t Batch 540 \t Validation Loss: 39.17246817836055\n",
      "Epoch 65 \t Batch 560 \t Validation Loss: 38.86182812963213\n",
      "Epoch 65 \t Batch 580 \t Validation Loss: 38.48340647467251\n",
      "Epoch 65 \t Batch 600 \t Validation Loss: 38.62782302856445\n",
      "Epoch 65 Training Loss: 46.587474165721964 Validation Loss: 39.2421557779436\n",
      "Epoch 65 completed\n",
      "Epoch 66 \t Batch 20 \t Training Loss: 46.54647331237793\n",
      "Epoch 66 \t Batch 40 \t Training Loss: 46.90024833679199\n",
      "Epoch 66 \t Batch 60 \t Training Loss: 46.560634994506835\n",
      "Epoch 66 \t Batch 80 \t Training Loss: 46.48356614112854\n",
      "Epoch 66 \t Batch 100 \t Training Loss: 46.38343421936035\n",
      "Epoch 66 \t Batch 120 \t Training Loss: 46.358734703063966\n",
      "Epoch 66 \t Batch 140 \t Training Loss: 46.41683485848563\n",
      "Epoch 66 \t Batch 160 \t Training Loss: 46.29039645195007\n",
      "Epoch 66 \t Batch 180 \t Training Loss: 46.32937766181098\n",
      "Epoch 66 \t Batch 200 \t Training Loss: 46.29344358444214\n",
      "Epoch 66 \t Batch 220 \t Training Loss: 46.251761159029876\n",
      "Epoch 66 \t Batch 240 \t Training Loss: 46.301223357518516\n",
      "Epoch 66 \t Batch 260 \t Training Loss: 46.26226747952975\n",
      "Epoch 66 \t Batch 280 \t Training Loss: 46.266653646741595\n",
      "Epoch 66 \t Batch 300 \t Training Loss: 46.19159989674886\n",
      "Epoch 66 \t Batch 320 \t Training Loss: 46.23501995801926\n",
      "Epoch 66 \t Batch 340 \t Training Loss: 46.3460402544807\n",
      "Epoch 66 \t Batch 360 \t Training Loss: 46.28823300467597\n",
      "Epoch 66 \t Batch 380 \t Training Loss: 46.31366943560148\n",
      "Epoch 66 \t Batch 400 \t Training Loss: 46.41931464195252\n",
      "Epoch 66 \t Batch 420 \t Training Loss: 46.43646208445231\n",
      "Epoch 66 \t Batch 440 \t Training Loss: 46.44097779013894\n",
      "Epoch 66 \t Batch 460 \t Training Loss: 46.4147517328677\n",
      "Epoch 66 \t Batch 480 \t Training Loss: 46.37900612354279\n",
      "Epoch 66 \t Batch 500 \t Training Loss: 46.389016662597655\n",
      "Epoch 66 \t Batch 520 \t Training Loss: 46.35888437124399\n",
      "Epoch 66 \t Batch 540 \t Training Loss: 46.39411387266936\n",
      "Epoch 66 \t Batch 560 \t Training Loss: 46.418103899274556\n",
      "Epoch 66 \t Batch 580 \t Training Loss: 46.40657261815564\n",
      "Epoch 66 \t Batch 600 \t Training Loss: 46.40327442804972\n",
      "Epoch 66 \t Batch 620 \t Training Loss: 46.40953403595955\n",
      "Epoch 66 \t Batch 640 \t Training Loss: 46.41249675750733\n",
      "Epoch 66 \t Batch 660 \t Training Loss: 46.418882404674186\n",
      "Epoch 66 \t Batch 680 \t Training Loss: 46.413363669900335\n",
      "Epoch 66 \t Batch 700 \t Training Loss: 46.455991903032576\n",
      "Epoch 66 \t Batch 720 \t Training Loss: 46.5001591735416\n",
      "Epoch 66 \t Batch 740 \t Training Loss: 46.52473036276328\n",
      "Epoch 66 \t Batch 760 \t Training Loss: 46.5193694114685\n",
      "Epoch 66 \t Batch 780 \t Training Loss: 46.53039982624543\n",
      "Epoch 66 \t Batch 800 \t Training Loss: 46.52322075366974\n",
      "Epoch 66 \t Batch 820 \t Training Loss: 46.53896568577464\n",
      "Epoch 66 \t Batch 840 \t Training Loss: 46.57894430614653\n",
      "Epoch 66 \t Batch 860 \t Training Loss: 46.60190713128378\n",
      "Epoch 66 \t Batch 880 \t Training Loss: 46.600328627499664\n",
      "Epoch 66 \t Batch 900 \t Training Loss: 46.58983220418294\n",
      "Epoch 66 \t Batch 20 \t Validation Loss: 43.374366760253906\n",
      "Epoch 66 \t Batch 40 \t Validation Loss: 38.38965590000153\n",
      "Epoch 66 \t Batch 60 \t Validation Loss: 41.782850408554076\n",
      "Epoch 66 \t Batch 80 \t Validation Loss: 39.999712634086606\n",
      "Epoch 66 \t Batch 100 \t Validation Loss: 37.52409633636475\n",
      "Epoch 66 \t Batch 120 \t Validation Loss: 36.1988882223765\n",
      "Epoch 66 \t Batch 140 \t Validation Loss: 35.00059611456735\n",
      "Epoch 66 \t Batch 160 \t Validation Loss: 35.737616908550265\n",
      "Epoch 66 \t Batch 180 \t Validation Loss: 38.01431805822585\n",
      "Epoch 66 \t Batch 200 \t Validation Loss: 38.70963776111603\n",
      "Epoch 66 \t Batch 220 \t Validation Loss: 39.38105070374229\n",
      "Epoch 66 \t Batch 240 \t Validation Loss: 39.25069966316223\n",
      "Epoch 66 \t Batch 260 \t Validation Loss: 40.89291613285358\n",
      "Epoch 66 \t Batch 280 \t Validation Loss: 41.59285636629377\n",
      "Epoch 66 \t Batch 300 \t Validation Loss: 42.1466897392273\n",
      "Epoch 66 \t Batch 320 \t Validation Loss: 42.28540573120117\n",
      "Epoch 66 \t Batch 340 \t Validation Loss: 42.027103384803326\n",
      "Epoch 66 \t Batch 360 \t Validation Loss: 41.67989127900865\n",
      "Epoch 66 \t Batch 380 \t Validation Loss: 41.74938575845016\n",
      "Epoch 66 \t Batch 400 \t Validation Loss: 41.230104413032535\n",
      "Epoch 66 \t Batch 420 \t Validation Loss: 41.11099358513242\n",
      "Epoch 66 \t Batch 440 \t Validation Loss: 40.73677060387352\n",
      "Epoch 66 \t Batch 460 \t Validation Loss: 40.820430880007535\n",
      "Epoch 66 \t Batch 480 \t Validation Loss: 41.18448804616928\n",
      "Epoch 66 \t Batch 500 \t Validation Loss: 40.7886661529541\n",
      "Epoch 66 \t Batch 520 \t Validation Loss: 40.466833670322714\n",
      "Epoch 66 \t Batch 540 \t Validation Loss: 40.140491028185245\n",
      "Epoch 66 \t Batch 560 \t Validation Loss: 39.85133819069181\n",
      "Epoch 66 \t Batch 580 \t Validation Loss: 39.54012487839008\n",
      "Epoch 66 \t Batch 600 \t Validation Loss: 39.69081286271413\n",
      "Epoch 66 Training Loss: 46.590890017044586 Validation Loss: 40.27606221452936\n",
      "Epoch 66 completed\n",
      "Epoch 67 \t Batch 20 \t Training Loss: 45.25023002624512\n",
      "Epoch 67 \t Batch 40 \t Training Loss: 46.335488891601564\n",
      "Epoch 67 \t Batch 60 \t Training Loss: 46.293833351135255\n",
      "Epoch 67 \t Batch 80 \t Training Loss: 46.40566263198853\n",
      "Epoch 67 \t Batch 100 \t Training Loss: 46.47984302520752\n",
      "Epoch 67 \t Batch 120 \t Training Loss: 46.534798844655356\n",
      "Epoch 67 \t Batch 140 \t Training Loss: 46.48137411390032\n",
      "Epoch 67 \t Batch 160 \t Training Loss: 46.460361456871034\n",
      "Epoch 67 \t Batch 180 \t Training Loss: 46.40103221469455\n",
      "Epoch 67 \t Batch 200 \t Training Loss: 46.16365703582764\n",
      "Epoch 67 \t Batch 220 \t Training Loss: 46.25236585790461\n",
      "Epoch 67 \t Batch 240 \t Training Loss: 46.245349105199175\n",
      "Epoch 67 \t Batch 260 \t Training Loss: 46.328878652132474\n",
      "Epoch 67 \t Batch 280 \t Training Loss: 46.28652740206037\n",
      "Epoch 67 \t Batch 300 \t Training Loss: 46.343228098551435\n",
      "Epoch 67 \t Batch 320 \t Training Loss: 46.36095888614655\n",
      "Epoch 67 \t Batch 340 \t Training Loss: 46.338203239440915\n",
      "Epoch 67 \t Batch 360 \t Training Loss: 46.30488183763292\n",
      "Epoch 67 \t Batch 380 \t Training Loss: 46.325027315240156\n",
      "Epoch 67 \t Batch 400 \t Training Loss: 46.319164247512816\n",
      "Epoch 67 \t Batch 420 \t Training Loss: 46.31209823063442\n",
      "Epoch 67 \t Batch 440 \t Training Loss: 46.29230270385742\n",
      "Epoch 67 \t Batch 460 \t Training Loss: 46.25743517668351\n",
      "Epoch 67 \t Batch 480 \t Training Loss: 46.28529183864593\n",
      "Epoch 67 \t Batch 500 \t Training Loss: 46.28870969390869\n",
      "Epoch 67 \t Batch 520 \t Training Loss: 46.349065413841835\n",
      "Epoch 67 \t Batch 540 \t Training Loss: 46.350474244576915\n",
      "Epoch 67 \t Batch 560 \t Training Loss: 46.384655359813145\n",
      "Epoch 67 \t Batch 580 \t Training Loss: 46.40597430919779\n",
      "Epoch 67 \t Batch 600 \t Training Loss: 46.44653129577637\n",
      "Epoch 67 \t Batch 620 \t Training Loss: 46.50586427257907\n",
      "Epoch 67 \t Batch 640 \t Training Loss: 46.54307472705841\n",
      "Epoch 67 \t Batch 660 \t Training Loss: 46.586794419722125\n",
      "Epoch 67 \t Batch 680 \t Training Loss: 46.62084451563218\n",
      "Epoch 67 \t Batch 700 \t Training Loss: 46.62264870779855\n",
      "Epoch 67 \t Batch 720 \t Training Loss: 46.61558515760634\n",
      "Epoch 67 \t Batch 740 \t Training Loss: 46.60164835130846\n",
      "Epoch 67 \t Batch 760 \t Training Loss: 46.588574178595294\n",
      "Epoch 67 \t Batch 780 \t Training Loss: 46.63535373883369\n",
      "Epoch 67 \t Batch 800 \t Training Loss: 46.64868757724762\n",
      "Epoch 67 \t Batch 820 \t Training Loss: 46.608810448065036\n",
      "Epoch 67 \t Batch 840 \t Training Loss: 46.59927033923921\n",
      "Epoch 67 \t Batch 860 \t Training Loss: 46.62178169073061\n",
      "Epoch 67 \t Batch 880 \t Training Loss: 46.62285174889998\n",
      "Epoch 67 \t Batch 900 \t Training Loss: 46.574915152655706\n",
      "Epoch 67 \t Batch 20 \t Validation Loss: 44.194619750976564\n",
      "Epoch 67 \t Batch 40 \t Validation Loss: 38.52068257331848\n",
      "Epoch 67 \t Batch 60 \t Validation Loss: 42.25411734580994\n",
      "Epoch 67 \t Batch 80 \t Validation Loss: 40.73785194158554\n",
      "Epoch 67 \t Batch 100 \t Validation Loss: 38.11683508872986\n",
      "Epoch 67 \t Batch 120 \t Validation Loss: 36.77868336836497\n",
      "Epoch 67 \t Batch 140 \t Validation Loss: 35.515573671885896\n",
      "Epoch 67 \t Batch 160 \t Validation Loss: 36.18159518837929\n",
      "Epoch 67 \t Batch 180 \t Validation Loss: 38.578069268332584\n",
      "Epoch 67 \t Batch 200 \t Validation Loss: 39.21824801921844\n",
      "Epoch 67 \t Batch 220 \t Validation Loss: 39.92313909097151\n",
      "Epoch 67 \t Batch 240 \t Validation Loss: 39.80107985734939\n",
      "Epoch 67 \t Batch 260 \t Validation Loss: 41.462511407412016\n",
      "Epoch 67 \t Batch 280 \t Validation Loss: 42.16706062725612\n",
      "Epoch 67 \t Batch 300 \t Validation Loss: 42.77191721598307\n",
      "Epoch 67 \t Batch 320 \t Validation Loss: 42.91723594069481\n",
      "Epoch 67 \t Batch 340 \t Validation Loss: 42.597360162174\n",
      "Epoch 67 \t Batch 360 \t Validation Loss: 42.2177092605167\n",
      "Epoch 67 \t Batch 380 \t Validation Loss: 42.24182777906719\n",
      "Epoch 67 \t Batch 400 \t Validation Loss: 41.65333109378815\n",
      "Epoch 67 \t Batch 420 \t Validation Loss: 41.48913696834019\n",
      "Epoch 67 \t Batch 440 \t Validation Loss: 41.0344231562181\n",
      "Epoch 67 \t Batch 460 \t Validation Loss: 41.08575746702111\n",
      "Epoch 67 \t Batch 480 \t Validation Loss: 41.42604970137278\n",
      "Epoch 67 \t Batch 500 \t Validation Loss: 41.01832015228271\n",
      "Epoch 67 \t Batch 520 \t Validation Loss: 40.65129271470583\n",
      "Epoch 67 \t Batch 540 \t Validation Loss: 40.272414958035505\n",
      "Epoch 67 \t Batch 560 \t Validation Loss: 39.94372388465064\n",
      "Epoch 67 \t Batch 580 \t Validation Loss: 39.564680466158634\n",
      "Epoch 67 \t Batch 600 \t Validation Loss: 39.6871729071935\n",
      "Epoch 67 Training Loss: 46.5558531832929 Validation Loss: 40.26021842987507\n",
      "Epoch 67 completed\n",
      "Epoch 68 \t Batch 20 \t Training Loss: 46.64287815093994\n",
      "Epoch 68 \t Batch 40 \t Training Loss: 46.04656114578247\n",
      "Epoch 68 \t Batch 60 \t Training Loss: 46.837670453389485\n",
      "Epoch 68 \t Batch 80 \t Training Loss: 46.782260465621945\n",
      "Epoch 68 \t Batch 100 \t Training Loss: 46.80737968444824\n",
      "Epoch 68 \t Batch 120 \t Training Loss: 46.64070380528768\n",
      "Epoch 68 \t Batch 140 \t Training Loss: 46.566257286071775\n",
      "Epoch 68 \t Batch 160 \t Training Loss: 46.50635676383972\n",
      "Epoch 68 \t Batch 180 \t Training Loss: 46.31435254414876\n",
      "Epoch 68 \t Batch 200 \t Training Loss: 46.41247247695923\n",
      "Epoch 68 \t Batch 220 \t Training Loss: 46.342204128612174\n",
      "Epoch 68 \t Batch 240 \t Training Loss: 46.15543514887492\n",
      "Epoch 68 \t Batch 260 \t Training Loss: 46.22385774759146\n",
      "Epoch 68 \t Batch 280 \t Training Loss: 46.250281524658206\n",
      "Epoch 68 \t Batch 300 \t Training Loss: 46.18496171315511\n",
      "Epoch 68 \t Batch 320 \t Training Loss: 46.23734748363495\n",
      "Epoch 68 \t Batch 340 \t Training Loss: 46.22804149178898\n",
      "Epoch 68 \t Batch 360 \t Training Loss: 46.18168987698025\n",
      "Epoch 68 \t Batch 380 \t Training Loss: 46.19843767065751\n",
      "Epoch 68 \t Batch 400 \t Training Loss: 46.16005319595337\n",
      "Epoch 68 \t Batch 420 \t Training Loss: 46.17427956717355\n",
      "Epoch 68 \t Batch 440 \t Training Loss: 46.21461205915971\n",
      "Epoch 68 \t Batch 460 \t Training Loss: 46.19904758619226\n",
      "Epoch 68 \t Batch 480 \t Training Loss: 46.2948618888855\n",
      "Epoch 68 \t Batch 500 \t Training Loss: 46.2702092666626\n",
      "Epoch 68 \t Batch 520 \t Training Loss: 46.30696094219501\n",
      "Epoch 68 \t Batch 540 \t Training Loss: 46.3316849037453\n",
      "Epoch 68 \t Batch 560 \t Training Loss: 46.279982812064034\n",
      "Epoch 68 \t Batch 580 \t Training Loss: 46.32729469167775\n",
      "Epoch 68 \t Batch 600 \t Training Loss: 46.349530703226726\n",
      "Epoch 68 \t Batch 620 \t Training Loss: 46.334564861174556\n",
      "Epoch 68 \t Batch 640 \t Training Loss: 46.30669050812721\n",
      "Epoch 68 \t Batch 660 \t Training Loss: 46.3237914056489\n",
      "Epoch 68 \t Batch 680 \t Training Loss: 46.30519183663761\n",
      "Epoch 68 \t Batch 700 \t Training Loss: 46.32239366803851\n",
      "Epoch 68 \t Batch 720 \t Training Loss: 46.33035414483812\n",
      "Epoch 68 \t Batch 740 \t Training Loss: 46.34229421873351\n",
      "Epoch 68 \t Batch 760 \t Training Loss: 46.397859066411065\n",
      "Epoch 68 \t Batch 780 \t Training Loss: 46.4056603602874\n",
      "Epoch 68 \t Batch 800 \t Training Loss: 46.42049354076386\n",
      "Epoch 68 \t Batch 820 \t Training Loss: 46.47352794088968\n",
      "Epoch 68 \t Batch 840 \t Training Loss: 46.504626828148254\n",
      "Epoch 68 \t Batch 860 \t Training Loss: 46.49778443713521\n",
      "Epoch 68 \t Batch 880 \t Training Loss: 46.5364485653964\n",
      "Epoch 68 \t Batch 900 \t Training Loss: 46.551271472507054\n",
      "Epoch 68 \t Batch 20 \t Validation Loss: 37.87694163322449\n",
      "Epoch 68 \t Batch 40 \t Validation Loss: 33.73584585189819\n",
      "Epoch 68 \t Batch 60 \t Validation Loss: 36.766609859466556\n",
      "Epoch 68 \t Batch 80 \t Validation Loss: 35.52650672197342\n",
      "Epoch 68 \t Batch 100 \t Validation Loss: 33.87306990623474\n",
      "Epoch 68 \t Batch 120 \t Validation Loss: 33.20241490205129\n",
      "Epoch 68 \t Batch 140 \t Validation Loss: 32.452665649141586\n",
      "Epoch 68 \t Batch 160 \t Validation Loss: 33.537205630540846\n",
      "Epoch 68 \t Batch 180 \t Validation Loss: 36.20485387908088\n",
      "Epoch 68 \t Batch 200 \t Validation Loss: 37.12136623859406\n",
      "Epoch 68 \t Batch 220 \t Validation Loss: 37.982723370465365\n",
      "Epoch 68 \t Batch 240 \t Validation Loss: 38.02895961999893\n",
      "Epoch 68 \t Batch 260 \t Validation Loss: 39.793818987332855\n",
      "Epoch 68 \t Batch 280 \t Validation Loss: 40.59847004413605\n",
      "Epoch 68 \t Batch 300 \t Validation Loss: 41.34563296318054\n",
      "Epoch 68 \t Batch 320 \t Validation Loss: 41.581329342722896\n",
      "Epoch 68 \t Batch 340 \t Validation Loss: 41.368793877433326\n",
      "Epoch 68 \t Batch 360 \t Validation Loss: 41.064523344569736\n",
      "Epoch 68 \t Batch 380 \t Validation Loss: 41.14267043816416\n",
      "Epoch 68 \t Batch 400 \t Validation Loss: 40.61865568399429\n",
      "Epoch 68 \t Batch 420 \t Validation Loss: 40.529128065563384\n",
      "Epoch 68 \t Batch 440 \t Validation Loss: 40.139040444113995\n",
      "Epoch 68 \t Batch 460 \t Validation Loss: 40.23977843160215\n",
      "Epoch 68 \t Batch 480 \t Validation Loss: 40.638446203867595\n",
      "Epoch 68 \t Batch 500 \t Validation Loss: 40.28664492034912\n",
      "Epoch 68 \t Batch 520 \t Validation Loss: 39.964954864061795\n",
      "Epoch 68 \t Batch 540 \t Validation Loss: 39.611508500134505\n",
      "Epoch 68 \t Batch 560 \t Validation Loss: 39.34231874261584\n",
      "Epoch 68 \t Batch 580 \t Validation Loss: 39.053748676694674\n",
      "Epoch 68 \t Batch 600 \t Validation Loss: 39.183723080952966\n",
      "Epoch 68 Training Loss: 46.53350009315017 Validation Loss: 39.827065229415894\n",
      "Epoch 68 completed\n",
      "Epoch 69 \t Batch 20 \t Training Loss: 45.66737003326416\n",
      "Epoch 69 \t Batch 40 \t Training Loss: 46.134175872802736\n",
      "Epoch 69 \t Batch 60 \t Training Loss: 45.70748914082845\n",
      "Epoch 69 \t Batch 80 \t Training Loss: 46.03664755821228\n",
      "Epoch 69 \t Batch 100 \t Training Loss: 45.92702651977539\n",
      "Epoch 69 \t Batch 120 \t Training Loss: 46.14633289972941\n",
      "Epoch 69 \t Batch 140 \t Training Loss: 46.045846176147464\n",
      "Epoch 69 \t Batch 160 \t Training Loss: 46.133959436416625\n",
      "Epoch 69 \t Batch 180 \t Training Loss: 46.130669657389326\n",
      "Epoch 69 \t Batch 200 \t Training Loss: 46.087282524108886\n",
      "Epoch 69 \t Batch 220 \t Training Loss: 46.086208031394264\n",
      "Epoch 69 \t Batch 240 \t Training Loss: 46.04390862782796\n",
      "Epoch 69 \t Batch 260 \t Training Loss: 46.11484837165246\n",
      "Epoch 69 \t Batch 280 \t Training Loss: 46.07163649967739\n",
      "Epoch 69 \t Batch 300 \t Training Loss: 46.13486324310303\n",
      "Epoch 69 \t Batch 320 \t Training Loss: 46.138855803012845\n",
      "Epoch 69 \t Batch 340 \t Training Loss: 46.12036707261029\n",
      "Epoch 69 \t Batch 360 \t Training Loss: 46.19333632787069\n",
      "Epoch 69 \t Batch 380 \t Training Loss: 46.17673555675306\n",
      "Epoch 69 \t Batch 400 \t Training Loss: 46.22469548225403\n",
      "Epoch 69 \t Batch 420 \t Training Loss: 46.25908164069766\n",
      "Epoch 69 \t Batch 440 \t Training Loss: 46.304440611059015\n",
      "Epoch 69 \t Batch 460 \t Training Loss: 46.35395706840183\n",
      "Epoch 69 \t Batch 480 \t Training Loss: 46.41430335839589\n",
      "Epoch 69 \t Batch 500 \t Training Loss: 46.372173774719236\n",
      "Epoch 69 \t Batch 520 \t Training Loss: 46.339818734389084\n",
      "Epoch 69 \t Batch 540 \t Training Loss: 46.40172864419443\n",
      "Epoch 69 \t Batch 560 \t Training Loss: 46.44607653617859\n",
      "Epoch 69 \t Batch 580 \t Training Loss: 46.431019993486075\n",
      "Epoch 69 \t Batch 600 \t Training Loss: 46.45610001881917\n",
      "Epoch 69 \t Batch 620 \t Training Loss: 46.445465752386276\n",
      "Epoch 69 \t Batch 640 \t Training Loss: 46.40216079950333\n",
      "Epoch 69 \t Batch 660 \t Training Loss: 46.48496279860988\n",
      "Epoch 69 \t Batch 680 \t Training Loss: 46.52300397087546\n",
      "Epoch 69 \t Batch 700 \t Training Loss: 46.50535545894078\n",
      "Epoch 69 \t Batch 720 \t Training Loss: 46.52344490687052\n",
      "Epoch 69 \t Batch 740 \t Training Loss: 46.53944561932538\n",
      "Epoch 69 \t Batch 760 \t Training Loss: 46.53387044103522\n",
      "Epoch 69 \t Batch 780 \t Training Loss: 46.55434645628318\n",
      "Epoch 69 \t Batch 800 \t Training Loss: 46.53998028755188\n",
      "Epoch 69 \t Batch 820 \t Training Loss: 46.54175516919392\n",
      "Epoch 69 \t Batch 840 \t Training Loss: 46.541086460295176\n",
      "Epoch 69 \t Batch 860 \t Training Loss: 46.530758902083996\n",
      "Epoch 69 \t Batch 880 \t Training Loss: 46.53252628066323\n",
      "Epoch 69 \t Batch 900 \t Training Loss: 46.53783499823676\n",
      "Epoch 69 \t Batch 20 \t Validation Loss: 33.407461643218994\n",
      "Epoch 69 \t Batch 40 \t Validation Loss: 30.54761266708374\n",
      "Epoch 69 \t Batch 60 \t Validation Loss: 33.02648005485535\n",
      "Epoch 69 \t Batch 80 \t Validation Loss: 32.2110494017601\n",
      "Epoch 69 \t Batch 100 \t Validation Loss: 31.279192781448366\n",
      "Epoch 69 \t Batch 120 \t Validation Loss: 31.036817558606465\n",
      "Epoch 69 \t Batch 140 \t Validation Loss: 30.635614606312345\n",
      "Epoch 69 \t Batch 160 \t Validation Loss: 32.05982132554054\n",
      "Epoch 69 \t Batch 180 \t Validation Loss: 35.18520522647434\n",
      "Epoch 69 \t Batch 200 \t Validation Loss: 36.38743761062622\n",
      "Epoch 69 \t Batch 220 \t Validation Loss: 37.49508408633145\n",
      "Epoch 69 \t Batch 240 \t Validation Loss: 37.681769120693204\n",
      "Epoch 69 \t Batch 260 \t Validation Loss: 39.66043269450848\n",
      "Epoch 69 \t Batch 280 \t Validation Loss: 40.64054387296949\n",
      "Epoch 69 \t Batch 300 \t Validation Loss: 41.52043447176615\n",
      "Epoch 69 \t Batch 320 \t Validation Loss: 41.83906130492687\n",
      "Epoch 69 \t Batch 340 \t Validation Loss: 41.642885255813596\n",
      "Epoch 69 \t Batch 360 \t Validation Loss: 41.39512049622006\n",
      "Epoch 69 \t Batch 380 \t Validation Loss: 41.520725935383844\n",
      "Epoch 69 \t Batch 400 \t Validation Loss: 40.98679030179977\n",
      "Epoch 69 \t Batch 420 \t Validation Loss: 40.908676444916495\n",
      "Epoch 69 \t Batch 440 \t Validation Loss: 40.50913283391432\n",
      "Epoch 69 \t Batch 460 \t Validation Loss: 40.59777669699296\n",
      "Epoch 69 \t Batch 480 \t Validation Loss: 41.003549573818844\n",
      "Epoch 69 \t Batch 500 \t Validation Loss: 40.65465635108948\n",
      "Epoch 69 \t Batch 520 \t Validation Loss: 40.326353852565475\n",
      "Epoch 69 \t Batch 540 \t Validation Loss: 39.99322278234694\n",
      "Epoch 69 \t Batch 560 \t Validation Loss: 39.75598251308713\n",
      "Epoch 69 \t Batch 580 \t Validation Loss: 39.552447346983286\n",
      "Epoch 69 \t Batch 600 \t Validation Loss: 39.70329384009043\n",
      "Epoch 69 Training Loss: 46.54082675380561 Validation Loss: 40.30836394390503\n",
      "Epoch 69 completed\n",
      "Epoch 70 \t Batch 20 \t Training Loss: 47.61556529998779\n",
      "Epoch 70 \t Batch 40 \t Training Loss: 46.76229667663574\n",
      "Epoch 70 \t Batch 60 \t Training Loss: 46.577957725524904\n",
      "Epoch 70 \t Batch 80 \t Training Loss: 46.540435218811034\n",
      "Epoch 70 \t Batch 100 \t Training Loss: 46.71430442810058\n",
      "Epoch 70 \t Batch 120 \t Training Loss: 46.772653834025064\n",
      "Epoch 70 \t Batch 140 \t Training Loss: 46.71733867100307\n",
      "Epoch 70 \t Batch 160 \t Training Loss: 46.69656562805176\n",
      "Epoch 70 \t Batch 180 \t Training Loss: 46.80942567189535\n",
      "Epoch 70 \t Batch 200 \t Training Loss: 46.788390922546384\n",
      "Epoch 70 \t Batch 220 \t Training Loss: 46.74805140061812\n",
      "Epoch 70 \t Batch 240 \t Training Loss: 46.80075348218282\n",
      "Epoch 70 \t Batch 260 \t Training Loss: 46.75396484961877\n",
      "Epoch 70 \t Batch 280 \t Training Loss: 46.80505195345197\n",
      "Epoch 70 \t Batch 300 \t Training Loss: 46.76452429453532\n",
      "Epoch 70 \t Batch 320 \t Training Loss: 46.61249074935913\n",
      "Epoch 70 \t Batch 340 \t Training Loss: 46.66010187934427\n",
      "Epoch 70 \t Batch 360 \t Training Loss: 46.663812202877466\n",
      "Epoch 70 \t Batch 380 \t Training Loss: 46.61206310673764\n",
      "Epoch 70 \t Batch 400 \t Training Loss: 46.59507781982422\n",
      "Epoch 70 \t Batch 420 \t Training Loss: 46.55490734463646\n",
      "Epoch 70 \t Batch 440 \t Training Loss: 46.58728633360429\n",
      "Epoch 70 \t Batch 460 \t Training Loss: 46.596093882685125\n",
      "Epoch 70 \t Batch 480 \t Training Loss: 46.58659932613373\n",
      "Epoch 70 \t Batch 500 \t Training Loss: 46.67128016662598\n",
      "Epoch 70 \t Batch 520 \t Training Loss: 46.69058933991652\n",
      "Epoch 70 \t Batch 540 \t Training Loss: 46.62148257361518\n",
      "Epoch 70 \t Batch 560 \t Training Loss: 46.673516028268\n",
      "Epoch 70 \t Batch 580 \t Training Loss: 46.65476652342698\n",
      "Epoch 70 \t Batch 600 \t Training Loss: 46.59375393549601\n",
      "Epoch 70 \t Batch 620 \t Training Loss: 46.568086888713225\n",
      "Epoch 70 \t Batch 640 \t Training Loss: 46.555551236867906\n",
      "Epoch 70 \t Batch 660 \t Training Loss: 46.563208770751956\n",
      "Epoch 70 \t Batch 680 \t Training Loss: 46.55779928319595\n",
      "Epoch 70 \t Batch 700 \t Training Loss: 46.524895488194055\n",
      "Epoch 70 \t Batch 720 \t Training Loss: 46.564837270312836\n",
      "Epoch 70 \t Batch 740 \t Training Loss: 46.52901516476193\n",
      "Epoch 70 \t Batch 760 \t Training Loss: 46.515194330717385\n",
      "Epoch 70 \t Batch 780 \t Training Loss: 46.535065269470216\n",
      "Epoch 70 \t Batch 800 \t Training Loss: 46.51425758361816\n",
      "Epoch 70 \t Batch 820 \t Training Loss: 46.56564914424245\n",
      "Epoch 70 \t Batch 840 \t Training Loss: 46.577412691570466\n",
      "Epoch 70 \t Batch 860 \t Training Loss: 46.54641771538313\n",
      "Epoch 70 \t Batch 880 \t Training Loss: 46.557123470306394\n",
      "Epoch 70 \t Batch 900 \t Training Loss: 46.524588652716744\n",
      "Epoch 70 \t Batch 20 \t Validation Loss: 34.45941843986511\n",
      "Epoch 70 \t Batch 40 \t Validation Loss: 31.91112883090973\n",
      "Epoch 70 \t Batch 60 \t Validation Loss: 34.3347984790802\n",
      "Epoch 70 \t Batch 80 \t Validation Loss: 33.56446418762207\n",
      "Epoch 70 \t Batch 100 \t Validation Loss: 32.094612197875975\n",
      "Epoch 70 \t Batch 120 \t Validation Loss: 31.51471461455027\n",
      "Epoch 70 \t Batch 140 \t Validation Loss: 30.882498148509434\n",
      "Epoch 70 \t Batch 160 \t Validation Loss: 32.0645448744297\n",
      "Epoch 70 \t Batch 180 \t Validation Loss: 34.76220737563239\n",
      "Epoch 70 \t Batch 200 \t Validation Loss: 35.80636967182159\n",
      "Epoch 70 \t Batch 220 \t Validation Loss: 36.67749466896057\n",
      "Epoch 70 \t Batch 240 \t Validation Loss: 36.74214426676432\n",
      "Epoch 70 \t Batch 260 \t Validation Loss: 38.54583914096539\n",
      "Epoch 70 \t Batch 280 \t Validation Loss: 39.41541767460959\n",
      "Epoch 70 \t Batch 300 \t Validation Loss: 40.15500332196554\n",
      "Epoch 70 \t Batch 320 \t Validation Loss: 40.41551742851734\n",
      "Epoch 70 \t Batch 340 \t Validation Loss: 40.282352646659405\n",
      "Epoch 70 \t Batch 360 \t Validation Loss: 40.01020848751068\n",
      "Epoch 70 \t Batch 380 \t Validation Loss: 40.16220003680179\n",
      "Epoch 70 \t Batch 400 \t Validation Loss: 39.74040869951248\n",
      "Epoch 70 \t Batch 420 \t Validation Loss: 39.745690148217335\n",
      "Epoch 70 \t Batch 440 \t Validation Loss: 39.4858870527961\n",
      "Epoch 70 \t Batch 460 \t Validation Loss: 39.634620967118636\n",
      "Epoch 70 \t Batch 480 \t Validation Loss: 40.0681590616703\n",
      "Epoch 70 \t Batch 500 \t Validation Loss: 39.73546492958069\n",
      "Epoch 70 \t Batch 520 \t Validation Loss: 39.46533099871415\n",
      "Epoch 70 \t Batch 540 \t Validation Loss: 39.17655889193217\n",
      "Epoch 70 \t Batch 560 \t Validation Loss: 38.98176035370145\n",
      "Epoch 70 \t Batch 580 \t Validation Loss: 38.75664762792916\n",
      "Epoch 70 \t Batch 600 \t Validation Loss: 38.94363242308299\n",
      "Epoch 70 Training Loss: 46.51426406411074 Validation Loss: 39.57164544099337\n",
      "Epoch 70 completed\n",
      "Epoch 71 \t Batch 20 \t Training Loss: 46.561499786376956\n",
      "Epoch 71 \t Batch 40 \t Training Loss: 46.5105016708374\n",
      "Epoch 71 \t Batch 60 \t Training Loss: 46.114591153462726\n",
      "Epoch 71 \t Batch 80 \t Training Loss: 46.67980589866638\n",
      "Epoch 71 \t Batch 100 \t Training Loss: 46.79527576446533\n",
      "Epoch 71 \t Batch 120 \t Training Loss: 47.13944622675578\n",
      "Epoch 71 \t Batch 140 \t Training Loss: 47.27438250950404\n",
      "Epoch 71 \t Batch 160 \t Training Loss: 47.11353840827942\n",
      "Epoch 71 \t Batch 180 \t Training Loss: 46.94590004814996\n",
      "Epoch 71 \t Batch 200 \t Training Loss: 46.83656513214111\n",
      "Epoch 71 \t Batch 220 \t Training Loss: 46.787927800958805\n",
      "Epoch 71 \t Batch 240 \t Training Loss: 46.770248889923096\n",
      "Epoch 71 \t Batch 260 \t Training Loss: 46.68818626403809\n",
      "Epoch 71 \t Batch 280 \t Training Loss: 46.664173752920966\n",
      "Epoch 71 \t Batch 300 \t Training Loss: 46.558821144104\n",
      "Epoch 71 \t Batch 320 \t Training Loss: 46.456152009963986\n",
      "Epoch 71 \t Batch 340 \t Training Loss: 46.465974493587716\n",
      "Epoch 71 \t Batch 360 \t Training Loss: 46.516793696085614\n",
      "Epoch 71 \t Batch 380 \t Training Loss: 46.466296567414936\n",
      "Epoch 71 \t Batch 400 \t Training Loss: 46.5100208568573\n",
      "Epoch 71 \t Batch 420 \t Training Loss: 46.44657626379104\n",
      "Epoch 71 \t Batch 440 \t Training Loss: 46.46954676021229\n",
      "Epoch 71 \t Batch 460 \t Training Loss: 46.46814478998599\n",
      "Epoch 71 \t Batch 480 \t Training Loss: 46.51887730757395\n",
      "Epoch 71 \t Batch 500 \t Training Loss: 46.55086610412598\n",
      "Epoch 71 \t Batch 520 \t Training Loss: 46.5021236199599\n",
      "Epoch 71 \t Batch 540 \t Training Loss: 46.51601407792833\n",
      "Epoch 71 \t Batch 560 \t Training Loss: 46.48740852219718\n",
      "Epoch 71 \t Batch 580 \t Training Loss: 46.48884533849256\n",
      "Epoch 71 \t Batch 600 \t Training Loss: 46.509052244822186\n",
      "Epoch 71 \t Batch 620 \t Training Loss: 46.52250023503457\n",
      "Epoch 71 \t Batch 640 \t Training Loss: 46.54361139535904\n",
      "Epoch 71 \t Batch 660 \t Training Loss: 46.540015891104034\n",
      "Epoch 71 \t Batch 680 \t Training Loss: 46.515978313894834\n",
      "Epoch 71 \t Batch 700 \t Training Loss: 46.51789069584438\n",
      "Epoch 71 \t Batch 720 \t Training Loss: 46.49215120739407\n",
      "Epoch 71 \t Batch 740 \t Training Loss: 46.509828505644926\n",
      "Epoch 71 \t Batch 760 \t Training Loss: 46.48984876431917\n",
      "Epoch 71 \t Batch 780 \t Training Loss: 46.53289514688345\n",
      "Epoch 71 \t Batch 800 \t Training Loss: 46.504059205055235\n",
      "Epoch 71 \t Batch 820 \t Training Loss: 46.49981049793522\n",
      "Epoch 71 \t Batch 840 \t Training Loss: 46.50770417622157\n",
      "Epoch 71 \t Batch 860 \t Training Loss: 46.514060379738034\n",
      "Epoch 71 \t Batch 880 \t Training Loss: 46.51241224028848\n",
      "Epoch 71 \t Batch 900 \t Training Loss: 46.5030140431722\n",
      "Epoch 71 \t Batch 20 \t Validation Loss: 36.210361528396604\n",
      "Epoch 71 \t Batch 40 \t Validation Loss: 31.86246829032898\n",
      "Epoch 71 \t Batch 60 \t Validation Loss: 34.92226297060649\n",
      "Epoch 71 \t Batch 80 \t Validation Loss: 33.778740698099135\n",
      "Epoch 71 \t Batch 100 \t Validation Loss: 32.53627602100372\n",
      "Epoch 71 \t Batch 120 \t Validation Loss: 32.11694589853287\n",
      "Epoch 71 \t Batch 140 \t Validation Loss: 31.502984220641\n",
      "Epoch 71 \t Batch 160 \t Validation Loss: 32.804847499728204\n",
      "Epoch 71 \t Batch 180 \t Validation Loss: 35.694093897607594\n",
      "Epoch 71 \t Batch 200 \t Validation Loss: 36.72865829706192\n",
      "Epoch 71 \t Batch 220 \t Validation Loss: 37.724725075201555\n",
      "Epoch 71 \t Batch 240 \t Validation Loss: 37.82942849198977\n",
      "Epoch 71 \t Batch 260 \t Validation Loss: 39.724498717601485\n",
      "Epoch 71 \t Batch 280 \t Validation Loss: 40.67473731551851\n",
      "Epoch 71 \t Batch 300 \t Validation Loss: 41.4882022968928\n",
      "Epoch 71 \t Batch 320 \t Validation Loss: 41.779645739495756\n",
      "Epoch 71 \t Batch 340 \t Validation Loss: 41.572034520261425\n",
      "Epoch 71 \t Batch 360 \t Validation Loss: 41.310136279794904\n",
      "Epoch 71 \t Batch 380 \t Validation Loss: 41.48106712918533\n",
      "Epoch 71 \t Batch 400 \t Validation Loss: 40.99968919634819\n",
      "Epoch 71 \t Batch 420 \t Validation Loss: 40.936053374835424\n",
      "Epoch 71 \t Batch 440 \t Validation Loss: 40.62929634072564\n",
      "Epoch 71 \t Batch 460 \t Validation Loss: 40.769079825152524\n",
      "Epoch 71 \t Batch 480 \t Validation Loss: 41.156750002503394\n",
      "Epoch 71 \t Batch 500 \t Validation Loss: 40.79807799434662\n",
      "Epoch 71 \t Batch 520 \t Validation Loss: 40.55191213075931\n",
      "Epoch 71 \t Batch 540 \t Validation Loss: 40.229223451790986\n",
      "Epoch 71 \t Batch 560 \t Validation Loss: 39.993249878713065\n",
      "Epoch 71 \t Batch 580 \t Validation Loss: 39.81221736628434\n",
      "Epoch 71 \t Batch 600 \t Validation Loss: 39.958549025058744\n",
      "Epoch 71 Training Loss: 46.48457949190129 Validation Loss: 40.59346860027932\n",
      "Epoch 71 completed\n",
      "Epoch 72 \t Batch 20 \t Training Loss: 47.68538990020752\n",
      "Epoch 72 \t Batch 40 \t Training Loss: 46.852629852294925\n",
      "Epoch 72 \t Batch 60 \t Training Loss: 46.69385611216227\n",
      "Epoch 72 \t Batch 80 \t Training Loss: 47.02008867263794\n",
      "Epoch 72 \t Batch 100 \t Training Loss: 46.74069164276123\n",
      "Epoch 72 \t Batch 120 \t Training Loss: 46.823131783803305\n",
      "Epoch 72 \t Batch 140 \t Training Loss: 46.812503242492674\n",
      "Epoch 72 \t Batch 160 \t Training Loss: 46.787910151481626\n",
      "Epoch 72 \t Batch 180 \t Training Loss: 46.66289439731174\n",
      "Epoch 72 \t Batch 200 \t Training Loss: 46.62810188293457\n",
      "Epoch 72 \t Batch 220 \t Training Loss: 46.75780823447487\n",
      "Epoch 72 \t Batch 240 \t Training Loss: 46.683510557810465\n",
      "Epoch 72 \t Batch 260 \t Training Loss: 46.782042092543385\n",
      "Epoch 72 \t Batch 280 \t Training Loss: 46.77982620511736\n",
      "Epoch 72 \t Batch 300 \t Training Loss: 46.7327789179484\n",
      "Epoch 72 \t Batch 320 \t Training Loss: 46.67989128828049\n",
      "Epoch 72 \t Batch 340 \t Training Loss: 46.63705472385182\n",
      "Epoch 72 \t Batch 360 \t Training Loss: 46.6456603580051\n",
      "Epoch 72 \t Batch 380 \t Training Loss: 46.62725178567987\n",
      "Epoch 72 \t Batch 400 \t Training Loss: 46.645071029663086\n",
      "Epoch 72 \t Batch 420 \t Training Loss: 46.57418236505418\n",
      "Epoch 72 \t Batch 440 \t Training Loss: 46.509072026339446\n",
      "Epoch 72 \t Batch 460 \t Training Loss: 46.52874733468761\n",
      "Epoch 72 \t Batch 480 \t Training Loss: 46.523014879226686\n",
      "Epoch 72 \t Batch 500 \t Training Loss: 46.526039932250974\n",
      "Epoch 72 \t Batch 520 \t Training Loss: 46.51147524026724\n",
      "Epoch 72 \t Batch 540 \t Training Loss: 46.50269210956715\n",
      "Epoch 72 \t Batch 560 \t Training Loss: 46.460054186412265\n",
      "Epoch 72 \t Batch 580 \t Training Loss: 46.48338088331551\n",
      "Epoch 72 \t Batch 600 \t Training Loss: 46.51044834136963\n",
      "Epoch 72 \t Batch 620 \t Training Loss: 46.52800244362123\n",
      "Epoch 72 \t Batch 640 \t Training Loss: 46.516832953691484\n",
      "Epoch 72 \t Batch 660 \t Training Loss: 46.50703510515618\n",
      "Epoch 72 \t Batch 680 \t Training Loss: 46.51639363905963\n",
      "Epoch 72 \t Batch 700 \t Training Loss: 46.51984656197684\n",
      "Epoch 72 \t Batch 720 \t Training Loss: 46.50134703848097\n",
      "Epoch 72 \t Batch 740 \t Training Loss: 46.45784049678493\n",
      "Epoch 72 \t Batch 760 \t Training Loss: 46.49326438401875\n",
      "Epoch 72 \t Batch 780 \t Training Loss: 46.496631920643345\n",
      "Epoch 72 \t Batch 800 \t Training Loss: 46.4943008184433\n",
      "Epoch 72 \t Batch 820 \t Training Loss: 46.48473146950327\n",
      "Epoch 72 \t Batch 840 \t Training Loss: 46.46951861608596\n",
      "Epoch 72 \t Batch 860 \t Training Loss: 46.47824923493141\n",
      "Epoch 72 \t Batch 880 \t Training Loss: 46.49142952832309\n",
      "Epoch 72 \t Batch 900 \t Training Loss: 46.489885550604924\n",
      "Epoch 72 \t Batch 20 \t Validation Loss: 37.59390997886658\n",
      "Epoch 72 \t Batch 40 \t Validation Loss: 33.522172355651854\n",
      "Epoch 72 \t Batch 60 \t Validation Loss: 36.81962691942851\n",
      "Epoch 72 \t Batch 80 \t Validation Loss: 35.740417337417604\n",
      "Epoch 72 \t Batch 100 \t Validation Loss: 34.2371799659729\n",
      "Epoch 72 \t Batch 120 \t Validation Loss: 33.42662072181702\n",
      "Epoch 72 \t Batch 140 \t Validation Loss: 32.60132146562849\n",
      "Epoch 72 \t Batch 160 \t Validation Loss: 33.88304764032364\n",
      "Epoch 72 \t Batch 180 \t Validation Loss: 37.00998323228624\n",
      "Epoch 72 \t Batch 200 \t Validation Loss: 38.08296238899231\n",
      "Epoch 72 \t Batch 220 \t Validation Loss: 39.09059393622658\n",
      "Epoch 72 \t Batch 240 \t Validation Loss: 39.241418890158336\n",
      "Epoch 72 \t Batch 260 \t Validation Loss: 41.15571806614216\n",
      "Epoch 72 \t Batch 280 \t Validation Loss: 42.057646931920736\n",
      "Epoch 72 \t Batch 300 \t Validation Loss: 43.00326951662699\n",
      "Epoch 72 \t Batch 320 \t Validation Loss: 43.30399279892445\n",
      "Epoch 72 \t Batch 340 \t Validation Loss: 43.04035927548128\n",
      "Epoch 72 \t Batch 360 \t Validation Loss: 42.77842014100816\n",
      "Epoch 72 \t Batch 380 \t Validation Loss: 42.859926730708075\n",
      "Epoch 72 \t Batch 400 \t Validation Loss: 42.28110888004303\n",
      "Epoch 72 \t Batch 420 \t Validation Loss: 42.16080385389782\n",
      "Epoch 72 \t Batch 440 \t Validation Loss: 41.733948200399226\n",
      "Epoch 72 \t Batch 460 \t Validation Loss: 41.823424600518265\n",
      "Epoch 72 \t Batch 480 \t Validation Loss: 42.20608235597611\n",
      "Epoch 72 \t Batch 500 \t Validation Loss: 41.84436362838745\n",
      "Epoch 72 \t Batch 520 \t Validation Loss: 41.50032110947829\n",
      "Epoch 72 \t Batch 540 \t Validation Loss: 41.079788483513724\n",
      "Epoch 72 \t Batch 560 \t Validation Loss: 40.7054910830089\n",
      "Epoch 72 \t Batch 580 \t Validation Loss: 40.28177859865386\n",
      "Epoch 72 \t Batch 600 \t Validation Loss: 40.356483662923175\n",
      "Epoch 72 Training Loss: 46.48240561781689 Validation Loss: 40.881664771538276\n",
      "Epoch 72 completed\n",
      "Epoch 73 \t Batch 20 \t Training Loss: 45.95599937438965\n",
      "Epoch 73 \t Batch 40 \t Training Loss: 45.72997713088989\n",
      "Epoch 73 \t Batch 60 \t Training Loss: 46.426011339823404\n",
      "Epoch 73 \t Batch 80 \t Training Loss: 46.88599152565003\n",
      "Epoch 73 \t Batch 100 \t Training Loss: 47.02378070831299\n",
      "Epoch 73 \t Batch 120 \t Training Loss: 46.86683495839437\n",
      "Epoch 73 \t Batch 140 \t Training Loss: 46.64226485661098\n",
      "Epoch 73 \t Batch 160 \t Training Loss: 46.521595406532285\n",
      "Epoch 73 \t Batch 180 \t Training Loss: 46.40600662231445\n",
      "Epoch 73 \t Batch 200 \t Training Loss: 46.23351665496826\n",
      "Epoch 73 \t Batch 220 \t Training Loss: 46.205888071927156\n",
      "Epoch 73 \t Batch 240 \t Training Loss: 46.206406625111896\n",
      "Epoch 73 \t Batch 260 \t Training Loss: 46.234361692575305\n",
      "Epoch 73 \t Batch 280 \t Training Loss: 46.22527715138027\n",
      "Epoch 73 \t Batch 300 \t Training Loss: 46.24119593302409\n",
      "Epoch 73 \t Batch 320 \t Training Loss: 46.2104373216629\n",
      "Epoch 73 \t Batch 340 \t Training Loss: 46.18391972710104\n",
      "Epoch 73 \t Batch 360 \t Training Loss: 46.09919401804606\n",
      "Epoch 73 \t Batch 380 \t Training Loss: 46.169012963144404\n",
      "Epoch 73 \t Batch 400 \t Training Loss: 46.225287971496584\n",
      "Epoch 73 \t Batch 420 \t Training Loss: 46.314626639229914\n",
      "Epoch 73 \t Batch 440 \t Training Loss: 46.348346935619006\n",
      "Epoch 73 \t Batch 460 \t Training Loss: 46.31774204088294\n",
      "Epoch 73 \t Batch 480 \t Training Loss: 46.34775364398956\n",
      "Epoch 73 \t Batch 500 \t Training Loss: 46.31682688903808\n",
      "Epoch 73 \t Batch 520 \t Training Loss: 46.27590714234572\n",
      "Epoch 73 \t Batch 540 \t Training Loss: 46.32629034960711\n",
      "Epoch 73 \t Batch 560 \t Training Loss: 46.33378016608102\n",
      "Epoch 73 \t Batch 580 \t Training Loss: 46.37627499679039\n",
      "Epoch 73 \t Batch 600 \t Training Loss: 46.34238054275513\n",
      "Epoch 73 \t Batch 620 \t Training Loss: 46.332115911668346\n",
      "Epoch 73 \t Batch 640 \t Training Loss: 46.313784116506575\n",
      "Epoch 73 \t Batch 660 \t Training Loss: 46.33402066664262\n",
      "Epoch 73 \t Batch 680 \t Training Loss: 46.4051775932312\n",
      "Epoch 73 \t Batch 700 \t Training Loss: 46.39973168509347\n",
      "Epoch 73 \t Batch 720 \t Training Loss: 46.428497950236\n",
      "Epoch 73 \t Batch 740 \t Training Loss: 46.45982629415151\n",
      "Epoch 73 \t Batch 760 \t Training Loss: 46.476436489506774\n",
      "Epoch 73 \t Batch 780 \t Training Loss: 46.456914432232196\n",
      "Epoch 73 \t Batch 800 \t Training Loss: 46.4906054353714\n",
      "Epoch 73 \t Batch 820 \t Training Loss: 46.50418980761272\n",
      "Epoch 73 \t Batch 840 \t Training Loss: 46.50261489777338\n",
      "Epoch 73 \t Batch 860 \t Training Loss: 46.47550700431646\n",
      "Epoch 73 \t Batch 880 \t Training Loss: 46.47382822036743\n",
      "Epoch 73 \t Batch 900 \t Training Loss: 46.48245729658339\n",
      "Epoch 73 \t Batch 20 \t Validation Loss: 30.57442421913147\n",
      "Epoch 73 \t Batch 40 \t Validation Loss: 28.35567526817322\n",
      "Epoch 73 \t Batch 60 \t Validation Loss: 30.482194487253825\n",
      "Epoch 73 \t Batch 80 \t Validation Loss: 29.928360950946807\n",
      "Epoch 73 \t Batch 100 \t Validation Loss: 29.632712392807008\n",
      "Epoch 73 \t Batch 120 \t Validation Loss: 29.79833931128184\n",
      "Epoch 73 \t Batch 140 \t Validation Loss: 29.61708869252886\n",
      "Epoch 73 \t Batch 160 \t Validation Loss: 31.19983450770378\n",
      "Epoch 73 \t Batch 180 \t Validation Loss: 34.3953721470303\n",
      "Epoch 73 \t Batch 200 \t Validation Loss: 35.63629914283752\n",
      "Epoch 73 \t Batch 220 \t Validation Loss: 36.83093066649003\n",
      "Epoch 73 \t Batch 240 \t Validation Loss: 37.084584820270535\n",
      "Epoch 73 \t Batch 260 \t Validation Loss: 39.11276655563941\n",
      "Epoch 73 \t Batch 280 \t Validation Loss: 40.15225750037602\n",
      "Epoch 73 \t Batch 300 \t Validation Loss: 41.050662253697716\n",
      "Epoch 73 \t Batch 320 \t Validation Loss: 41.40046301186085\n",
      "Epoch 73 \t Batch 340 \t Validation Loss: 41.237486527947816\n",
      "Epoch 73 \t Batch 360 \t Validation Loss: 41.01404572327932\n",
      "Epoch 73 \t Batch 380 \t Validation Loss: 41.186007160889474\n",
      "Epoch 73 \t Batch 400 \t Validation Loss: 40.690606853961945\n",
      "Epoch 73 \t Batch 420 \t Validation Loss: 40.66063907941182\n",
      "Epoch 73 \t Batch 440 \t Validation Loss: 40.29029204845428\n",
      "Epoch 73 \t Batch 460 \t Validation Loss: 40.39637599820676\n",
      "Epoch 73 \t Batch 480 \t Validation Loss: 40.8221895635128\n",
      "Epoch 73 \t Batch 500 \t Validation Loss: 40.500950063705446\n",
      "Epoch 73 \t Batch 520 \t Validation Loss: 40.169334710561316\n",
      "Epoch 73 \t Batch 540 \t Validation Loss: 39.79058397964195\n",
      "Epoch 73 \t Batch 560 \t Validation Loss: 39.48064523935318\n",
      "Epoch 73 \t Batch 580 \t Validation Loss: 39.163145889084916\n",
      "Epoch 73 \t Batch 600 \t Validation Loss: 39.28498533725738\n",
      "Epoch 73 Training Loss: 46.450326813476295 Validation Loss: 39.87323054864809\n",
      "Epoch 73 completed\n",
      "Epoch 74 \t Batch 20 \t Training Loss: 47.31504383087158\n",
      "Epoch 74 \t Batch 40 \t Training Loss: 47.65089626312256\n",
      "Epoch 74 \t Batch 60 \t Training Loss: 47.61313813527425\n",
      "Epoch 74 \t Batch 80 \t Training Loss: 46.85546932220459\n",
      "Epoch 74 \t Batch 100 \t Training Loss: 46.85122283935547\n",
      "Epoch 74 \t Batch 120 \t Training Loss: 46.96857811609904\n",
      "Epoch 74 \t Batch 140 \t Training Loss: 46.80436785561698\n",
      "Epoch 74 \t Batch 160 \t Training Loss: 46.483767819404605\n",
      "Epoch 74 \t Batch 180 \t Training Loss: 46.483924590216745\n",
      "Epoch 74 \t Batch 200 \t Training Loss: 46.505930709838864\n",
      "Epoch 74 \t Batch 220 \t Training Loss: 46.50906193473122\n",
      "Epoch 74 \t Batch 240 \t Training Loss: 46.44437747001648\n",
      "Epoch 74 \t Batch 260 \t Training Loss: 46.57196526160607\n",
      "Epoch 74 \t Batch 280 \t Training Loss: 46.549702821459086\n",
      "Epoch 74 \t Batch 300 \t Training Loss: 46.57808053334554\n",
      "Epoch 74 \t Batch 320 \t Training Loss: 46.544971919059755\n",
      "Epoch 74 \t Batch 340 \t Training Loss: 46.6034408120548\n",
      "Epoch 74 \t Batch 360 \t Training Loss: 46.6075554423862\n",
      "Epoch 74 \t Batch 380 \t Training Loss: 46.556954223231266\n",
      "Epoch 74 \t Batch 400 \t Training Loss: 46.523844938278195\n",
      "Epoch 74 \t Batch 420 \t Training Loss: 46.527909160795666\n",
      "Epoch 74 \t Batch 440 \t Training Loss: 46.463006591796876\n",
      "Epoch 74 \t Batch 460 \t Training Loss: 46.467832855556324\n",
      "Epoch 74 \t Batch 480 \t Training Loss: 46.41344513098399\n",
      "Epoch 74 \t Batch 500 \t Training Loss: 46.414608306884766\n",
      "Epoch 74 \t Batch 520 \t Training Loss: 46.42037408535297\n",
      "Epoch 74 \t Batch 540 \t Training Loss: 46.45737490477385\n",
      "Epoch 74 \t Batch 560 \t Training Loss: 46.456740965162005\n",
      "Epoch 74 \t Batch 580 \t Training Loss: 46.446297421948664\n",
      "Epoch 74 \t Batch 600 \t Training Loss: 46.473830706278484\n",
      "Epoch 74 \t Batch 620 \t Training Loss: 46.47012218352287\n",
      "Epoch 74 \t Batch 640 \t Training Loss: 46.45180936455726\n",
      "Epoch 74 \t Batch 660 \t Training Loss: 46.43210744568796\n",
      "Epoch 74 \t Batch 680 \t Training Loss: 46.42419266981237\n",
      "Epoch 74 \t Batch 700 \t Training Loss: 46.42667140960693\n",
      "Epoch 74 \t Batch 720 \t Training Loss: 46.43091117540995\n",
      "Epoch 74 \t Batch 740 \t Training Loss: 46.43460527883994\n",
      "Epoch 74 \t Batch 760 \t Training Loss: 46.46454953143471\n",
      "Epoch 74 \t Batch 780 \t Training Loss: 46.43872080582839\n",
      "Epoch 74 \t Batch 800 \t Training Loss: 46.453801517486575\n",
      "Epoch 74 \t Batch 820 \t Training Loss: 46.44527659997708\n",
      "Epoch 74 \t Batch 840 \t Training Loss: 46.46602242333548\n",
      "Epoch 74 \t Batch 860 \t Training Loss: 46.445870350682455\n",
      "Epoch 74 \t Batch 880 \t Training Loss: 46.45194740728898\n",
      "Epoch 74 \t Batch 900 \t Training Loss: 46.43363381703695\n",
      "Epoch 74 \t Batch 20 \t Validation Loss: 45.69532532691956\n",
      "Epoch 74 \t Batch 40 \t Validation Loss: 40.01024336814881\n",
      "Epoch 74 \t Batch 60 \t Validation Loss: 43.74303515752157\n",
      "Epoch 74 \t Batch 80 \t Validation Loss: 42.04691340923309\n",
      "Epoch 74 \t Batch 100 \t Validation Loss: 39.27205381393433\n",
      "Epoch 74 \t Batch 120 \t Validation Loss: 37.76391072273255\n",
      "Epoch 74 \t Batch 140 \t Validation Loss: 36.401576954977855\n",
      "Epoch 74 \t Batch 160 \t Validation Loss: 37.049037432670595\n",
      "Epoch 74 \t Batch 180 \t Validation Loss: 39.486463165283205\n",
      "Epoch 74 \t Batch 200 \t Validation Loss: 40.14150821685791\n",
      "Epoch 74 \t Batch 220 \t Validation Loss: 40.78212464939464\n",
      "Epoch 74 \t Batch 240 \t Validation Loss: 40.63972380956014\n",
      "Epoch 74 \t Batch 260 \t Validation Loss: 42.269133134988635\n",
      "Epoch 74 \t Batch 280 \t Validation Loss: 42.96179344654083\n",
      "Epoch 74 \t Batch 300 \t Validation Loss: 43.65369435946147\n",
      "Epoch 74 \t Batch 320 \t Validation Loss: 43.81047324836254\n",
      "Epoch 74 \t Batch 340 \t Validation Loss: 43.49967775625341\n",
      "Epoch 74 \t Batch 360 \t Validation Loss: 43.13375119368235\n",
      "Epoch 74 \t Batch 380 \t Validation Loss: 43.138103653255264\n",
      "Epoch 74 \t Batch 400 \t Validation Loss: 42.54077447652817\n",
      "Epoch 74 \t Batch 420 \t Validation Loss: 42.39831650370643\n",
      "Epoch 74 \t Batch 440 \t Validation Loss: 41.96352392326702\n",
      "Epoch 74 \t Batch 460 \t Validation Loss: 42.00777270897575\n",
      "Epoch 74 \t Batch 480 \t Validation Loss: 42.351565382877986\n",
      "Epoch 74 \t Batch 500 \t Validation Loss: 41.95953729820251\n",
      "Epoch 74 \t Batch 520 \t Validation Loss: 41.60294923232152\n",
      "Epoch 74 \t Batch 540 \t Validation Loss: 41.217743423249985\n",
      "Epoch 74 \t Batch 560 \t Validation Loss: 40.9313097017152\n",
      "Epoch 74 \t Batch 580 \t Validation Loss: 40.69185714886106\n",
      "Epoch 74 \t Batch 600 \t Validation Loss: 40.796653615633645\n",
      "Epoch 74 Training Loss: 46.451863599837544 Validation Loss: 41.36652182758628\n",
      "Epoch 74 completed\n",
      "Epoch 75 \t Batch 20 \t Training Loss: 44.342495918273926\n",
      "Epoch 75 \t Batch 40 \t Training Loss: 45.67998142242432\n",
      "Epoch 75 \t Batch 60 \t Training Loss: 46.25968621571859\n",
      "Epoch 75 \t Batch 80 \t Training Loss: 46.11754274368286\n",
      "Epoch 75 \t Batch 100 \t Training Loss: 46.29655216217041\n",
      "Epoch 75 \t Batch 120 \t Training Loss: 46.13646624883016\n",
      "Epoch 75 \t Batch 140 \t Training Loss: 46.16777877807617\n",
      "Epoch 75 \t Batch 160 \t Training Loss: 46.41841037273407\n",
      "Epoch 75 \t Batch 180 \t Training Loss: 46.30449930826823\n",
      "Epoch 75 \t Batch 200 \t Training Loss: 46.368802280426024\n",
      "Epoch 75 \t Batch 220 \t Training Loss: 46.33854722109708\n",
      "Epoch 75 \t Batch 240 \t Training Loss: 46.309379227956136\n",
      "Epoch 75 \t Batch 260 \t Training Loss: 46.32526428516095\n",
      "Epoch 75 \t Batch 280 \t Training Loss: 46.20638969966343\n",
      "Epoch 75 \t Batch 300 \t Training Loss: 46.21964462280273\n",
      "Epoch 75 \t Batch 320 \t Training Loss: 46.1665862083435\n",
      "Epoch 75 \t Batch 340 \t Training Loss: 46.21465503468233\n",
      "Epoch 75 \t Batch 360 \t Training Loss: 46.25761443244086\n",
      "Epoch 75 \t Batch 380 \t Training Loss: 46.2374971188997\n",
      "Epoch 75 \t Batch 400 \t Training Loss: 46.216567249298095\n",
      "Epoch 75 \t Batch 420 \t Training Loss: 46.2124004636492\n",
      "Epoch 75 \t Batch 440 \t Training Loss: 46.25171481912786\n",
      "Epoch 75 \t Batch 460 \t Training Loss: 46.24712685294773\n",
      "Epoch 75 \t Batch 480 \t Training Loss: 46.275440009435016\n",
      "Epoch 75 \t Batch 500 \t Training Loss: 46.26557595062256\n",
      "Epoch 75 \t Batch 520 \t Training Loss: 46.302465402162994\n",
      "Epoch 75 \t Batch 540 \t Training Loss: 46.34140131208632\n",
      "Epoch 75 \t Batch 560 \t Training Loss: 46.35786390985761\n",
      "Epoch 75 \t Batch 580 \t Training Loss: 46.38429416130329\n",
      "Epoch 75 \t Batch 600 \t Training Loss: 46.40321472167969\n",
      "Epoch 75 \t Batch 620 \t Training Loss: 46.414020150707614\n",
      "Epoch 75 \t Batch 640 \t Training Loss: 46.36678567528725\n",
      "Epoch 75 \t Batch 660 \t Training Loss: 46.34910397385106\n",
      "Epoch 75 \t Batch 680 \t Training Loss: 46.32075592489804\n",
      "Epoch 75 \t Batch 700 \t Training Loss: 46.35146785191127\n",
      "Epoch 75 \t Batch 720 \t Training Loss: 46.38462954627143\n",
      "Epoch 75 \t Batch 740 \t Training Loss: 46.39712461007608\n",
      "Epoch 75 \t Batch 760 \t Training Loss: 46.406895205849096\n",
      "Epoch 75 \t Batch 780 \t Training Loss: 46.422534600282326\n",
      "Epoch 75 \t Batch 800 \t Training Loss: 46.392356338500974\n",
      "Epoch 75 \t Batch 820 \t Training Loss: 46.393443279731564\n",
      "Epoch 75 \t Batch 840 \t Training Loss: 46.44590643474034\n",
      "Epoch 75 \t Batch 860 \t Training Loss: 46.43450189191242\n",
      "Epoch 75 \t Batch 880 \t Training Loss: 46.44399152235551\n",
      "Epoch 75 \t Batch 900 \t Training Loss: 46.44149403889974\n",
      "Epoch 75 \t Batch 20 \t Validation Loss: 41.86884961128235\n",
      "Epoch 75 \t Batch 40 \t Validation Loss: 36.531472611427304\n",
      "Epoch 75 \t Batch 60 \t Validation Loss: 40.06899781227112\n",
      "Epoch 75 \t Batch 80 \t Validation Loss: 38.457767045497896\n",
      "Epoch 75 \t Batch 100 \t Validation Loss: 36.52051251411438\n",
      "Epoch 75 \t Batch 120 \t Validation Loss: 35.62014976342519\n",
      "Epoch 75 \t Batch 140 \t Validation Loss: 34.608647094454085\n",
      "Epoch 75 \t Batch 160 \t Validation Loss: 35.47519755959511\n",
      "Epoch 75 \t Batch 180 \t Validation Loss: 38.20451366106669\n",
      "Epoch 75 \t Batch 200 \t Validation Loss: 39.00300020694733\n",
      "Epoch 75 \t Batch 220 \t Validation Loss: 39.90006917173212\n",
      "Epoch 75 \t Batch 240 \t Validation Loss: 39.91413735548655\n",
      "Epoch 75 \t Batch 260 \t Validation Loss: 41.73262805571923\n",
      "Epoch 75 \t Batch 280 \t Validation Loss: 42.575789540154595\n",
      "Epoch 75 \t Batch 300 \t Validation Loss: 43.28693866729736\n",
      "Epoch 75 \t Batch 320 \t Validation Loss: 43.4964968085289\n",
      "Epoch 75 \t Batch 340 \t Validation Loss: 43.193682614494776\n",
      "Epoch 75 \t Batch 360 \t Validation Loss: 42.822372778256735\n",
      "Epoch 75 \t Batch 380 \t Validation Loss: 42.861902184235426\n",
      "Epoch 75 \t Batch 400 \t Validation Loss: 42.2208801817894\n",
      "Epoch 75 \t Batch 420 \t Validation Loss: 42.04283605076018\n",
      "Epoch 75 \t Batch 440 \t Validation Loss: 41.56819368058985\n",
      "Epoch 75 \t Batch 460 \t Validation Loss: 41.58790889200957\n",
      "Epoch 75 \t Batch 480 \t Validation Loss: 41.92102466622988\n",
      "Epoch 75 \t Batch 500 \t Validation Loss: 41.51042706108093\n",
      "Epoch 75 \t Batch 520 \t Validation Loss: 41.0928756750547\n",
      "Epoch 75 \t Batch 540 \t Validation Loss: 40.68827902475993\n",
      "Epoch 75 \t Batch 560 \t Validation Loss: 40.344179013797216\n",
      "Epoch 75 \t Batch 580 \t Validation Loss: 39.94098792898244\n",
      "Epoch 75 \t Batch 600 \t Validation Loss: 40.04632312138875\n",
      "Epoch 75 Training Loss: 46.44492050137619 Validation Loss: 40.60086822200131\n",
      "Epoch 75 completed\n",
      "Epoch 76 \t Batch 20 \t Training Loss: 46.825297355651855\n",
      "Epoch 76 \t Batch 40 \t Training Loss: 46.43468074798584\n",
      "Epoch 76 \t Batch 60 \t Training Loss: 46.465290260314944\n",
      "Epoch 76 \t Batch 80 \t Training Loss: 46.50234861373902\n",
      "Epoch 76 \t Batch 100 \t Training Loss: 46.02513336181641\n",
      "Epoch 76 \t Batch 120 \t Training Loss: 46.28169215520223\n",
      "Epoch 76 \t Batch 140 \t Training Loss: 46.13715956551688\n",
      "Epoch 76 \t Batch 160 \t Training Loss: 46.2321836233139\n",
      "Epoch 76 \t Batch 180 \t Training Loss: 46.189966117011174\n",
      "Epoch 76 \t Batch 200 \t Training Loss: 46.18435096740723\n",
      "Epoch 76 \t Batch 220 \t Training Loss: 46.30330005992543\n",
      "Epoch 76 \t Batch 240 \t Training Loss: 46.37348095575968\n",
      "Epoch 76 \t Batch 260 \t Training Loss: 46.323624126727765\n",
      "Epoch 76 \t Batch 280 \t Training Loss: 46.407446193695066\n",
      "Epoch 76 \t Batch 300 \t Training Loss: 46.430381762186684\n",
      "Epoch 76 \t Batch 320 \t Training Loss: 46.43713738918304\n",
      "Epoch 76 \t Batch 340 \t Training Loss: 46.3545311422909\n",
      "Epoch 76 \t Batch 360 \t Training Loss: 46.47075924343533\n",
      "Epoch 76 \t Batch 380 \t Training Loss: 46.38267520101447\n",
      "Epoch 76 \t Batch 400 \t Training Loss: 46.42125347137451\n",
      "Epoch 76 \t Batch 420 \t Training Loss: 46.39998219807943\n",
      "Epoch 76 \t Batch 440 \t Training Loss: 46.408900833129884\n",
      "Epoch 76 \t Batch 460 \t Training Loss: 46.40548067507537\n",
      "Epoch 76 \t Batch 480 \t Training Loss: 46.361650904019676\n",
      "Epoch 76 \t Batch 500 \t Training Loss: 46.3057374420166\n",
      "Epoch 76 \t Batch 520 \t Training Loss: 46.32179384231567\n",
      "Epoch 76 \t Batch 540 \t Training Loss: 46.315047405384206\n",
      "Epoch 76 \t Batch 560 \t Training Loss: 46.30454707145691\n",
      "Epoch 76 \t Batch 580 \t Training Loss: 46.27144195293558\n",
      "Epoch 76 \t Batch 600 \t Training Loss: 46.2979702758789\n",
      "Epoch 76 \t Batch 620 \t Training Loss: 46.304584761588806\n",
      "Epoch 76 \t Batch 640 \t Training Loss: 46.3562805891037\n",
      "Epoch 76 \t Batch 660 \t Training Loss: 46.37712695959843\n",
      "Epoch 76 \t Batch 680 \t Training Loss: 46.402047202166386\n",
      "Epoch 76 \t Batch 700 \t Training Loss: 46.398128989083425\n",
      "Epoch 76 \t Batch 720 \t Training Loss: 46.401149129867555\n",
      "Epoch 76 \t Batch 740 \t Training Loss: 46.3702516761986\n",
      "Epoch 76 \t Batch 760 \t Training Loss: 46.37486925627056\n",
      "Epoch 76 \t Batch 780 \t Training Loss: 46.39565903101212\n",
      "Epoch 76 \t Batch 800 \t Training Loss: 46.36403338909149\n",
      "Epoch 76 \t Batch 820 \t Training Loss: 46.382087828473345\n",
      "Epoch 76 \t Batch 840 \t Training Loss: 46.35068341209775\n",
      "Epoch 76 \t Batch 860 \t Training Loss: 46.379328151081886\n",
      "Epoch 76 \t Batch 880 \t Training Loss: 46.384685256264426\n",
      "Epoch 76 \t Batch 900 \t Training Loss: 46.3791918182373\n",
      "Epoch 76 \t Batch 20 \t Validation Loss: 44.94379081726074\n",
      "Epoch 76 \t Batch 40 \t Validation Loss: 38.92977330684662\n",
      "Epoch 76 \t Batch 60 \t Validation Loss: 42.72882227897644\n",
      "Epoch 76 \t Batch 80 \t Validation Loss: 41.06627421379089\n",
      "Epoch 76 \t Batch 100 \t Validation Loss: 38.87605928421021\n",
      "Epoch 76 \t Batch 120 \t Validation Loss: 37.9093590259552\n",
      "Epoch 76 \t Batch 140 \t Validation Loss: 36.7484297479902\n",
      "Epoch 76 \t Batch 160 \t Validation Loss: 37.38267787694931\n",
      "Epoch 76 \t Batch 180 \t Validation Loss: 39.8438453356425\n",
      "Epoch 76 \t Batch 200 \t Validation Loss: 40.49091967582703\n",
      "Epoch 76 \t Batch 220 \t Validation Loss: 41.20127017281272\n",
      "Epoch 76 \t Batch 240 \t Validation Loss: 41.07436660925547\n",
      "Epoch 76 \t Batch 260 \t Validation Loss: 42.738385559962346\n",
      "Epoch 76 \t Batch 280 \t Validation Loss: 43.464597177505496\n",
      "Epoch 76 \t Batch 300 \t Validation Loss: 44.12676773071289\n",
      "Epoch 76 \t Batch 320 \t Validation Loss: 44.27748714089394\n",
      "Epoch 76 \t Batch 340 \t Validation Loss: 43.94274703194113\n",
      "Epoch 76 \t Batch 360 \t Validation Loss: 43.54746442370944\n",
      "Epoch 76 \t Batch 380 \t Validation Loss: 43.561576953687165\n",
      "Epoch 76 \t Batch 400 \t Validation Loss: 42.94671577453613\n",
      "Epoch 76 \t Batch 420 \t Validation Loss: 42.78575700805301\n",
      "Epoch 76 \t Batch 440 \t Validation Loss: 42.35143493088809\n",
      "Epoch 76 \t Batch 460 \t Validation Loss: 42.36817438913428\n",
      "Epoch 76 \t Batch 480 \t Validation Loss: 42.69459144473076\n",
      "Epoch 76 \t Batch 500 \t Validation Loss: 42.274384141922\n",
      "Epoch 76 \t Batch 520 \t Validation Loss: 41.8839848279953\n",
      "Epoch 76 \t Batch 540 \t Validation Loss: 41.45303298279091\n",
      "Epoch 76 \t Batch 560 \t Validation Loss: 41.09283937215805\n",
      "Epoch 76 \t Batch 580 \t Validation Loss: 40.675465269746454\n",
      "Epoch 76 \t Batch 600 \t Validation Loss: 40.75084759871165\n",
      "Epoch 76 Training Loss: 46.396844999090796 Validation Loss: 41.29555144402888\n",
      "Epoch 76 completed\n",
      "Epoch 77 \t Batch 20 \t Training Loss: 46.416867065429685\n",
      "Epoch 77 \t Batch 40 \t Training Loss: 46.22951574325562\n",
      "Epoch 77 \t Batch 60 \t Training Loss: 46.13245525360107\n",
      "Epoch 77 \t Batch 80 \t Training Loss: 45.946282958984376\n",
      "Epoch 77 \t Batch 100 \t Training Loss: 46.396086845397946\n",
      "Epoch 77 \t Batch 120 \t Training Loss: 46.3313977877299\n",
      "Epoch 77 \t Batch 140 \t Training Loss: 46.48009673527309\n",
      "Epoch 77 \t Batch 160 \t Training Loss: 46.56845695972443\n",
      "Epoch 77 \t Batch 180 \t Training Loss: 46.50897352430555\n",
      "Epoch 77 \t Batch 200 \t Training Loss: 46.496167430877684\n",
      "Epoch 77 \t Batch 220 \t Training Loss: 46.51857643127441\n",
      "Epoch 77 \t Batch 240 \t Training Loss: 46.62470027605693\n",
      "Epoch 77 \t Batch 260 \t Training Loss: 46.547123600886415\n",
      "Epoch 77 \t Batch 280 \t Training Loss: 46.443720885685515\n",
      "Epoch 77 \t Batch 300 \t Training Loss: 46.414337361653644\n",
      "Epoch 77 \t Batch 320 \t Training Loss: 46.336156892776486\n",
      "Epoch 77 \t Batch 340 \t Training Loss: 46.424239506441005\n",
      "Epoch 77 \t Batch 360 \t Training Loss: 46.3887476073371\n",
      "Epoch 77 \t Batch 380 \t Training Loss: 46.278240233973456\n",
      "Epoch 77 \t Batch 400 \t Training Loss: 46.37340516090393\n",
      "Epoch 77 \t Batch 420 \t Training Loss: 46.36824177333287\n",
      "Epoch 77 \t Batch 440 \t Training Loss: 46.32171760038896\n",
      "Epoch 77 \t Batch 460 \t Training Loss: 46.371422187141754\n",
      "Epoch 77 \t Batch 480 \t Training Loss: 46.34198674360911\n",
      "Epoch 77 \t Batch 500 \t Training Loss: 46.32573486328125\n",
      "Epoch 77 \t Batch 520 \t Training Loss: 46.3897904762855\n",
      "Epoch 77 \t Batch 540 \t Training Loss: 46.387695990668405\n",
      "Epoch 77 \t Batch 560 \t Training Loss: 46.37559563773019\n",
      "Epoch 77 \t Batch 580 \t Training Loss: 46.361895442831106\n",
      "Epoch 77 \t Batch 600 \t Training Loss: 46.33993937810262\n",
      "Epoch 77 \t Batch 620 \t Training Loss: 46.32359225365423\n",
      "Epoch 77 \t Batch 640 \t Training Loss: 46.34141688942909\n",
      "Epoch 77 \t Batch 660 \t Training Loss: 46.33021253528017\n",
      "Epoch 77 \t Batch 680 \t Training Loss: 46.34807393130134\n",
      "Epoch 77 \t Batch 700 \t Training Loss: 46.31989931379046\n",
      "Epoch 77 \t Batch 720 \t Training Loss: 46.302998786502414\n",
      "Epoch 77 \t Batch 740 \t Training Loss: 46.32859803277093\n",
      "Epoch 77 \t Batch 760 \t Training Loss: 46.37449291128861\n",
      "Epoch 77 \t Batch 780 \t Training Loss: 46.37974752279428\n",
      "Epoch 77 \t Batch 800 \t Training Loss: 46.35525822639465\n",
      "Epoch 77 \t Batch 820 \t Training Loss: 46.35027417438786\n",
      "Epoch 77 \t Batch 840 \t Training Loss: 46.34737687337966\n",
      "Epoch 77 \t Batch 860 \t Training Loss: 46.35248592287995\n",
      "Epoch 77 \t Batch 880 \t Training Loss: 46.36861107132652\n",
      "Epoch 77 \t Batch 900 \t Training Loss: 46.3920112186008\n",
      "Epoch 77 \t Batch 20 \t Validation Loss: 38.14976849555969\n",
      "Epoch 77 \t Batch 40 \t Validation Loss: 34.16256017684937\n",
      "Epoch 77 \t Batch 60 \t Validation Loss: 37.15550758043925\n",
      "Epoch 77 \t Batch 80 \t Validation Loss: 36.09806268215179\n",
      "Epoch 77 \t Batch 100 \t Validation Loss: 34.56198692321777\n",
      "Epoch 77 \t Batch 120 \t Validation Loss: 33.87122402191162\n",
      "Epoch 77 \t Batch 140 \t Validation Loss: 33.06752890178135\n",
      "Epoch 77 \t Batch 160 \t Validation Loss: 34.17064263820648\n",
      "Epoch 77 \t Batch 180 \t Validation Loss: 37.01268331739637\n",
      "Epoch 77 \t Batch 200 \t Validation Loss: 37.976941919326784\n",
      "Epoch 77 \t Batch 220 \t Validation Loss: 38.964763129841195\n",
      "Epoch 77 \t Batch 240 \t Validation Loss: 39.04009475708008\n",
      "Epoch 77 \t Batch 260 \t Validation Loss: 40.926502799987794\n",
      "Epoch 77 \t Batch 280 \t Validation Loss: 41.865616583824156\n",
      "Epoch 77 \t Batch 300 \t Validation Loss: 42.62365976015727\n",
      "Epoch 77 \t Batch 320 \t Validation Loss: 42.894118431210515\n",
      "Epoch 77 \t Batch 340 \t Validation Loss: 42.644448939491724\n",
      "Epoch 77 \t Batch 360 \t Validation Loss: 42.3297252257665\n",
      "Epoch 77 \t Batch 380 \t Validation Loss: 42.44831920172039\n",
      "Epoch 77 \t Batch 400 \t Validation Loss: 41.90029433012009\n",
      "Epoch 77 \t Batch 420 \t Validation Loss: 41.77907557941619\n",
      "Epoch 77 \t Batch 440 \t Validation Loss: 41.4053376046094\n",
      "Epoch 77 \t Batch 460 \t Validation Loss: 41.48851947991744\n",
      "Epoch 77 \t Batch 480 \t Validation Loss: 41.84929944475492\n",
      "Epoch 77 \t Batch 500 \t Validation Loss: 41.46153535270691\n",
      "Epoch 77 \t Batch 520 \t Validation Loss: 41.12979864340562\n",
      "Epoch 77 \t Batch 540 \t Validation Loss: 40.73575882735076\n",
      "Epoch 77 \t Batch 560 \t Validation Loss: 40.40829352651323\n",
      "Epoch 77 \t Batch 580 \t Validation Loss: 40.069230924803634\n",
      "Epoch 77 \t Batch 600 \t Validation Loss: 40.173793977101646\n",
      "Epoch 77 Training Loss: 46.40149215731522 Validation Loss: 40.77485502849925\n",
      "Epoch 77 completed\n",
      "Epoch 78 \t Batch 20 \t Training Loss: 47.705124855041504\n",
      "Epoch 78 \t Batch 40 \t Training Loss: 46.532816123962405\n",
      "Epoch 78 \t Batch 60 \t Training Loss: 46.6405642191569\n",
      "Epoch 78 \t Batch 80 \t Training Loss: 46.28747267723084\n",
      "Epoch 78 \t Batch 100 \t Training Loss: 46.40072052001953\n",
      "Epoch 78 \t Batch 120 \t Training Loss: 46.430167770385744\n",
      "Epoch 78 \t Batch 140 \t Training Loss: 46.48232370104108\n",
      "Epoch 78 \t Batch 160 \t Training Loss: 46.40263159275055\n",
      "Epoch 78 \t Batch 180 \t Training Loss: 46.4317969640096\n",
      "Epoch 78 \t Batch 200 \t Training Loss: 46.41843378067016\n",
      "Epoch 78 \t Batch 220 \t Training Loss: 46.44271091114391\n",
      "Epoch 78 \t Batch 240 \t Training Loss: 46.44096048672994\n",
      "Epoch 78 \t Batch 260 \t Training Loss: 46.51937992389386\n",
      "Epoch 78 \t Batch 280 \t Training Loss: 46.4768524987357\n",
      "Epoch 78 \t Batch 300 \t Training Loss: 46.40518388112386\n",
      "Epoch 78 \t Batch 320 \t Training Loss: 46.35667394399643\n",
      "Epoch 78 \t Batch 340 \t Training Loss: 46.33168570574592\n",
      "Epoch 78 \t Batch 360 \t Training Loss: 46.370141643948024\n",
      "Epoch 78 \t Batch 380 \t Training Loss: 46.382020719427814\n",
      "Epoch 78 \t Batch 400 \t Training Loss: 46.443055610656735\n",
      "Epoch 78 \t Batch 420 \t Training Loss: 46.4224212374006\n",
      "Epoch 78 \t Batch 440 \t Training Loss: 46.4350156177174\n",
      "Epoch 78 \t Batch 460 \t Training Loss: 46.443096807728644\n",
      "Epoch 78 \t Batch 480 \t Training Loss: 46.39193573792775\n",
      "Epoch 78 \t Batch 500 \t Training Loss: 46.390578414916995\n",
      "Epoch 78 \t Batch 520 \t Training Loss: 46.382140210958624\n",
      "Epoch 78 \t Batch 540 \t Training Loss: 46.37386389838325\n",
      "Epoch 78 \t Batch 560 \t Training Loss: 46.377454444340295\n",
      "Epoch 78 \t Batch 580 \t Training Loss: 46.38533028569715\n",
      "Epoch 78 \t Batch 600 \t Training Loss: 46.35613849639893\n",
      "Epoch 78 \t Batch 620 \t Training Loss: 46.399393881520915\n",
      "Epoch 78 \t Batch 640 \t Training Loss: 46.34315936565399\n",
      "Epoch 78 \t Batch 660 \t Training Loss: 46.334868945497455\n",
      "Epoch 78 \t Batch 680 \t Training Loss: 46.33878271439496\n",
      "Epoch 78 \t Batch 700 \t Training Loss: 46.386244381495885\n",
      "Epoch 78 \t Batch 720 \t Training Loss: 46.392020887798736\n",
      "Epoch 78 \t Batch 740 \t Training Loss: 46.419145542866474\n",
      "Epoch 78 \t Batch 760 \t Training Loss: 46.4638714238217\n",
      "Epoch 78 \t Batch 780 \t Training Loss: 46.481414540608725\n",
      "Epoch 78 \t Batch 800 \t Training Loss: 46.50520426273346\n",
      "Epoch 78 \t Batch 820 \t Training Loss: 46.501756454095606\n",
      "Epoch 78 \t Batch 840 \t Training Loss: 46.481221993764244\n",
      "Epoch 78 \t Batch 860 \t Training Loss: 46.44901891752731\n",
      "Epoch 78 \t Batch 880 \t Training Loss: 46.42822856036099\n",
      "Epoch 78 \t Batch 900 \t Training Loss: 46.40326921251085\n",
      "Epoch 78 \t Batch 20 \t Validation Loss: 44.40024886131287\n",
      "Epoch 78 \t Batch 40 \t Validation Loss: 39.165792322158815\n",
      "Epoch 78 \t Batch 60 \t Validation Loss: 42.75817586580912\n",
      "Epoch 78 \t Batch 80 \t Validation Loss: 41.27439169883728\n",
      "Epoch 78 \t Batch 100 \t Validation Loss: 38.93511177062988\n",
      "Epoch 78 \t Batch 120 \t Validation Loss: 37.66124617258708\n",
      "Epoch 78 \t Batch 140 \t Validation Loss: 36.450420883723666\n",
      "Epoch 78 \t Batch 160 \t Validation Loss: 37.03030798435211\n",
      "Epoch 78 \t Batch 180 \t Validation Loss: 39.35658721394009\n",
      "Epoch 78 \t Batch 200 \t Validation Loss: 39.99010246753693\n",
      "Epoch 78 \t Batch 220 \t Validation Loss: 40.66389851570129\n",
      "Epoch 78 \t Batch 240 \t Validation Loss: 40.518034132321674\n",
      "Epoch 78 \t Batch 260 \t Validation Loss: 42.1627016214224\n",
      "Epoch 78 \t Batch 280 \t Validation Loss: 42.87890071528299\n",
      "Epoch 78 \t Batch 300 \t Validation Loss: 43.443019259770715\n",
      "Epoch 78 \t Batch 320 \t Validation Loss: 43.56966196000576\n",
      "Epoch 78 \t Batch 340 \t Validation Loss: 43.24140023904688\n",
      "Epoch 78 \t Batch 360 \t Validation Loss: 42.85278536743588\n",
      "Epoch 78 \t Batch 380 \t Validation Loss: 42.874494354348435\n",
      "Epoch 78 \t Batch 400 \t Validation Loss: 42.286280477046965\n",
      "Epoch 78 \t Batch 420 \t Validation Loss: 42.108766680672055\n",
      "Epoch 78 \t Batch 440 \t Validation Loss: 41.69185830679807\n",
      "Epoch 78 \t Batch 460 \t Validation Loss: 41.75033576592155\n",
      "Epoch 78 \t Batch 480 \t Validation Loss: 42.07151080171267\n",
      "Epoch 78 \t Batch 500 \t Validation Loss: 41.64569505882263\n",
      "Epoch 78 \t Batch 520 \t Validation Loss: 41.30747843338893\n",
      "Epoch 78 \t Batch 540 \t Validation Loss: 40.94018101338987\n",
      "Epoch 78 \t Batch 560 \t Validation Loss: 40.63222287552697\n",
      "Epoch 78 \t Batch 580 \t Validation Loss: 40.316883654429994\n",
      "Epoch 78 \t Batch 600 \t Validation Loss: 40.443874128659566\n",
      "Epoch 78 Training Loss: 46.391380884135174 Validation Loss: 41.012446590832305\n",
      "Epoch 78 completed\n",
      "Epoch 79 \t Batch 20 \t Training Loss: 46.51994037628174\n",
      "Epoch 79 \t Batch 40 \t Training Loss: 46.30640993118286\n",
      "Epoch 79 \t Batch 60 \t Training Loss: 46.30763206481934\n",
      "Epoch 79 \t Batch 80 \t Training Loss: 46.48350462913513\n",
      "Epoch 79 \t Batch 100 \t Training Loss: 46.412708587646485\n",
      "Epoch 79 \t Batch 120 \t Training Loss: 46.357865206400554\n",
      "Epoch 79 \t Batch 140 \t Training Loss: 46.33668286459787\n",
      "Epoch 79 \t Batch 160 \t Training Loss: 46.48825132846832\n",
      "Epoch 79 \t Batch 180 \t Training Loss: 46.345536422729495\n",
      "Epoch 79 \t Batch 200 \t Training Loss: 46.24128219604492\n",
      "Epoch 79 \t Batch 220 \t Training Loss: 46.268560738997024\n",
      "Epoch 79 \t Batch 240 \t Training Loss: 46.279471000035606\n",
      "Epoch 79 \t Batch 260 \t Training Loss: 46.29150060506967\n",
      "Epoch 79 \t Batch 280 \t Training Loss: 46.284186485835484\n",
      "Epoch 79 \t Batch 300 \t Training Loss: 46.30919740041097\n",
      "Epoch 79 \t Batch 320 \t Training Loss: 46.362796139717105\n",
      "Epoch 79 \t Batch 340 \t Training Loss: 46.419863139881805\n",
      "Epoch 79 \t Batch 360 \t Training Loss: 46.363120004865856\n",
      "Epoch 79 \t Batch 380 \t Training Loss: 46.402360303778394\n",
      "Epoch 79 \t Batch 400 \t Training Loss: 46.36280220031738\n",
      "Epoch 79 \t Batch 420 \t Training Loss: 46.370032946268715\n",
      "Epoch 79 \t Batch 440 \t Training Loss: 46.40621527758512\n",
      "Epoch 79 \t Batch 460 \t Training Loss: 46.451560086789335\n",
      "Epoch 79 \t Batch 480 \t Training Loss: 46.37279074192047\n",
      "Epoch 79 \t Batch 500 \t Training Loss: 46.361404228210446\n",
      "Epoch 79 \t Batch 520 \t Training Loss: 46.37048245209914\n",
      "Epoch 79 \t Batch 540 \t Training Loss: 46.40050688849555\n",
      "Epoch 79 \t Batch 560 \t Training Loss: 46.43211381775992\n",
      "Epoch 79 \t Batch 580 \t Training Loss: 46.405322508976376\n",
      "Epoch 79 \t Batch 600 \t Training Loss: 46.398054421742756\n",
      "Epoch 79 \t Batch 620 \t Training Loss: 46.41973617922875\n",
      "Epoch 79 \t Batch 640 \t Training Loss: 46.38990055322647\n",
      "Epoch 79 \t Batch 660 \t Training Loss: 46.40298919677734\n",
      "Epoch 79 \t Batch 680 \t Training Loss: 46.38876221039716\n",
      "Epoch 79 \t Batch 700 \t Training Loss: 46.40248615809849\n",
      "Epoch 79 \t Batch 720 \t Training Loss: 46.42253378762139\n",
      "Epoch 79 \t Batch 740 \t Training Loss: 46.401572820302604\n",
      "Epoch 79 \t Batch 760 \t Training Loss: 46.39674700184872\n",
      "Epoch 79 \t Batch 780 \t Training Loss: 46.38930199451936\n",
      "Epoch 79 \t Batch 800 \t Training Loss: 46.353560543060304\n",
      "Epoch 79 \t Batch 820 \t Training Loss: 46.340324397203396\n",
      "Epoch 79 \t Batch 840 \t Training Loss: 46.354945509774346\n",
      "Epoch 79 \t Batch 860 \t Training Loss: 46.344770755324255\n",
      "Epoch 79 \t Batch 880 \t Training Loss: 46.31081539934332\n",
      "Epoch 79 \t Batch 900 \t Training Loss: 46.324489496019154\n",
      "Epoch 79 \t Batch 20 \t Validation Loss: 31.802511501312257\n",
      "Epoch 79 \t Batch 40 \t Validation Loss: 29.694858837127686\n",
      "Epoch 79 \t Batch 60 \t Validation Loss: 31.859048986434935\n",
      "Epoch 79 \t Batch 80 \t Validation Loss: 31.246011364459992\n",
      "Epoch 79 \t Batch 100 \t Validation Loss: 30.693418130874633\n",
      "Epoch 79 \t Batch 120 \t Validation Loss: 30.641815145810444\n",
      "Epoch 79 \t Batch 140 \t Validation Loss: 30.28768500600542\n",
      "Epoch 79 \t Batch 160 \t Validation Loss: 31.620406156778337\n",
      "Epoch 79 \t Batch 180 \t Validation Loss: 34.6112774848938\n",
      "Epoch 79 \t Batch 200 \t Validation Loss: 35.713271131515505\n",
      "Epoch 79 \t Batch 220 \t Validation Loss: 36.75724567066539\n",
      "Epoch 79 \t Batch 240 \t Validation Loss: 36.95504604180654\n",
      "Epoch 79 \t Batch 260 \t Validation Loss: 38.83710635992197\n",
      "Epoch 79 \t Batch 280 \t Validation Loss: 39.76915671144213\n",
      "Epoch 79 \t Batch 300 \t Validation Loss: 40.610186134974164\n",
      "Epoch 79 \t Batch 320 \t Validation Loss: 40.94438621103764\n",
      "Epoch 79 \t Batch 340 \t Validation Loss: 40.78032585312339\n",
      "Epoch 79 \t Batch 360 \t Validation Loss: 40.5276955180698\n",
      "Epoch 79 \t Batch 380 \t Validation Loss: 40.670408319172104\n",
      "Epoch 79 \t Batch 400 \t Validation Loss: 40.20942358016968\n",
      "Epoch 79 \t Batch 420 \t Validation Loss: 40.16581786473592\n",
      "Epoch 79 \t Batch 440 \t Validation Loss: 39.848740083521065\n",
      "Epoch 79 \t Batch 460 \t Validation Loss: 39.96496478370998\n",
      "Epoch 79 \t Batch 480 \t Validation Loss: 40.370819040139516\n",
      "Epoch 79 \t Batch 500 \t Validation Loss: 40.03173278427124\n",
      "Epoch 79 \t Batch 520 \t Validation Loss: 39.75552829595713\n",
      "Epoch 79 \t Batch 540 \t Validation Loss: 39.414554546497484\n",
      "Epoch 79 \t Batch 560 \t Validation Loss: 39.15159727845873\n",
      "Epoch 79 \t Batch 580 \t Validation Loss: 38.86821520904015\n",
      "Epoch 79 \t Batch 600 \t Validation Loss: 39.02265746116638\n",
      "Epoch 79 Training Loss: 46.35005233332011 Validation Loss: 39.63014346903021\n",
      "Epoch 79 completed\n",
      "Epoch 80 \t Batch 20 \t Training Loss: 45.023087882995604\n",
      "Epoch 80 \t Batch 40 \t Training Loss: 45.81898431777954\n",
      "Epoch 80 \t Batch 60 \t Training Loss: 45.964599736531575\n",
      "Epoch 80 \t Batch 80 \t Training Loss: 46.453391313552856\n",
      "Epoch 80 \t Batch 100 \t Training Loss: 46.3814155960083\n",
      "Epoch 80 \t Batch 120 \t Training Loss: 46.42436917622884\n",
      "Epoch 80 \t Batch 140 \t Training Loss: 46.406040000915525\n",
      "Epoch 80 \t Batch 160 \t Training Loss: 46.29650661945343\n",
      "Epoch 80 \t Batch 180 \t Training Loss: 46.515998395284015\n",
      "Epoch 80 \t Batch 200 \t Training Loss: 46.470980663299564\n",
      "Epoch 80 \t Batch 220 \t Training Loss: 46.342475058815694\n",
      "Epoch 80 \t Batch 240 \t Training Loss: 46.21609539985657\n",
      "Epoch 80 \t Batch 260 \t Training Loss: 46.34838372744047\n",
      "Epoch 80 \t Batch 280 \t Training Loss: 46.42602438245501\n",
      "Epoch 80 \t Batch 300 \t Training Loss: 46.400563405354816\n",
      "Epoch 80 \t Batch 320 \t Training Loss: 46.327745616436005\n",
      "Epoch 80 \t Batch 340 \t Training Loss: 46.37605192521039\n",
      "Epoch 80 \t Batch 360 \t Training Loss: 46.377045684390595\n",
      "Epoch 80 \t Batch 380 \t Training Loss: 46.29819450378418\n",
      "Epoch 80 \t Batch 400 \t Training Loss: 46.310294218063355\n",
      "Epoch 80 \t Batch 420 \t Training Loss: 46.285086441040036\n",
      "Epoch 80 \t Batch 440 \t Training Loss: 46.227685676921496\n",
      "Epoch 80 \t Batch 460 \t Training Loss: 46.26299022176991\n",
      "Epoch 80 \t Batch 480 \t Training Loss: 46.26457645893097\n",
      "Epoch 80 \t Batch 500 \t Training Loss: 46.27452975463867\n",
      "Epoch 80 \t Batch 520 \t Training Loss: 46.25531327907856\n",
      "Epoch 80 \t Batch 540 \t Training Loss: 46.22840131830286\n",
      "Epoch 80 \t Batch 560 \t Training Loss: 46.22444044521877\n",
      "Epoch 80 \t Batch 580 \t Training Loss: 46.259644258433376\n",
      "Epoch 80 \t Batch 600 \t Training Loss: 46.29646827697754\n",
      "Epoch 80 \t Batch 620 \t Training Loss: 46.31330926341395\n",
      "Epoch 80 \t Batch 640 \t Training Loss: 46.344642555713655\n",
      "Epoch 80 \t Batch 660 \t Training Loss: 46.356931218233974\n",
      "Epoch 80 \t Batch 680 \t Training Loss: 46.3470549807829\n",
      "Epoch 80 \t Batch 700 \t Training Loss: 46.33742456163679\n",
      "Epoch 80 \t Batch 720 \t Training Loss: 46.32354695002238\n",
      "Epoch 80 \t Batch 740 \t Training Loss: 46.31417698215794\n",
      "Epoch 80 \t Batch 760 \t Training Loss: 46.32137971677278\n",
      "Epoch 80 \t Batch 780 \t Training Loss: 46.351121677496494\n",
      "Epoch 80 \t Batch 800 \t Training Loss: 46.38212833881378\n",
      "Epoch 80 \t Batch 820 \t Training Loss: 46.32495091135909\n",
      "Epoch 80 \t Batch 840 \t Training Loss: 46.32845046633766\n",
      "Epoch 80 \t Batch 860 \t Training Loss: 46.31797895209734\n",
      "Epoch 80 \t Batch 880 \t Training Loss: 46.33612280758945\n",
      "Epoch 80 \t Batch 900 \t Training Loss: 46.34947995079888\n",
      "Epoch 80 \t Batch 20 \t Validation Loss: 32.96924724578857\n",
      "Epoch 80 \t Batch 40 \t Validation Loss: 30.887313055992127\n",
      "Epoch 80 \t Batch 60 \t Validation Loss: 33.09600461324056\n",
      "Epoch 80 \t Batch 80 \t Validation Loss: 32.30498431921005\n",
      "Epoch 80 \t Batch 100 \t Validation Loss: 31.455196409225465\n",
      "Epoch 80 \t Batch 120 \t Validation Loss: 31.27768731911977\n",
      "Epoch 80 \t Batch 140 \t Validation Loss: 30.82780225617545\n",
      "Epoch 80 \t Batch 160 \t Validation Loss: 32.23646714091301\n",
      "Epoch 80 \t Batch 180 \t Validation Loss: 35.44133528073629\n",
      "Epoch 80 \t Batch 200 \t Validation Loss: 36.596026616096495\n",
      "Epoch 80 \t Batch 220 \t Validation Loss: 37.77341817942533\n",
      "Epoch 80 \t Batch 240 \t Validation Loss: 38.012045641740166\n",
      "Epoch 80 \t Batch 260 \t Validation Loss: 40.03951429953942\n",
      "Epoch 80 \t Batch 280 \t Validation Loss: 41.085462253434315\n",
      "Epoch 80 \t Batch 300 \t Validation Loss: 41.96763058026632\n",
      "Epoch 80 \t Batch 320 \t Validation Loss: 42.32094447314739\n",
      "Epoch 80 \t Batch 340 \t Validation Loss: 42.136641465916355\n",
      "Epoch 80 \t Batch 360 \t Validation Loss: 41.863254123263886\n",
      "Epoch 80 \t Batch 380 \t Validation Loss: 42.01851471850747\n",
      "Epoch 80 \t Batch 400 \t Validation Loss: 41.48709808826447\n",
      "Epoch 80 \t Batch 420 \t Validation Loss: 41.392883659544445\n",
      "Epoch 80 \t Batch 440 \t Validation Loss: 41.03817173350941\n",
      "Epoch 80 \t Batch 460 \t Validation Loss: 41.13477329585863\n",
      "Epoch 80 \t Batch 480 \t Validation Loss: 41.51831133762995\n",
      "Epoch 80 \t Batch 500 \t Validation Loss: 41.1587604598999\n",
      "Epoch 80 \t Batch 520 \t Validation Loss: 40.83986680324261\n",
      "Epoch 80 \t Batch 540 \t Validation Loss: 40.45538383060031\n",
      "Epoch 80 \t Batch 560 \t Validation Loss: 40.11125658239637\n",
      "Epoch 80 \t Batch 580 \t Validation Loss: 39.74255582381939\n",
      "Epoch 80 \t Batch 600 \t Validation Loss: 39.847636059125264\n",
      "Epoch 80 Training Loss: 46.33914783190745 Validation Loss: 40.419412735220675\n",
      "Epoch 80 completed\n",
      "Epoch 81 \t Batch 20 \t Training Loss: 46.71775531768799\n",
      "Epoch 81 \t Batch 40 \t Training Loss: 46.195631408691405\n",
      "Epoch 81 \t Batch 60 \t Training Loss: 46.004120381673175\n",
      "Epoch 81 \t Batch 80 \t Training Loss: 46.02554388046265\n",
      "Epoch 81 \t Batch 100 \t Training Loss: 46.51832443237305\n",
      "Epoch 81 \t Batch 120 \t Training Loss: 46.401757621765135\n",
      "Epoch 81 \t Batch 140 \t Training Loss: 46.414975329807824\n",
      "Epoch 81 \t Batch 160 \t Training Loss: 46.46290082931519\n",
      "Epoch 81 \t Batch 180 \t Training Loss: 46.37706843482123\n",
      "Epoch 81 \t Batch 200 \t Training Loss: 46.327943305969235\n",
      "Epoch 81 \t Batch 220 \t Training Loss: 46.15125158483332\n",
      "Epoch 81 \t Batch 240 \t Training Loss: 46.078633546829224\n",
      "Epoch 81 \t Batch 260 \t Training Loss: 46.11119101597713\n",
      "Epoch 81 \t Batch 280 \t Training Loss: 46.06627781731742\n",
      "Epoch 81 \t Batch 300 \t Training Loss: 46.076144205729165\n",
      "Epoch 81 \t Batch 320 \t Training Loss: 46.058925688266754\n",
      "Epoch 81 \t Batch 340 \t Training Loss: 46.09482212066651\n",
      "Epoch 81 \t Batch 360 \t Training Loss: 46.09964291254679\n",
      "Epoch 81 \t Batch 380 \t Training Loss: 46.099990613836994\n",
      "Epoch 81 \t Batch 400 \t Training Loss: 46.154306116104124\n",
      "Epoch 81 \t Batch 420 \t Training Loss: 46.27018464406331\n",
      "Epoch 81 \t Batch 440 \t Training Loss: 46.238795852661134\n",
      "Epoch 81 \t Batch 460 \t Training Loss: 46.269849992835006\n",
      "Epoch 81 \t Batch 480 \t Training Loss: 46.25812623500824\n",
      "Epoch 81 \t Batch 500 \t Training Loss: 46.28461994934082\n",
      "Epoch 81 \t Batch 520 \t Training Loss: 46.29254713792067\n",
      "Epoch 81 \t Batch 540 \t Training Loss: 46.30067021405255\n",
      "Epoch 81 \t Batch 560 \t Training Loss: 46.30450302532741\n",
      "Epoch 81 \t Batch 580 \t Training Loss: 46.29978731089625\n",
      "Epoch 81 \t Batch 600 \t Training Loss: 46.331078612009684\n",
      "Epoch 81 \t Batch 620 \t Training Loss: 46.27507169785038\n",
      "Epoch 81 \t Batch 640 \t Training Loss: 46.268700557947156\n",
      "Epoch 81 \t Batch 660 \t Training Loss: 46.28857017863881\n",
      "Epoch 81 \t Batch 680 \t Training Loss: 46.26065716462977\n",
      "Epoch 81 \t Batch 700 \t Training Loss: 46.25351116725377\n",
      "Epoch 81 \t Batch 720 \t Training Loss: 46.23738227420383\n",
      "Epoch 81 \t Batch 740 \t Training Loss: 46.24572215209136\n",
      "Epoch 81 \t Batch 760 \t Training Loss: 46.27243573540135\n",
      "Epoch 81 \t Batch 780 \t Training Loss: 46.292584898532965\n",
      "Epoch 81 \t Batch 800 \t Training Loss: 46.276714134216306\n",
      "Epoch 81 \t Batch 820 \t Training Loss: 46.27985869616997\n",
      "Epoch 81 \t Batch 840 \t Training Loss: 46.2855044955299\n",
      "Epoch 81 \t Batch 860 \t Training Loss: 46.31877056387968\n",
      "Epoch 81 \t Batch 880 \t Training Loss: 46.33485415198586\n",
      "Epoch 81 \t Batch 900 \t Training Loss: 46.34139447530111\n",
      "Epoch 81 \t Batch 20 \t Validation Loss: 40.938383436203004\n",
      "Epoch 81 \t Batch 40 \t Validation Loss: 36.256979894638064\n",
      "Epoch 81 \t Batch 60 \t Validation Loss: 39.582875903447466\n",
      "Epoch 81 \t Batch 80 \t Validation Loss: 38.29752303361893\n",
      "Epoch 81 \t Batch 100 \t Validation Loss: 36.23947228431702\n",
      "Epoch 81 \t Batch 120 \t Validation Loss: 35.1466006676356\n",
      "Epoch 81 \t Batch 140 \t Validation Loss: 34.152314397266935\n",
      "Epoch 81 \t Batch 160 \t Validation Loss: 35.098570364713666\n",
      "Epoch 81 \t Batch 180 \t Validation Loss: 37.67190096643236\n",
      "Epoch 81 \t Batch 200 \t Validation Loss: 38.499139981269835\n",
      "Epoch 81 \t Batch 220 \t Validation Loss: 39.31117592291398\n",
      "Epoch 81 \t Batch 240 \t Validation Loss: 39.28600035508474\n",
      "Epoch 81 \t Batch 260 \t Validation Loss: 41.057594420359685\n",
      "Epoch 81 \t Batch 280 \t Validation Loss: 41.888215844971796\n",
      "Epoch 81 \t Batch 300 \t Validation Loss: 42.582991139094034\n",
      "Epoch 81 \t Batch 320 \t Validation Loss: 42.77514826357365\n",
      "Epoch 81 \t Batch 340 \t Validation Loss: 42.53171893007615\n",
      "Epoch 81 \t Batch 360 \t Validation Loss: 42.22243204116821\n",
      "Epoch 81 \t Batch 380 \t Validation Loss: 42.31591402354994\n",
      "Epoch 81 \t Batch 400 \t Validation Loss: 41.786330919265744\n",
      "Epoch 81 \t Batch 420 \t Validation Loss: 41.68738432384673\n",
      "Epoch 81 \t Batch 440 \t Validation Loss: 41.32512991211631\n",
      "Epoch 81 \t Batch 460 \t Validation Loss: 41.423632111756696\n",
      "Epoch 81 \t Batch 480 \t Validation Loss: 41.78128600517909\n",
      "Epoch 81 \t Batch 500 \t Validation Loss: 41.41406035232544\n",
      "Epoch 81 \t Batch 520 \t Validation Loss: 41.11578172903795\n",
      "Epoch 81 \t Batch 540 \t Validation Loss: 40.75774783381709\n",
      "Epoch 81 \t Batch 560 \t Validation Loss: 40.485259120804926\n",
      "Epoch 81 \t Batch 580 \t Validation Loss: 40.22108183564811\n",
      "Epoch 81 \t Batch 600 \t Validation Loss: 40.34124882698059\n",
      "Epoch 81 Training Loss: 46.34942158539916 Validation Loss: 41.00002782375782\n",
      "Epoch 81 completed\n",
      "Epoch 82 \t Batch 20 \t Training Loss: 44.60020790100098\n",
      "Epoch 82 \t Batch 40 \t Training Loss: 45.062935256958006\n",
      "Epoch 82 \t Batch 60 \t Training Loss: 45.3429069519043\n",
      "Epoch 82 \t Batch 80 \t Training Loss: 45.86906027793884\n",
      "Epoch 82 \t Batch 100 \t Training Loss: 46.01343994140625\n",
      "Epoch 82 \t Batch 120 \t Training Loss: 46.001167456309\n",
      "Epoch 82 \t Batch 140 \t Training Loss: 46.111962127685544\n",
      "Epoch 82 \t Batch 160 \t Training Loss: 46.049602675437924\n",
      "Epoch 82 \t Batch 180 \t Training Loss: 46.1159645292494\n",
      "Epoch 82 \t Batch 200 \t Training Loss: 46.095175170898436\n",
      "Epoch 82 \t Batch 220 \t Training Loss: 46.17318996082653\n",
      "Epoch 82 \t Batch 240 \t Training Loss: 46.141080077489214\n",
      "Epoch 82 \t Batch 260 \t Training Loss: 46.14802197676438\n",
      "Epoch 82 \t Batch 280 \t Training Loss: 46.094582080841064\n",
      "Epoch 82 \t Batch 300 \t Training Loss: 46.130453478495276\n",
      "Epoch 82 \t Batch 320 \t Training Loss: 46.01734048128128\n",
      "Epoch 82 \t Batch 340 \t Training Loss: 46.00585231781006\n",
      "Epoch 82 \t Batch 360 \t Training Loss: 46.14403194851346\n",
      "Epoch 82 \t Batch 380 \t Training Loss: 46.13149805570904\n",
      "Epoch 82 \t Batch 400 \t Training Loss: 46.072334413528445\n",
      "Epoch 82 \t Batch 420 \t Training Loss: 46.07192269279843\n",
      "Epoch 82 \t Batch 440 \t Training Loss: 46.0486198425293\n",
      "Epoch 82 \t Batch 460 \t Training Loss: 46.07416966479757\n",
      "Epoch 82 \t Batch 480 \t Training Loss: 46.11084436575572\n",
      "Epoch 82 \t Batch 500 \t Training Loss: 46.10947172546387\n",
      "Epoch 82 \t Batch 520 \t Training Loss: 46.14690085190993\n",
      "Epoch 82 \t Batch 540 \t Training Loss: 46.12157283359104\n",
      "Epoch 82 \t Batch 560 \t Training Loss: 46.11434373855591\n",
      "Epoch 82 \t Batch 580 \t Training Loss: 46.10193426855679\n",
      "Epoch 82 \t Batch 600 \t Training Loss: 46.09771380106608\n",
      "Epoch 82 \t Batch 620 \t Training Loss: 46.15355825116558\n",
      "Epoch 82 \t Batch 640 \t Training Loss: 46.147434359788896\n",
      "Epoch 82 \t Batch 660 \t Training Loss: 46.13997852441037\n",
      "Epoch 82 \t Batch 680 \t Training Loss: 46.15646892996396\n",
      "Epoch 82 \t Batch 700 \t Training Loss: 46.167771056038994\n",
      "Epoch 82 \t Batch 720 \t Training Loss: 46.22234401702881\n",
      "Epoch 82 \t Batch 740 \t Training Loss: 46.19871126638876\n",
      "Epoch 82 \t Batch 760 \t Training Loss: 46.21685972715679\n",
      "Epoch 82 \t Batch 780 \t Training Loss: 46.21792664161095\n",
      "Epoch 82 \t Batch 800 \t Training Loss: 46.21729906082153\n",
      "Epoch 82 \t Batch 820 \t Training Loss: 46.256637661631515\n",
      "Epoch 82 \t Batch 840 \t Training Loss: 46.27431252343314\n",
      "Epoch 82 \t Batch 860 \t Training Loss: 46.28394200968188\n",
      "Epoch 82 \t Batch 880 \t Training Loss: 46.270881951938975\n",
      "Epoch 82 \t Batch 900 \t Training Loss: 46.293115073310005\n",
      "Epoch 82 \t Batch 20 \t Validation Loss: 31.685819387435913\n",
      "Epoch 82 \t Batch 40 \t Validation Loss: 29.696183156967162\n",
      "Epoch 82 \t Batch 60 \t Validation Loss: 31.971202023824056\n",
      "Epoch 82 \t Batch 80 \t Validation Loss: 31.417665469646455\n",
      "Epoch 82 \t Batch 100 \t Validation Loss: 30.71788197517395\n",
      "Epoch 82 \t Batch 120 \t Validation Loss: 30.651006031036378\n",
      "Epoch 82 \t Batch 140 \t Validation Loss: 30.34508282797677\n",
      "Epoch 82 \t Batch 160 \t Validation Loss: 31.78155908584595\n",
      "Epoch 82 \t Batch 180 \t Validation Loss: 34.687022897932266\n",
      "Epoch 82 \t Batch 200 \t Validation Loss: 35.85952869415283\n",
      "Epoch 82 \t Batch 220 \t Validation Loss: 36.89005918502808\n",
      "Epoch 82 \t Batch 240 \t Validation Loss: 37.054945623874666\n",
      "Epoch 82 \t Batch 260 \t Validation Loss: 38.97416637127216\n",
      "Epoch 82 \t Batch 280 \t Validation Loss: 39.93889360087258\n",
      "Epoch 82 \t Batch 300 \t Validation Loss: 40.739047651290896\n",
      "Epoch 82 \t Batch 320 \t Validation Loss: 41.05247355401516\n",
      "Epoch 82 \t Batch 340 \t Validation Loss: 40.92740686360528\n",
      "Epoch 82 \t Batch 360 \t Validation Loss: 40.70918702284495\n",
      "Epoch 82 \t Batch 380 \t Validation Loss: 40.913105229327556\n",
      "Epoch 82 \t Batch 400 \t Validation Loss: 40.52656141996383\n",
      "Epoch 82 \t Batch 420 \t Validation Loss: 40.50916427203587\n",
      "Epoch 82 \t Batch 440 \t Validation Loss: 40.27569060542367\n",
      "Epoch 82 \t Batch 460 \t Validation Loss: 40.45308323943097\n",
      "Epoch 82 \t Batch 480 \t Validation Loss: 40.862040517727536\n",
      "Epoch 82 \t Batch 500 \t Validation Loss: 40.53562701606751\n",
      "Epoch 82 \t Batch 520 \t Validation Loss: 40.34724326317127\n",
      "Epoch 82 \t Batch 540 \t Validation Loss: 40.021526953026104\n",
      "Epoch 82 \t Batch 560 \t Validation Loss: 39.78783067805426\n",
      "Epoch 82 \t Batch 580 \t Validation Loss: 39.55335245296873\n",
      "Epoch 82 \t Batch 600 \t Validation Loss: 39.69390624841054\n",
      "Epoch 82 Training Loss: 46.297912431257245 Validation Loss: 40.33130410894171\n",
      "Epoch 82 completed\n",
      "Epoch 83 \t Batch 20 \t Training Loss: 46.10402946472168\n",
      "Epoch 83 \t Batch 40 \t Training Loss: 46.381482696533205\n",
      "Epoch 83 \t Batch 60 \t Training Loss: 46.53495616912842\n",
      "Epoch 83 \t Batch 80 \t Training Loss: 46.32116250991821\n",
      "Epoch 83 \t Batch 100 \t Training Loss: 46.28747200012207\n",
      "Epoch 83 \t Batch 120 \t Training Loss: 46.28056154251099\n",
      "Epoch 83 \t Batch 140 \t Training Loss: 46.14666862487793\n",
      "Epoch 83 \t Batch 160 \t Training Loss: 46.09450755119324\n",
      "Epoch 83 \t Batch 180 \t Training Loss: 45.76050646040175\n",
      "Epoch 83 \t Batch 200 \t Training Loss: 45.77891437530518\n",
      "Epoch 83 \t Batch 220 \t Training Loss: 45.895964778553356\n",
      "Epoch 83 \t Batch 240 \t Training Loss: 46.08300048510234\n",
      "Epoch 83 \t Batch 260 \t Training Loss: 46.200199890136716\n",
      "Epoch 83 \t Batch 280 \t Training Loss: 46.19069463184902\n",
      "Epoch 83 \t Batch 300 \t Training Loss: 46.15827439626058\n",
      "Epoch 83 \t Batch 320 \t Training Loss: 46.0915408372879\n",
      "Epoch 83 \t Batch 340 \t Training Loss: 46.140977197534895\n",
      "Epoch 83 \t Batch 360 \t Training Loss: 46.07161548402574\n",
      "Epoch 83 \t Batch 380 \t Training Loss: 46.19345007444683\n",
      "Epoch 83 \t Batch 400 \t Training Loss: 46.21059874534607\n",
      "Epoch 83 \t Batch 420 \t Training Loss: 46.25741855984642\n",
      "Epoch 83 \t Batch 440 \t Training Loss: 46.25317224155773\n",
      "Epoch 83 \t Batch 460 \t Training Loss: 46.31216581593389\n",
      "Epoch 83 \t Batch 480 \t Training Loss: 46.27998010317484\n",
      "Epoch 83 \t Batch 500 \t Training Loss: 46.29380544281006\n",
      "Epoch 83 \t Batch 520 \t Training Loss: 46.295260884211615\n",
      "Epoch 83 \t Batch 540 \t Training Loss: 46.30180937449138\n",
      "Epoch 83 \t Batch 560 \t Training Loss: 46.250947625296455\n",
      "Epoch 83 \t Batch 580 \t Training Loss: 46.26422419054755\n",
      "Epoch 83 \t Batch 600 \t Training Loss: 46.25770975112915\n",
      "Epoch 83 \t Batch 620 \t Training Loss: 46.25398710927656\n",
      "Epoch 83 \t Batch 640 \t Training Loss: 46.2650962471962\n",
      "Epoch 83 \t Batch 660 \t Training Loss: 46.253966464418355\n",
      "Epoch 83 \t Batch 680 \t Training Loss: 46.2227388494155\n",
      "Epoch 83 \t Batch 700 \t Training Loss: 46.23262164524623\n",
      "Epoch 83 \t Batch 720 \t Training Loss: 46.26892302301195\n",
      "Epoch 83 \t Batch 740 \t Training Loss: 46.2386968406471\n",
      "Epoch 83 \t Batch 760 \t Training Loss: 46.29180538277877\n",
      "Epoch 83 \t Batch 780 \t Training Loss: 46.28992081666604\n",
      "Epoch 83 \t Batch 800 \t Training Loss: 46.2989311504364\n",
      "Epoch 83 \t Batch 820 \t Training Loss: 46.2686642809612\n",
      "Epoch 83 \t Batch 840 \t Training Loss: 46.27179180326916\n",
      "Epoch 83 \t Batch 860 \t Training Loss: 46.268188108399855\n",
      "Epoch 83 \t Batch 880 \t Training Loss: 46.23824529214339\n",
      "Epoch 83 \t Batch 900 \t Training Loss: 46.26760271708171\n",
      "Epoch 83 \t Batch 20 \t Validation Loss: 34.98363213539123\n",
      "Epoch 83 \t Batch 40 \t Validation Loss: 31.637000679969788\n",
      "Epoch 83 \t Batch 60 \t Validation Loss: 34.35197393099467\n",
      "Epoch 83 \t Batch 80 \t Validation Loss: 33.517739057540894\n",
      "Epoch 83 \t Batch 100 \t Validation Loss: 32.21849088668823\n",
      "Epoch 83 \t Batch 120 \t Validation Loss: 31.706707032521567\n",
      "Epoch 83 \t Batch 140 \t Validation Loss: 31.114228779929025\n",
      "Epoch 83 \t Batch 160 \t Validation Loss: 32.34940390586853\n",
      "Epoch 83 \t Batch 180 \t Validation Loss: 35.17257630030314\n",
      "Epoch 83 \t Batch 200 \t Validation Loss: 36.22694814682007\n",
      "Epoch 83 \t Batch 220 \t Validation Loss: 37.207293995943935\n",
      "Epoch 83 \t Batch 240 \t Validation Loss: 37.331000395615895\n",
      "Epoch 83 \t Batch 260 \t Validation Loss: 39.200625746066756\n",
      "Epoch 83 \t Batch 280 \t Validation Loss: 40.123723319598604\n",
      "Epoch 83 \t Batch 300 \t Validation Loss: 40.88858338991801\n",
      "Epoch 83 \t Batch 320 \t Validation Loss: 41.16857058703899\n",
      "Epoch 83 \t Batch 340 \t Validation Loss: 40.99424335535835\n",
      "Epoch 83 \t Batch 360 \t Validation Loss: 40.72346605459849\n",
      "Epoch 83 \t Batch 380 \t Validation Loss: 40.87703579099555\n",
      "Epoch 83 \t Batch 400 \t Validation Loss: 40.41018780946732\n",
      "Epoch 83 \t Batch 420 \t Validation Loss: 40.350616757074995\n",
      "Epoch 83 \t Batch 440 \t Validation Loss: 40.02334246852181\n",
      "Epoch 83 \t Batch 460 \t Validation Loss: 40.148736897758816\n",
      "Epoch 83 \t Batch 480 \t Validation Loss: 40.54233155846596\n",
      "Epoch 83 \t Batch 500 \t Validation Loss: 40.19750008964539\n",
      "Epoch 83 \t Batch 520 \t Validation Loss: 39.92640894743113\n",
      "Epoch 83 \t Batch 540 \t Validation Loss: 39.57587958795053\n",
      "Epoch 83 \t Batch 560 \t Validation Loss: 39.30489247185843\n",
      "Epoch 83 \t Batch 580 \t Validation Loss: 39.02000289785451\n",
      "Epoch 83 \t Batch 600 \t Validation Loss: 39.15681062062581\n",
      "Epoch 83 Training Loss: 46.29900445802911 Validation Loss: 39.7726622339967\n",
      "Epoch 83 completed\n",
      "Epoch 84 \t Batch 20 \t Training Loss: 45.53537368774414\n",
      "Epoch 84 \t Batch 40 \t Training Loss: 45.08979597091675\n",
      "Epoch 84 \t Batch 60 \t Training Loss: 45.366726303100585\n",
      "Epoch 84 \t Batch 80 \t Training Loss: 45.231072473526\n",
      "Epoch 84 \t Batch 100 \t Training Loss: 45.42711330413818\n",
      "Epoch 84 \t Batch 120 \t Training Loss: 45.62607065836588\n",
      "Epoch 84 \t Batch 140 \t Training Loss: 45.746929223196844\n",
      "Epoch 84 \t Batch 160 \t Training Loss: 46.06850388050079\n",
      "Epoch 84 \t Batch 180 \t Training Loss: 46.11692089504666\n",
      "Epoch 84 \t Batch 200 \t Training Loss: 46.16603578567505\n",
      "Epoch 84 \t Batch 220 \t Training Loss: 46.09455491846258\n",
      "Epoch 84 \t Batch 240 \t Training Loss: 46.22827436129252\n",
      "Epoch 84 \t Batch 260 \t Training Loss: 46.214232121981105\n",
      "Epoch 84 \t Batch 280 \t Training Loss: 46.26490185601371\n",
      "Epoch 84 \t Batch 300 \t Training Loss: 46.24865426381429\n",
      "Epoch 84 \t Batch 320 \t Training Loss: 46.28979051113129\n",
      "Epoch 84 \t Batch 340 \t Training Loss: 46.27828684414134\n",
      "Epoch 84 \t Batch 360 \t Training Loss: 46.2442703988817\n",
      "Epoch 84 \t Batch 380 \t Training Loss: 46.25873767451236\n",
      "Epoch 84 \t Batch 400 \t Training Loss: 46.2737219619751\n",
      "Epoch 84 \t Batch 420 \t Training Loss: 46.231951768057684\n",
      "Epoch 84 \t Batch 440 \t Training Loss: 46.19024777845903\n",
      "Epoch 84 \t Batch 460 \t Training Loss: 46.173923674873684\n",
      "Epoch 84 \t Batch 480 \t Training Loss: 46.18252607981364\n",
      "Epoch 84 \t Batch 500 \t Training Loss: 46.16940938568115\n",
      "Epoch 84 \t Batch 520 \t Training Loss: 46.11805520424476\n",
      "Epoch 84 \t Batch 540 \t Training Loss: 46.15882400230125\n",
      "Epoch 84 \t Batch 560 \t Training Loss: 46.135578175953455\n",
      "Epoch 84 \t Batch 580 \t Training Loss: 46.18200274500354\n",
      "Epoch 84 \t Batch 600 \t Training Loss: 46.143755423227944\n",
      "Epoch 84 \t Batch 620 \t Training Loss: 46.17655899909235\n",
      "Epoch 84 \t Batch 640 \t Training Loss: 46.22489436268806\n",
      "Epoch 84 \t Batch 660 \t Training Loss: 46.30929505897291\n",
      "Epoch 84 \t Batch 680 \t Training Loss: 46.3076935599832\n",
      "Epoch 84 \t Batch 700 \t Training Loss: 46.294389637538366\n",
      "Epoch 84 \t Batch 720 \t Training Loss: 46.27809077368842\n",
      "Epoch 84 \t Batch 740 \t Training Loss: 46.25312581964441\n",
      "Epoch 84 \t Batch 760 \t Training Loss: 46.29203238738211\n",
      "Epoch 84 \t Batch 780 \t Training Loss: 46.332839217552774\n",
      "Epoch 84 \t Batch 800 \t Training Loss: 46.31863681316376\n",
      "Epoch 84 \t Batch 820 \t Training Loss: 46.33714200461783\n",
      "Epoch 84 \t Batch 840 \t Training Loss: 46.315550204685756\n",
      "Epoch 84 \t Batch 860 \t Training Loss: 46.31584934412047\n",
      "Epoch 84 \t Batch 880 \t Training Loss: 46.28235812187195\n",
      "Epoch 84 \t Batch 900 \t Training Loss: 46.26845706939697\n",
      "Epoch 84 \t Batch 20 \t Validation Loss: 50.4193416595459\n",
      "Epoch 84 \t Batch 40 \t Validation Loss: 43.771706748008725\n",
      "Epoch 84 \t Batch 60 \t Validation Loss: 48.03285150527954\n",
      "Epoch 84 \t Batch 80 \t Validation Loss: 46.07869929075241\n",
      "Epoch 84 \t Batch 100 \t Validation Loss: 42.42266507148743\n",
      "Epoch 84 \t Batch 120 \t Validation Loss: 40.371537518501285\n",
      "Epoch 84 \t Batch 140 \t Validation Loss: 38.622979579653055\n",
      "Epoch 84 \t Batch 160 \t Validation Loss: 39.13758475184441\n",
      "Epoch 84 \t Batch 180 \t Validation Loss: 41.6722074508667\n",
      "Epoch 84 \t Batch 200 \t Validation Loss: 42.28807731628418\n",
      "Epoch 84 \t Batch 220 \t Validation Loss: 42.982702645388514\n",
      "Epoch 84 \t Batch 240 \t Validation Loss: 42.82900307973226\n",
      "Epoch 84 \t Batch 260 \t Validation Loss: 44.481193513136645\n",
      "Epoch 84 \t Batch 280 \t Validation Loss: 45.174334859848024\n",
      "Epoch 84 \t Batch 300 \t Validation Loss: 45.89156098047892\n",
      "Epoch 84 \t Batch 320 \t Validation Loss: 46.03321050405502\n",
      "Epoch 84 \t Batch 340 \t Validation Loss: 45.63282591875862\n",
      "Epoch 84 \t Batch 360 \t Validation Loss: 45.202618005540636\n",
      "Epoch 84 \t Batch 380 \t Validation Loss: 45.16956201352571\n",
      "Epoch 84 \t Batch 400 \t Validation Loss: 44.4901313829422\n",
      "Epoch 84 \t Batch 420 \t Validation Loss: 44.285799780346096\n",
      "Epoch 84 \t Batch 440 \t Validation Loss: 43.79420215433294\n",
      "Epoch 84 \t Batch 460 \t Validation Loss: 43.775867458011795\n",
      "Epoch 84 \t Batch 480 \t Validation Loss: 44.08032277027766\n",
      "Epoch 84 \t Batch 500 \t Validation Loss: 43.63539640808106\n",
      "Epoch 84 \t Batch 520 \t Validation Loss: 43.20372381210327\n",
      "Epoch 84 \t Batch 540 \t Validation Loss: 42.73468512429132\n",
      "Epoch 84 \t Batch 560 \t Validation Loss: 42.31405351502555\n",
      "Epoch 84 \t Batch 580 \t Validation Loss: 41.833058524953906\n",
      "Epoch 84 \t Batch 600 \t Validation Loss: 41.882874984741214\n",
      "Epoch 84 Training Loss: 46.281803251742794 Validation Loss: 42.414986508233206\n",
      "Epoch 84 completed\n",
      "Epoch 85 \t Batch 20 \t Training Loss: 46.26006965637207\n",
      "Epoch 85 \t Batch 40 \t Training Loss: 46.01669607162476\n",
      "Epoch 85 \t Batch 60 \t Training Loss: 46.027146085103354\n",
      "Epoch 85 \t Batch 80 \t Training Loss: 46.148955774307254\n",
      "Epoch 85 \t Batch 100 \t Training Loss: 46.07887027740478\n",
      "Epoch 85 \t Batch 120 \t Training Loss: 46.15688002904256\n",
      "Epoch 85 \t Batch 140 \t Training Loss: 46.43755781991141\n",
      "Epoch 85 \t Batch 160 \t Training Loss: 46.41654098033905\n",
      "Epoch 85 \t Batch 180 \t Training Loss: 46.411854108174644\n",
      "Epoch 85 \t Batch 200 \t Training Loss: 46.24846715927124\n",
      "Epoch 85 \t Batch 220 \t Training Loss: 46.254815812544386\n",
      "Epoch 85 \t Batch 240 \t Training Loss: 46.26981043815613\n",
      "Epoch 85 \t Batch 260 \t Training Loss: 46.18344416985145\n",
      "Epoch 85 \t Batch 280 \t Training Loss: 46.10898057392665\n",
      "Epoch 85 \t Batch 300 \t Training Loss: 46.09452035268148\n",
      "Epoch 85 \t Batch 320 \t Training Loss: 46.10561276674271\n",
      "Epoch 85 \t Batch 340 \t Training Loss: 46.15803693883559\n",
      "Epoch 85 \t Batch 360 \t Training Loss: 46.22865001890394\n",
      "Epoch 85 \t Batch 380 \t Training Loss: 46.163612305490595\n",
      "Epoch 85 \t Batch 400 \t Training Loss: 46.20349666595459\n",
      "Epoch 85 \t Batch 420 \t Training Loss: 46.1570214680263\n",
      "Epoch 85 \t Batch 440 \t Training Loss: 46.209793324904005\n",
      "Epoch 85 \t Batch 460 \t Training Loss: 46.19480375206989\n",
      "Epoch 85 \t Batch 480 \t Training Loss: 46.25338632265727\n",
      "Epoch 85 \t Batch 500 \t Training Loss: 46.24663226318359\n",
      "Epoch 85 \t Batch 520 \t Training Loss: 46.32212327810434\n",
      "Epoch 85 \t Batch 540 \t Training Loss: 46.32867112336336\n",
      "Epoch 85 \t Batch 560 \t Training Loss: 46.29812748091562\n",
      "Epoch 85 \t Batch 580 \t Training Loss: 46.28697782056085\n",
      "Epoch 85 \t Batch 600 \t Training Loss: 46.28332724253337\n",
      "Epoch 85 \t Batch 620 \t Training Loss: 46.274841610077885\n",
      "Epoch 85 \t Batch 640 \t Training Loss: 46.275544840097425\n",
      "Epoch 85 \t Batch 660 \t Training Loss: 46.30745090715813\n",
      "Epoch 85 \t Batch 680 \t Training Loss: 46.304556156607234\n",
      "Epoch 85 \t Batch 700 \t Training Loss: 46.31103183746338\n",
      "Epoch 85 \t Batch 720 \t Training Loss: 46.337455378638374\n",
      "Epoch 85 \t Batch 740 \t Training Loss: 46.294372331773914\n",
      "Epoch 85 \t Batch 760 \t Training Loss: 46.29150581359863\n",
      "Epoch 85 \t Batch 780 \t Training Loss: 46.293458361503404\n",
      "Epoch 85 \t Batch 800 \t Training Loss: 46.2457560300827\n",
      "Epoch 85 \t Batch 820 \t Training Loss: 46.27472501615198\n",
      "Epoch 85 \t Batch 840 \t Training Loss: 46.28811304001581\n",
      "Epoch 85 \t Batch 860 \t Training Loss: 46.286324931299966\n",
      "Epoch 85 \t Batch 880 \t Training Loss: 46.278423846851695\n",
      "Epoch 85 \t Batch 900 \t Training Loss: 46.27824272579617\n",
      "Epoch 85 \t Batch 20 \t Validation Loss: 34.2320191860199\n",
      "Epoch 85 \t Batch 40 \t Validation Loss: 31.38182032108307\n",
      "Epoch 85 \t Batch 60 \t Validation Loss: 34.132182661692305\n",
      "Epoch 85 \t Batch 80 \t Validation Loss: 33.16619782447815\n",
      "Epoch 85 \t Batch 100 \t Validation Loss: 31.996944522857667\n",
      "Epoch 85 \t Batch 120 \t Validation Loss: 31.47724852561951\n",
      "Epoch 85 \t Batch 140 \t Validation Loss: 30.894760867527552\n",
      "Epoch 85 \t Batch 160 \t Validation Loss: 32.23286592960358\n",
      "Epoch 85 \t Batch 180 \t Validation Loss: 35.107728762096826\n",
      "Epoch 85 \t Batch 200 \t Validation Loss: 36.20066824436188\n",
      "Epoch 85 \t Batch 220 \t Validation Loss: 37.25603425719521\n",
      "Epoch 85 \t Batch 240 \t Validation Loss: 37.40607755978902\n",
      "Epoch 85 \t Batch 260 \t Validation Loss: 39.32001412098224\n",
      "Epoch 85 \t Batch 280 \t Validation Loss: 40.274262465749466\n",
      "Epoch 85 \t Batch 300 \t Validation Loss: 41.03188645998637\n",
      "Epoch 85 \t Batch 320 \t Validation Loss: 41.32435432374477\n",
      "Epoch 85 \t Batch 340 \t Validation Loss: 41.160489685395184\n",
      "Epoch 85 \t Batch 360 \t Validation Loss: 40.90285322401259\n",
      "Epoch 85 \t Batch 380 \t Validation Loss: 41.05896119569477\n",
      "Epoch 85 \t Batch 400 \t Validation Loss: 40.565460958480834\n",
      "Epoch 85 \t Batch 420 \t Validation Loss: 40.52732652936663\n",
      "Epoch 85 \t Batch 440 \t Validation Loss: 40.16860827099193\n",
      "Epoch 85 \t Batch 460 \t Validation Loss: 40.27454073947409\n",
      "Epoch 85 \t Batch 480 \t Validation Loss: 40.700878028074904\n",
      "Epoch 85 \t Batch 500 \t Validation Loss: 40.370799774169924\n",
      "Epoch 85 \t Batch 520 \t Validation Loss: 40.039170382573054\n",
      "Epoch 85 \t Batch 540 \t Validation Loss: 39.69186591042413\n",
      "Epoch 85 \t Batch 560 \t Validation Loss: 39.411656682831904\n",
      "Epoch 85 \t Batch 580 \t Validation Loss: 39.110683280024034\n",
      "Epoch 85 \t Batch 600 \t Validation Loss: 39.2601833375295\n",
      "Epoch 85 Training Loss: 46.26499226361091 Validation Loss: 39.88516264147573\n",
      "Epoch 85 completed\n",
      "Epoch 86 \t Batch 20 \t Training Loss: 46.78342533111572\n",
      "Epoch 86 \t Batch 40 \t Training Loss: 46.6168496131897\n",
      "Epoch 86 \t Batch 60 \t Training Loss: 46.46003112792969\n",
      "Epoch 86 \t Batch 80 \t Training Loss: 46.44078545570373\n",
      "Epoch 86 \t Batch 100 \t Training Loss: 46.43268421173096\n",
      "Epoch 86 \t Batch 120 \t Training Loss: 46.30683002471924\n",
      "Epoch 86 \t Batch 140 \t Training Loss: 46.19227583748954\n",
      "Epoch 86 \t Batch 160 \t Training Loss: 46.321695756912234\n",
      "Epoch 86 \t Batch 180 \t Training Loss: 46.49886514875624\n",
      "Epoch 86 \t Batch 200 \t Training Loss: 46.4924271774292\n",
      "Epoch 86 \t Batch 220 \t Training Loss: 46.58985982374711\n",
      "Epoch 86 \t Batch 240 \t Training Loss: 46.41748751004537\n",
      "Epoch 86 \t Batch 260 \t Training Loss: 46.399182671767015\n",
      "Epoch 86 \t Batch 280 \t Training Loss: 46.36283982140677\n",
      "Epoch 86 \t Batch 300 \t Training Loss: 46.38448844909668\n",
      "Epoch 86 \t Batch 320 \t Training Loss: 46.39525512456894\n",
      "Epoch 86 \t Batch 340 \t Training Loss: 46.44582300747142\n",
      "Epoch 86 \t Batch 360 \t Training Loss: 46.454112678103975\n",
      "Epoch 86 \t Batch 380 \t Training Loss: 46.416519345735246\n",
      "Epoch 86 \t Batch 400 \t Training Loss: 46.39065426826477\n",
      "Epoch 86 \t Batch 420 \t Training Loss: 46.422641518002465\n",
      "Epoch 86 \t Batch 440 \t Training Loss: 46.42756092765114\n",
      "Epoch 86 \t Batch 460 \t Training Loss: 46.46863493712052\n",
      "Epoch 86 \t Batch 480 \t Training Loss: 46.48507882754008\n",
      "Epoch 86 \t Batch 500 \t Training Loss: 46.456998321533206\n",
      "Epoch 86 \t Batch 520 \t Training Loss: 46.4693652006296\n",
      "Epoch 86 \t Batch 540 \t Training Loss: 46.4568382051256\n",
      "Epoch 86 \t Batch 560 \t Training Loss: 46.40265805380685\n",
      "Epoch 86 \t Batch 580 \t Training Loss: 46.342227008424956\n",
      "Epoch 86 \t Batch 600 \t Training Loss: 46.35742167154948\n",
      "Epoch 86 \t Batch 620 \t Training Loss: 46.36275234837686\n",
      "Epoch 86 \t Batch 640 \t Training Loss: 46.35654688477516\n",
      "Epoch 86 \t Batch 660 \t Training Loss: 46.35797709840717\n",
      "Epoch 86 \t Batch 680 \t Training Loss: 46.38488288767198\n",
      "Epoch 86 \t Batch 700 \t Training Loss: 46.426943146841865\n",
      "Epoch 86 \t Batch 720 \t Training Loss: 46.40724470350477\n",
      "Epoch 86 \t Batch 740 \t Training Loss: 46.412625596329974\n",
      "Epoch 86 \t Batch 760 \t Training Loss: 46.38121346925434\n",
      "Epoch 86 \t Batch 780 \t Training Loss: 46.36153561518743\n",
      "Epoch 86 \t Batch 800 \t Training Loss: 46.32592717170715\n",
      "Epoch 86 \t Batch 820 \t Training Loss: 46.361268197036374\n",
      "Epoch 86 \t Batch 840 \t Training Loss: 46.32470704487392\n",
      "Epoch 86 \t Batch 860 \t Training Loss: 46.2967729080555\n",
      "Epoch 86 \t Batch 880 \t Training Loss: 46.29265727563338\n",
      "Epoch 86 \t Batch 900 \t Training Loss: 46.265483411153156\n",
      "Epoch 86 \t Batch 20 \t Validation Loss: 33.78014159202576\n",
      "Epoch 86 \t Batch 40 \t Validation Loss: 30.974484610557557\n",
      "Epoch 86 \t Batch 60 \t Validation Loss: 33.65422585805257\n",
      "Epoch 86 \t Batch 80 \t Validation Loss: 32.979006063938144\n",
      "Epoch 86 \t Batch 100 \t Validation Loss: 31.83688054084778\n",
      "Epoch 86 \t Batch 120 \t Validation Loss: 31.427677575747172\n",
      "Epoch 86 \t Batch 140 \t Validation Loss: 30.898527124949865\n",
      "Epoch 86 \t Batch 160 \t Validation Loss: 32.161886614561084\n",
      "Epoch 86 \t Batch 180 \t Validation Loss: 34.83708169195387\n",
      "Epoch 86 \t Batch 200 \t Validation Loss: 35.775729622840885\n",
      "Epoch 86 \t Batch 220 \t Validation Loss: 36.709474203803325\n",
      "Epoch 86 \t Batch 240 \t Validation Loss: 36.79533512592316\n",
      "Epoch 86 \t Batch 260 \t Validation Loss: 38.638698313786435\n",
      "Epoch 86 \t Batch 280 \t Validation Loss: 39.54103036608015\n",
      "Epoch 86 \t Batch 300 \t Validation Loss: 40.239014574686685\n",
      "Epoch 86 \t Batch 320 \t Validation Loss: 40.48670380115509\n",
      "Epoch 86 \t Batch 340 \t Validation Loss: 40.322092847263114\n",
      "Epoch 86 \t Batch 360 \t Validation Loss: 40.079541805055406\n",
      "Epoch 86 \t Batch 380 \t Validation Loss: 40.23484057376259\n",
      "Epoch 86 \t Batch 400 \t Validation Loss: 39.778855628967285\n",
      "Epoch 86 \t Batch 420 \t Validation Loss: 39.72786998748779\n",
      "Epoch 86 \t Batch 440 \t Validation Loss: 39.41197364547036\n",
      "Epoch 86 \t Batch 460 \t Validation Loss: 39.55913731948189\n",
      "Epoch 86 \t Batch 480 \t Validation Loss: 39.96068833271662\n",
      "Epoch 86 \t Batch 500 \t Validation Loss: 39.61616759109497\n",
      "Epoch 86 \t Batch 520 \t Validation Loss: 39.35592995423537\n",
      "Epoch 86 \t Batch 540 \t Validation Loss: 39.02650522302698\n",
      "Epoch 86 \t Batch 560 \t Validation Loss: 38.76244882856096\n",
      "Epoch 86 \t Batch 580 \t Validation Loss: 38.499750617454794\n",
      "Epoch 86 \t Batch 600 \t Validation Loss: 38.64637675603231\n",
      "Epoch 86 Training Loss: 46.26688028915888 Validation Loss: 39.25546603078966\n",
      "Epoch 86 completed\n",
      "Epoch 87 \t Batch 20 \t Training Loss: 45.74241409301758\n",
      "Epoch 87 \t Batch 40 \t Training Loss: 47.031885623931885\n",
      "Epoch 87 \t Batch 60 \t Training Loss: 46.86193161010742\n",
      "Epoch 87 \t Batch 80 \t Training Loss: 46.742614126205446\n",
      "Epoch 87 \t Batch 100 \t Training Loss: 46.54193035125732\n",
      "Epoch 87 \t Batch 120 \t Training Loss: 46.401549402872725\n",
      "Epoch 87 \t Batch 140 \t Training Loss: 46.5353974206107\n",
      "Epoch 87 \t Batch 160 \t Training Loss: 46.48534090518952\n",
      "Epoch 87 \t Batch 180 \t Training Loss: 46.5100393931071\n",
      "Epoch 87 \t Batch 200 \t Training Loss: 46.51601879119873\n",
      "Epoch 87 \t Batch 220 \t Training Loss: 46.54225586977872\n",
      "Epoch 87 \t Batch 240 \t Training Loss: 46.56781328519185\n",
      "Epoch 87 \t Batch 260 \t Training Loss: 46.52835495288556\n",
      "Epoch 87 \t Batch 280 \t Training Loss: 46.528966317858014\n",
      "Epoch 87 \t Batch 300 \t Training Loss: 46.44335576375325\n",
      "Epoch 87 \t Batch 320 \t Training Loss: 46.40814455747604\n",
      "Epoch 87 \t Batch 340 \t Training Loss: 46.296425482806036\n",
      "Epoch 87 \t Batch 360 \t Training Loss: 46.19393695195516\n",
      "Epoch 87 \t Batch 380 \t Training Loss: 46.146096289785284\n",
      "Epoch 87 \t Batch 400 \t Training Loss: 46.161357145309445\n",
      "Epoch 87 \t Batch 420 \t Training Loss: 46.16421474275135\n",
      "Epoch 87 \t Batch 440 \t Training Loss: 46.15529888326471\n",
      "Epoch 87 \t Batch 460 \t Training Loss: 46.157417620783264\n",
      "Epoch 87 \t Batch 480 \t Training Loss: 46.14258708159129\n",
      "Epoch 87 \t Batch 500 \t Training Loss: 46.19982025909424\n",
      "Epoch 87 \t Batch 520 \t Training Loss: 46.172095445486214\n",
      "Epoch 87 \t Batch 540 \t Training Loss: 46.20931330786811\n",
      "Epoch 87 \t Batch 560 \t Training Loss: 46.21822022710528\n",
      "Epoch 87 \t Batch 580 \t Training Loss: 46.20677588890339\n",
      "Epoch 87 \t Batch 600 \t Training Loss: 46.21854665756226\n",
      "Epoch 87 \t Batch 620 \t Training Loss: 46.225804765762824\n",
      "Epoch 87 \t Batch 640 \t Training Loss: 46.21625424027443\n",
      "Epoch 87 \t Batch 660 \t Training Loss: 46.202763516975175\n",
      "Epoch 87 \t Batch 680 \t Training Loss: 46.172449740241554\n",
      "Epoch 87 \t Batch 700 \t Training Loss: 46.183337385995046\n",
      "Epoch 87 \t Batch 720 \t Training Loss: 46.20199744966295\n",
      "Epoch 87 \t Batch 740 \t Training Loss: 46.22674551783381\n",
      "Epoch 87 \t Batch 760 \t Training Loss: 46.20749573456614\n",
      "Epoch 87 \t Batch 780 \t Training Loss: 46.22960679959028\n",
      "Epoch 87 \t Batch 800 \t Training Loss: 46.23488910198212\n",
      "Epoch 87 \t Batch 820 \t Training Loss: 46.22170557161657\n",
      "Epoch 87 \t Batch 840 \t Training Loss: 46.244284071241104\n",
      "Epoch 87 \t Batch 860 \t Training Loss: 46.256057961042536\n",
      "Epoch 87 \t Batch 880 \t Training Loss: 46.25129800276323\n",
      "Epoch 87 \t Batch 900 \t Training Loss: 46.25358010609945\n",
      "Epoch 87 \t Batch 20 \t Validation Loss: 38.70132098197937\n",
      "Epoch 87 \t Batch 40 \t Validation Loss: 34.84620261192322\n",
      "Epoch 87 \t Batch 60 \t Validation Loss: 37.85279434521993\n",
      "Epoch 87 \t Batch 80 \t Validation Loss: 36.7762793302536\n",
      "Epoch 87 \t Batch 100 \t Validation Loss: 35.04068658828735\n",
      "Epoch 87 \t Batch 120 \t Validation Loss: 34.159612035751344\n",
      "Epoch 87 \t Batch 140 \t Validation Loss: 33.263775771004816\n",
      "Epoch 87 \t Batch 160 \t Validation Loss: 34.24782178401947\n",
      "Epoch 87 \t Batch 180 \t Validation Loss: 36.83520277871026\n",
      "Epoch 87 \t Batch 200 \t Validation Loss: 37.73693998336792\n",
      "Epoch 87 \t Batch 220 \t Validation Loss: 38.580269293351606\n",
      "Epoch 87 \t Batch 240 \t Validation Loss: 38.602408436934155\n",
      "Epoch 87 \t Batch 260 \t Validation Loss: 40.40920143494239\n",
      "Epoch 87 \t Batch 280 \t Validation Loss: 41.255792941365925\n",
      "Epoch 87 \t Batch 300 \t Validation Loss: 41.91766934394836\n",
      "Epoch 87 \t Batch 320 \t Validation Loss: 42.12096367180347\n",
      "Epoch 87 \t Batch 340 \t Validation Loss: 41.87818541526794\n",
      "Epoch 87 \t Batch 360 \t Validation Loss: 41.560151335928175\n",
      "Epoch 87 \t Batch 380 \t Validation Loss: 41.66211641964159\n",
      "Epoch 87 \t Batch 400 \t Validation Loss: 41.147791330814364\n",
      "Epoch 87 \t Batch 420 \t Validation Loss: 41.047909507297334\n",
      "Epoch 87 \t Batch 440 \t Validation Loss: 40.68135451620275\n",
      "Epoch 87 \t Batch 460 \t Validation Loss: 40.79893829304239\n",
      "Epoch 87 \t Batch 480 \t Validation Loss: 41.17112805644671\n",
      "Epoch 87 \t Batch 500 \t Validation Loss: 40.8020695400238\n",
      "Epoch 87 \t Batch 520 \t Validation Loss: 40.508477564958426\n",
      "Epoch 87 \t Batch 540 \t Validation Loss: 40.180081362194485\n",
      "Epoch 87 \t Batch 560 \t Validation Loss: 39.93022465876171\n",
      "Epoch 87 \t Batch 580 \t Validation Loss: 39.66753090332294\n",
      "Epoch 87 \t Batch 600 \t Validation Loss: 39.81976699670156\n",
      "Epoch 87 Training Loss: 46.24729652779183 Validation Loss: 40.418596179454354\n",
      "Epoch 87 completed\n",
      "Epoch 88 \t Batch 20 \t Training Loss: 48.15069046020508\n",
      "Epoch 88 \t Batch 40 \t Training Loss: 47.475357151031496\n",
      "Epoch 88 \t Batch 60 \t Training Loss: 47.46087525685628\n",
      "Epoch 88 \t Batch 80 \t Training Loss: 47.30780701637268\n",
      "Epoch 88 \t Batch 100 \t Training Loss: 46.894065399169925\n",
      "Epoch 88 \t Batch 120 \t Training Loss: 46.66014334360759\n",
      "Epoch 88 \t Batch 140 \t Training Loss: 46.474826077052526\n",
      "Epoch 88 \t Batch 160 \t Training Loss: 46.43824162483215\n",
      "Epoch 88 \t Batch 180 \t Training Loss: 46.372866778903536\n",
      "Epoch 88 \t Batch 200 \t Training Loss: 46.290858364105226\n",
      "Epoch 88 \t Batch 220 \t Training Loss: 46.380907422846015\n",
      "Epoch 88 \t Batch 240 \t Training Loss: 46.428648408253984\n",
      "Epoch 88 \t Batch 260 \t Training Loss: 46.37491001716027\n",
      "Epoch 88 \t Batch 280 \t Training Loss: 46.435301072256905\n",
      "Epoch 88 \t Batch 300 \t Training Loss: 46.41934862772624\n",
      "Epoch 88 \t Batch 320 \t Training Loss: 46.37698500156402\n",
      "Epoch 88 \t Batch 340 \t Training Loss: 46.4256139867446\n",
      "Epoch 88 \t Batch 360 \t Training Loss: 46.367389625973175\n",
      "Epoch 88 \t Batch 380 \t Training Loss: 46.41679478695518\n",
      "Epoch 88 \t Batch 400 \t Training Loss: 46.452846183776856\n",
      "Epoch 88 \t Batch 420 \t Training Loss: 46.3965559550694\n",
      "Epoch 88 \t Batch 440 \t Training Loss: 46.44194179014726\n",
      "Epoch 88 \t Batch 460 \t Training Loss: 46.410729665341584\n",
      "Epoch 88 \t Batch 480 \t Training Loss: 46.34115908940633\n",
      "Epoch 88 \t Batch 500 \t Training Loss: 46.37321472167969\n",
      "Epoch 88 \t Batch 520 \t Training Loss: 46.392958626380334\n",
      "Epoch 88 \t Batch 540 \t Training Loss: 46.35367647806803\n",
      "Epoch 88 \t Batch 560 \t Training Loss: 46.37198946135385\n",
      "Epoch 88 \t Batch 580 \t Training Loss: 46.319936410311996\n",
      "Epoch 88 \t Batch 600 \t Training Loss: 46.28822945276896\n",
      "Epoch 88 \t Batch 620 \t Training Loss: 46.29824822948825\n",
      "Epoch 88 \t Batch 640 \t Training Loss: 46.29600319862366\n",
      "Epoch 88 \t Batch 660 \t Training Loss: 46.255817245714596\n",
      "Epoch 88 \t Batch 680 \t Training Loss: 46.253981556611905\n",
      "Epoch 88 \t Batch 700 \t Training Loss: 46.22756375994001\n",
      "Epoch 88 \t Batch 720 \t Training Loss: 46.20630366537306\n",
      "Epoch 88 \t Batch 740 \t Training Loss: 46.25483139656686\n",
      "Epoch 88 \t Batch 760 \t Training Loss: 46.32123866332205\n",
      "Epoch 88 \t Batch 780 \t Training Loss: 46.32718338110508\n",
      "Epoch 88 \t Batch 800 \t Training Loss: 46.32574244022369\n",
      "Epoch 88 \t Batch 820 \t Training Loss: 46.29048279552925\n",
      "Epoch 88 \t Batch 840 \t Training Loss: 46.27445502962385\n",
      "Epoch 88 \t Batch 860 \t Training Loss: 46.26315033491268\n",
      "Epoch 88 \t Batch 880 \t Training Loss: 46.23798688541759\n",
      "Epoch 88 \t Batch 900 \t Training Loss: 46.24825624254015\n",
      "Epoch 88 \t Batch 20 \t Validation Loss: 31.4603120803833\n",
      "Epoch 88 \t Batch 40 \t Validation Loss: 29.03142113685608\n",
      "Epoch 88 \t Batch 60 \t Validation Loss: 31.3849574247996\n",
      "Epoch 88 \t Batch 80 \t Validation Loss: 30.997923171520235\n",
      "Epoch 88 \t Batch 100 \t Validation Loss: 30.19160168647766\n",
      "Epoch 88 \t Batch 120 \t Validation Loss: 30.015389585494994\n",
      "Epoch 88 \t Batch 140 \t Validation Loss: 29.654527705056328\n",
      "Epoch 88 \t Batch 160 \t Validation Loss: 31.124324357509614\n",
      "Epoch 88 \t Batch 180 \t Validation Loss: 34.104217259089154\n",
      "Epoch 88 \t Batch 200 \t Validation Loss: 35.33605600833893\n",
      "Epoch 88 \t Batch 220 \t Validation Loss: 36.45290211330761\n",
      "Epoch 88 \t Batch 240 \t Validation Loss: 36.661058433850606\n",
      "Epoch 88 \t Batch 260 \t Validation Loss: 38.6456677546868\n",
      "Epoch 88 \t Batch 280 \t Validation Loss: 39.69127081802913\n",
      "Epoch 88 \t Batch 300 \t Validation Loss: 40.46595837593079\n",
      "Epoch 88 \t Batch 320 \t Validation Loss: 40.773184755444525\n",
      "Epoch 88 \t Batch 340 \t Validation Loss: 40.62226290141835\n",
      "Epoch 88 \t Batch 360 \t Validation Loss: 40.392763294114005\n",
      "Epoch 88 \t Batch 380 \t Validation Loss: 40.58354737382186\n",
      "Epoch 88 \t Batch 400 \t Validation Loss: 40.14500290155411\n",
      "Epoch 88 \t Batch 420 \t Validation Loss: 40.14179794901893\n",
      "Epoch 88 \t Batch 440 \t Validation Loss: 39.850832451473586\n",
      "Epoch 88 \t Batch 460 \t Validation Loss: 39.997737583906755\n",
      "Epoch 88 \t Batch 480 \t Validation Loss: 40.40050781766573\n",
      "Epoch 88 \t Batch 500 \t Validation Loss: 40.07171550178528\n",
      "Epoch 88 \t Batch 520 \t Validation Loss: 39.80506770977607\n",
      "Epoch 88 \t Batch 540 \t Validation Loss: 39.47776540297049\n",
      "Epoch 88 \t Batch 560 \t Validation Loss: 39.25792324032102\n",
      "Epoch 88 \t Batch 580 \t Validation Loss: 39.04117410758446\n",
      "Epoch 88 \t Batch 600 \t Validation Loss: 39.207193967501325\n",
      "Epoch 88 Training Loss: 46.220414921917595 Validation Loss: 39.84534915855953\n",
      "Epoch 88 completed\n",
      "Epoch 89 \t Batch 20 \t Training Loss: 45.711941719055176\n",
      "Epoch 89 \t Batch 40 \t Training Loss: 46.37732305526733\n",
      "Epoch 89 \t Batch 60 \t Training Loss: 45.85856952667236\n",
      "Epoch 89 \t Batch 80 \t Training Loss: 46.37404799461365\n",
      "Epoch 89 \t Batch 100 \t Training Loss: 46.291178436279296\n",
      "Epoch 89 \t Batch 120 \t Training Loss: 45.980043983459474\n",
      "Epoch 89 \t Batch 140 \t Training Loss: 46.104570388793945\n",
      "Epoch 89 \t Batch 160 \t Training Loss: 46.05091843605042\n",
      "Epoch 89 \t Batch 180 \t Training Loss: 46.19849552578396\n",
      "Epoch 89 \t Batch 200 \t Training Loss: 46.31776769638061\n",
      "Epoch 89 \t Batch 220 \t Training Loss: 46.30021100477739\n",
      "Epoch 89 \t Batch 240 \t Training Loss: 46.343194246292114\n",
      "Epoch 89 \t Batch 260 \t Training Loss: 46.27969078650841\n",
      "Epoch 89 \t Batch 280 \t Training Loss: 46.29928451265607\n",
      "Epoch 89 \t Batch 300 \t Training Loss: 46.26914465586344\n",
      "Epoch 89 \t Batch 320 \t Training Loss: 46.23289172649383\n",
      "Epoch 89 \t Batch 340 \t Training Loss: 46.28131251615636\n",
      "Epoch 89 \t Batch 360 \t Training Loss: 46.250211556752525\n",
      "Epoch 89 \t Batch 380 \t Training Loss: 46.259531653554816\n",
      "Epoch 89 \t Batch 400 \t Training Loss: 46.235318880081174\n",
      "Epoch 89 \t Batch 420 \t Training Loss: 46.259488396417524\n",
      "Epoch 89 \t Batch 440 \t Training Loss: 46.22100706967441\n",
      "Epoch 89 \t Batch 460 \t Training Loss: 46.27155596691629\n",
      "Epoch 89 \t Batch 480 \t Training Loss: 46.256339526176454\n",
      "Epoch 89 \t Batch 500 \t Training Loss: 46.24693007659912\n",
      "Epoch 89 \t Batch 520 \t Training Loss: 46.30085800611056\n",
      "Epoch 89 \t Batch 540 \t Training Loss: 46.265168790464045\n",
      "Epoch 89 \t Batch 560 \t Training Loss: 46.24757737432208\n",
      "Epoch 89 \t Batch 580 \t Training Loss: 46.22131263469828\n",
      "Epoch 89 \t Batch 600 \t Training Loss: 46.25272775014241\n",
      "Epoch 89 \t Batch 620 \t Training Loss: 46.263088189401934\n",
      "Epoch 89 \t Batch 640 \t Training Loss: 46.26251292228699\n",
      "Epoch 89 \t Batch 660 \t Training Loss: 46.24103809125496\n",
      "Epoch 89 \t Batch 680 \t Training Loss: 46.27975885727826\n",
      "Epoch 89 \t Batch 700 \t Training Loss: 46.33534896850586\n",
      "Epoch 89 \t Batch 720 \t Training Loss: 46.3091181702084\n",
      "Epoch 89 \t Batch 740 \t Training Loss: 46.320463855846505\n",
      "Epoch 89 \t Batch 760 \t Training Loss: 46.2965583600496\n",
      "Epoch 89 \t Batch 780 \t Training Loss: 46.26930440755991\n",
      "Epoch 89 \t Batch 800 \t Training Loss: 46.27704599380493\n",
      "Epoch 89 \t Batch 820 \t Training Loss: 46.29716252582829\n",
      "Epoch 89 \t Batch 840 \t Training Loss: 46.274872412000384\n",
      "Epoch 89 \t Batch 860 \t Training Loss: 46.26275868083155\n",
      "Epoch 89 \t Batch 880 \t Training Loss: 46.23244106986306\n",
      "Epoch 89 \t Batch 900 \t Training Loss: 46.219418445163306\n",
      "Epoch 89 \t Batch 20 \t Validation Loss: 29.37552089691162\n",
      "Epoch 89 \t Batch 40 \t Validation Loss: 27.33347170352936\n",
      "Epoch 89 \t Batch 60 \t Validation Loss: 29.654660002390543\n",
      "Epoch 89 \t Batch 80 \t Validation Loss: 29.312438988685606\n",
      "Epoch 89 \t Batch 100 \t Validation Loss: 28.94284450531006\n",
      "Epoch 89 \t Batch 120 \t Validation Loss: 29.0563059647878\n",
      "Epoch 89 \t Batch 140 \t Validation Loss: 28.91676002229963\n",
      "Epoch 89 \t Batch 160 \t Validation Loss: 30.398176324367522\n",
      "Epoch 89 \t Batch 180 \t Validation Loss: 33.34822936587864\n",
      "Epoch 89 \t Batch 200 \t Validation Loss: 34.53936578750611\n",
      "Epoch 89 \t Batch 220 \t Validation Loss: 35.62707439769398\n",
      "Epoch 89 \t Batch 240 \t Validation Loss: 35.860349508126575\n",
      "Epoch 89 \t Batch 260 \t Validation Loss: 37.81063007208017\n",
      "Epoch 89 \t Batch 280 \t Validation Loss: 38.793864485195705\n",
      "Epoch 89 \t Batch 300 \t Validation Loss: 39.580966736475624\n",
      "Epoch 89 \t Batch 320 \t Validation Loss: 39.90505471527577\n",
      "Epoch 89 \t Batch 340 \t Validation Loss: 39.796300245733825\n",
      "Epoch 89 \t Batch 360 \t Validation Loss: 39.59575654665629\n",
      "Epoch 89 \t Batch 380 \t Validation Loss: 39.79053198663812\n",
      "Epoch 89 \t Batch 400 \t Validation Loss: 39.376320886611936\n",
      "Epoch 89 \t Batch 420 \t Validation Loss: 39.34223465692429\n",
      "Epoch 89 \t Batch 440 \t Validation Loss: 39.04227881431579\n",
      "Epoch 89 \t Batch 460 \t Validation Loss: 39.21514745380568\n",
      "Epoch 89 \t Batch 480 \t Validation Loss: 39.649419824282326\n",
      "Epoch 89 \t Batch 500 \t Validation Loss: 39.32515229415893\n",
      "Epoch 89 \t Batch 520 \t Validation Loss: 39.087977220461916\n",
      "Epoch 89 \t Batch 540 \t Validation Loss: 38.81596339367054\n",
      "Epoch 89 \t Batch 560 \t Validation Loss: 38.60349214928491\n",
      "Epoch 89 \t Batch 580 \t Validation Loss: 38.377087469758656\n",
      "Epoch 89 \t Batch 600 \t Validation Loss: 38.5682737048467\n",
      "Epoch 89 Training Loss: 46.22685855295562 Validation Loss: 39.18730522905077\n",
      "Epoch 89 completed\n",
      "Epoch 90 \t Batch 20 \t Training Loss: 45.208403968811034\n",
      "Epoch 90 \t Batch 40 \t Training Loss: 45.94939403533935\n",
      "Epoch 90 \t Batch 60 \t Training Loss: 45.70857855478923\n",
      "Epoch 90 \t Batch 80 \t Training Loss: 45.63190822601318\n",
      "Epoch 90 \t Batch 100 \t Training Loss: 45.7572290802002\n",
      "Epoch 90 \t Batch 120 \t Training Loss: 45.91655610402425\n",
      "Epoch 90 \t Batch 140 \t Training Loss: 46.01317868913923\n",
      "Epoch 90 \t Batch 160 \t Training Loss: 46.110777378082275\n",
      "Epoch 90 \t Batch 180 \t Training Loss: 45.869816610548234\n",
      "Epoch 90 \t Batch 200 \t Training Loss: 45.93532020568848\n",
      "Epoch 90 \t Batch 220 \t Training Loss: 46.135043595053936\n",
      "Epoch 90 \t Batch 240 \t Training Loss: 46.14872679710388\n",
      "Epoch 90 \t Batch 260 \t Training Loss: 46.1088309948261\n",
      "Epoch 90 \t Batch 280 \t Training Loss: 46.040478352137974\n",
      "Epoch 90 \t Batch 300 \t Training Loss: 46.014871495564776\n",
      "Epoch 90 \t Batch 320 \t Training Loss: 46.117280507087706\n",
      "Epoch 90 \t Batch 340 \t Training Loss: 46.13586102653952\n",
      "Epoch 90 \t Batch 360 \t Training Loss: 46.14343814849853\n",
      "Epoch 90 \t Batch 380 \t Training Loss: 46.05617854469701\n",
      "Epoch 90 \t Batch 400 \t Training Loss: 46.00953169822693\n",
      "Epoch 90 \t Batch 420 \t Training Loss: 45.99351369766962\n",
      "Epoch 90 \t Batch 440 \t Training Loss: 45.93214147741144\n",
      "Epoch 90 \t Batch 460 \t Training Loss: 45.941176215462065\n",
      "Epoch 90 \t Batch 480 \t Training Loss: 45.953527275721235\n",
      "Epoch 90 \t Batch 500 \t Training Loss: 45.96962674713135\n",
      "Epoch 90 \t Batch 520 \t Training Loss: 45.97359865628756\n",
      "Epoch 90 \t Batch 540 \t Training Loss: 46.02006738450792\n",
      "Epoch 90 \t Batch 560 \t Training Loss: 46.04428116253444\n",
      "Epoch 90 \t Batch 580 \t Training Loss: 46.0559692843207\n",
      "Epoch 90 \t Batch 600 \t Training Loss: 46.088166122436526\n",
      "Epoch 90 \t Batch 620 \t Training Loss: 46.144332055122625\n",
      "Epoch 90 \t Batch 640 \t Training Loss: 46.144760489463806\n",
      "Epoch 90 \t Batch 660 \t Training Loss: 46.1426497372714\n",
      "Epoch 90 \t Batch 680 \t Training Loss: 46.17732301038854\n",
      "Epoch 90 \t Batch 700 \t Training Loss: 46.17222880772182\n",
      "Epoch 90 \t Batch 720 \t Training Loss: 46.192015186945596\n",
      "Epoch 90 \t Batch 740 \t Training Loss: 46.17683858098211\n",
      "Epoch 90 \t Batch 760 \t Training Loss: 46.16310017736335\n",
      "Epoch 90 \t Batch 780 \t Training Loss: 46.20651486714681\n",
      "Epoch 90 \t Batch 800 \t Training Loss: 46.21552921772003\n",
      "Epoch 90 \t Batch 820 \t Training Loss: 46.21604739398491\n",
      "Epoch 90 \t Batch 840 \t Training Loss: 46.22135066077823\n",
      "Epoch 90 \t Batch 860 \t Training Loss: 46.20639083330021\n",
      "Epoch 90 \t Batch 880 \t Training Loss: 46.22299221645702\n",
      "Epoch 90 \t Batch 900 \t Training Loss: 46.1896147579617\n",
      "Epoch 90 \t Batch 20 \t Validation Loss: 36.67441062927246\n",
      "Epoch 90 \t Batch 40 \t Validation Loss: 33.15995800495148\n",
      "Epoch 90 \t Batch 60 \t Validation Loss: 36.10734092394511\n",
      "Epoch 90 \t Batch 80 \t Validation Loss: 35.12133880853653\n",
      "Epoch 90 \t Batch 100 \t Validation Loss: 33.95149868965149\n",
      "Epoch 90 \t Batch 120 \t Validation Loss: 33.55680221716563\n",
      "Epoch 90 \t Batch 140 \t Validation Loss: 32.86894131387983\n",
      "Epoch 90 \t Batch 160 \t Validation Loss: 33.905141192674634\n",
      "Epoch 90 \t Batch 180 \t Validation Loss: 36.57599817381965\n",
      "Epoch 90 \t Batch 200 \t Validation Loss: 37.42774970531464\n",
      "Epoch 90 \t Batch 220 \t Validation Loss: 38.34798394549977\n",
      "Epoch 90 \t Batch 240 \t Validation Loss: 38.41209326585134\n",
      "Epoch 90 \t Batch 260 \t Validation Loss: 40.212473216423625\n",
      "Epoch 90 \t Batch 280 \t Validation Loss: 41.044164769990104\n",
      "Epoch 90 \t Batch 300 \t Validation Loss: 41.73803983370463\n",
      "Epoch 90 \t Batch 320 \t Validation Loss: 41.96946969926357\n",
      "Epoch 90 \t Batch 340 \t Validation Loss: 41.72883716190562\n",
      "Epoch 90 \t Batch 360 \t Validation Loss: 41.410260083940294\n",
      "Epoch 90 \t Batch 380 \t Validation Loss: 41.49184331893921\n",
      "Epoch 90 \t Batch 400 \t Validation Loss: 40.93600910663605\n",
      "Epoch 90 \t Batch 420 \t Validation Loss: 40.82661170959473\n",
      "Epoch 90 \t Batch 440 \t Validation Loss: 40.410405492782594\n",
      "Epoch 90 \t Batch 460 \t Validation Loss: 40.49663467407227\n",
      "Epoch 90 \t Batch 480 \t Validation Loss: 40.87280301650365\n",
      "Epoch 90 \t Batch 500 \t Validation Loss: 40.49640196609497\n",
      "Epoch 90 \t Batch 520 \t Validation Loss: 40.13786085568942\n",
      "Epoch 90 \t Batch 540 \t Validation Loss: 39.77702396534107\n",
      "Epoch 90 \t Batch 560 \t Validation Loss: 39.45435538632529\n",
      "Epoch 90 \t Batch 580 \t Validation Loss: 39.05590413027796\n",
      "Epoch 90 \t Batch 600 \t Validation Loss: 39.20567165851593\n",
      "Epoch 90 Training Loss: 46.180391825402445 Validation Loss: 39.77694424561092\n",
      "Epoch 90 completed\n",
      "Epoch 91 \t Batch 20 \t Training Loss: 45.07871837615967\n",
      "Epoch 91 \t Batch 40 \t Training Loss: 46.17297248840332\n",
      "Epoch 91 \t Batch 60 \t Training Loss: 46.08787078857422\n",
      "Epoch 91 \t Batch 80 \t Training Loss: 46.21605772972107\n",
      "Epoch 91 \t Batch 100 \t Training Loss: 45.92985202789307\n",
      "Epoch 91 \t Batch 120 \t Training Loss: 46.014516067504886\n",
      "Epoch 91 \t Batch 140 \t Training Loss: 46.18588485717773\n",
      "Epoch 91 \t Batch 160 \t Training Loss: 46.27891061306\n",
      "Epoch 91 \t Batch 180 \t Training Loss: 46.38933832380507\n",
      "Epoch 91 \t Batch 200 \t Training Loss: 46.33484649658203\n",
      "Epoch 91 \t Batch 220 \t Training Loss: 46.2622203653509\n",
      "Epoch 91 \t Batch 240 \t Training Loss: 46.181902964909874\n",
      "Epoch 91 \t Batch 260 \t Training Loss: 46.13635748349703\n",
      "Epoch 91 \t Batch 280 \t Training Loss: 46.25474237714495\n",
      "Epoch 91 \t Batch 300 \t Training Loss: 46.23780771891276\n",
      "Epoch 91 \t Batch 320 \t Training Loss: 46.276946341991426\n",
      "Epoch 91 \t Batch 340 \t Training Loss: 46.2915740293615\n",
      "Epoch 91 \t Batch 360 \t Training Loss: 46.28611741595798\n",
      "Epoch 91 \t Batch 380 \t Training Loss: 46.32388500414397\n",
      "Epoch 91 \t Batch 400 \t Training Loss: 46.3130730342865\n",
      "Epoch 91 \t Batch 420 \t Training Loss: 46.30945695241292\n",
      "Epoch 91 \t Batch 440 \t Training Loss: 46.260298858989366\n",
      "Epoch 91 \t Batch 460 \t Training Loss: 46.27343765756358\n",
      "Epoch 91 \t Batch 480 \t Training Loss: 46.3023121436437\n",
      "Epoch 91 \t Batch 500 \t Training Loss: 46.277779647827145\n",
      "Epoch 91 \t Batch 520 \t Training Loss: 46.27241090627817\n",
      "Epoch 91 \t Batch 540 \t Training Loss: 46.25004411626745\n",
      "Epoch 91 \t Batch 560 \t Training Loss: 46.2563686983926\n",
      "Epoch 91 \t Batch 580 \t Training Loss: 46.259248424398486\n",
      "Epoch 91 \t Batch 600 \t Training Loss: 46.232854181925454\n",
      "Epoch 91 \t Batch 620 \t Training Loss: 46.25562005812122\n",
      "Epoch 91 \t Batch 640 \t Training Loss: 46.21821283102035\n",
      "Epoch 91 \t Batch 660 \t Training Loss: 46.23347944201845\n",
      "Epoch 91 \t Batch 680 \t Training Loss: 46.19538326263428\n",
      "Epoch 91 \t Batch 700 \t Training Loss: 46.1921443939209\n",
      "Epoch 91 \t Batch 720 \t Training Loss: 46.22754241625468\n",
      "Epoch 91 \t Batch 740 \t Training Loss: 46.222290600957095\n",
      "Epoch 91 \t Batch 760 \t Training Loss: 46.169962019669384\n",
      "Epoch 91 \t Batch 780 \t Training Loss: 46.16436275090927\n",
      "Epoch 91 \t Batch 800 \t Training Loss: 46.18102983951569\n",
      "Epoch 91 \t Batch 820 \t Training Loss: 46.194983812657796\n",
      "Epoch 91 \t Batch 840 \t Training Loss: 46.153036930447534\n",
      "Epoch 91 \t Batch 860 \t Training Loss: 46.15287011612293\n",
      "Epoch 91 \t Batch 880 \t Training Loss: 46.132355611974546\n",
      "Epoch 91 \t Batch 900 \t Training Loss: 46.172695791456434\n",
      "Epoch 91 \t Batch 20 \t Validation Loss: 33.64833550453186\n",
      "Epoch 91 \t Batch 40 \t Validation Loss: 30.27797350883484\n",
      "Epoch 91 \t Batch 60 \t Validation Loss: 33.018345896403\n",
      "Epoch 91 \t Batch 80 \t Validation Loss: 32.34304069280624\n",
      "Epoch 91 \t Batch 100 \t Validation Loss: 31.279596757888793\n",
      "Epoch 91 \t Batch 120 \t Validation Loss: 30.895000394185384\n",
      "Epoch 91 \t Batch 140 \t Validation Loss: 30.417044230869838\n",
      "Epoch 91 \t Batch 160 \t Validation Loss: 31.81727001667023\n",
      "Epoch 91 \t Batch 180 \t Validation Loss: 34.71496378580729\n",
      "Epoch 91 \t Batch 200 \t Validation Loss: 35.82202365875244\n",
      "Epoch 91 \t Batch 220 \t Validation Loss: 36.86403959447687\n",
      "Epoch 91 \t Batch 240 \t Validation Loss: 37.03498144944509\n",
      "Epoch 91 \t Batch 260 \t Validation Loss: 38.92437786322374\n",
      "Epoch 91 \t Batch 280 \t Validation Loss: 39.862826224735805\n",
      "Epoch 91 \t Batch 300 \t Validation Loss: 40.666432552337646\n",
      "Epoch 91 \t Batch 320 \t Validation Loss: 40.98535784482956\n",
      "Epoch 91 \t Batch 340 \t Validation Loss: 40.81658370635089\n",
      "Epoch 91 \t Batch 360 \t Validation Loss: 40.59550828403897\n",
      "Epoch 91 \t Batch 380 \t Validation Loss: 40.77290083734613\n",
      "Epoch 91 \t Batch 400 \t Validation Loss: 40.33644701957703\n",
      "Epoch 91 \t Batch 420 \t Validation Loss: 40.29419521604265\n",
      "Epoch 91 \t Batch 440 \t Validation Loss: 40.01167981407859\n",
      "Epoch 91 \t Batch 460 \t Validation Loss: 40.1719692810722\n",
      "Epoch 91 \t Batch 480 \t Validation Loss: 40.58212869962056\n",
      "Epoch 91 \t Batch 500 \t Validation Loss: 40.247184185028075\n",
      "Epoch 91 \t Batch 520 \t Validation Loss: 40.03481843288128\n",
      "Epoch 91 \t Batch 540 \t Validation Loss: 39.735448897326435\n",
      "Epoch 91 \t Batch 560 \t Validation Loss: 39.52097563402993\n",
      "Epoch 91 \t Batch 580 \t Validation Loss: 39.34583978324101\n",
      "Epoch 91 \t Batch 600 \t Validation Loss: 39.50491398175557\n",
      "Epoch 91 Training Loss: 46.18841695421525 Validation Loss: 40.22294041707918\n",
      "Epoch 91 completed\n",
      "Epoch 92 \t Batch 20 \t Training Loss: 45.72510261535645\n",
      "Epoch 92 \t Batch 40 \t Training Loss: 46.15182380676269\n",
      "Epoch 92 \t Batch 60 \t Training Loss: 46.42229461669922\n",
      "Epoch 92 \t Batch 80 \t Training Loss: 46.06859245300293\n",
      "Epoch 92 \t Batch 100 \t Training Loss: 45.95010314941406\n",
      "Epoch 92 \t Batch 120 \t Training Loss: 46.065038967132566\n",
      "Epoch 92 \t Batch 140 \t Training Loss: 46.05751746041434\n",
      "Epoch 92 \t Batch 160 \t Training Loss: 46.05217700004577\n",
      "Epoch 92 \t Batch 180 \t Training Loss: 45.959922875298396\n",
      "Epoch 92 \t Batch 200 \t Training Loss: 46.03857400894165\n",
      "Epoch 92 \t Batch 220 \t Training Loss: 46.02695144306529\n",
      "Epoch 92 \t Batch 240 \t Training Loss: 46.08917999267578\n",
      "Epoch 92 \t Batch 260 \t Training Loss: 46.13066083467924\n",
      "Epoch 92 \t Batch 280 \t Training Loss: 46.18611418860299\n",
      "Epoch 92 \t Batch 300 \t Training Loss: 46.24151320139567\n",
      "Epoch 92 \t Batch 320 \t Training Loss: 46.22991734743118\n",
      "Epoch 92 \t Batch 340 \t Training Loss: 46.2932949851541\n",
      "Epoch 92 \t Batch 360 \t Training Loss: 46.2185533841451\n",
      "Epoch 92 \t Batch 380 \t Training Loss: 46.31084087773373\n",
      "Epoch 92 \t Batch 400 \t Training Loss: 46.26124891281128\n",
      "Epoch 92 \t Batch 420 \t Training Loss: 46.22880552382696\n",
      "Epoch 92 \t Batch 440 \t Training Loss: 46.18231984918768\n",
      "Epoch 92 \t Batch 460 \t Training Loss: 46.132029765585195\n",
      "Epoch 92 \t Batch 480 \t Training Loss: 46.13490238189697\n",
      "Epoch 92 \t Batch 500 \t Training Loss: 46.09802415466309\n",
      "Epoch 92 \t Batch 520 \t Training Loss: 46.02392108623798\n",
      "Epoch 92 \t Batch 540 \t Training Loss: 46.07788141038683\n",
      "Epoch 92 \t Batch 560 \t Training Loss: 46.10433922495161\n",
      "Epoch 92 \t Batch 580 \t Training Loss: 46.1234499372285\n",
      "Epoch 92 \t Batch 600 \t Training Loss: 46.105138543446856\n",
      "Epoch 92 \t Batch 620 \t Training Loss: 46.10415434683523\n",
      "Epoch 92 \t Batch 640 \t Training Loss: 46.09079501032829\n",
      "Epoch 92 \t Batch 660 \t Training Loss: 46.148524839227846\n",
      "Epoch 92 \t Batch 680 \t Training Loss: 46.15747194851146\n",
      "Epoch 92 \t Batch 700 \t Training Loss: 46.16564724513463\n",
      "Epoch 92 \t Batch 720 \t Training Loss: 46.2030223051707\n",
      "Epoch 92 \t Batch 740 \t Training Loss: 46.19073943576297\n",
      "Epoch 92 \t Batch 760 \t Training Loss: 46.208704737613076\n",
      "Epoch 92 \t Batch 780 \t Training Loss: 46.18915453690749\n",
      "Epoch 92 \t Batch 800 \t Training Loss: 46.17708402633667\n",
      "Epoch 92 \t Batch 820 \t Training Loss: 46.16304388744075\n",
      "Epoch 92 \t Batch 840 \t Training Loss: 46.15087788899739\n",
      "Epoch 92 \t Batch 860 \t Training Loss: 46.19073276076206\n",
      "Epoch 92 \t Batch 880 \t Training Loss: 46.21045355796814\n",
      "Epoch 92 \t Batch 900 \t Training Loss: 46.194119228786896\n",
      "Epoch 92 \t Batch 20 \t Validation Loss: 35.49451832771301\n",
      "Epoch 92 \t Batch 40 \t Validation Loss: 31.146564388275145\n",
      "Epoch 92 \t Batch 60 \t Validation Loss: 34.474412552515666\n",
      "Epoch 92 \t Batch 80 \t Validation Loss: 33.482412511110304\n",
      "Epoch 92 \t Batch 100 \t Validation Loss: 32.13899790287018\n",
      "Epoch 92 \t Batch 120 \t Validation Loss: 31.579426610469817\n",
      "Epoch 92 \t Batch 140 \t Validation Loss: 30.969077958379472\n",
      "Epoch 92 \t Batch 160 \t Validation Loss: 32.19465688765049\n",
      "Epoch 92 \t Batch 180 \t Validation Loss: 34.85428122679392\n",
      "Epoch 92 \t Batch 200 \t Validation Loss: 35.88722847223282\n",
      "Epoch 92 \t Batch 220 \t Validation Loss: 36.837790083885196\n",
      "Epoch 92 \t Batch 240 \t Validation Loss: 36.92400525212288\n",
      "Epoch 92 \t Batch 260 \t Validation Loss: 38.781794516856856\n",
      "Epoch 92 \t Batch 280 \t Validation Loss: 39.694262775353025\n",
      "Epoch 92 \t Batch 300 \t Validation Loss: 40.37899134794871\n",
      "Epoch 92 \t Batch 320 \t Validation Loss: 40.612779854238035\n",
      "Epoch 92 \t Batch 340 \t Validation Loss: 40.43193707606372\n",
      "Epoch 92 \t Batch 360 \t Validation Loss: 40.15774882766936\n",
      "Epoch 92 \t Batch 380 \t Validation Loss: 40.327298416589436\n",
      "Epoch 92 \t Batch 400 \t Validation Loss: 39.8658125269413\n",
      "Epoch 92 \t Batch 420 \t Validation Loss: 39.82309166703905\n",
      "Epoch 92 \t Batch 440 \t Validation Loss: 39.49418456879529\n",
      "Epoch 92 \t Batch 460 \t Validation Loss: 39.64435699400695\n",
      "Epoch 92 \t Batch 480 \t Validation Loss: 40.049436288078624\n",
      "Epoch 92 \t Batch 500 \t Validation Loss: 39.70555054569245\n",
      "Epoch 92 \t Batch 520 \t Validation Loss: 39.45097462489055\n",
      "Epoch 92 \t Batch 540 \t Validation Loss: 39.1285059178317\n",
      "Epoch 92 \t Batch 560 \t Validation Loss: 38.87184873904501\n",
      "Epoch 92 \t Batch 580 \t Validation Loss: 38.59339570259226\n",
      "Epoch 92 \t Batch 600 \t Validation Loss: 38.75225566784541\n",
      "Epoch 92 Training Loss: 46.17390162344197 Validation Loss: 39.36632073002976\n",
      "Epoch 92 completed\n",
      "Epoch 93 \t Batch 20 \t Training Loss: 45.317428016662596\n",
      "Epoch 93 \t Batch 40 \t Training Loss: 45.732967376708984\n",
      "Epoch 93 \t Batch 60 \t Training Loss: 45.79804884592692\n",
      "Epoch 93 \t Batch 80 \t Training Loss: 46.066797733306885\n",
      "Epoch 93 \t Batch 100 \t Training Loss: 45.9038391494751\n",
      "Epoch 93 \t Batch 120 \t Training Loss: 45.9182448387146\n",
      "Epoch 93 \t Batch 140 \t Training Loss: 46.109197916303366\n",
      "Epoch 93 \t Batch 160 \t Training Loss: 46.152925324440005\n",
      "Epoch 93 \t Batch 180 \t Training Loss: 46.282610003153486\n",
      "Epoch 93 \t Batch 200 \t Training Loss: 46.30806982040405\n",
      "Epoch 93 \t Batch 220 \t Training Loss: 46.305714364485304\n",
      "Epoch 93 \t Batch 240 \t Training Loss: 46.33217034339905\n",
      "Epoch 93 \t Batch 260 \t Training Loss: 46.45285603449895\n",
      "Epoch 93 \t Batch 280 \t Training Loss: 46.3752944946289\n",
      "Epoch 93 \t Batch 300 \t Training Loss: 46.328267631530764\n",
      "Epoch 93 \t Batch 320 \t Training Loss: 46.30791929960251\n",
      "Epoch 93 \t Batch 340 \t Training Loss: 46.283102024302764\n",
      "Epoch 93 \t Batch 360 \t Training Loss: 46.30847087436252\n",
      "Epoch 93 \t Batch 380 \t Training Loss: 46.35603535300807\n",
      "Epoch 93 \t Batch 400 \t Training Loss: 46.30279960632324\n",
      "Epoch 93 \t Batch 420 \t Training Loss: 46.251536950610934\n",
      "Epoch 93 \t Batch 440 \t Training Loss: 46.285260833393444\n",
      "Epoch 93 \t Batch 460 \t Training Loss: 46.26813526153565\n",
      "Epoch 93 \t Batch 480 \t Training Loss: 46.24029298623403\n",
      "Epoch 93 \t Batch 500 \t Training Loss: 46.2698948135376\n",
      "Epoch 93 \t Batch 520 \t Training Loss: 46.28172922867995\n",
      "Epoch 93 \t Batch 540 \t Training Loss: 46.2613615459866\n",
      "Epoch 93 \t Batch 560 \t Training Loss: 46.25829339027405\n",
      "Epoch 93 \t Batch 580 \t Training Loss: 46.266171093644765\n",
      "Epoch 93 \t Batch 600 \t Training Loss: 46.26207004547119\n",
      "Epoch 93 \t Batch 620 \t Training Loss: 46.2548990126579\n",
      "Epoch 93 \t Batch 640 \t Training Loss: 46.26181559562683\n",
      "Epoch 93 \t Batch 660 \t Training Loss: 46.22862836086389\n",
      "Epoch 93 \t Batch 680 \t Training Loss: 46.24367923175588\n",
      "Epoch 93 \t Batch 700 \t Training Loss: 46.25452182769775\n",
      "Epoch 93 \t Batch 720 \t Training Loss: 46.27529526286655\n",
      "Epoch 93 \t Batch 740 \t Training Loss: 46.2793585442208\n",
      "Epoch 93 \t Batch 760 \t Training Loss: 46.25344400907817\n",
      "Epoch 93 \t Batch 780 \t Training Loss: 46.24536707951472\n",
      "Epoch 93 \t Batch 800 \t Training Loss: 46.221441383361814\n",
      "Epoch 93 \t Batch 820 \t Training Loss: 46.21586854981213\n",
      "Epoch 93 \t Batch 840 \t Training Loss: 46.223655986785886\n",
      "Epoch 93 \t Batch 860 \t Training Loss: 46.21413736565169\n",
      "Epoch 93 \t Batch 880 \t Training Loss: 46.188709085637875\n",
      "Epoch 93 \t Batch 900 \t Training Loss: 46.15859060499403\n",
      "Epoch 93 \t Batch 20 \t Validation Loss: 36.13831405639648\n",
      "Epoch 93 \t Batch 40 \t Validation Loss: 32.47060828208923\n",
      "Epoch 93 \t Batch 60 \t Validation Loss: 35.33744330406189\n",
      "Epoch 93 \t Batch 80 \t Validation Loss: 34.38617188930512\n",
      "Epoch 93 \t Batch 100 \t Validation Loss: 33.115087356567386\n",
      "Epoch 93 \t Batch 120 \t Validation Loss: 32.623696597417194\n",
      "Epoch 93 \t Batch 140 \t Validation Loss: 31.973314911978587\n",
      "Epoch 93 \t Batch 160 \t Validation Loss: 33.155952179431914\n",
      "Epoch 93 \t Batch 180 \t Validation Loss: 35.91526638666789\n",
      "Epoch 93 \t Batch 200 \t Validation Loss: 36.89025987625122\n",
      "Epoch 93 \t Batch 220 \t Validation Loss: 37.88078671368686\n",
      "Epoch 93 \t Batch 240 \t Validation Loss: 37.98099832137426\n",
      "Epoch 93 \t Batch 260 \t Validation Loss: 39.85758477357717\n",
      "Epoch 93 \t Batch 280 \t Validation Loss: 40.76278729438782\n",
      "Epoch 93 \t Batch 300 \t Validation Loss: 41.474957644144695\n",
      "Epoch 93 \t Batch 320 \t Validation Loss: 41.73600034117699\n",
      "Epoch 93 \t Batch 340 \t Validation Loss: 41.53104259266573\n",
      "Epoch 93 \t Batch 360 \t Validation Loss: 41.25400629043579\n",
      "Epoch 93 \t Batch 380 \t Validation Loss: 41.37035171609176\n",
      "Epoch 93 \t Batch 400 \t Validation Loss: 40.827710032463074\n",
      "Epoch 93 \t Batch 420 \t Validation Loss: 40.741643063227336\n",
      "Epoch 93 \t Batch 440 \t Validation Loss: 40.339073872566225\n",
      "Epoch 93 \t Batch 460 \t Validation Loss: 40.41432185587676\n",
      "Epoch 93 \t Batch 480 \t Validation Loss: 40.812401086091995\n",
      "Epoch 93 \t Batch 500 \t Validation Loss: 40.46207899284363\n",
      "Epoch 93 \t Batch 520 \t Validation Loss: 40.11424889197716\n",
      "Epoch 93 \t Batch 540 \t Validation Loss: 39.78507027449431\n",
      "Epoch 93 \t Batch 560 \t Validation Loss: 39.5175995179585\n",
      "Epoch 93 \t Batch 580 \t Validation Loss: 39.243692115257524\n",
      "Epoch 93 \t Batch 600 \t Validation Loss: 39.40480500857036\n",
      "Epoch 93 Training Loss: 46.147708181466506 Validation Loss: 40.050483034802724\n",
      "Epoch 93 completed\n",
      "Epoch 94 \t Batch 20 \t Training Loss: 45.86645851135254\n",
      "Epoch 94 \t Batch 40 \t Training Loss: 45.7159309387207\n",
      "Epoch 94 \t Batch 60 \t Training Loss: 45.806552696228025\n",
      "Epoch 94 \t Batch 80 \t Training Loss: 45.55965805053711\n",
      "Epoch 94 \t Batch 100 \t Training Loss: 45.601401824951175\n",
      "Epoch 94 \t Batch 120 \t Training Loss: 45.954966894785564\n",
      "Epoch 94 \t Batch 140 \t Training Loss: 46.018731471470424\n",
      "Epoch 94 \t Batch 160 \t Training Loss: 45.941640329360965\n",
      "Epoch 94 \t Batch 180 \t Training Loss: 46.00807151794434\n",
      "Epoch 94 \t Batch 200 \t Training Loss: 46.06586692810058\n",
      "Epoch 94 \t Batch 220 \t Training Loss: 45.9408034064553\n",
      "Epoch 94 \t Batch 240 \t Training Loss: 46.01257171630859\n",
      "Epoch 94 \t Batch 260 \t Training Loss: 46.186317209097055\n",
      "Epoch 94 \t Batch 280 \t Training Loss: 46.067977741786414\n",
      "Epoch 94 \t Batch 300 \t Training Loss: 46.08866074879964\n",
      "Epoch 94 \t Batch 320 \t Training Loss: 46.12522611618042\n",
      "Epoch 94 \t Batch 340 \t Training Loss: 46.10080059275908\n",
      "Epoch 94 \t Batch 360 \t Training Loss: 46.097429837120906\n",
      "Epoch 94 \t Batch 380 \t Training Loss: 46.108063035262255\n",
      "Epoch 94 \t Batch 400 \t Training Loss: 46.09772385597229\n",
      "Epoch 94 \t Batch 420 \t Training Loss: 46.10419862838018\n",
      "Epoch 94 \t Batch 440 \t Training Loss: 46.097431590340356\n",
      "Epoch 94 \t Batch 460 \t Training Loss: 46.10449670708698\n",
      "Epoch 94 \t Batch 480 \t Training Loss: 46.13843497435252\n",
      "Epoch 94 \t Batch 500 \t Training Loss: 46.151232002258304\n",
      "Epoch 94 \t Batch 520 \t Training Loss: 46.17355795640212\n",
      "Epoch 94 \t Batch 540 \t Training Loss: 46.18847469753689\n",
      "Epoch 94 \t Batch 560 \t Training Loss: 46.14136219024658\n",
      "Epoch 94 \t Batch 580 \t Training Loss: 46.14604118610251\n",
      "Epoch 94 \t Batch 600 \t Training Loss: 46.128313306172686\n",
      "Epoch 94 \t Batch 620 \t Training Loss: 46.137440792206796\n",
      "Epoch 94 \t Batch 640 \t Training Loss: 46.094068604707715\n",
      "Epoch 94 \t Batch 660 \t Training Loss: 46.06197576233835\n",
      "Epoch 94 \t Batch 680 \t Training Loss: 46.07891943314496\n",
      "Epoch 94 \t Batch 700 \t Training Loss: 46.057965147835866\n",
      "Epoch 94 \t Batch 720 \t Training Loss: 46.12234336535136\n",
      "Epoch 94 \t Batch 740 \t Training Loss: 46.1130101848293\n",
      "Epoch 94 \t Batch 760 \t Training Loss: 46.12405222340634\n",
      "Epoch 94 \t Batch 780 \t Training Loss: 46.09697763491899\n",
      "Epoch 94 \t Batch 800 \t Training Loss: 46.097330384254455\n",
      "Epoch 94 \t Batch 820 \t Training Loss: 46.10469586907364\n",
      "Epoch 94 \t Batch 840 \t Training Loss: 46.11595688774472\n",
      "Epoch 94 \t Batch 860 \t Training Loss: 46.09812707679216\n",
      "Epoch 94 \t Batch 880 \t Training Loss: 46.11085079366511\n",
      "Epoch 94 \t Batch 900 \t Training Loss: 46.10723773108588\n",
      "Epoch 94 \t Batch 20 \t Validation Loss: 34.91336898803711\n",
      "Epoch 94 \t Batch 40 \t Validation Loss: 32.236102199554445\n",
      "Epoch 94 \t Batch 60 \t Validation Loss: 34.45437534650167\n",
      "Epoch 94 \t Batch 80 \t Validation Loss: 33.82141032218933\n",
      "Epoch 94 \t Batch 100 \t Validation Loss: 32.81694486618042\n",
      "Epoch 94 \t Batch 120 \t Validation Loss: 32.452303965886436\n",
      "Epoch 94 \t Batch 140 \t Validation Loss: 31.891119820731028\n",
      "Epoch 94 \t Batch 160 \t Validation Loss: 33.048912358284\n",
      "Epoch 94 \t Batch 180 \t Validation Loss: 35.87098302311367\n",
      "Epoch 94 \t Batch 200 \t Validation Loss: 36.89915640354157\n",
      "Epoch 94 \t Batch 220 \t Validation Loss: 37.86555425037037\n",
      "Epoch 94 \t Batch 240 \t Validation Loss: 37.97220013936361\n",
      "Epoch 94 \t Batch 260 \t Validation Loss: 39.84361627285297\n",
      "Epoch 94 \t Batch 280 \t Validation Loss: 40.7471873828343\n",
      "Epoch 94 \t Batch 300 \t Validation Loss: 41.51511098225912\n",
      "Epoch 94 \t Batch 320 \t Validation Loss: 41.76863903999329\n",
      "Epoch 94 \t Batch 340 \t Validation Loss: 41.54705809424905\n",
      "Epoch 94 \t Batch 360 \t Validation Loss: 41.24923140207927\n",
      "Epoch 94 \t Batch 380 \t Validation Loss: 41.368482785475884\n",
      "Epoch 94 \t Batch 400 \t Validation Loss: 40.847974772453306\n",
      "Epoch 94 \t Batch 420 \t Validation Loss: 40.76327084132603\n",
      "Epoch 94 \t Batch 440 \t Validation Loss: 40.38643175038425\n",
      "Epoch 94 \t Batch 460 \t Validation Loss: 40.49489928535793\n",
      "Epoch 94 \t Batch 480 \t Validation Loss: 40.87686498562495\n",
      "Epoch 94 \t Batch 500 \t Validation Loss: 40.50446153640747\n",
      "Epoch 94 \t Batch 520 \t Validation Loss: 40.19916716172145\n",
      "Epoch 94 \t Batch 540 \t Validation Loss: 39.866907640739726\n",
      "Epoch 94 \t Batch 560 \t Validation Loss: 39.59110021761486\n",
      "Epoch 94 \t Batch 580 \t Validation Loss: 39.28077102036312\n",
      "Epoch 94 \t Batch 600 \t Validation Loss: 39.42753413995107\n",
      "Epoch 94 Training Loss: 46.13536729885421 Validation Loss: 40.05021575364199\n",
      "Epoch 94 completed\n",
      "Epoch 95 \t Batch 20 \t Training Loss: 47.219885444641115\n",
      "Epoch 95 \t Batch 40 \t Training Loss: 46.577064990997314\n",
      "Epoch 95 \t Batch 60 \t Training Loss: 46.761467170715335\n",
      "Epoch 95 \t Batch 80 \t Training Loss: 46.810841751098636\n",
      "Epoch 95 \t Batch 100 \t Training Loss: 46.678502044677735\n",
      "Epoch 95 \t Batch 120 \t Training Loss: 46.46293668746948\n",
      "Epoch 95 \t Batch 140 \t Training Loss: 46.49766208103725\n",
      "Epoch 95 \t Batch 160 \t Training Loss: 46.55863497257233\n",
      "Epoch 95 \t Batch 180 \t Training Loss: 46.5322546005249\n",
      "Epoch 95 \t Batch 200 \t Training Loss: 46.50072750091553\n",
      "Epoch 95 \t Batch 220 \t Training Loss: 46.53620152906938\n",
      "Epoch 95 \t Batch 240 \t Training Loss: 46.43857719103495\n",
      "Epoch 95 \t Batch 260 \t Training Loss: 46.44858093261719\n",
      "Epoch 95 \t Batch 280 \t Training Loss: 46.35123558044434\n",
      "Epoch 95 \t Batch 300 \t Training Loss: 46.27782889048258\n",
      "Epoch 95 \t Batch 320 \t Training Loss: 46.390232026576996\n",
      "Epoch 95 \t Batch 340 \t Training Loss: 46.33459667878992\n",
      "Epoch 95 \t Batch 360 \t Training Loss: 46.25840161641439\n",
      "Epoch 95 \t Batch 380 \t Training Loss: 46.21540457073011\n",
      "Epoch 95 \t Batch 400 \t Training Loss: 46.25216981887817\n",
      "Epoch 95 \t Batch 420 \t Training Loss: 46.328618231273836\n",
      "Epoch 95 \t Batch 440 \t Training Loss: 46.31236691908403\n",
      "Epoch 95 \t Batch 460 \t Training Loss: 46.291848331948984\n",
      "Epoch 95 \t Batch 480 \t Training Loss: 46.311142834027606\n",
      "Epoch 95 \t Batch 500 \t Training Loss: 46.25253741455078\n",
      "Epoch 95 \t Batch 520 \t Training Loss: 46.23677116540762\n",
      "Epoch 95 \t Batch 540 \t Training Loss: 46.2337407430013\n",
      "Epoch 95 \t Batch 560 \t Training Loss: 46.17828519003732\n",
      "Epoch 95 \t Batch 580 \t Training Loss: 46.16500445398791\n",
      "Epoch 95 \t Batch 600 \t Training Loss: 46.15383036931356\n",
      "Epoch 95 \t Batch 620 \t Training Loss: 46.156650857002504\n",
      "Epoch 95 \t Batch 640 \t Training Loss: 46.15996875762939\n",
      "Epoch 95 \t Batch 660 \t Training Loss: 46.197868607260965\n",
      "Epoch 95 \t Batch 680 \t Training Loss: 46.19368575040032\n",
      "Epoch 95 \t Batch 700 \t Training Loss: 46.21497281210763\n",
      "Epoch 95 \t Batch 720 \t Training Loss: 46.193403752644855\n",
      "Epoch 95 \t Batch 740 \t Training Loss: 46.1784881385597\n",
      "Epoch 95 \t Batch 760 \t Training Loss: 46.171493269267835\n",
      "Epoch 95 \t Batch 780 \t Training Loss: 46.159001262371355\n",
      "Epoch 95 \t Batch 800 \t Training Loss: 46.16832204818726\n",
      "Epoch 95 \t Batch 820 \t Training Loss: 46.1705230805932\n",
      "Epoch 95 \t Batch 840 \t Training Loss: 46.13731448309762\n",
      "Epoch 95 \t Batch 860 \t Training Loss: 46.149505859197575\n",
      "Epoch 95 \t Batch 880 \t Training Loss: 46.13270814202048\n",
      "Epoch 95 \t Batch 900 \t Training Loss: 46.120896394517686\n",
      "Epoch 95 \t Batch 20 \t Validation Loss: 50.055929851531985\n",
      "Epoch 95 \t Batch 40 \t Validation Loss: 42.96340374946594\n",
      "Epoch 95 \t Batch 60 \t Validation Loss: 46.799956734975176\n",
      "Epoch 95 \t Batch 80 \t Validation Loss: 44.951176404953\n",
      "Epoch 95 \t Batch 100 \t Validation Loss: 41.75328227996826\n",
      "Epoch 95 \t Batch 120 \t Validation Loss: 39.90368310610453\n",
      "Epoch 95 \t Batch 140 \t Validation Loss: 38.285052980695454\n",
      "Epoch 95 \t Batch 160 \t Validation Loss: 38.73134188652038\n",
      "Epoch 95 \t Batch 180 \t Validation Loss: 40.99786971939935\n",
      "Epoch 95 \t Batch 200 \t Validation Loss: 41.55720050334931\n",
      "Epoch 95 \t Batch 220 \t Validation Loss: 42.1751736424186\n",
      "Epoch 95 \t Batch 240 \t Validation Loss: 41.999753109614055\n",
      "Epoch 95 \t Batch 260 \t Validation Loss: 43.631008239892815\n",
      "Epoch 95 \t Batch 280 \t Validation Loss: 44.32976133142199\n",
      "Epoch 95 \t Batch 300 \t Validation Loss: 44.89744413693746\n",
      "Epoch 95 \t Batch 320 \t Validation Loss: 44.989878442883494\n",
      "Epoch 95 \t Batch 340 \t Validation Loss: 44.614047861099245\n",
      "Epoch 95 \t Batch 360 \t Validation Loss: 44.19610192245907\n",
      "Epoch 95 \t Batch 380 \t Validation Loss: 44.21024005036605\n",
      "Epoch 95 \t Batch 400 \t Validation Loss: 43.5854537653923\n",
      "Epoch 95 \t Batch 420 \t Validation Loss: 43.394128851663496\n",
      "Epoch 95 \t Batch 440 \t Validation Loss: 42.93725383281708\n",
      "Epoch 95 \t Batch 460 \t Validation Loss: 42.96166814513828\n",
      "Epoch 95 \t Batch 480 \t Validation Loss: 43.24529946843783\n",
      "Epoch 95 \t Batch 500 \t Validation Loss: 42.814929536819456\n",
      "Epoch 95 \t Batch 520 \t Validation Loss: 42.48275996171511\n",
      "Epoch 95 \t Batch 540 \t Validation Loss: 42.084292074486065\n",
      "Epoch 95 \t Batch 560 \t Validation Loss: 41.77325262853078\n",
      "Epoch 95 \t Batch 580 \t Validation Loss: 41.46909881295829\n",
      "Epoch 95 \t Batch 600 \t Validation Loss: 41.55870760758718\n",
      "Epoch 95 Training Loss: 46.13630552718268 Validation Loss: 42.1421562309389\n",
      "Epoch 95 completed\n",
      "Epoch 96 \t Batch 20 \t Training Loss: 46.30370464324951\n",
      "Epoch 96 \t Batch 40 \t Training Loss: 45.980620574951175\n",
      "Epoch 96 \t Batch 60 \t Training Loss: 45.86997152964274\n",
      "Epoch 96 \t Batch 80 \t Training Loss: 46.02517871856689\n",
      "Epoch 96 \t Batch 100 \t Training Loss: 46.1781551361084\n",
      "Epoch 96 \t Batch 120 \t Training Loss: 46.279509703318276\n",
      "Epoch 96 \t Batch 140 \t Training Loss: 46.157737486703056\n",
      "Epoch 96 \t Batch 160 \t Training Loss: 46.12204225063324\n",
      "Epoch 96 \t Batch 180 \t Training Loss: 46.04862090216743\n",
      "Epoch 96 \t Batch 200 \t Training Loss: 46.005091209411624\n",
      "Epoch 96 \t Batch 220 \t Training Loss: 45.81418399810791\n",
      "Epoch 96 \t Batch 240 \t Training Loss: 45.76006339391073\n",
      "Epoch 96 \t Batch 260 \t Training Loss: 45.71279346759503\n",
      "Epoch 96 \t Batch 280 \t Training Loss: 45.744272872379845\n",
      "Epoch 96 \t Batch 300 \t Training Loss: 45.751069424947104\n",
      "Epoch 96 \t Batch 320 \t Training Loss: 45.864699971675876\n",
      "Epoch 96 \t Batch 340 \t Training Loss: 45.85397669848274\n",
      "Epoch 96 \t Batch 360 \t Training Loss: 45.88600307040744\n",
      "Epoch 96 \t Batch 380 \t Training Loss: 45.8853586397673\n",
      "Epoch 96 \t Batch 400 \t Training Loss: 46.00136344909668\n",
      "Epoch 96 \t Batch 420 \t Training Loss: 46.064563505990165\n",
      "Epoch 96 \t Batch 440 \t Training Loss: 46.135957249728115\n",
      "Epoch 96 \t Batch 460 \t Training Loss: 46.150891552800715\n",
      "Epoch 96 \t Batch 480 \t Training Loss: 46.15231164296468\n",
      "Epoch 96 \t Batch 500 \t Training Loss: 46.15020036315918\n",
      "Epoch 96 \t Batch 520 \t Training Loss: 46.07798807437603\n",
      "Epoch 96 \t Batch 540 \t Training Loss: 46.126436367741334\n",
      "Epoch 96 \t Batch 560 \t Training Loss: 46.21262127331325\n",
      "Epoch 96 \t Batch 580 \t Training Loss: 46.19357112358357\n",
      "Epoch 96 \t Batch 600 \t Training Loss: 46.21450089772542\n",
      "Epoch 96 \t Batch 620 \t Training Loss: 46.22875923648957\n",
      "Epoch 96 \t Batch 640 \t Training Loss: 46.22998654842377\n",
      "Epoch 96 \t Batch 660 \t Training Loss: 46.22565787344268\n",
      "Epoch 96 \t Batch 680 \t Training Loss: 46.20313809338738\n",
      "Epoch 96 \t Batch 700 \t Training Loss: 46.213108160836356\n",
      "Epoch 96 \t Batch 720 \t Training Loss: 46.1728364361657\n",
      "Epoch 96 \t Batch 740 \t Training Loss: 46.144830451140535\n",
      "Epoch 96 \t Batch 760 \t Training Loss: 46.12088914168508\n",
      "Epoch 96 \t Batch 780 \t Training Loss: 46.1191119609735\n",
      "Epoch 96 \t Batch 800 \t Training Loss: 46.13095901012421\n",
      "Epoch 96 \t Batch 820 \t Training Loss: 46.12364974138213\n",
      "Epoch 96 \t Batch 840 \t Training Loss: 46.15797782625471\n",
      "Epoch 96 \t Batch 860 \t Training Loss: 46.1298109365064\n",
      "Epoch 96 \t Batch 880 \t Training Loss: 46.117541790008545\n",
      "Epoch 96 \t Batch 900 \t Training Loss: 46.09123595343696\n",
      "Epoch 96 \t Batch 20 \t Validation Loss: 38.9085328578949\n",
      "Epoch 96 \t Batch 40 \t Validation Loss: 35.36815083026886\n",
      "Epoch 96 \t Batch 60 \t Validation Loss: 38.28400309880575\n",
      "Epoch 96 \t Batch 80 \t Validation Loss: 37.39170414209366\n",
      "Epoch 96 \t Batch 100 \t Validation Loss: 35.61695881843567\n",
      "Epoch 96 \t Batch 120 \t Validation Loss: 34.72567414442698\n",
      "Epoch 96 \t Batch 140 \t Validation Loss: 33.78040755816868\n",
      "Epoch 96 \t Batch 160 \t Validation Loss: 34.72828251719475\n",
      "Epoch 96 \t Batch 180 \t Validation Loss: 37.48177399105496\n",
      "Epoch 96 \t Batch 200 \t Validation Loss: 38.320030193328854\n",
      "Epoch 96 \t Batch 220 \t Validation Loss: 39.22896936590021\n",
      "Epoch 96 \t Batch 240 \t Validation Loss: 39.25446228981018\n",
      "Epoch 96 \t Batch 260 \t Validation Loss: 41.096779463841365\n",
      "Epoch 96 \t Batch 280 \t Validation Loss: 41.99023280824934\n",
      "Epoch 96 \t Batch 300 \t Validation Loss: 42.70547870000203\n",
      "Epoch 96 \t Batch 320 \t Validation Loss: 42.92585099339485\n",
      "Epoch 96 \t Batch 340 \t Validation Loss: 42.659820775424734\n",
      "Epoch 96 \t Batch 360 \t Validation Loss: 42.31437821388245\n",
      "Epoch 96 \t Batch 380 \t Validation Loss: 42.37676177777742\n",
      "Epoch 96 \t Batch 400 \t Validation Loss: 41.763877263069155\n",
      "Epoch 96 \t Batch 420 \t Validation Loss: 41.618898616518294\n",
      "Epoch 96 \t Batch 440 \t Validation Loss: 41.16774320819161\n",
      "Epoch 96 \t Batch 460 \t Validation Loss: 41.20942453508792\n",
      "Epoch 96 \t Batch 480 \t Validation Loss: 41.57121947805087\n",
      "Epoch 96 \t Batch 500 \t Validation Loss: 41.18779738807678\n",
      "Epoch 96 \t Batch 520 \t Validation Loss: 40.79939350164854\n",
      "Epoch 96 \t Batch 540 \t Validation Loss: 40.42662731276618\n",
      "Epoch 96 \t Batch 560 \t Validation Loss: 40.11504498379571\n",
      "Epoch 96 \t Batch 580 \t Validation Loss: 39.75897495499973\n",
      "Epoch 96 \t Batch 600 \t Validation Loss: 39.884337878227235\n",
      "Epoch 96 Training Loss: 46.104770057203986 Validation Loss: 40.46604321993791\n",
      "Epoch 96 completed\n",
      "Epoch 97 \t Batch 20 \t Training Loss: 47.419326591491696\n",
      "Epoch 97 \t Batch 40 \t Training Loss: 46.790352725982665\n",
      "Epoch 97 \t Batch 60 \t Training Loss: 46.24163392384847\n",
      "Epoch 97 \t Batch 80 \t Training Loss: 45.985637521743776\n",
      "Epoch 97 \t Batch 100 \t Training Loss: 46.1649596786499\n",
      "Epoch 97 \t Batch 120 \t Training Loss: 45.98528102238973\n",
      "Epoch 97 \t Batch 140 \t Training Loss: 46.01945168631418\n",
      "Epoch 97 \t Batch 160 \t Training Loss: 45.9700389623642\n",
      "Epoch 97 \t Batch 180 \t Training Loss: 45.95659972296821\n",
      "Epoch 97 \t Batch 200 \t Training Loss: 46.120732345581054\n",
      "Epoch 97 \t Batch 220 \t Training Loss: 45.93212684284557\n",
      "Epoch 97 \t Batch 240 \t Training Loss: 45.9875289440155\n",
      "Epoch 97 \t Batch 260 \t Training Loss: 46.04938922295204\n",
      "Epoch 97 \t Batch 280 \t Training Loss: 46.03841973713466\n",
      "Epoch 97 \t Batch 300 \t Training Loss: 46.024142328898115\n",
      "Epoch 97 \t Batch 320 \t Training Loss: 45.90021287202835\n",
      "Epoch 97 \t Batch 340 \t Training Loss: 45.9319740856395\n",
      "Epoch 97 \t Batch 360 \t Training Loss: 45.93451684315999\n",
      "Epoch 97 \t Batch 380 \t Training Loss: 45.92315451973363\n",
      "Epoch 97 \t Batch 400 \t Training Loss: 45.904763793945314\n",
      "Epoch 97 \t Batch 420 \t Training Loss: 45.92884309859503\n",
      "Epoch 97 \t Batch 440 \t Training Loss: 46.00666516043923\n",
      "Epoch 97 \t Batch 460 \t Training Loss: 45.921185510054876\n",
      "Epoch 97 \t Batch 480 \t Training Loss: 45.848259353637694\n",
      "Epoch 97 \t Batch 500 \t Training Loss: 45.87411281585693\n",
      "Epoch 97 \t Batch 520 \t Training Loss: 45.90150065788856\n",
      "Epoch 97 \t Batch 540 \t Training Loss: 45.95276891920302\n",
      "Epoch 97 \t Batch 560 \t Training Loss: 45.94991327694484\n",
      "Epoch 97 \t Batch 580 \t Training Loss: 45.90668331014699\n",
      "Epoch 97 \t Batch 600 \t Training Loss: 45.90683006286621\n",
      "Epoch 97 \t Batch 620 \t Training Loss: 45.93086949625323\n",
      "Epoch 97 \t Batch 640 \t Training Loss: 45.900733053684235\n",
      "Epoch 97 \t Batch 660 \t Training Loss: 45.93558733969024\n",
      "Epoch 97 \t Batch 680 \t Training Loss: 45.931384675643024\n",
      "Epoch 97 \t Batch 700 \t Training Loss: 45.95574325561523\n",
      "Epoch 97 \t Batch 720 \t Training Loss: 45.97924365467495\n",
      "Epoch 97 \t Batch 740 \t Training Loss: 45.99288282652159\n",
      "Epoch 97 \t Batch 760 \t Training Loss: 46.01612561878405\n",
      "Epoch 97 \t Batch 780 \t Training Loss: 45.96267189612755\n",
      "Epoch 97 \t Batch 800 \t Training Loss: 45.97746730804443\n",
      "Epoch 97 \t Batch 820 \t Training Loss: 45.9561353171744\n",
      "Epoch 97 \t Batch 840 \t Training Loss: 46.00230127970378\n",
      "Epoch 97 \t Batch 860 \t Training Loss: 46.01544877429341\n",
      "Epoch 97 \t Batch 880 \t Training Loss: 46.03209782947194\n",
      "Epoch 97 \t Batch 900 \t Training Loss: 46.04783220503065\n",
      "Epoch 97 \t Batch 20 \t Validation Loss: 40.279524612426755\n",
      "Epoch 97 \t Batch 40 \t Validation Loss: 35.384895706176756\n",
      "Epoch 97 \t Batch 60 \t Validation Loss: 38.47080314954122\n",
      "Epoch 97 \t Batch 80 \t Validation Loss: 37.339038503170016\n",
      "Epoch 97 \t Batch 100 \t Validation Loss: 35.320790452957155\n",
      "Epoch 97 \t Batch 120 \t Validation Loss: 34.30158140659332\n",
      "Epoch 97 \t Batch 140 \t Validation Loss: 33.324202694211685\n",
      "Epoch 97 \t Batch 160 \t Validation Loss: 34.334872990846634\n",
      "Epoch 97 \t Batch 180 \t Validation Loss: 37.056750774383545\n",
      "Epoch 97 \t Batch 200 \t Validation Loss: 37.90982717514038\n",
      "Epoch 97 \t Batch 220 \t Validation Loss: 38.808272032304245\n",
      "Epoch 97 \t Batch 240 \t Validation Loss: 38.848282178243004\n",
      "Epoch 97 \t Batch 260 \t Validation Loss: 40.682433872956494\n",
      "Epoch 97 \t Batch 280 \t Validation Loss: 41.55782290867397\n",
      "Epoch 97 \t Batch 300 \t Validation Loss: 42.29650660832723\n",
      "Epoch 97 \t Batch 320 \t Validation Loss: 42.528388530015945\n",
      "Epoch 97 \t Batch 340 \t Validation Loss: 42.26436945971321\n",
      "Epoch 97 \t Batch 360 \t Validation Loss: 41.9447391351064\n",
      "Epoch 97 \t Batch 380 \t Validation Loss: 42.01263030704699\n",
      "Epoch 97 \t Batch 400 \t Validation Loss: 41.41827003479004\n",
      "Epoch 97 \t Batch 420 \t Validation Loss: 41.29029383659363\n",
      "Epoch 97 \t Batch 440 \t Validation Loss: 40.85128600597382\n",
      "Epoch 97 \t Batch 460 \t Validation Loss: 40.9149228365525\n",
      "Epoch 97 \t Batch 480 \t Validation Loss: 41.288076641162235\n",
      "Epoch 97 \t Batch 500 \t Validation Loss: 40.914948236465456\n",
      "Epoch 97 \t Batch 520 \t Validation Loss: 40.53387993849241\n",
      "Epoch 97 \t Batch 540 \t Validation Loss: 40.14723118852686\n",
      "Epoch 97 \t Batch 560 \t Validation Loss: 39.82843125377383\n",
      "Epoch 97 \t Batch 580 \t Validation Loss: 39.47971572382697\n",
      "Epoch 97 \t Batch 600 \t Validation Loss: 39.59910557905833\n",
      "Epoch 97 Training Loss: 46.0627732188517 Validation Loss: 40.19215108047832\n",
      "Epoch 97 completed\n",
      "Epoch 98 \t Batch 20 \t Training Loss: 45.91841850280762\n",
      "Epoch 98 \t Batch 40 \t Training Loss: 45.55567350387573\n",
      "Epoch 98 \t Batch 60 \t Training Loss: 45.47738850911458\n",
      "Epoch 98 \t Batch 80 \t Training Loss: 45.451793336868285\n",
      "Epoch 98 \t Batch 100 \t Training Loss: 45.26342620849609\n",
      "Epoch 98 \t Batch 120 \t Training Loss: 45.35205364227295\n",
      "Epoch 98 \t Batch 140 \t Training Loss: 45.626717294965474\n",
      "Epoch 98 \t Batch 160 \t Training Loss: 45.71372935771942\n",
      "Epoch 98 \t Batch 180 \t Training Loss: 45.73959835900201\n",
      "Epoch 98 \t Batch 200 \t Training Loss: 45.750266456604\n",
      "Epoch 98 \t Batch 220 \t Training Loss: 45.82261695861816\n",
      "Epoch 98 \t Batch 240 \t Training Loss: 45.86208836237589\n",
      "Epoch 98 \t Batch 260 \t Training Loss: 45.9157498726478\n",
      "Epoch 98 \t Batch 280 \t Training Loss: 46.006229877471924\n",
      "Epoch 98 \t Batch 300 \t Training Loss: 46.08087342580159\n",
      "Epoch 98 \t Batch 320 \t Training Loss: 46.020832777023315\n",
      "Epoch 98 \t Batch 340 \t Training Loss: 46.06865641650032\n",
      "Epoch 98 \t Batch 360 \t Training Loss: 46.04987673229641\n",
      "Epoch 98 \t Batch 380 \t Training Loss: 45.99041863491661\n",
      "Epoch 98 \t Batch 400 \t Training Loss: 46.03516731262207\n",
      "Epoch 98 \t Batch 420 \t Training Loss: 46.02434535253616\n",
      "Epoch 98 \t Batch 440 \t Training Loss: 46.03898892836137\n",
      "Epoch 98 \t Batch 460 \t Training Loss: 46.02864241392716\n",
      "Epoch 98 \t Batch 480 \t Training Loss: 45.97803416252136\n",
      "Epoch 98 \t Batch 500 \t Training Loss: 46.01454331207275\n",
      "Epoch 98 \t Batch 520 \t Training Loss: 46.05513883737417\n",
      "Epoch 98 \t Batch 540 \t Training Loss: 46.04873325913041\n",
      "Epoch 98 \t Batch 560 \t Training Loss: 46.09661667006356\n",
      "Epoch 98 \t Batch 580 \t Training Loss: 46.0752571303269\n",
      "Epoch 98 \t Batch 600 \t Training Loss: 46.066619408925376\n",
      "Epoch 98 \t Batch 620 \t Training Loss: 46.05999111667756\n",
      "Epoch 98 \t Batch 640 \t Training Loss: 46.05282212495804\n",
      "Epoch 98 \t Batch 660 \t Training Loss: 46.01710829301314\n",
      "Epoch 98 \t Batch 680 \t Training Loss: 45.990765588423784\n",
      "Epoch 98 \t Batch 700 \t Training Loss: 46.015708193097794\n",
      "Epoch 98 \t Batch 720 \t Training Loss: 46.00904570155674\n",
      "Epoch 98 \t Batch 740 \t Training Loss: 46.012487674403836\n",
      "Epoch 98 \t Batch 760 \t Training Loss: 46.01996744557431\n",
      "Epoch 98 \t Batch 780 \t Training Loss: 46.033394852662695\n",
      "Epoch 98 \t Batch 800 \t Training Loss: 46.07794788360596\n",
      "Epoch 98 \t Batch 820 \t Training Loss: 46.10054470620504\n",
      "Epoch 98 \t Batch 840 \t Training Loss: 46.092498611268546\n",
      "Epoch 98 \t Batch 860 \t Training Loss: 46.09547442946323\n",
      "Epoch 98 \t Batch 880 \t Training Loss: 46.11287098797885\n",
      "Epoch 98 \t Batch 900 \t Training Loss: 46.09840130700005\n",
      "Epoch 98 \t Batch 20 \t Validation Loss: 37.861186790466306\n",
      "Epoch 98 \t Batch 40 \t Validation Loss: 33.25692927837372\n",
      "Epoch 98 \t Batch 60 \t Validation Loss: 36.512778441111244\n",
      "Epoch 98 \t Batch 80 \t Validation Loss: 35.38506894111633\n",
      "Epoch 98 \t Batch 100 \t Validation Loss: 33.72686367034912\n",
      "Epoch 98 \t Batch 120 \t Validation Loss: 33.01442572275798\n",
      "Epoch 98 \t Batch 140 \t Validation Loss: 32.251127297537664\n",
      "Epoch 98 \t Batch 160 \t Validation Loss: 33.35592174530029\n",
      "Epoch 98 \t Batch 180 \t Validation Loss: 36.09143083890279\n",
      "Epoch 98 \t Batch 200 \t Validation Loss: 36.98069314479828\n",
      "Epoch 98 \t Batch 220 \t Validation Loss: 37.90195580829273\n",
      "Epoch 98 \t Batch 240 \t Validation Loss: 37.978293744723004\n",
      "Epoch 98 \t Batch 260 \t Validation Loss: 39.80203577921941\n",
      "Epoch 98 \t Batch 280 \t Validation Loss: 40.6576481444495\n",
      "Epoch 98 \t Batch 300 \t Validation Loss: 41.42377786954244\n",
      "Epoch 98 \t Batch 320 \t Validation Loss: 41.67144254148006\n",
      "Epoch 98 \t Batch 340 \t Validation Loss: 41.4507387974683\n",
      "Epoch 98 \t Batch 360 \t Validation Loss: 41.16122014257643\n",
      "Epoch 98 \t Batch 380 \t Validation Loss: 41.25246604618273\n",
      "Epoch 98 \t Batch 400 \t Validation Loss: 40.69802831172943\n",
      "Epoch 98 \t Batch 420 \t Validation Loss: 40.588297187714346\n",
      "Epoch 98 \t Batch 440 \t Validation Loss: 40.17641811587594\n",
      "Epoch 98 \t Batch 460 \t Validation Loss: 40.25510518239892\n",
      "Epoch 98 \t Batch 480 \t Validation Loss: 40.64341106216113\n",
      "Epoch 98 \t Batch 500 \t Validation Loss: 40.289069120407106\n",
      "Epoch 98 \t Batch 520 \t Validation Loss: 39.942438525419966\n",
      "Epoch 98 \t Batch 540 \t Validation Loss: 39.575846428341336\n",
      "Epoch 98 \t Batch 560 \t Validation Loss: 39.27527031557901\n",
      "Epoch 98 \t Batch 580 \t Validation Loss: 38.97196916382888\n",
      "Epoch 98 \t Batch 600 \t Validation Loss: 39.103785308202106\n",
      "Epoch 98 Training Loss: 46.081511964454776 Validation Loss: 39.687727990088526\n",
      "Epoch 98 completed\n",
      "Epoch 99 \t Batch 20 \t Training Loss: 47.05653305053711\n",
      "Epoch 99 \t Batch 40 \t Training Loss: 46.4298770904541\n",
      "Epoch 99 \t Batch 60 \t Training Loss: 46.41250489552816\n",
      "Epoch 99 \t Batch 80 \t Training Loss: 46.172768640518186\n",
      "Epoch 99 \t Batch 100 \t Training Loss: 46.15502666473389\n",
      "Epoch 99 \t Batch 120 \t Training Loss: 46.07018938064575\n",
      "Epoch 99 \t Batch 140 \t Training Loss: 46.009454699925016\n",
      "Epoch 99 \t Batch 160 \t Training Loss: 46.13007967472076\n",
      "Epoch 99 \t Batch 180 \t Training Loss: 45.98158751593696\n",
      "Epoch 99 \t Batch 200 \t Training Loss: 46.03020641326904\n",
      "Epoch 99 \t Batch 220 \t Training Loss: 46.0063666603782\n",
      "Epoch 99 \t Batch 240 \t Training Loss: 45.99704469045003\n",
      "Epoch 99 \t Batch 260 \t Training Loss: 46.083954737736626\n",
      "Epoch 99 \t Batch 280 \t Training Loss: 46.07433207375663\n",
      "Epoch 99 \t Batch 300 \t Training Loss: 46.094290288289386\n",
      "Epoch 99 \t Batch 320 \t Training Loss: 46.15808609724045\n",
      "Epoch 99 \t Batch 340 \t Training Loss: 46.19884158863741\n",
      "Epoch 99 \t Batch 360 \t Training Loss: 46.22957888709174\n",
      "Epoch 99 \t Batch 380 \t Training Loss: 46.17837321632787\n",
      "Epoch 99 \t Batch 400 \t Training Loss: 46.13413355827331\n",
      "Epoch 99 \t Batch 420 \t Training Loss: 46.074576078142435\n",
      "Epoch 99 \t Batch 440 \t Training Loss: 46.12846800197254\n",
      "Epoch 99 \t Batch 460 \t Training Loss: 46.162477684021\n",
      "Epoch 99 \t Batch 480 \t Training Loss: 46.10023560523987\n",
      "Epoch 99 \t Batch 500 \t Training Loss: 46.105626502990724\n",
      "Epoch 99 \t Batch 520 \t Training Loss: 46.101898926955\n",
      "Epoch 99 \t Batch 540 \t Training Loss: 46.11376472755715\n",
      "Epoch 99 \t Batch 560 \t Training Loss: 46.120529999051776\n",
      "Epoch 99 \t Batch 580 \t Training Loss: 46.1107211408944\n",
      "Epoch 99 \t Batch 600 \t Training Loss: 46.06604764938354\n",
      "Epoch 99 \t Batch 620 \t Training Loss: 46.059187778349845\n",
      "Epoch 99 \t Batch 640 \t Training Loss: 46.0632543027401\n",
      "Epoch 99 \t Batch 660 \t Training Loss: 46.05628510677453\n",
      "Epoch 99 \t Batch 680 \t Training Loss: 46.012233425589166\n",
      "Epoch 99 \t Batch 700 \t Training Loss: 46.06323174612863\n",
      "Epoch 99 \t Batch 720 \t Training Loss: 46.091174003813\n",
      "Epoch 99 \t Batch 740 \t Training Loss: 46.08365613576528\n",
      "Epoch 99 \t Batch 760 \t Training Loss: 46.078173782950955\n",
      "Epoch 99 \t Batch 780 \t Training Loss: 46.088436889648435\n",
      "Epoch 99 \t Batch 800 \t Training Loss: 46.075019812583925\n",
      "Epoch 99 \t Batch 820 \t Training Loss: 46.08081029101116\n",
      "Epoch 99 \t Batch 840 \t Training Loss: 46.09694790158953\n",
      "Epoch 99 \t Batch 860 \t Training Loss: 46.09924190432526\n",
      "Epoch 99 \t Batch 880 \t Training Loss: 46.11334899122065\n",
      "Epoch 99 \t Batch 900 \t Training Loss: 46.09258218553331\n",
      "Epoch 99 \t Batch 20 \t Validation Loss: 37.98600625991821\n",
      "Epoch 99 \t Batch 40 \t Validation Loss: 34.002027654647826\n",
      "Epoch 99 \t Batch 60 \t Validation Loss: 37.05846110979716\n",
      "Epoch 99 \t Batch 80 \t Validation Loss: 36.16032363176346\n",
      "Epoch 99 \t Batch 100 \t Validation Loss: 34.73723431587219\n",
      "Epoch 99 \t Batch 120 \t Validation Loss: 34.17720592021942\n",
      "Epoch 99 \t Batch 140 \t Validation Loss: 33.47382245063782\n",
      "Epoch 99 \t Batch 160 \t Validation Loss: 34.45971440672874\n",
      "Epoch 99 \t Batch 180 \t Validation Loss: 36.941384347279865\n",
      "Epoch 99 \t Batch 200 \t Validation Loss: 37.74613568305969\n",
      "Epoch 99 \t Batch 220 \t Validation Loss: 38.522533572803844\n",
      "Epoch 99 \t Batch 240 \t Validation Loss: 38.51046112378438\n",
      "Epoch 99 \t Batch 260 \t Validation Loss: 40.219699987998375\n",
      "Epoch 99 \t Batch 280 \t Validation Loss: 41.041659688949586\n",
      "Epoch 99 \t Batch 300 \t Validation Loss: 41.71149784723918\n",
      "Epoch 99 \t Batch 320 \t Validation Loss: 41.913321030139926\n",
      "Epoch 99 \t Batch 340 \t Validation Loss: 41.728158120548024\n",
      "Epoch 99 \t Batch 360 \t Validation Loss: 41.44828438229031\n",
      "Epoch 99 \t Batch 380 \t Validation Loss: 41.57727853373477\n",
      "Epoch 99 \t Batch 400 \t Validation Loss: 41.158814182281496\n",
      "Epoch 99 \t Batch 420 \t Validation Loss: 41.069958718617755\n",
      "Epoch 99 \t Batch 440 \t Validation Loss: 40.81171049638228\n",
      "Epoch 99 \t Batch 460 \t Validation Loss: 40.95852489056794\n",
      "Epoch 99 \t Batch 480 \t Validation Loss: 41.334462292989095\n",
      "Epoch 99 \t Batch 500 \t Validation Loss: 40.971409660339354\n",
      "Epoch 99 \t Batch 520 \t Validation Loss: 40.744083558596095\n",
      "Epoch 99 \t Batch 540 \t Validation Loss: 40.41364889851323\n",
      "Epoch 99 \t Batch 560 \t Validation Loss: 40.11678212370191\n",
      "Epoch 99 \t Batch 580 \t Validation Loss: 39.79509214203933\n",
      "Epoch 99 \t Batch 600 \t Validation Loss: 39.92288754145304\n",
      "Epoch 99 Training Loss: 46.08419063073078 Validation Loss: 40.51501719363324\n",
      "Epoch 99 completed\n",
      "Epoch 100 \t Batch 20 \t Training Loss: 44.73870716094971\n",
      "Epoch 100 \t Batch 40 \t Training Loss: 45.735836791992185\n",
      "Epoch 100 \t Batch 60 \t Training Loss: 45.28890794118245\n",
      "Epoch 100 \t Batch 80 \t Training Loss: 45.483227777481076\n",
      "Epoch 100 \t Batch 100 \t Training Loss: 45.62798240661621\n",
      "Epoch 100 \t Batch 120 \t Training Loss: 45.71716105143229\n",
      "Epoch 100 \t Batch 140 \t Training Loss: 45.691623497009275\n",
      "Epoch 100 \t Batch 160 \t Training Loss: 46.033607721328735\n",
      "Epoch 100 \t Batch 180 \t Training Loss: 45.82740463680691\n",
      "Epoch 100 \t Batch 200 \t Training Loss: 45.93588977813721\n",
      "Epoch 100 \t Batch 220 \t Training Loss: 45.98129163221879\n",
      "Epoch 100 \t Batch 240 \t Training Loss: 45.99379885991414\n",
      "Epoch 100 \t Batch 260 \t Training Loss: 46.003372104351335\n",
      "Epoch 100 \t Batch 280 \t Training Loss: 45.98184678213937\n",
      "Epoch 100 \t Batch 300 \t Training Loss: 46.001173210144046\n",
      "Epoch 100 \t Batch 320 \t Training Loss: 46.05125269889832\n",
      "Epoch 100 \t Batch 340 \t Training Loss: 46.08181913039264\n",
      "Epoch 100 \t Batch 360 \t Training Loss: 46.078218163384335\n",
      "Epoch 100 \t Batch 380 \t Training Loss: 46.088392307883815\n",
      "Epoch 100 \t Batch 400 \t Training Loss: 46.07148127555847\n",
      "Epoch 100 \t Batch 420 \t Training Loss: 46.05360093797956\n",
      "Epoch 100 \t Batch 440 \t Training Loss: 46.09236663471569\n",
      "Epoch 100 \t Batch 460 \t Training Loss: 46.076109247622284\n",
      "Epoch 100 \t Batch 480 \t Training Loss: 46.059863328933716\n",
      "Epoch 100 \t Batch 500 \t Training Loss: 46.029127006530764\n",
      "Epoch 100 \t Batch 520 \t Training Loss: 46.04386460230901\n",
      "Epoch 100 \t Batch 540 \t Training Loss: 46.03993141033031\n",
      "Epoch 100 \t Batch 560 \t Training Loss: 46.01998952456883\n",
      "Epoch 100 \t Batch 580 \t Training Loss: 46.06099450341586\n",
      "Epoch 100 \t Batch 600 \t Training Loss: 46.045933424631755\n",
      "Epoch 100 \t Batch 620 \t Training Loss: 46.082730791645666\n",
      "Epoch 100 \t Batch 640 \t Training Loss: 46.08744317889214\n",
      "Epoch 100 \t Batch 660 \t Training Loss: 46.080011801286176\n",
      "Epoch 100 \t Batch 680 \t Training Loss: 46.03315673154943\n",
      "Epoch 100 \t Batch 700 \t Training Loss: 46.019998005458284\n",
      "Epoch 100 \t Batch 720 \t Training Loss: 46.02150132921007\n",
      "Epoch 100 \t Batch 740 \t Training Loss: 46.029618139524715\n",
      "Epoch 100 \t Batch 760 \t Training Loss: 45.96224733653821\n",
      "Epoch 100 \t Batch 780 \t Training Loss: 45.99832022740291\n",
      "Epoch 100 \t Batch 800 \t Training Loss: 46.02668211460114\n",
      "Epoch 100 \t Batch 820 \t Training Loss: 46.03529918019365\n",
      "Epoch 100 \t Batch 840 \t Training Loss: 46.05740443184262\n",
      "Epoch 100 \t Batch 860 \t Training Loss: 46.07942754168843\n",
      "Epoch 100 \t Batch 880 \t Training Loss: 46.05248211513866\n",
      "Epoch 100 \t Batch 900 \t Training Loss: 46.04550576951769\n",
      "Epoch 100 \t Batch 20 \t Validation Loss: 36.85996890068054\n",
      "Epoch 100 \t Batch 40 \t Validation Loss: 33.33137600421905\n",
      "Epoch 100 \t Batch 60 \t Validation Loss: 36.26115953127543\n",
      "Epoch 100 \t Batch 80 \t Validation Loss: 35.36032251119614\n",
      "Epoch 100 \t Batch 100 \t Validation Loss: 33.81890288352966\n",
      "Epoch 100 \t Batch 120 \t Validation Loss: 33.08256238301595\n",
      "Epoch 100 \t Batch 140 \t Validation Loss: 32.35227026258196\n",
      "Epoch 100 \t Batch 160 \t Validation Loss: 33.607211124897006\n",
      "Epoch 100 \t Batch 180 \t Validation Loss: 36.49915787908766\n",
      "Epoch 100 \t Batch 200 \t Validation Loss: 37.493401408195496\n",
      "Epoch 100 \t Batch 220 \t Validation Loss: 38.55666559826244\n",
      "Epoch 100 \t Batch 240 \t Validation Loss: 38.706509403387706\n",
      "Epoch 100 \t Batch 260 \t Validation Loss: 40.63771257400513\n",
      "Epoch 100 \t Batch 280 \t Validation Loss: 41.55921778338296\n",
      "Epoch 100 \t Batch 300 \t Validation Loss: 42.3128308836619\n",
      "Epoch 100 \t Batch 320 \t Validation Loss: 42.59217782914639\n",
      "Epoch 100 \t Batch 340 \t Validation Loss: 42.34806296685163\n",
      "Epoch 100 \t Batch 360 \t Validation Loss: 42.078978673617044\n",
      "Epoch 100 \t Batch 380 \t Validation Loss: 42.191135263442995\n",
      "Epoch 100 \t Batch 400 \t Validation Loss: 41.60962098836899\n",
      "Epoch 100 \t Batch 420 \t Validation Loss: 41.46797233990261\n",
      "Epoch 100 \t Batch 440 \t Validation Loss: 41.03171528686177\n",
      "Epoch 100 \t Batch 460 \t Validation Loss: 41.112126132716305\n",
      "Epoch 100 \t Batch 480 \t Validation Loss: 41.48407224615415\n",
      "Epoch 100 \t Batch 500 \t Validation Loss: 41.106750185012814\n",
      "Epoch 100 \t Batch 520 \t Validation Loss: 40.76251389796917\n",
      "Epoch 100 \t Batch 540 \t Validation Loss: 40.40306071175469\n",
      "Epoch 100 \t Batch 560 \t Validation Loss: 40.09280626092638\n",
      "Epoch 100 \t Batch 580 \t Validation Loss: 39.76727213037425\n",
      "Epoch 100 \t Batch 600 \t Validation Loss: 39.89297490755717\n",
      "Epoch 100 Training Loss: 46.05081145142001 Validation Loss: 40.484126741235904\n",
      "Epoch 100 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataloader import *\n",
    "\n",
    "model = SimpleFCN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "class RMSE(nn.Module):\n",
    "    \"\"\" \n",
    "        Weighted RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        self.mse = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    def __call__(self, prediction, target, weights = 1):\n",
    "        # prediction = prediction[:, 0]\n",
    "        return torch.sqrt(torch.mean(weights * self.mse(prediction,target)))\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.latlon = True\n",
    "        self.bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "        self.bm = True\n",
    "        self.patch_size = [15,15]\n",
    "        self.norm_strat = 'pct'\n",
    "        self.norm = False\n",
    "\n",
    "args = Args()\n",
    "fnames = ['data_nonan_0-5.h5', 'data_nonan_1-5.h5', 'data_nonan_2-5.h5', 'data_nonan_3-5.h5', 'data_nonan_4-5.h5']\n",
    "mode = 'train'\n",
    "ds_training = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "trainloader = DataLoader(dataset = ds_training, batch_size = 512, shuffle = True, num_workers = 8)\n",
    "mode = 'val'\n",
    "ds_validation = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "validloader = DataLoader(dataset = ds_validation, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(\"inputs.shape: \", inputs.shape)\n",
    "        # print(\"targets.shape: \", targets.shape)\n",
    "        # # # print(outputs)\n",
    "        # print(\"outputs.shape: \", outputs.shape)\n",
    "        # loss1 = criterion(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Training Loss: {train_loss / i}')\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    i=0\n",
    "    model.eval()\n",
    "    for inputs, targets in validloader:\n",
    "        i+=1\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,:,7,7].squeeze(),targets)\n",
    "        loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "        valid_loss += loss.item()\n",
    "        if i%20==0:\n",
    "            print(f'Epoch {epoch+1} \\t Batch {i} \\t Validation Loss: {valid_loss / i}')\n",
    " \n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss / len(trainloader)} Validation Loss: {valid_loss / len(validloader)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model3.pth')\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 \t Testing Loss: 42.53355712890625\n",
      "Batch 40 \t Testing Loss: 43.28306503295899\n",
      "Batch 60 \t Testing Loss: 40.05320833524068\n",
      "Batch 80 \t Testing Loss: 53.31889432668686\n",
      "Batch 100 \t Testing Loss: 58.16570910453797\n",
      "Batch 120 \t Testing Loss: 55.65926862557729\n",
      "Batch 140 \t Testing Loss: 54.927022504806516\n",
      "Batch 160 \t Testing Loss: 55.88878775238991\n",
      "Batch 180 \t Testing Loss: 57.40653795136346\n",
      "Batch 200 \t Testing Loss: 57.75407019138336\n",
      "Batch 220 \t Testing Loss: 57.22346431125294\n",
      "Batch 240 \t Testing Loss: 56.70166890223821\n",
      "Batch 260 \t Testing Loss: 57.22100104185251\n",
      "Batch 280 \t Testing Loss: 57.034873427663534\n",
      "Batch 300 \t Testing Loss: 56.52579403877258\n",
      "Batch 320 \t Testing Loss: 56.625859341025354\n",
      "Batch 340 \t Testing Loss: 56.93372951675864\n",
      "Batch 360 \t Testing Loss: 57.88346780935923\n",
      "Batch 380 \t Testing Loss: 58.69027750868546\n",
      "Batch 400 \t Testing Loss: 59.485480926036836\n",
      "Batch 420 \t Testing Loss: 60.16552920341492\n",
      "Batch 440 \t Testing Loss: 60.13528670180928\n",
      "Batch 460 \t Testing Loss: 60.84647957967675\n",
      "Batch 480 \t Testing Loss: 61.08307608962059\n",
      "Batch 500 \t Testing Loss: 61.59072723579407\n",
      "Batch 520 \t Testing Loss: 62.474772117688104\n",
      "Testing Loss: 62.969508012135826\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load('saved_model3.pth'))\n",
    "\n",
    "# Set mode to 'test'\n",
    "mode = 'test'\n",
    "ds_testing = GEDIDataset({'h5':'/scratch2/biomass_estimation/code/ml/data', 'norm': '/scratch2/biomass_estimation/code/ml/data', 'map': '/scratch2/biomass_estimation/code/ml/data/'}, fnames = fnames, chunk_size = 1, mode = mode, args = args)\n",
    "testloader = DataLoader(dataset = ds_testing, batch_size = 512, shuffle = False, num_workers = 8)\n",
    "\n",
    "# Testing loop\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "i = 0\n",
    "for inputs, targets in testloader:\n",
    "    i += 1\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = RMSE()(outputs[:,:,7,7].squeeze(), targets)\n",
    "    test_loss += loss.item()\n",
    "    if i % 20 == 0:\n",
    "        print(f'Batch {i} \\t Testing Loss: {test_loss / i}')\n",
    "\n",
    "print(f'Testing Loss: {test_loss / len(testloader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
